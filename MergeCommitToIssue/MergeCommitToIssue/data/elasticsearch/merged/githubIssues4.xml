<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Make the heuristic to compute the default shard size less aggressive.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19659</link><project id="" key="" /><description>The current heuristic to compute a default shard size is pretty aggressive,
it returns `max(10, number_of_shards * size)` as a value for the shard size.
I think making it less aggressive has the benefit that it would reduce the
likelyness of running into OOME when there are many shards (yearly
aggregations with time-based indices can make numbers of shards in the
thousands) and make the use of breadth-first more likely/efficient.

This commit replaces the heuristic with `size * 1.5 + 10`, which is enough
to have good accuracy on zipfian distributions.
</description><key id="168110462">19659</key><summary>Make the heuristic to compute the default shard size less aggressive.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T14:31:11Z</created><updated>2016-07-29T16:01:39Z</updated><resolved>2016-07-29T08:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-28T19:31:59Z" id="236000534">I've checked with your method to test the heuristic. The ratio of error is indeed bigger with the new heuristic but it's so close that it should not be a problem.
I am +1 for the aggressive heuristic.

You should also rephrase the doc https://github.com/elastic/elasticsearch/blob/master/docs/reference/aggregations/bucket/terms-aggregation.asciidoc:

```
The default `shard_size` is a multiple of the `size` parameter which is dependant on the number of shards.
```
</comment><comment author="jpountz" created="2016-07-29T08:00:37Z" id="236118908">Thanks @jimferenczi !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restarting a crashed master node can cause deleted indices to show up as unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19658</link><project id="" key="" /><description>**Elasticsearch version**:
2.2.0
**JVM version**:
OpenJDK Runtime Environment (build 1.8.0_91-b14)

**OS version**:
Amazon linux 

Kernel version:
Linux ip-10-51-101-219 3.14.35-28.38.amzn1.x86_64 #1 SMP Wed Mar 11 22:50:37 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
In a cluster which has a pool of dedicated master servers, if one master server crashes and then subsequently indices are deleted restarting the failed master causes the deleted indices to reappear as unallocated shards.

**Steps to reproduce**:
1. Induce master server to crash by exhausting heap memory
2. Wait for election to complete
3. Delete indices from cluster
4. Restart failed master node
</description><key id="168110006">19658</key><summary>Restarting a crashed master node can cause deleted indices to show up as unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnewing1</reporter><labels /><created>2016-07-28T14:29:28Z</created><updated>2016-07-28T14:42:44Z</updated><resolved>2016-07-28T14:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-28T14:42:39Z" id="235915828">This is a known issue which we solved in 5.0 by introducing index deletion tombstones, see https://github.com/elastic/elasticsearch/pull/17265.

Basically what happens is, in order to determine index deletions, we compare the current cluster state to the previous cluster state and if there is an index in the previous cluster state that isn't in the current one, we know its an index deletion.  If a node is offline while an index deletion happens, then it comes back online, it has missed the cluster state update that conveyed this deletion, so it instead imports the index as dangling.

Index tombstones solve this problem in 5.0 by explicitly recording index deletions in the cluster state and when a node (re)joins the cluster, we check the tombstones to make sure we don't import dangling indices that should in fact be deleted.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES 2.3.4: same query being run several times</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19657</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.4

**JVM version**: 1.7.0_101

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

Our setup is as follows:
An index over 11 nodes each of them holding 2 shards one primary and one replica (not of the same shard).
When running a slow query of search_type  QUERY_THEN_FETCH and looking at the elasticsearch slow query logs we see that on each of the nodes we are getting 3 calls of the exact same query with timestamps miliseconds apart.
We were wondering whether this is a "feature" of the slow query logging or whether there are really 3 distinct calls of the same query performed on the index and if the latter, what would be an explanation for this.

The query we are running is:
{"index":["index-*****************_"],"search_type":"count","ignore_unavailable":true}
{"size":0,"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30m","time_zone":"+03:00","min_doc_count":1,"extended_bounds":{"min":1469318400000,"max":1469393384478}},"aggs":{"3":{"terms":{"field":"level","size":5,"order":{"_count":"desc"}}}}}},"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"_"}},"filter":{"bool":{"must":[{"query":{"match":{"type":{"query":"mainlog","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1469318400000,"lte":1469393384478}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}},"fragment_size":2147483647}}

And the slow query log (after some processing on our end) is:
[2016-07-28 07:53:00,561][INFO ][index.search.slowlog.query] [index-*****************_]took[5.2s], took_millis[5209], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[11], source[{"size":0,"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30m","time_zone":"+03:00","min_doc_count":1,"extended_bounds":{"min":1469318400000,"max":1469393384478}},"aggs":{"3":{"terms":{"field":"level","size":5,"order":{"_count":"desc"}}}}}},"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"_"}},"filter":{"bool":{"must":[{"query":{"match":{"type":{"query":"mainlog","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1469318400000,"lte":1469393384478}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}},"fragment_size":2147483647}}], extra_source[],

(the other two instances are exactly the same with a slight variance on the time taken)
</description><key id="168105663">19657</key><summary>ES 2.3.4: same query being run several times</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">doron-logzio</reporter><labels><label>:Logging</label><label>feedback_needed</label></labels><created>2016-07-28T14:12:40Z</created><updated>2017-03-31T13:43:44Z</updated><resolved>2017-03-31T13:43:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-28T18:49:10Z" id="235989029">The slow log records queries at the shard level. You have more than one shard from the same index on the same node.
</comment><comment author="doron-logzio" created="2016-07-31T06:40:51Z" id="236414373">Thanks. This does clarify things a bit, but:
As stated there are only two shards per node (one of them even a replica) and still I get 3(!!!) slowquery logs.

Would there be any other possible reason?
</comment><comment author="clintongormley" created="2016-08-11T11:59:53Z" id="239141244">I've tested this out locally and I get one slow log message per shard, only.  (It doesn't matter if it is primary or replica - a search can hit either one).

If you are seeing something different then please could you put together a small recreation that demonstrates the issue?
</comment><comment author="doron-logzio" created="2016-08-11T12:03:48Z" id="239142040">thanks for this!
Will try to put a "small recreation"
</comment><comment author="colings86" created="2017-03-31T13:43:44Z" id="290715628">no further feedback</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[1.7.3] Two identical ES clusters with identical data return different results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19656</link><project id="" key="" /><description>Hi Elastic! Yesterday we were migrating from an old cluster to a new one and experienced a really bizzare issue with a simple terms aggregation. We've got identical versions/ES-binaries (build hash is the same) and data in both clusters - yet one returns correct terms data for one query and the other does not.

The query in question:

``` json
{
  "aggregations": {
    "colours": {
      "aggregations": {
        "offset__terms": {
          "terms": {
            "field": "offset",
            "shard_size": 0,
            "size": 10
          }
        }
      },
      "nested": {
        "path": "colours"
      }
    }
  },
  "size": 0,
  "timeout": "60s"
}
```

Run against docs:

``` json
{
    "colours": [
        {
            "offset": 1,
            "basic_name": "black"
        }
    ]
}
```

One cluster returns:

``` json
"aggregations": {
    "colours": {
        "doc_count": 1,
        "offset__terms": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
            ]
        }
    }
}
```

And the other:

``` json
"aggregations": {
    "colours": {
        "doc_count": 1,
        "offset__terms": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": 1,
                    "doc_count": 1
                }
            ]
        }
    }
}
```

We fixed the issue by changing the nested field name from `offset` to `colours.offset` - however I'm still baffled as to why the two clusters would return different results. Additionally when using other fields nested under colours both the short and full path syntax work - this _only_ affects the `offset` key!
## 

ES version info (both clusters identical):

``` json
{
  "status" : 200,
  "name" : "cluster-X-01",
  "cluster_name" : "cluster-X",
  "version" : {
    "number" : "1.7.3",
    "build_hash" : "05d4530971ef0ea46d0f4fa6ee64dbc8df659682",
    "build_timestamp" : "2015-10-15T09:14:17Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="168104737">19656</key><summary>[1.7.3] Two identical ES clusters with identical data return different results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fizzadar</reporter><labels /><created>2016-07-28T14:08:45Z</created><updated>2016-07-28T18:47:56Z</updated><resolved>2016-07-28T18:47:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-28T18:47:56Z" id="235988657">This is due to a mapping bug which is fixed in 2.x.  You have more than one field called `offset` and it just chooses the first one it finds.  Hence, specifying the full path fixes the issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add index pattern wildcards support to _cat/shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19655</link><project id="" key="" /><description>`/_cat/shards` does not support wildcards for indices (like `/_cat/shards/boo*`) but it should according to the documentation.

closes #19634
</description><key id="168102300">19655</key><summary>Add index pattern wildcards support to _cat/shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:CAT API</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T13:58:14Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-08-01T09:10:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-29T14:06:33Z" id="236189435">Depends on #19688
</comment><comment author="tlrx" created="2016-08-01T08:48:05Z" id="236523711">@javanna Now #19688 has been merged I rebased this one. Can you please have a last look? Thanks!
</comment><comment author="javanna" created="2016-08-01T09:05:35Z" id="236527620">LGTM thanks a lot @tlrx 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES 2.3.4 accepts two fields with the same name and different types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19654</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.4

**Description of the problem including expected versus actual behavior**:
We are indexing logs to version 2.3.4. We got some index mapper parsing exceptions and checking the mapping I found out that the mapping contains two fields, under two different types, with the same name but their types are different.

Is it the desired behaviour? 

I had the understanding that under an index it could never happen. I would expect elasticsearch to accept the first type and reject the second one.

Here is the relevant part of the resulting mapping:
Please see 'body' field below

```
{
    "test-index": {
        "mappings": {
            "type-b": {
                "properties": {
                    "body": {
                        "type": "string"
                    }
                }
            },
            "type-a": {
                "properties": {
                    "extraData": {
                        "properties": {
                            "body": {
                                "properties": {
                                    "inner-field": {
                                        "type": "long"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

**Steps to reproduce**:
1.  POST into type-a {"extraData": { "body": { "inner-field":5 } } }
2. POST into type-b {"body": "text"}
3. Check the mapping

**Provide logs (if relevant)**:
These are the mapper parsing errors, on some cases:

&gt; {"type":"mapper_parsing_exception","reason":"failed to parse [body]","caused_by":{"type":"illegal_argument_exception","reason":"unknown property [linkId]"}}

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="168093197">19654</key><summary>ES 2.3.4 accepts two fields with the same name and different types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">segalziv</reporter><labels /><created>2016-07-28T13:18:15Z</created><updated>2016-07-28T16:27:42Z</updated><resolved>2016-07-28T16:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-28T16:05:56Z" id="235942445">Those fields in your mappings do not have the same _path_. Your body fields have full paths of `extraData.body` and `body`, so they are considered different fields (the full path is used when indexing into lucene).

The exception you see is unrelated to the steps to reproduce you have here. You have an invalid settings in your mapping for the body field (which is not shown here).
</comment><comment author="segalziv" created="2016-07-28T16:27:42Z" id="235948825">Ops, missed that. Thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add response body to ResponseException error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19653</link><project id="" key="" /><description>Consuming the response body to make it part of the exception message means that it may not be readable anymore later, depending on whether the entity is repeatable or not. Turns out that the response body tells a lot about the error itself, and considering that we don't expect bodies to be incredibly big for errors, we can wrap the entity into a BufferedHttpEntity to make it repeatable.

Closes #19653 
</description><key id="168086040">19653</key><summary>Add response body to ResponseException error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T12:41:59Z</created><updated>2016-07-28T13:53:15Z</updated><resolved>2016-07-28T13:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-28T12:42:08Z" id="235882974">@nik9000 this is for you
</comment><comment author="nik9000" created="2016-07-28T12:53:18Z" id="235885495">&gt; @nik9000 this is for you

Thanks!
</comment><comment author="nik9000" created="2016-07-28T13:46:54Z" id="235899060">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/ResponseExceptionTests.java</file></files><comments><comment>Merge pull request #19653 from javanna/enhancement/response_exception_body_message</comment></comments></commit></commits></item><item><title>Update TESTING.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19652</link><project id="" key="" /><description /><key id="168084942">19652</key><summary>Update TESTING.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adityasinghraghav</reporter><labels><label>docs</label></labels><created>2016-07-28T12:36:31Z</created><updated>2016-07-28T12:38:40Z</updated><resolved>2016-07-28T12:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-28T12:36:32Z" id="235881765">Can one of the admins verify this patch?
</comment><comment author="clintongormley" created="2016-07-28T12:38:18Z" id="235882164">thanks @adityasinghraghav - merged
</comment><comment author="javanna" created="2016-07-28T12:38:40Z" id="235882249">LOL thanks a lot @adityasinghraghav 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update TESTING.asciidoc (#19652)</comment></comments></commit></commits></item><item><title>Shield authentication failed to ES 2.3.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19651</link><project id="" key="" /><description>Hi Team,

I installed ES 2.3.4 &amp; latest shield and I can able to create esadmin user.

But we didn't have license for shield latest version I am getting below exception in ES logs,

License will expire on [Saturday, July 30, 2016]. If you have a new license, please update it.
Otherwise, please reach out to your support contact

Commercial plugins operate with reduced functionality on license expiration:
- shield
- Cluster health, cluster stats and indices stats operations are blocked
- All data operations (read and write) continue to work

Also I tried verifying the shield esadmin user by executing the following curl command,
curl --user esadmin:Gemmot08 -XGET '10.247.0.5:9200?pretty=true'

I am getting below error,

{
  "error" : {
    "root_cause" : [ {
      "type" : "security_exception",
      "reason" : "unable to authenticate user [esadmin] for REST request [/?pretty=true]",
      "header" : {
        "WWW-Authenticate" : "Basic realm=\"shield\""
      }
    } ],
    "type" : "security_exception",
    "reason" : "unable to authenticate user [esadmin] for REST request [/?pretty=true]",
    "header" : {
      "WWW-Authenticate" : "Basic realm=\"shield\""
    }
  },
  "status" : 401
}

Due to that, I couldn't login into the ES HQ &amp; marvel plugins for monitoring purpose.

Is the shield not working because of license update?

Please kindly let us know so that I can work the team to get the latest shield license.

Thanks,
Ganeshbabu R
</description><key id="168084146">19651</key><summary>Shield authentication failed to ES 2.3.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaneshbabuRamamoorthy</reporter><labels /><created>2016-07-28T12:32:10Z</created><updated>2016-07-28T12:37:45Z</updated><resolved>2016-07-28T12:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-28T12:37:45Z" id="235882051">Hi @GaneshbabuRamamoorthy 

You need to contact support:

&gt; Otherwise, please reach out to your support contact
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix NPE when simulating a pipeline with no id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19650</link><project id="" key="" /><description>When you simulate a pipeline without specifying an id against a node where the request is redirected to a master node,
the request and the response is throwing a NPE:

```
java.lang.NullPointerException
    at __randomizedtesting.SeedInfo.seed([3B9536AC6AA23C06:DD62280CF765DA1F]:0)
    at org.elasticsearch.common.io.stream.StreamOutput.writeString(StreamOutput.java:300)
    at org.elasticsearch.action.ingest.SimulatePipelineRequest.writeTo(SimulatePipelineRequest.java:92)
    at org.elasticsearch.transport.local.LocalTransport.sendRequest(LocalTransport.java:222)
    at org.elasticsearch.test.transport.AssertingLocalTransport.sendRequest(AssertingLocalTransport.java:95)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:470)
    at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:51)
    at org.elasticsearch.client.transport.support.TransportProxyClient.lambda$execute$441(TransportProxyClient.java:63)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:233)
    at org.elasticsearch.client.transport.support.TransportProxyClient.execute(TransportProxyClient.java:63)
    at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:309)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:67)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)
    at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:710)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)
    at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:62)
    at org.elasticsearch.ingest.bano.BanoProcessorIntegrationTest.testSimulateProcessorConfigTarget(BanoProcessorIntegrationTest.java:139)
```

This patch fixes this and adds some random tests.
</description><key id="168073769">19650</key><summary>Fix NPE when simulating a pipeline with no id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T11:29:39Z</created><updated>2016-07-28T12:55:32Z</updated><resolved>2016-07-28T12:55:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-28T11:57:08Z" id="235874346">LGTM - Thx for fixing!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Makes `m` case sensitive in TimeValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19649</link><project id="" key="" /><description>The reason for this change is that currently if a user specifies e.g.`2M`
meaning 2 months as a time value instead of throwing an exception
explaining that time units in months are not supported (due to months
having variable time spans) we instead will parse this to 2 minutes.
This could be surprising to a user and could mean put a lot of load on
the cluster performing a task that was never intended and whose results
will be useless anyway.

It is generally accepted that `m` indicates minutes and `M` indicates
months with time values so this is consistent with the expectations a
user might have around specifying time units.

A concrete example of where this causes issues is in the decay score
function which uses TimeValue to parse the scale and offset parameters
of the decay into millisecond values to use in the calculation.

Relates to #19619
</description><key id="168064295">19649</key><summary>Makes `m` case sensitive in TimeValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T10:33:09Z</created><updated>2016-07-28T12:00:57Z</updated><resolved>2016-07-28T12:00:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-28T10:34:28Z" id="235859748">LGTM.

Thanks a lot. Will help!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file></files><comments><comment>#19649 Makes `m` case sensitive in TimeValue</comment></comments></commit></commits></item><item><title>Add fields index and type to rated documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19648</link><project id="" key="" /><description>@cbuescher this adds index and type information to the document ratings. It also add roundtrip parsing for the RatedDocument XContent part. Would be great if you could have a look. If all is fine and jenkins is happy, please feel free to go ahead and hit merge as soon as tests are through.
</description><key id="168055273">19648</key><summary>Add fields index and type to rated documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>feature</label><label>WIP</label></labels><created>2016-07-28T09:46:56Z</created><updated>2016-08-10T09:57:10Z</updated><resolved>2016-08-10T09:57:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-07-28T17:15:22Z" id="235962061">@MaineC this looks good, left one small comment. Also it looks to me like the added index/type information isn't used for linking rated docs and search results in the metrics yet. I think it would be good to add that to this PR as well.
</comment><comment author="MaineC" created="2016-08-08T12:44:26Z" id="238224223">@cbuescher Update. I added functionality to check index/type information when comparing rated docs to actual hits retrieved. Would be great if you could have a look. 
</comment><comment author="MaineC" created="2016-08-09T09:14:49Z" id="238498142">@cbuescher merged base in, can you take a look please?
</comment><comment author="cbuescher" created="2016-08-09T10:08:54Z" id="238510654">@MaineC left a few small comments, other than that I like the change and the introduction of the RatedDocumentKey object.
</comment><comment author="MaineC" created="2016-08-09T10:53:14Z" id="238519392">.oO(Glad github lets you re-open closed pull requests - browser made this side jump in just the right moment for me to hit the wrong button instead of into the textfield I'm typing in right now...)

@cbuescher addressed your comments, thanks for pointing me to ConstructingObjectParser (and thanks to @nik9000 for including really helpful documentation on how to actually use it!)
</comment><comment author="cbuescher" created="2016-08-09T14:14:47Z" id="238566287">LGTM, thanks for the clarification
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds index and type information to rated documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19647</link><project id="" key="" /><description>@cbuescher this adds index and type information to the document ratings. It also add roundtrip parsing for the RatedDocument XContent part. Would be great if you could have a look. If all is fine and jenkins is happy, please feel free to go ahead and hit merge as soon as tests are through.
</description><key id="168051876">19647</key><summary>Adds index and type information to rated documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>feature</label><label>WIP</label></labels><created>2016-07-28T09:29:55Z</created><updated>2016-07-28T09:45:11Z</updated><resolved>2016-07-28T09:45:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-07-28T09:30:55Z" id="235846126">This PR commit history looks broken ... checking what went wrong.
</comment><comment author="MaineC" created="2016-07-28T09:45:11Z" id="235849333">_sigh_ wrong target branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster hangs and node disconnects due to exessive traffic on transport layer network card stopping pings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19646</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
Issue Tested on v 2.3.1 &amp; v2.3.4 of Elasticsearch

**JVM version**:
      "version" : "1.8.0_92",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "25.92-b14",

**OS version**:
CentOS Linux release 7.2.1511 (Core)

**Description of the problem including expected versus actual behavior**:

_Background information_
Our clusters consist of approximately 1500 or so indexes (5 shards per index), we are running a group of  aggregated queries across 1175 of the available indexes. On the test system in question (although the issue affects both our development and much more powerful live environment) there is between 1.5 and 2.5 TB of data (including 1 replica per shard).

**Expected**
The frontend of our system issues complex queries and often runs multiple queries at once. The queries are complex with a number of aggregations. The queries normally run on the ES backend and the frontend code then renders the results to the users.

Cluster state remains Green.

**Actual**
Staring the queries from the frontend with the cluster running and occasionally logging GC entries.

The cluster starts processing as expected. After a short amount of time one or more cluster nodes get disconnected from the cluster. This is seen in the logs on both the data node and the master instance it was attempting to communicate with.

_Master log entry_

```
[2016-07-27 15:51:14,198][DEBUG][action.admin.cluster.node.stats] [qa-es-01-master] failed to execute on node [0Ob8bwWzRJ2MmFoteZ-o1g]
ReceiveTimeoutTransportException[[qa-es-02-01][192.168.253.2:9300][cluster:monitor/nodes/stats[n]] request_id [177174] timed out after [15000ms]]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:679)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-07-27 15:52:06,645][INFO ][cluster.routing.allocation] [qa-es-01-master] Cluster health status changed from [GREEN] to [RED] (reason: [[{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false}] failed]).
[2016-07-27 15:52:06,650][INFO ][cluster.service          ] [qa-es-01-master] removed {{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false},}, reason: zen-disco-node_failed({qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout
```

_Failed node log entry_

```
[2016-07-27 15:52:28,547][WARN ][monitor.jvm              ] [qa-es-02-01] [gc][old][8175][2] duration [1.9m], collections [1]/[1.9m], total [1.9m]/[1.9m], memory [23.3gb]-&gt;[17.2gb]/[23.8gb], all_pools {[young] [838.6mb]-&gt;[19.2mb]/[865.3mb]}{[survivor] [108.1mb]-&gt;[0b]/[108.1mb]}{[old] [22.4gb]-&gt;[17.2gb]/[22.9gb]}
[2016-07-27 15:52:28,618][INFO ][discovery.zen            ] [qa-es-02-01] master_left [{qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2016-07-27 15:52:28,621][WARN ][discovery.zen            ] [qa-es-02-01] master left (reason = failed to ping, tried [3] times, each with  maximum [30s] timeout), current nodes: {{magnesium}{ucSxWsLbS_CFHzrgL-9bLw}{10.91.119.10}{10.91.119.10:9300}{data=false, master=false},{qa-es-02-master}{N-b9DErbTz2-N7DOYVx3yQ}{192.168.253.2}{192.168.253.2:9302}{host=qa-es-02, data=false, master=true},{qa-es-03-02}{M5sKFMoNSEKQFTFQjwQqyA}{192.168.253.3}{192.168.253.3:9301}{host=qa-es-03, master=false},{qa-es-01-01}{oM7dd8SjTNyTTXPn-65k6g}{192.168.253.1}{192.168.253.1:9300}{host=qa-es-01, master=false},{qa-es-03-master}{fQD12LrVRHWNmdTBHP-tNg}{192.168.253.3}{192.168.253.3:9302}{host=qa-es-03, data=false, master=true},{qa-es-01-02}{pgqJ5ybRQH2RGnxK7VkYrA}{192.168.253.1}{192.168.253.1:9301}{host=qa-es-01, master=false},{qa-es-03-01}{pCwkC8fCQj-B-wIWxCCxww}{192.168.253.3}{192.168.253.3:9300}{host=qa-es-03, master=false},{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false},{qa-es-02-02}{Vi5LgogOQNehMD5Yze5l5g}{192.168.253.2}{192.168.253.2:9301}{host=qa-es-02, master=false},}
[2016-07-27 15:52:28,625][INFO ][cluster.service          ] [qa-es-02-01] removed {{qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true},}, reason: zen-disco-master_failed ({qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true})
```

Each physical host has 3 nodes running on it. 1 Master node and 2 data nodes, host awareness is set for the nodes also. 

The cluster state changes to red (even though there are replicas available), normally from this point on the cluster will respond to queries such as /_cluster/health and /_nodes  but the failed nodes will not rejoin the cluster.

If i try to use the OS command to stop nodes that have timed out the command is ignored and just hangs. I have to kill -9 the process to stop the instance, this needs to be done on both the master and the node that failed. Usually even killing those 2 instances does not help, the cluster continues to throw errors about being unable to ping 

Even once stopped and restarting the individual instances they still fail to connect back to the rest of the cluster reporting timeouts to other instance's IPs however I can ping all the IPs of the cluster. 

I eventually realised that the problem is the bandwidth available in the network card used for the transport layer. Once this becomes saturated with traffic from the cluster the pings between nodes become queued at the network interface. By the time they are processed the other instances have already timed out the expected ping.

To confirm this was the issue I reset our cluster and modified it so rather than having 2 network cards em1 for the http traffic and em2 for the transport layer traffic i setup the machine so that it has

em1 - http traffic (1 Gb/s)
em2 and em3 as bonded interface bond0 with mode 0 round robin giving me a single (2Gb/s) interface.

Running the same query as above allowed the query to run without issue and there were no errors on the cluster. This would seem like the solution however the number of layers of aggregation in our system can change dynamically and even with the 2 x 1Gb/s interfaces acting as one adding another aggregation then caused the same original problem. 

**Steps to reproduce**:
1. On a cluster with a large amount of data and a large number of indexes and shards start a complex aggregation query. The query needs to create enough traffic to overwhelm the network card's available bandwidth.
2. Because of the saturation of the network card used for the transport layer the cluster pings fail to arrive in a timely manor resulting in the node being removed from the cluster by the master and the node thinking the master has gone away.

**Provide logs (if relevant)**:

**Thoughts on a solution**
In previous experience with clusters would normally have some disk quorum device which arbitrates similar issues, I think with elasticsearch this isn't required but being able to define a dedicated LAN (possibly LANs to allow redundancy) like you can split http and transport traffic would remove this problem entirely. As an example a solution such as:

em1 - HTTP Traffic 
em2 - Interconnect between nodes traffic (possibly including cluster state traffic)
em3 - transport layer traffic (i.e. results of searches, indexing etc......)

In this sense bonding or teaming of cards a the OS level would allow a user to provide resiliency whilst the segregation of traffic will protect the cluster from saturation of the transport layer with network IO. 

I also have a post on the discussion forum for the same issue: [https://discuss.elastic.co/t/cluster-nodes-get-disconnected-and-out-of-sync-due-to-ping-timeouts-caused-by-transport-load/56505/](https://discuss.elastic.co/t/cluster-nodes-get-disconnected-and-out-of-sync-due-to-ping-timeouts-caused-by-transport-load/56505/)

Kind Regards

Lee
</description><key id="168036837">19646</key><summary>Cluster hangs and node disconnects due to exessive traffic on transport layer network card stopping pings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Cardy165</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2016-07-28T08:06:08Z</created><updated>2016-07-28T19:14:53Z</updated><resolved>2016-07-28T19:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-28T12:11:00Z" id="235876987">The problem here is the long GCs you're getting, essentially because you're overwhelming the heap with your aggregations (probably because you're generating way too many buckets).

You need to simplify your aggs structure.  Imagine if you have 1,000 buckets and each has another 1,000 buckets, and each of those has another 1,000 buckets.  That's a trillion buckets!  It's just not going to work.  In fact, we've recently added a circuit breaker which will prevent such aggs from running https://github.com/elastic/elasticsearch/pull/19394
</comment><comment author="Cardy165" created="2016-07-28T12:50:44Z" id="235884899">That would make sense if our query was generating thousands of buckets. 

The first aggregation returns 2 buckets
the second as documented in the linked discussion is fixed at 5 buckets. 
The last aggregation could be one of several and at worst case would be 8000 buckets. 

There is GC going on on the cluster however doubling the capacity of the interface which is used for transports allows the query to run and return results, if the issue was purely a GC one then increasing the bandwidth available to the transport layer would not change the original behaviour.
</comment><comment author="clintongormley" created="2016-07-28T18:45:17Z" id="235987919">8000 buckets x 1175 indices x 5 shards that have to be handled by the coordinating node.  

Most of this traffic is across the transport layer, not http, so I very much doubt that separating the interfaces would really help here.  That said, I'll reopen this for further discussion.

/cc @bleskes 
</comment><comment author="bleskes" created="2016-07-28T19:14:52Z" id="235996110">&gt; The first aggregation returns 2 buckets
&gt; the second as documented in the linked discussion is fixed at 5 buckets. 
&gt; The last aggregation could be one of several and at worst case would be 8000 buckets

This is much more than @clintongormley said, as the 8000 buckets on the lowest level need to be multiplied by the upper levels as well - assuming `sex` (see later how I came up with) has just two values, that will be 2 x 5 (for age ranges) x lower level buckets x 1175 x 5 shards.

&gt; Most of this traffic is across the transport layer, not http, so I very much doubt that separating the interfaces would really help here. That said, I'll reopen this for further discussion.

Agreed. All the traffic here is highly likely on the transport layer, so separation is not a big deal. Also you can try it out since you can bind the http host to another IP than the transport one. See [here](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-http.html).

All in all I think you have two issues:
1) Long GCs cause nodes to not respond to pings - I see a 1.9m GC, cleaning about 6GB of mem.
2) Network saturation (presumably by queries) which prevents the cluster to normally operate. For pings and any other reasons, ES nodes should be able to freely communicate with each other.

You should either increase the capacity of your cluster to meet what you do with it or try to reduce the number of shards and see how it helps.

Closing this again. I suggest you continue the discussion on the thread you already have going with @danielmitterdorfer  on discuss.elastic.co : https://discuss.elastic.co/t/cluster-nodes-get-disconnected-and-out-of-sync-due-to-ping-timeouts-caused-by-transport-load/56505
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removing isCreated and isFound from the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19645</link><project id="" key="" /><description>Closes #19631.
This is cleanup work from #19566, where @nik9000 suggested trying to nuke the isCreated and isFound methods. I've combined nuking the two methods with removing UpdateHelper.Operation in favor of DocWriteResponse.Operation here.
</description><key id="168035112">19645</key><summary>Removing isCreated and isFound from the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">a2lin</reporter><labels><label>:CRUD</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-07-28T07:55:44Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-07-29T18:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-28T07:55:45Z" id="235825723">Can one of the admins verify this patch?
</comment><comment author="clintongormley" created="2016-07-28T12:06:18Z" id="235876092">This should be for v6 only, so we'll have to wait until we master moves to v6
</comment><comment author="nik9000" created="2016-07-28T12:11:19Z" id="235877064">If it is just the Java methods that is ok for  5.0 I think. I'll look in a
bit when I'm properly awake.

On Jul 28, 2016 8:06 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; This should be for v6 only, so we'll have to wait until we master moves to
&gt; v6
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19645#issuecomment-235876092,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLolx4_BPcUUMzQYfKGU0ls_9r9nbEks5qaJs-gaJpZM4JW9EI
&gt; .
</comment><comment author="nik9000" created="2016-07-28T13:11:22Z" id="235889881">@clintongormley this only removes these from the Java API and keeps the REST API. We've been fairly willing to make breaking changes like this in the 5.0 process in the past (like `refresh` -&gt; `refreshPolicy`).
</comment><comment author="nik9000" created="2016-07-28T13:42:33Z" id="235897854">@a2lin:

It looks like we're talking about whether this should be in 6.0 or 5.0. I thought 5.0 but maybe I was wrong. If it has to wait until 6.0 that means we'll leave it stalled until we branch 5.0 from master.

As far as code review, I left a few line notes:
- I'd prefer not to use `not` for the assertions at all.
- I prefer `assertEquals(foo, bar)` over `assertThat(bar, equalTo(foo))` but not enough to make you change it everywhere. If you prefer it my way then go ahead and change it.
- There are a few error messages that we 

None of these are major. The middle one'd be a fair amount of work but only if you want to do it. The non-test changes all look good to me.
</comment><comment author="nik9000" created="2016-07-28T14:53:39Z" id="235919327">Another thing: can you rename `_operation` to `operation`? I can do it if you don't want, but I figure you might be up for it. You can do it in this PR or another, either is fine with me.

The reason we want to rename it is that `_` is used for document metadata. At least that is what we're thinking right now. And `_operation` isn't really document metadata, it is response data.
</comment><comment author="nik9000" created="2016-07-28T15:38:40Z" id="235933974">&gt; Another thing: can you rename _operation to operation? I can do it if you don't want, but I figure you might be up for it. You can do it in this PR or another, either is fine with me.
&gt; 
&gt; The reason we want to rename it is that _ is used for document metadata. At least that is what we're thinking right now. And _operation isn't really document metadata, it is response data.

Ignore that. I'll open up another issue. I had a voice chat with a few people and I'll open up another issue.
</comment><comment author="nik9000" created="2016-07-28T16:09:10Z" id="235943394">&gt; Ignore that. I'll open up another issue. I had a voice chat with a few people and I'll open up another issue.

https://github.com/elastic/elasticsearch/issues/19664
</comment><comment author="a2lin" created="2016-07-28T16:13:44Z" id="235944732">@nik9000 is it still a good idea to change all the hamcrest matchers to assertEquals if the other parts of the test have assertThat/equalTo? I've changed the ones where assertEquals seemed to be the majority to try and be consistent. Let me know if you want the others to be changed too?
</comment><comment author="nik9000" created="2016-07-28T16:15:37Z" id="235945295">&gt; @nik9000 is it still a good idea to change all the hamcrest matchers to assertEquals if the other parts of the test have assertThat/equalTo?

It is up to you. I'd do it for the lines that I touched but you certainly don't have to.
</comment><comment author="nik9000" created="2016-07-28T16:18:40Z" id="235946193">OK. It looks good to me and I can merge it if @clintongormley is ok with it going into 5.0 and you ( @a2lin ) are done with it.
</comment><comment author="a2lin" created="2016-07-28T16:27:40Z" id="235948803">I am done with it, but I haven't run gradle check on this yet to make sure the tests pass.
</comment><comment author="a2lin" created="2016-07-28T16:28:09Z" id="235948973">I'll probably do that in about 8 hours as I'm out of morning-time :)
</comment><comment author="nik9000" created="2016-07-28T17:30:39Z" id="235966326">@elasticmachine , test this
</comment><comment author="nik9000" created="2016-07-28T18:07:23Z" id="235977103">Jenkins caught this:
core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingIT.java:103: Line is longer than 140 characters (found 168).

Maybe there is more, not sure.
</comment><comment author="a2lin" created="2016-07-29T07:55:14Z" id="236117980">@nik9000 I think this code should pass CI now. I've also made the assertEquals change.
</comment><comment author="nik9000" created="2016-07-29T15:57:19Z" id="236219485">I've pulled this back to `v5.0.0` because I believe that is correct. I also replaced `breaking` with the new `breaking-java` that tells folks that this really only breaks anyone that links to the java code (transport client, plugins, etc).

I'll review again.
</comment><comment author="nik9000" created="2016-07-29T16:00:18Z" id="236220278">OK - looks good to me. I'll merge in a bit.

@a2lin , do you have any interest in https://github.com/elastic/elasticsearch/issues/19664, hopefully the final change in this sequence before 6.0.
</comment><comment author="a2lin" created="2016-07-29T16:04:16Z" id="236221289">Sure, I'll give #19664 a shot. Thanks for the reviews!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postgresql storedprocedure to elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19644</link><project id="" key="" /><description>Hi,

I have to index Postgresql Stored Procedure which is having million of records. Can someone let me know the process how to do indexing the Stored Procedure into elasticsearch.  Without using parameter i want to download whole data in elasticsearch. Please help me. Thanks in advance.

bin=/home/elasticsearch-jdbc-2.3.1.0/bin
lib=/home/elasticsearch-jdbc-2.3.1.0/lib
 echo ' {
        "jdbc" : {
        "driver": "org.postgresql.Driver",
        "url": "jdbc:postgresql://localhost:5432/testdb",
        "user": "b2g_mns",
        "password": "b2gmns",
       "sql" : [
            {
                "callable" : true,
                "statement" : "{call  cloud10k.sp_fs_comp_search(?::varchar, ?::varchar, ?::varchar, ?::bigint)}",
        "parameter" : [
                                "CITI", "ALL", "ALL", 0
                                 ]
                           }
                        ],
                "index" : "my_jdbc_river03",
                "type" : "my_jdbc_river03"
       }
 }' | java \
      -cp "/home/elasticsearch-jdbc-2.3.1.0/lib/*" \
      -Dlog4j.configurationFile=/home/elasticsearch-jdbc-2.3.1.0/bin/log4j2.xml \
       org.xbib.tools.Runner \
       org.xbib.tools.JDBCImporter

Thanks,
Pavan Kumar.
</description><key id="168024298">19644</key><summary>Postgresql storedprocedure to elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kavurupavan</reporter><labels /><created>2016-07-28T06:42:20Z</created><updated>2016-07-28T07:13:13Z</updated><resolved>2016-07-28T07:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-28T07:07:59Z" id="235816751">Please ask questions on discuss.elastic.co
</comment><comment author="kavurupavan" created="2016-07-28T07:13:13Z" id="235817618">Hi sir,
I asked i didn't receive any response.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>search template mustache loop quoting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19643</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.4

**JVM version**: java version "1.8.0_65", Java(TM) SE Runtime Environment (build 1.8.0_65-b17), Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)

**OS version**: OS X 10.11.5 (15F34)

**Description of the problem including expected versus actual behavior**: mustache loop inserting extra quotes and commas.

**Steps to reproduce**:
1. Use example from https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#_validating_templates
   
   ```
   GET /_render/template
   {
     "inline": {
       "query": {
         "terms": {
           "status": [
             "{{#status}}",
             "{{.}}",
             "{{/status}}"
           ]
         }
       }
     },
     "params": {
       "status": [ "pending", "published" ]
     }
   }
   ```
2. Expected (from the documentation)
   
   ```
   {
     "template_output": {
       "query": {
         "terms": {
           "status": [ 
             "pending",
             "published"
           ]
         }
       }
     }
   }
   ```
3. Actual Result
   
   ```
   {
     "template_output": {
       "query": {
         "terms": {
           "status": [
             "",
             "pending",
             "",
             "published",
             ""
           ]
         }
       }
     }
   }
   ```
</description><key id="168013393">19643</key><summary>search template mustache loop quoting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jpcarey</reporter><labels><label>:Search Templates</label><label>discuss</label><label>docs</label></labels><created>2016-07-28T04:50:04Z</created><updated>2016-07-29T09:37:56Z</updated><resolved>2016-07-29T09:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-28T11:10:08Z" id="235866005">This is unfortunately a known issue of Mustache. We added a helper function to better render arrays of values in #18856 using the notation `{{#join}}my_iterable{{/join}}`. It will be released in the next 2.4 / 5.0 versions.
</comment><comment author="clintongormley" created="2016-07-28T11:50:32Z" id="235873171">Closed by https://github.com/elastic/elasticsearch/commit/8afb8da9cf74ae1d465700909a47655a2ea2481f and https://github.com/elastic/elasticsearch/commit/69c25bf8d98f281da37eac570d078dfd879966bd
</comment><comment author="jpcarey" created="2016-07-28T15:37:56Z" id="235933740">@clintongormley @tlrx is there a workaround for the currently impacted versions?  I think we should update or remove that particular example for those versions, as one would be more prone to hitting this issue by having a bad example in our docs.  I spent about an hour pulling apart a configured watch before I realized what the actual issue was.  This might be worth a separate bug, but hitting a _search template with the described issue from above provides a fairly useless response error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "query_parsing_exception",
            "reason": "[_na] query malformed, no field after start_object",
            "index": "logstash-uptime-test",
            "line": 1,
            "col": 2
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "logstash-uptime-test",
            "node": "4sTl4inGTWC0sHRirlCrZg",
            "reason": {
               "type": "query_parsing_exception",
               "reason": "[_na] query malformed, no field after start_object",
               "index": "logstash-uptime-test",
               "line": 1,
               "col": 2
            }
         }
      ]
   },
   "status": 400
}
```
</comment><comment author="clintongormley" created="2016-07-29T09:37:56Z" id="236137901">@jpcarey i updated the 2.x docs to reflect the real rendering (as mentioned by you above).  The exception that you're getting means you're doing something different from the example, eg this works:

```
PUT t/t/1
{
  "status": "pending"
}

GET /_search/template
{
  "inline": {
    "query": {
      "terms": {
        "status": [
          "{{#status}}",
          "{{.}}",
          "{{/status}}"
        ]
      }
    }
  },
  "params": {
    "status": [ "pending", "published" ]
  }
}
```

Try using `/_render/template` with the code giving you this error and you will be able to see where it is going wrong.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Removed array-of-string example from search template</comment></comments></commit></commits></item><item><title>when index with 0 replica, if lost some primary shards, the lost shards will keep unassigned status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19642</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2.0

**JVM version**:
jdk-8u71-linux-x64

**OS version**:
CentOS release 6.7 (Final)

**Description of the problem including expected versus actual behavior**:
   it is a little strange that as long as the cluster has more then 0 replications, the lost unassigned shards will be reassigned, but with 0 replications, they will keep unassigned status.
  is this a normal feature that only support manual reassign the lost primary shards when no replications ?
</description><key id="168006458">19642</key><summary>when index with 0 replica, if lost some primary shards, the lost shards will keep unassigned status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasstionzyf</reporter><labels /><created>2016-07-28T03:28:15Z</created><updated>2016-07-28T06:30:42Z</updated><resolved>2016-07-28T06:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-28T06:30:42Z" id="235810647">Please ask questions on discuss.elastic.co

Yes. This is expected.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adjacent wildcards in string query causes incorrect result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19641</link><project id="" key="" /><description>Elasticsearch version: 2.1.1

JVM version: java version "1.7.0_79" (Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode))

OS version: CentOS release 6.7 (Final)

Description of the problem including expected versus actual behavior:
Expected correct results for query "U??ZB4", but the result always hits 0.

Steps to reproduce:
 1.
execute search:
{"query":{"bool":{"filter":{"bool":{"should":{"wildcard":{"plateno":"U?SZB4"}}}}}}}
gets correct results:
"hits": { "total": 1,"plateno": "UBSZB4"}
 2.
execute search:
{"query":{"bool":{"filter":{"bool":{"should":{"wildcard":{"plateno":"U??ZB4"}}}}}}}
gets incorrect results:
"hits": { "total": 0}
</description><key id="168002672">19641</key><summary>Adjacent wildcards in string query causes incorrect result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">518jelly</reporter><labels /><created>2016-07-28T02:48:56Z</created><updated>2016-07-28T08:03:05Z</updated><resolved>2016-07-28T08:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-28T08:03:05Z" id="235827134">I can't reproduce it with this script:

``` sh
curl -XDELETE "localhost:9200/test?pretty"
curl -XPUT "localhost:9200/test/doc/1?pretty&amp;refresh" -d '{
    "foo": "barbaz"
}'
curl -XGET "localhost:9200/test/_search?pretty" -d '{
    "query": {
        "wildcard": {
            "foo":"bar?az"
        }
    }
}'
curl -XGET "localhost:9200/test/_search?pretty" -d '{
    "query": {
        "wildcard": {
            "foo":"bar??z"
        }
    }
}'
```

My guess is that because wildcard queries are not analyzed it does not give the expected result. Depends on your mapping as well.

You can follow up on discuss.elastic.co where we can help with this question.
Not an issue. (tested in 5.0)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add package-info to o.e.test.rest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19640</link><project id="" key="" /><description>This removes two packages, consolidating them into their parent package
and adds `package-info.java` files to describe all of the packages under
`org.elasticsearch.test.rest`.
</description><key id="167977736">19640</key><summary>Add package-info to o.e.test.rest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T23:10:00Z</created><updated>2016-07-29T07:50:46Z</updated><resolved>2016-07-28T20:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T23:10:35Z" id="235749332">@javanna, can you look at this too? You reviewed some changes I made in this area yesterday, this is just cleaning it all up and better documenting it.
</comment><comment author="javanna" created="2016-07-28T07:49:03Z" id="235824349">LGTM thanks! shall we also merge `o.e.test.rest.yaml.parser` and `o.e.test.rest.yaml.section`? maybe also the `o.e.test.rest.yaml.support` could be removed and its classed moved one level up?
</comment><comment author="nik9000" created="2016-07-28T17:34:45Z" id="235967446">&gt; maybe also the o.e.test.rest.yaml.support could be removed and its classed moved one level up?

I _think_ I did that.
</comment><comment author="nik9000" created="2016-07-28T20:12:00Z" id="236010915">Thanks for the review @javanna! I didn't want to merge `o.e.test.rest.yaml.parser` into `o.e.test.rest.yaml.section` because I thought they both had enough files in them already.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Assert we return Location header with 201 CREATED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19639</link><project id="" key="" /><description>Add an assertion to the most popular way of turning the response object
into the actual http response. As it stands all places we return
`201 CREATED` we return the `Location` header. This will help to keep it
that way, though it won't catch all uses.

Followup to #19509
</description><key id="167974851">19639</key><summary>Assert we return Location header with 201 CREATED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T22:49:09Z</created><updated>2016-07-29T07:50:46Z</updated><resolved>2016-07-28T20:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T22:49:36Z" id="235745518">@javanna this is a followup to another one you reviewed for me. Can you have a look?
</comment><comment author="javanna" created="2016-07-28T07:41:10Z" id="235822732">LGTM
</comment><comment author="nik9000" created="2016-07-28T20:54:04Z" id="236022026">Thanks for reviewing @javanna !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change file changes listener for resource watcher to an interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19638</link><project id="" key="" /><description>Currently to use the ResourceWatcherService to watch files, you
implement a FileChangesListener. However, this is a class, not an
interface, even though it has no base state or anything like that, just
defining a few methods. This change converts FileChangesListener to an
interface.
</description><key id="167971987">19638</key><summary>Change file changes listener for resource watcher to an interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T22:30:05Z</created><updated>2016-07-28T11:05:24Z</updated><resolved>2016-07-27T22:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T22:32:40Z" id="235742090">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/watcher/FileChangesListener.java</file><file>core/src/test/java/org/elasticsearch/watcher/FileWatcherTests.java</file></files><comments><comment>Merge pull request #19638 from rjernst/filewatcher_interface</comment></comments></commit></commits></item><item><title>Only log running out of slots when out of slots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19637</link><project id="" key="" /><description>We were logging on every `refresh=wait_for`.
</description><key id="167971213">19637</key><summary>Only log running out of slots when out of slots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T22:25:03Z</created><updated>2016-07-28T11:04:55Z</updated><resolved>2016-07-27T22:26:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-27T22:25:43Z" id="235740642">LGTM, thanks for fixing!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java</file></files><comments><comment>Only log running out of slots when out of slots (#19637)</comment></comments></commit></commits></item><item><title>Use fewer threads when reindexing-from-remote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19636</link><project id="" key="" /><description>Reindex from remote uses the Elasticsearch client which uses apache
httpasyncclient which spins up 5 thread by default, 1 as a dispatcher
and 4 more to handle IO. This changes Reindex's usage so it only spins
up two thread - 1 dispatcher and one to handle io. It also renames the
threads to "es-client-$taskid-$thread_number". That way if we see any
thread sticking around we can trace it back to the task.
</description><key id="167964441">19636</key><summary>Use fewer threads when reindexing-from-remote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T21:46:05Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-07-29T18:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T21:46:14Z" id="235731424">@javanna I think this is another one for you.
</comment><comment author="javanna" created="2016-07-28T07:31:47Z" id="235820923">LGTM besides a minor comment I left
</comment><comment author="nik9000" created="2016-07-28T21:20:31Z" id="236028842">@javanna I refactored it so that I could test it and added a test.
</comment><comment author="nik9000" created="2016-07-29T15:44:12Z" id="236215996">@javanna I pushed another one.
</comment><comment author="javanna" created="2016-07-29T17:32:43Z" id="236242969">left a super minor, LGTM though
</comment><comment author="nik9000" created="2016-07-29T18:15:11Z" id="236254165">@javanna Thanks again for reviewing! Jenkins for a race condition in the test so I added an `assertBusy` and I'm merging. Thanks again!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make snapshots save in new directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19635</link><project id="" key="" /><description>**Describe the feature**:
When taking a snapshot, the files are saved to the `location` defined in the repository settings.
If another snapshot is being taken right after the first one, it will be saved to the same directory as the first one, mixing the files together (Possibly overriding some of them?).

It would be nice if each snapshot would be saved to a new directory. It can be possible to either set the name in the snapshot command or maybe use a timestamp of some sort.

So when running:

``` bash
# create repository
curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
    "type": "fs",
    "settings": {
        "location": "/mount/backups/my_backup",
        "compress": true
    }
}'

# create snapshot
curl -XPUT 'http://localhost:9200/_snapshot/my_backup/my_snapshot' -d '{
  "indices": "index_1,index_2",
  "ignore_unavailable": "true",
  "include_global_state": false
}'
```

It will be saved to `/mount/backups/my_backup/my_snapshot/` instead of `/mount/backups/my_backup/` directly.
</description><key id="167963501">19635</key><summary>Make snapshots save in new directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaharmor</reporter><labels /><created>2016-07-27T21:41:10Z</created><updated>2016-07-27T21:58:14Z</updated><resolved>2016-07-27T21:58:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-27T21:58:14Z" id="235734440">Please ask questions on https://discuss.elastic.co. We reserve github for confirmed bug reports and feature requests.

Also, please read [how snapshots work](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html)  carefully. In particular:

&gt; The index snapshot process is incremental. In the process of making the index snapshot Elasticsearch analyses the list of the index files that are already stored in the repository and copies only files that were created or changed since the last snapshot. That allows multiple snapshots to be preserved in the repository in a compact form.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/shards should support index pattern wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19634</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3 &amp; 2.3.4

**JVM version**: what is running in Cloud now

**OS version**: what is running in Cloud now

**Description of the problem including expected versus actual behavior**:
I’m trying to do some cat queries and I run into an issue that I can’t see any results when I try to use a wildcard in _cat/shards

**Steps to reproduce**:
0. Make sure you have some indices named topbeat*
1. GET _cat/indices/topbeat\* &lt;-- as expected
2. GET _cat/segments/topbeat\* &lt;-- as expected
3. GET _cat/shards/topbeat\* &lt;-- empty response

But if I point to a single index with its full name, it does work.

Not sure if this is a bug or an enhancement, the docs actually speak of "[index pattern](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-shards.html#cat-shards)", so I'm leaning towards bug.

**Provide logs (if relevant)**:
</description><key id="167951136">19634</key><summary>_cat/shards should support index pattern wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">loekvangool</reporter><labels><label>:CAT API</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T20:40:22Z</created><updated>2016-08-26T13:30:52Z</updated><resolved>2016-08-01T09:10:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>/_cat/shards should support wilcards for indices</comment></comments></commit></commits></item><item><title>Deprecate found and created in delete and index rest responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19633</link><project id="" key="" /><description>These parts of delete and index response have been replaced with the
`operation` field.
</description><key id="167941675">19633</key><summary>Deprecate found and created in delete and index rest responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>deprecation</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T19:55:35Z</created><updated>2016-07-28T18:49:28Z</updated><resolved>2016-07-28T14:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-28T10:51:58Z" id="235863114">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/test/java/org/elasticsearch/action/delete/DeleteResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexResponseTests.java</file></files><comments><comment>Remove deprecated created and found from index, delete and bulk (#25516)</comment></comments></commit><commit><files /><comments><comment>[docs] Deprecate found and created (#19633)</comment></comments></commit></commits></item><item><title>update_by_query Retry on conflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19632</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.0

**JVM version**: 1.8

**OS version**: ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
I am using update_by_query API I do not get anything in failures [] when version conflicts happen. Is there a way I can retry on version conflicts on update_by_query?

**Steps to reproduce**:
1. Run update_by_query on certain documents
2. Meanwhile modify some of the documents
3. Version conflicts arise

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="167922901">19632</key><summary>update_by_query Retry on conflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anaghasjoshi</reporter><labels><label>:Reindex API</label></labels><created>2016-07-27T18:27:14Z</created><updated>2017-03-31T14:22:36Z</updated><resolved>2017-03-31T14:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T18:42:07Z" id="235680372">&gt; Is there a way I can retry on version conflicts on update_by_query?

It doesn't support that right now. You can either ignore version conflicts or abort the entire request.
</comment><comment author="nik9000" created="2016-07-27T18:43:40Z" id="235680813">&gt; I am using update_by_query API I do not get anything in failures [] when version conflicts happen.

We have tests for this behavior. Can you provide a bash script or something like that that reproduces the problem?
</comment><comment author="anaghasjoshi" created="2016-07-27T20:41:10Z" id="235713925">There are no scripts. Our system gets a lot of data from various other systems, where they all tend to modify the same set of documents (sometimes when one has taken a snapshot for update and the other has requested to index). Is there no way we can identify the version conflicted documents?
</comment><comment author="nik9000" created="2016-07-27T20:53:14Z" id="235717264">&gt; Is there no way we can identify the version conflicted documents?

If you don't pass `conflicts=proceed` in the request it should fail the request when it hits the first conflict and you can retry yourself. There is no built in retry on conflict, specifically because we'd have to be fairly fancy to make sure that the query that you send still matches.

If you aren't passing `conflicts=proceed` and you are still aren't seeing the failures on conflicts then you've found a bug. We have plenty of tests in this area so I expect the bug is something I haven't thought of. So I'd like you to provide a thinned down example that I can run in a shell or that reproduces it.
</comment><comment author="serzh" created="2017-01-04T16:07:26Z" id="270409400">Hi, guys. Are there any progress on adding `retry_on_conflicts` feature for `update_by_query` API? 
I'm really interested in this feature.</comment><comment author="nik9000" created="2017-01-04T16:43:01Z" id="270419310">No. I haven't been looking at it. The trouble is that a conflict means "this document has changed" which might mean that the update is no longer valid. On the other hand we could retry the query against the document and if it still passes then retry. That is *technically* possible, but not built and fairly fiddly to build.  You might be better off writing update_by_query requests in such a way that you can rerun them if there are any conflicts.</comment><comment author="marshall007" created="2017-01-07T01:08:41Z" id="271050339">&gt; You can either ignore version conflicts or abort the entire request.

@nik9000 how do you ignore version conflicts? Were you just referring to `conflicts=proceed`? Is there a workaround for when your update script is just appending to an array or incrementing a value?</comment><comment author="nik9000" created="2017-01-07T01:16:23Z" id="271051126">Yes, conflicts=proceed is all I meant. There isn't anything for the race
case. For the append to array case you could redo the update by query
ignoring docs that already have the field. It would certainly be nicer if
we had a recheck feature for conflicts like those but it'd be fiddly to put
together.

On Fri, Jan 6, 2017, 8:08 PM Marshall Cottrell &lt;notifications@github.com&gt;
wrote:

&gt; You can either ignore version conflicts or abort the entire request.
&gt;
&gt; @nik9000 &lt;https://github.com/nik9000&gt; how do you ignore version
&gt; conflicts? Were you just referring to conflicts=proceed? Is there a
&gt; workaround for when your update script is just appending to an array or
&gt; incrementing a value?
&gt;
&gt; —
&gt; You are receiving this because you were mentioned.
&gt;
&gt;
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/19632#issuecomment-271050339&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AANLolayEw3utHZyjnvsSz7HQsx9WRmKks5rPuWcgaJpZM4JWfB-&gt;
&gt; .
&gt;
</comment><comment author="colings86" created="2017-03-31T14:16:28Z" id="290724183">@nik9000 Is this still an issue? Could this be closed?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove isCreated and isFound from the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19631</link><project id="" key="" /><description>Callers can just use `getOperation` instead.

If this were the middle of a release cycle or the methods were very very popular we'd deprecate the methods instead but as is, we can get this in before 5.0 and slot it into the breaking changes list.

Note that if we don't get to this before 5.0 is GA we probably _would_ want to deprecate the method instead and remove it in 6.0.
</description><key id="167909447">19631</key><summary>Remove isCreated and isFound from the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Java API</label><label>breaking</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T17:23:24Z</created><updated>2016-08-04T09:12:46Z</updated><resolved>2016-07-29T18:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T17:23:31Z" id="235657186">Relates to #19566.
</comment><comment author="nik9000" created="2016-07-27T17:25:04Z" id="235657689">@a2lin are you interested in doing this?
</comment><comment author="a2lin" created="2016-07-27T17:28:00Z" id="235658663">Yes, I will do this.
</comment><comment author="nik9000" created="2016-07-27T17:34:12Z" id="235660477">Awesome! I can't assign it to you so I'll assign it to myself as a stand in.
</comment><comment author="colings86" created="2016-07-28T08:42:43Z" id="235835425">I wonder about this change since we will remove `isCreated` and `isFound` in the Java API here but surely when we move to using only the Java http client we will end up adding these methods back since the REST API has `found` and `created` rather than `operation` in it's response. I am not against this change but it will be annoying for users if we remove these methods only to effectively force them to go back when we deprecate the transport client in favour of the Java http client so at the very least we need to bear this in mind with the java http client. /cc @javanna 
</comment><comment author="colings86" created="2016-07-28T08:46:01Z" id="235836068">Sorry, ignore the above comment, I hadn't seen that we have just added the `_operation` field to the response of these APIs. Sorry for the noise
</comment><comment author="nik9000" created="2016-07-28T14:51:52Z" id="235918745">&gt; Sorry, ignore the above comment, I hadn't seen that we have just added the _operation field to the response of these APIs. Sorry for the noise

It is cool! We'll have both in 5.0 and in 6.0 we'll drop `found` and `created`. I merged a PR to the docs this morning to explain that they are now deprecated.

I'd also like to rename `_operation` to `operation`. The logic goes: `_` is used for document metadata and `_operation` isn't document metadata.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>core/src/test/java/org/elasticsearch/action/IndicesRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkWithUpdatesIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/LegacyDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/DateMathIndexExpressionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexPrimaryRelocationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomIOExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/nested/SimpleNestedIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Remove isCreated and isFound from the Java API</comment></comments></commit></commits></item><item><title>Remove found and created from index, delete, update, and bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19630</link><project id="" key="" /><description>People can just use `operation` instead.
</description><key id="167908665">19630</key><summary>Remove found and created from index, delete, update, and bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>breaking</label><label>v6.0.0</label></labels><created>2016-07-27T17:19:32Z</created><updated>2017-07-07T17:58:46Z</updated><resolved>2017-07-07T17:58:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T17:19:49Z" id="235656129">At the time of filing this issue we don't have a branch that yet targets 6.0 so this can't be worked on.
</comment><comment author="nik9000" created="2016-07-27T17:21:03Z" id="235656494">Relates to #19566.
</comment><comment author="clintongormley" created="2016-07-27T19:01:54Z" id="235686280">@nik9000 we should add deprecation docs to 5.0 though
</comment><comment author="nik9000" created="2016-07-27T19:55:46Z" id="235701073">&gt; @nik9000 we should add deprecation docs to 5.0 though

#19633
</comment><comment author="olcbean" created="2017-06-29T08:25:35Z" id="311897853">hey @nik9000 
Now that #19633 is merged, I would like to have a go at implementing the remaining enhancement. I figured I will ask before doing the work, as the issue does not have an `adoptme` tag.
Thanks!</comment><comment author="nik9000" created="2017-06-29T13:18:08Z" id="311963399">@olcbean it is all yours! Thanks for asking! I'd totally forgotten about it.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/test/java/org/elasticsearch/action/delete/DeleteResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexResponseTests.java</file></files><comments><comment>Remove deprecated created and found from index, delete and bulk (#25516)</comment></comments></commit></commits></item><item><title>Remove forced_refresh from rest response if we didn't force a refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19629</link><project id="" key="" /><description>Right now on ever write requests we include `"forced_refresh": false`. This is probably more noise than we need. Lets only include it if it is `"forced_refresh": true`.
</description><key id="167908134">19629</key><summary>Remove forced_refresh from rest response if we didn't force a refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T17:16:38Z</created><updated>2016-08-04T09:12:46Z</updated><resolved>2016-07-29T19:20:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-27T17:17:27Z" id="235655485">I added blocker to this because we don't want to release a major version and people to come to rely on it. This is a small change so I expect it won't actually _block_ anything.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/DocWriteResponse.java</file><file>core/src/test/java/org/elasticsearch/action/DocWriteResponseTests.java</file></files><comments><comment>Only write forced_refresh if we forced a refresh</comment></comments></commit></commits></item><item><title>missing a comma in the code example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19628</link><project id="" key="" /><description>without the comma I get a `json_parse_exception`
</description><key id="167903881">19628</key><summary>missing a comma in the code example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaredmcqueen</reporter><labels><label>docs</label></labels><created>2016-07-27T16:55:22Z</created><updated>2016-07-27T16:59:38Z</updated><resolved>2016-07-27T16:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-27T16:55:24Z" id="235649245">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-27T16:59:17Z" id="235650372">Thanks! Merging.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify building from sources docs in README</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19627</link><project id="" key="" /><description /><key id="167881558">19627</key><summary>Clarify building from sources docs in README</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adityasinghraghav</reporter><labels><label>docs</label></labels><created>2016-07-27T15:20:58Z</created><updated>2016-07-27T18:24:58Z</updated><resolved>2016-07-27T18:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-27T15:20:59Z" id="235619984">Can one of the admins verify this patch?
</comment><comment author="dakrone" created="2016-07-27T16:03:35Z" id="235633822">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="nik9000" created="2016-07-27T17:42:13Z" id="235662781">I think this PR is too a funny branch. I think it should be to master?
</comment><comment author="dakrone" created="2016-07-27T17:55:08Z" id="235666517">@nik9000 I can cherry pick the commit to master easily at least
</comment><comment author="nik9000" created="2016-07-27T18:21:08Z" id="235674086">&gt; @nik9000 I can cherry pick the commit to master easily at least

Yeah, that sounds like a good idea.
</comment><comment author="dakrone" created="2016-07-27T18:24:57Z" id="235675253">@adityasinghraghav actually this is already fixed in the README for the master branch (it uses build/distribution), I think this is not needed since it is opened against a branch that doesn't need the change.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix bwc index tool for versions before 5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19626</link><project id="" key="" /><description /><key id="167878048">19626</key><summary>fix bwc index tool for versions before 5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-07-27T15:06:58Z</created><updated>2016-08-03T13:37:38Z</updated><resolved>2016-08-03T13:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-27T18:43:00Z" id="235680628">LGTM
</comment><comment author="rjernst" created="2016-07-27T19:16:33Z" id="235690432">LGTM too.
</comment><comment author="brwe" created="2016-07-28T09:53:36Z" id="235851165">@clintongormley pushed another commit. Thanks all for the reviews.
</comment><comment author="rjernst" created="2016-07-30T05:40:52Z" id="236345103">LGTM
</comment><comment author="brwe" created="2016-08-03T09:50:04Z" id="237193752">@rjernst pushed another commit, please check I was unsure where to put the spaces.
</comment><comment author="clintongormley" created="2016-08-03T13:36:36Z" id="237238824">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fix bwc index tool for versions before 5.0 (#19626)</comment></comments></commit></commits></item><item><title>Transport client is broken for released versions from 5.0.0-alpha5 RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19625</link><project id="" key="" /><description>The transport client has now been moved into its own project (using project here to distinguish a gradle module/project from and Elasticsearch plugin module) `:client:transport`. This module has the following compile dependencies:

```
&lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
      &lt;artifactId&gt;reindex&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
      &lt;artifactId&gt;lang-mustache&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
      &lt;artifactId&gt;transport-netty4&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
      &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
      &lt;artifactId&gt;transport-netty3&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
      &lt;artifactId&gt;percolator&lt;/artifactId&gt;
      &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
```

The problem is that the `org.elasticsearch.plugin` artifacts don't seem to be published to the maven repository as part of the release build and therefore a project that tries to import the transport client using:

```
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
        &lt;artifactId&gt;transport&lt;/artifactId&gt;
        &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
    &lt;/dependency&gt;
```

cannot resolve the dependencies rendering the transport client broken. The above dependencies are required dues to https://github.com/elastic/elasticsearch/blob/master/client/transport/src/main/java/org/elasticsearch/transport/client/PreBuiltTransportClient.java#L52-59.

In order to get the transport client to be usable for 5.0.0-alpha5 onwards it looks like we need to start publishing the artifacts for the modules in the maven repository.
</description><key id="167872383">19625</key><summary>Transport client is broken for released versions from 5.0.0-alpha5 RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>blocker</label><label>build</label><label>v5.0.0-beta1</label></labels><created>2016-07-27T14:45:21Z</created><updated>2016-09-17T09:29:06Z</updated><resolved>2016-08-11T14:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-27T14:49:26Z" id="235609366">Similar to #18131. I agree we should publish at the very least jars that can be used by the transport client.
Won't be an issue anymore I guess once we will definitely switch to REST Client only.
</comment><comment author="nik9000" created="2016-08-01T11:38:29Z" id="236557751">I believe this is closed by https://github.com/elastic/elasticsearch/pull/19667.
</comment><comment author="costin" created="2016-08-01T14:08:06Z" id="236590580">The issue still persists and currently I can't test the upcoming ES 5.0.0-alpha5.

First off, the snapshot in the Maven OSS repository is incomplete as neither netty3 or netty4 artifacts are present:
https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/plugin/

Now using the staging repo I can find netty3 deployed however its pom is invalid and cannot be used inside Gradle:

```
   &gt; Could not resolve org.elasticsearch.plugin:transport-netty3-client:5.0.0-alpha5.
     Required by:
         elasticsearch-hadoop:elasticsearch-hadoop-mr:5.0.0.BUILD-SNAPSHOT
      &gt; Could not resolve org.elasticsearch.plugin:transport-netty3-client:5.0.0-alpha5.
         &gt; inconsistent module metadata found. Descriptor: org.elasticsearch.plugin:transport-netty3:5.0.0-alpha5 Errors: bad module name: expected='transport-netty3-client' found='transport-netty3'
```

The pom definition does look invalid:

```
  &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
  &lt;artifactId&gt;transport-netty3&lt;/artifactId&gt;
  &lt;version&gt;5.0.0-alpha5&lt;/version&gt;
```

am I missing something?
</comment><comment author="nik9000" created="2016-08-01T14:13:25Z" id="236592099">Maybe we just need to cut another build?
</comment><comment author="costin" created="2016-08-01T16:02:20Z" id="236625144">@clintongormley fixed the staging repo live and things build locally; the CI should soon (~1h total) confirm that. However I'm still confused why the netty3/4 artifacts don't show up in the OSS Sonatype repo. The latest deployment is from [today](https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/elasticsearch/5.0.0-alpha5-SNAPSHOT/) but the modules are not there.
@rjernst any ideas?
</comment><comment author="rjernst" created="2016-08-01T16:05:44Z" id="236626070">Note that the reason Clint had to fix it up locally was because the "old" way (prior to unified release) that stuff gets created does not know about the hack I added for client jars of modules/plugins.

&gt; However I'm still confused why the netty3/4 artifacts don't show up in the OSS Sonatype repo

CI is caught in a weird world between old publishing, and the unified release snapshots. I just found the unified builds have been (silently) failing in CI unknowingly forever.  I'm working with infra to get that fixed, and then when we have clean runs, I will ask them to remove the old job (I did not even realize it was still running).
</comment><comment author="costin" created="2016-08-01T16:12:27Z" id="236627975">@rjernst Thanks. Fwiw, after upgrading to ES alpha5, Gradle was reporting a build as successful despite clearly failing in the logs. It was the execution time that gave it away.
</comment><comment author="rjernst" created="2016-08-01T16:17:00Z" id="236629316">&gt; Gradle was reporting a build as successful despite clearly failing in the logs

Can you expand on this or open a new issue with details?
</comment><comment author="costin" created="2016-08-01T16:38:52Z" id="236635297">Still exploring - it _might_ be something inside our build script.
</comment><comment author="clintongormley" created="2016-08-11T11:16:42Z" id="239133483">@costin any further info?
</comment><comment author="costin" created="2016-08-11T14:44:23Z" id="239182495">The transport plugin seem to be in maven so we're good.
As for the Gradle misbehaviour, I couldn't reproduce it but I'll keep on trying. However this is separate and has nothing to do with the issue itself which can be closed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throttling importing dangling shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19624</link><project id="" key="" /><description>If you have 1000s of shards and blow away your cluster state by mistake and try to bring an empty master online then the cluster will try to import all the dangling shards. The trouble is that each node tries to import all of its shards as soon as it comes online. It is quite possible that the master node doesn't have the heap to process the whole request, especially if dozens of nodes try to do this all at the same time.

Given that this is a recovery operation and not a super common one I don't think we should engineer fancy stuff like brackpressure. Instead I think we should throttle the number of dangling shards we try to import at once, maybe adding a randomized wait before the first import when an empty master comes online to stagger nodes.
</description><key id="167868397">19624</key><summary>Throttling importing dangling shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cluster</label><label>adoptme</label><label>enhancement</label></labels><created>2016-07-27T14:29:42Z</created><updated>2016-08-11T12:38:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-27T15:24:51Z" id="235621298">it's very hard to coordinate things between distributed nodes. I suggest something simple like a node level settings that makes the node only send 5 dangling indices at a time (by default). Once those are allocated the next batch can be sent.
</comment><comment author="jpountz" created="2016-07-29T09:38:15Z" id="236137960">Discussed in FixitFriday: @bleskes's suggestion above should help while being simple, so we are removing the `discuss` label and making it an `adoptme`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add parsing of type information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19623</link><project id="" key="" /><description>This adds parsing of which type should be targeted when executing queries for rank evaluation, as well as actually using this information down the line.

Missing is a test that fails w/o that change - is that something that should be added to the REST test or is there some other means to check this integration works?

@cbuescher your input would be most welcome.
</description><key id="167859952">19623</key><summary>Add parsing of type information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Rank Evaluation</label><label>feature</label><label>WIP</label></labels><created>2016-07-27T13:55:31Z</created><updated>2016-08-11T10:53:07Z</updated><resolved>2016-08-02T11:50:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-08-02T11:01:35Z" id="236872305">LGTM, lets work on removing the individual TODOs on the feature branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest client's ResponseException's message should include the body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19622</link><project id="" key="" /><description>The body of the response is super useful in figuring out why the request failed. Without it the message for the exception doesn't give you a clue what is wrong other than "400 error when trying this request" or "500 error when trying this other one".

I know we removed that functionality as part of adding the async request methods but I want it back please. I gave it a shot as part of #19310 but it was fiddly because InputStream so I figured it should be fixed in its own PR.
</description><key id="167854610">19622</key><summary>Rest client's ResponseException's message should include the body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Java REST Client</label><label>enhancement</label></labels><created>2016-07-27T13:32:20Z</created><updated>2016-07-28T13:53:28Z</updated><resolved>2016-07-28T13:53:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-27T13:36:45Z" id="235586859">We didn't remove the body from the exception, we just changed the way you get to it. It's just that it was pre-read before to avoid making an exception closeable, now you just have to read it through `responseException.getResponse().getEntity()`.
</comment><comment author="javanna" created="2016-07-27T13:39:46Z" id="235587688">Maybe you mean that the body was previously part of the exception message?
</comment><comment author="nik9000" created="2016-07-27T13:43:11Z" id="235588677">Yes! That. Sorry.

You can always get the body from the exception but the trouble with that is that you have to intentionally do it. And that only works if you are explicitly handling the exception. If you aren't then you only have the exception message. I think it'd be much more user friendly to have the body in the message.
</comment><comment author="javanna" created="2016-07-27T13:49:16Z" id="235590472">It would be better yes, but also a bit cumbersome because some entities are not repeatable hence if we read the body automatically users cannot get it back from the Response anymore. I could do tricks like I did in the `RequestLogger` to wrap the entity and make it repeatable, but I am not sure that is a good solution for every single exception. Or maybe just document the different and only way to get the body back. Which one do you prefer?
</comment><comment author="nik9000" created="2016-07-27T13:51:18Z" id="235591063">Normally I'd prefer the simple way, but I think the request logger trick might be a better choice here just because exception messages are so important for debugging and the body is really the best part of the message.
</comment><comment author="javanna" created="2016-07-27T13:52:19Z" id="235591385">I will look into this.
</comment><comment author="nik9000" created="2016-07-27T13:55:46Z" id="235592438">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/ResponseExceptionTests.java</file></files><comments><comment>Add response body to ResponseException error message</comment></comments></commit></commits></item><item><title>[TEST] Kill remaining lang-groovy messy tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19621</link><project id="" key="" /><description>After #13834 many tests that used Groovy scripts (for good or bad reason) in their tests have been moved in the lang-groovy module and the issue #13837 has been created to track these messy tests in order to clean them up.

The work started with #19280, #19302 and #19336 and this PR moves the remaining messy tests back in core, removes the dependency on Groovy, changes the scripts in order to use the mocked script engine, and change the tests  to integration tests.

It also moves `IndexLookupIT` test back (even if it has good chance to be removed soon) and fixes its tests.

closes #13837
</description><key id="167850943">19621</key><summary>[TEST] Kill remaining lang-groovy messy tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T13:15:38Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-08-01T15:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-28T10:03:49Z" id="235853492">@nik9000 Thanks for your review!

I addressed all your comments. I also made the `MockScriptEngine` stricter so that it now throws an exception when compiling undefined scripts.

Can you please have another look?
</comment><comment author="tlrx" created="2016-07-29T08:07:19Z" id="236120118">@nik9000 Do you need any more information or do you have any more comment? I'd like to get this in. Thanks
</comment><comment author="nik9000" created="2016-07-29T14:12:45Z" id="236191031">LGTM. Sorry to wedge this open so long.

Elasticmachine is complaining about:

```
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=FBA82156698EB076 -Dtests.class=org.elasticsearch.script.IndexLookupIT -Dtests.method="testCallWithDifferentFlagsFails" -Dtests.security.manager=true -Dtests.locale=mk -Dtests.timezone=America/Argentina/San_Juan
```

so I guess that is all that is left to solve before merging.
</comment><comment author="tlrx" created="2016-07-29T14:48:12Z" id="236200746">@nik9000 Thanks! I thought I fixed this test but obviously not. It should be OK now.
</comment><comment author="tlrx" created="2016-07-29T16:00:16Z" id="236220272">:(
</comment><comment author="nik9000" created="2016-07-29T16:04:29Z" id="236221349">Sad jenkins.
</comment><comment author="nik9000" created="2016-07-29T16:05:21Z" id="236221555">Lots of fun failures this time!

```
   &gt; Caused by: java.lang.IllegalArgumentException: No pre defined script matching [{ "match_all" : {}}] for script with name [null], did you declare the mocked script?
```
</comment><comment author="tlrx" created="2016-07-29T16:07:37Z" id="236222136">Yes, I made MockScriptEngine more strict and it now shouts when a script is used but not registered first. Looks like `TemplateQueryBuilderTests` uses mocked scripts and mustache scripts but the way the test is initialized only MockScriptEngine is used. It's almost a useless test.
</comment><comment author="nik9000" created="2016-08-01T14:22:36Z" id="236594730">Last change makes sense to me. LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Number format in "origin" attribute of Gauss decay functional score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19620</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 8

**OS version**: ubuntu 14.04

**Description of the problem including expected versus actual behavior**: Number_format error when using multiple numbers in "origin" attribute in Gauss scoring function. 

**Query**:
"gauss": { 
    "experience": { 
          "origin": "11, 12",
          "scale": "2",
          "offset": "0",
          "decay": 0.33
    }
}

**Output** :- {"error":{"root_cause":[{"type":"number_format_exception","reason":"For input string: "11,12""}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"profiles","node":"AHWbyD5qR3yYDAzNIOZ8rA","reason":{"type":"number_format_exception","reason":"For input string: "11,12""}}]},"status":400}
</description><key id="167849088">19620</key><summary>Number format in "origin" attribute of Gauss decay functional score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MalhotraKaran</reporter><labels /><created>2016-07-27T13:07:16Z</created><updated>2016-07-27T18:44:08Z</updated><resolved>2016-07-27T18:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T18:44:08Z" id="235680970">You can't use multiple origins in decay functions.  The `"lon,lat"` format is reserved for geo-point fields
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Years and months units aren't supported for 'scale' parameter in decay score function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19619</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3

**JVM version**: 1.8.0_66

**OS version**: linux 3.2.0-106-generic-pae

**Description of the problem including expected versus actual behavior**:
The documentation says that date units expressions work for 'scale' parameter of the decay function, however not all date time units seem work. 

As the user I expect an expression as '10y' or '10Y' work as '10 years'
Also I expect an expression as '2M' work as '2 month'.

Actually years pattern isn't supported and 'M' unit works as minutes.

There is a work-around to write '365d' instead of year and '30d' instead of month, however it will produce slightly different results if the current month has 31, 28 or 29 days or the current year is leap.
If years and months units can't be supported the documentation should have a complete list of supported units (now it isn't obvious)

**Steps to reproduce**:
1. Create a mapping with the indexed date field
2. Put some documents
3. Perform the fuctional score query with any decay function on your date field and scale with year or month expression 

**Logs**:
Caused by: ElasticsearchParseException[Failed to parse setting [GaussDecayFunctionParser.scale] with value [10y] as a time value: unit is missing or unrecognized]

(see the attachment for stack trace)
[log.txt](https://github.com/elastic/elasticsearch/files/385902/log.txt)

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="167834669">19619</key><summary>Years and months units aren't supported for 'scale' parameter in decay score function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">igorbunova</reporter><labels><label>:Dates</label><label>:Query DSL</label><label>discuss</label></labels><created>2016-07-27T11:46:20Z</created><updated>2016-07-28T15:27:00Z</updated><resolved>2016-07-28T12:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-27T11:52:55Z" id="235563369">See also #2473 #3841 #7666 #8939 #14802
</comment><comment author="clintongormley" created="2016-07-28T12:35:28Z" id="235881539">https://github.com/elastic/elasticsearch/pull/19649 fixes the misinterpretation of `M` as minutes.  For aggregations, we should try to fix time parsing so that eg `10y` works, but date histograms use calendars which is why it makes sense to do it there.  The decay functions don't use calendars - just a distance from a point.  So `10y` isn't going to take leap years etc into account.

I think it is correct to only expose the units that have a real meaning here.  If you want to use 3 months in decay functions, then you need to decide how many days that should be (because a day has a determined length, but a month doesn't).
</comment><comment author="rjernst" created="2016-07-28T15:27:00Z" id="235930096">&gt; I think it is correct to only expose the units that have a real meaning here.

By extension of that, we should not support days. Due to daylight savings time, 1 day a year has 23 hours, and 1 day has 25 hours. Hours is the highest granularity that does not change (if you don't count leapseconds!)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file></files><comments><comment>Makes `m` case sensitive in TimeValue</comment></comments></commit></commits></item><item><title>[DOCS] add java REST client docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19618</link><project id="" key="" /><description>Add some docs on how to get started with the Java REST client, some common configuration that may be needed and the sniffer component.
</description><key id="167822012">19618</key><summary>[DOCS] add java REST client docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T10:29:13Z</created><updated>2016-07-29T16:01:39Z</updated><resolved>2016-07-29T09:22:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-27T10:35:05Z" id="235549100">@nik9000 @clintongormley can you have a look please?
</comment><comment author="nik9000" created="2016-07-27T16:35:18Z" id="235643519">CI failure comes from branching from an unfortunate point.
</comment><comment author="nik9000" created="2016-07-27T17:32:00Z" id="235659855">Left some small things but I think it is ok.
</comment><comment author="nik9000" created="2016-07-27T17:33:02Z" id="235660149">&gt; ok

And by that I mean great that we have some docs for this! None of the comments I left are worth blocking merging it. I didn't render the docs locally and look at how they lay out though.
</comment><comment author="clintongormley" created="2016-07-28T08:53:49Z" id="235837929">Minor things, but LGTM. Perhaps put the docs in `docs/java-rest` instead of `docs/java`.  I was confused about which dir to build
</comment><comment author="javanna" created="2016-07-28T09:55:52Z" id="235851651">@clintongormley I addressed your comments and replied to the ones that I was not sure about.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] add java REST client docs (#19618)</comment></comments></commit></commits></item><item><title>add  /elasticsearch-2.3.3/lib/jts-1.13.jar to build path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19617</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="167819169">19617</key><summary>add  /elasticsearch-2.3.3/lib/jts-1.13.jar to build path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ucasyp</reporter><labels /><created>2016-07-27T10:13:23Z</created><updated>2016-07-27T11:55:31Z</updated><resolved>2016-07-27T11:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-27T11:55:31Z" id="235563861">JTS is marked as optional.

Which means that you simply have to add it to your `pom.xml` if you wish.

See https://github.com/elastic/elasticsearch/blob/2.4/core/pom.xml#L90-L94
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plain highlighter should ignore parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19616</link><project id="" key="" /><description>The plain highlighter fails when it tries to select the fragments based on a query containing either a `has_child` or `has_parent` query.

The plain highlighter should just ignore parent/child queries as it makes no sense to highlight a parent match with a `has_child` as the child documents are not available at highlight time. Instead if child document should be highlighted inner hits should be used.

Parent/child queries already have no effect when the `fvh` or `postings` highligher is used. The test added in this PR verifies that.

PR for #14999
</description><key id="167809656">19616</key><summary>Plain highlighter should ignore parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Highlighting</label><label>:Parent/Child</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-27T09:25:39Z</created><updated>2016-07-29T16:01:38Z</updated><resolved>2016-07-29T11:01:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2016-07-28T08:55:17Z" id="235838270">wow, didn't know github works this way, sorry about the noise.
</comment><comment author="martijnvg" created="2016-07-28T09:04:51Z" id="235840372">@HonzaKral np :) - This is totally unexpected.
</comment><comment author="jpountz" created="2016-07-29T07:48:14Z" id="236116800">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Phrase Suggester collate query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19615</link><project id="" key="" /><description>I'm not able to set the collate query in Phrase Suggester api.

I created a template query initially like this
`POST /_search/template/sc
{
    "template": {
        "query": {
              "query_string" : {
                "query" : "{{suggestion}}",
                "fields" : [ "SearchAllText" ],
                "default_operator" : "and",
                "analyze_wildcard" : true
              }
              }
    }
}`

and then tried to use that in PhraseSuggestionBuilder with collateQuery, but it doesn't seem to interpret correctly.

`QueryBuilder qb = templateQuery("sc", ScriptService.ScriptType.INDEXED, null);        PhraseSuggestionBuilder psb = SuggestBuilders.phraseSuggestion(name)
                .text(original)
                .field(SUGGEST_FIELD)
                .size(1)
                .addCandidateGenerator(generator
                        .suggestMode("popular")
                        .minWordLength(1)
                        .prefixLength(0))
                        .collateQuery(qb.toString());`

Above query gets translated to this:
`phraseSuggest": {
    "text": "xyz",
    "phrase": {
        "field": "SuggestCompletion",
        "size": 1.0,
        "direct_generator": [
            {
                "field": "SuggestCompletion",
                "suggest_mode": "popular",
                "prefix_length": 0.0,
                "min_word_length": 1.0
            }
        ],
        "collate": {
            "query": "{\n  \"template\" : {\n    \"id\" : \"sc\",\n    \"params\" : null\n  }\n}"
        }
    }
}
`
Can you please help me with this?
</description><key id="167803403">19615</key><summary>Phrase Suggester collate query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shyam-aryan</reporter><labels /><created>2016-07-27T08:52:57Z</created><updated>2016-07-27T18:08:37Z</updated><resolved>2016-07-27T18:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T18:08:37Z" id="235670474">Hi @shyam-aryan 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only. If it turns out that this is a bug, then feel free to reopen with the details.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch accepts invalid json with unpredictable behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19614</link><project id="" key="" /><description>When a key is present in json object multiple times it doesn't raise a parse error and only last value is used. This should instead raise `json_parse_exception`.

**Elasticsearch version**: verified on 2.x, 5.0.0-alpha3

**Steps to reproduce**:
1. `curl -X PUT localhost:9200/i -d '{"settings": {"number_of_replicas": 2}, "settings": {"number_of_shards": 1}}'`
2. `curl -X GET localhost:9200/i`
</description><key id="167803334">19614</key><summary>Elasticsearch accepts invalid json with unpredictable behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>bug</label></labels><created>2016-07-27T08:52:34Z</created><updated>2017-01-10T08:35:28Z</updated><resolved>2016-12-14T08:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-27T09:12:57Z" id="235531226">I could see this becoming a long discussion around whether that one is invalid json or not and whether we should return a parse exception or some other error. The json library we use for parsing allows this, then we should improve this on our end rather than being lenient.

This reminds me of #19547 too and is a very common problem with the way we pull parse json. It can easily be solved case by case but every single parser in our codebase is subject to this so it would be nice to have some generic solution for it. Not sure if there are alternatives to adding lots of ifs to all our pull parsers, we should evaluate that.
</comment><comment author="tlrx" created="2016-07-27T10:00:08Z" id="235542038">&gt; The json library we use for parsing allows this, then we should improve this on our end rather than being lenient.

For the record, the option is `JsonParser.STRICT_DUPLICATE_DETECTION` and has the following warning:

```
Note that enabling this feature will incur performance overhead 
due to having to store and check additional information: 
this typically adds 20-30% to execution time for basic parsing.
```
</comment><comment author="clintongormley" created="2016-07-27T18:11:49Z" id="235671436">According to the JSON spec, this isn't invalid JSON.  The spec doesn't mention how duplicate keys should be treated.  Many languages will simply overwrite older values with newer values, without generating any warning.  This is essentially what Elasticsearch does today, and i'm not sure it is worth a 20-30% penalty to prevent this behaviour.
</comment><comment author="HonzaKral" created="2016-07-27T19:19:37Z" id="235691305">Yes, strictly speaking (the rfc only says the keys **SHOULD** be unique), this is valid. I also agree that the performance penalty isn't worth it. It would, however, be nice to document this behavior and perhaps (if it's easy) have an option to turn on strict checking (ideally per request) - it would be useful as debugging tool and perhaps when running tests.
</comment><comment author="martijnvg" created="2016-07-28T09:56:16Z" id="235851745">Allowing duplicate keys adds a lot of confusion: https://discuss.elastic.co/t/using-the-remove-processor-for-ingest-node/56500

Maybe for certain apis we should enable strict parsing? (admin like APIs?)
</comment><comment author="jpountz" created="2016-07-29T09:52:22Z" id="236140859">Discussed in FixitFriday: let's play with the jackon feature to reject duplicated keys and make sure that it works and has a reasonable performance hit. If it is not satisfactory, then let's look into whether there are things that we can do at a higher level such as ObjectParser.
</comment><comment author="danielmitterdorfer" created="2016-12-01T12:46:10Z" id="264164983">### Macrobenchmark Results

We have run our whole macrobenchmark suite with `JsonParser.STRICT_DUPLICATE_DETECTION == false` (`baseline`) and `JsonParser.STRICT_DUPLICATE_DETECTION == true` (`STRICT_DUPLICATE_DETECTION`) and published the results. https://elasticsearch-benchmarks.elastic.co/19614/

We see at most a reduction in median indexing throughput of 3% for our macrobenchmark suite (PMC track). 

### Microbenchmark Results

I also double-checked a few scenarios with a microbenchmark and saw similar results (see https://gist.github.com/danielmitterdorfer/9236796a46f3956447171313a6a0b365):

Below are the results of both configurations showing the average time for one iteration (smaller is better).

`JsonParser.Feature.STRICT_DUPLICATE_DETECTION: false`:


```
Benchmark                      Mode  Cnt   Score   Error  Units
JsonParserBenchmark.largeJson  avgt   60  19.414 ± 0.044  us/op
JsonParserBenchmark.smallJson  avgt   60   0.479 ± 0.001  us/op
```

`JsonParser.Feature.STRICT_DUPLICATE_DETECTION: true`:

```
Benchmark                      Mode  Cnt   Score   Error  Units
JsonParserBenchmark.largeJson  avgt   60  20.642 ± 0.064  us/op
JsonParserBenchmark.smallJson  avgt   60   0.487 ± 0.001  us/op
```

For smaller JSON objects (49 bytes) the overhead of duplication check is 8ns or 1.6%. For a large JSON object (6440 bytes) the overhead of duplication check is in the range 1.12us [1] and 1.3us [2] or in the range 5.8% and 6.7%.

[1] best case duplication check enabled 20.578 us, worst case duplication check enabled: 19.458 us
[2] worst case duplication check enabled: 20.706 us, best case duplication check disabled: 19.370 us

Please refer to the gist for more details.</comment><comment author="jpountz" created="2016-12-01T13:08:34Z" id="264169209">Thanks @danielmitterdorfer. To me that means we should do it. We can have an undocumented escape hatch if we do not feel confident the overhead will be low in all cases.</comment><comment author="danielmitterdorfer" created="2016-12-01T13:45:04Z" id="264176362">&gt; We can have an undocumented escape hatch

@jpountz The [relevant code](https://github.com/elastic/elasticsearch/blob/27ff4f3/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java#L52-L60) is in a `static` block so we can't use our settings infrastructure. I guess that means we'd use a system property?</comment><comment author="jpountz" created="2016-12-01T13:51:24Z" id="264177732">That would work for me. Or we could handle it like `INDICES_MAX_CLAUSE_COUNT_SETTING` I suppose, which is a node setting that sets the static limit on the number of boolean clauses.</comment><comment author="tlrx" created="2016-12-05T09:24:58Z" id="264804574">Thanks @danielmitterdorfer. 

I agree with @jpountz, and a first step would be to see if our tests pass (I'm pretty sure we will have to adapt some of them). Also, the same JSON factory is used for both parsing and generating JSON: if we enable this feature then we'll also see if we generate duplicate keys somewhere, which is cool. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/JsonSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/YamlSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/ConstructingObjectParserTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/phrase/DirectCandidateGeneratorTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/yaml/parser/AbstractClientYamlTestFragmentParserTestCase.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/yaml/parser/DoSectionParserTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/yaml/restspec/ClientYamlSuiteRestApiParserFailingTests.java</file></files><comments><comment>Enable strict duplicate checks for all XContent types (#22225)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/JsonSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/ConstructingObjectParserTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/nested/SimpleNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/phrase/DirectCandidateGeneratorTests.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/TransportPercolateAction.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/yaml/restspec/ClientYamlSuiteRestApiParserFailingTests.java</file></files><comments><comment>Enable strict duplicate checks for JSON content</comment></comments></commit></commits></item><item><title>How to solve this problem???</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19613</link><project id="" key="" /><description>RemoteTransportException[[node6][10.25.113.127:9300][indices:data/read/search[phase/query/id]]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@75595924 on EsThreadPoolExecutor[search, queue capacity = 8000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1b43ef9e[Running, pool size = 13, active threads = 13, queued tasks = 8000, completed tasks = 49763771]]];
Caused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@75595924 on EsThreadPoolExecutor[search, queue capacity = 8000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1b43ef9e[Running, pool size = 13, active threads = 13, queued tasks = 8000, completed tasks = 49763771]]]
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:247)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="167798544">19613</key><summary>How to solve this problem???</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunxiaojian</reporter><labels /><created>2016-07-27T08:26:02Z</created><updated>2016-07-27T09:02:35Z</updated><resolved>2016-07-27T09:02:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-27T09:02:35Z" id="235528858">Please ask on discuss.elastic.co.
Also describe exactly what your are doing so we can better help.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes debug logging on index creation waiting for shards to be started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19612</link><project id="" key="" /><description>Before, the debug logging on timing out was logged whether the shards were started or not.  This fixes it to only log the warning when the shards starting was not acknowledged.
</description><key id="167731765">19612</key><summary>Fixes debug logging on index creation waiting for shards to be started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label></labels><created>2016-07-26T22:58:00Z</created><updated>2016-07-27T18:04:14Z</updated><resolved>2016-07-26T23:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-26T23:05:23Z" id="235433007">LGTM
</comment><comment author="abeyad" created="2016-07-26T23:16:52Z" id="235435117">@dakrone thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file></files><comments><comment>Fixes debug logging on index creation waiting for shards to be started (#19612)</comment></comments></commit></commits></item><item><title>Simplify src directory structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19611</link><project id="" key="" /><description>We currently follow the maven pattern of `src/main/java`, `src/test/java`, etc. Lucene, alternatively, removes one level of indirection, having `src/java` and `src/test`. I think it would be great to remove this extra directory level, as it makes working from a shell that much simpler (not everyone always works from an IDE).  Since it would be hard to maintain an open PR for this, I thought I would first open an issue to get agreement. If we agree, I would do it on a weekend to minimize merge conflicts for open PRs (although in my experience moves will merge ok, as long as too much wasn't change in a branch from the original file).

Note that from the build side, the change is trivial (only needs to be changed in BuildPlugin).

So the directory changes would be:
`src/main/java` -&gt; `src/java`
`src/main/resources` -&gt; `src/resources`
`src/test/java` -&gt; `src/test`
`src/test/resources` -&gt; `src/test-resources`
</description><key id="167731409">19611</key><summary>Simplify src directory structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>discuss</label></labels><created>2016-07-26T22:55:28Z</created><updated>2016-10-06T20:12:24Z</updated><resolved>2016-10-06T20:12:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-27T01:22:21Z" id="235454645">I really like using defaults. Gradle uses same default values as Maven as explained in https://docs.gradle.org/current/userguide/java_plugin.html

I would not modify it. So I'm -1 on this.
</comment><comment author="rjernst" created="2016-07-27T01:25:17Z" id="235455004">Why are using defaults more important than ease of use? This is an actual nuisance for those who navigate the directories from the command line.  We need to have less deep directories structures in general (read: less java packages), but this is an easy change that could help with navigating to every single class in the source.
</comment><comment author="rjernst" created="2016-07-27T01:26:20Z" id="235455157">And for those using IDEs, it will not change one thing for them, but for those using the shell, it helps.
</comment><comment author="nik9000" created="2016-07-27T01:26:25Z" id="235455174">I'm ambivalent. I don't think it'd save me any time and it is easier on
folks who aren't always in the code base if we follow the "standard"
directory lay out. But I don't see the slight change you are proposing to
be a big deal either way. It is close enough to the normal structure that
anyone used to maven will figure it out in seconds.

Where would you move src/test/resources? src/test-resources? I suspect I'd
get to both the same way, s&lt;tab&gt;t&lt;tab&gt;r.

On Jul 26, 2016 6:55 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; We currently follow the maven pattern of src/main/java, src/test/java,
&gt; etc. Lucene, alternatively, removes one level of indirection, having
&gt; src/java and src/test-java. I think it would be great to remove this
&gt; extra directory level, as it makes working from a shell that much simpler
&gt; (not everyone always works from an IDE). Since it would be hard to maintain
&gt; an open PR for this, I thought I would first open an issue to get
&gt; agreement. If we agree, I would do it on a weekend to minimize merge
&gt; conflicts for open PRs (although in my experience moves will merge ok, as
&gt; long as too much wasn't change in a branch from the original file).
&gt; 
&gt; Note that from the build side, the change is trivial (only needs to be
&gt; changed in BuildPlugin).
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19611, or mute the
&gt; thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLohsrgVCLLJSbn9oZw1_yBN9uVuGtks5qZpBlgaJpZM4JVrM3
&gt; .
</comment><comment author="rjernst" created="2016-07-27T01:27:41Z" id="235455361">&gt; Where would you move src/test/resources? src/test-resources

Yes, that is what lucene does.
</comment><comment author="nik9000" created="2016-07-27T01:30:45Z" id="235455782">I don't think it'd help me in the shell because I just tab complete through
all the directories. But it wouldn't hurt me either.

On Jul 26, 2016 9:26 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; And for those using IDEs, it will not change one thing for them, but for
&gt; those using the shell, it helps.
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19611#issuecomment-235455157,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLog7-x5j5qeJ770brCuBiiOzIt591ks5qZrPBgaJpZM4JVrM3
&gt; .
</comment><comment author="rmuir" created="2016-07-27T03:36:10Z" id="235475559">Yeah, i dont use your stupid shell, or your stupid IDE with its bogus tab completion.

Please, clean up the disorganized, overly complex filesystem hierarchy of thousands of undocumented needless packages and classes so the can be navigated.

This is literally :poop: code. Please, let's make it better.
</comment><comment author="rmuir" created="2016-07-27T03:47:15Z" id="235476861">It is easy to see the crappiness of the organization mathematically:
- 449 java packages
- 30 of which are documented.

Package depth in some cases (including src/main/java/....) gets to depths of 10 or 11.

The combination of these factors makes it nearly impossible to navigate this big pile of spaghetti. Lets remove needless levels of abstraction, please, that should be the number 1 priority for improving the elasticsearch code.
</comment><comment author="ywelsch" created="2016-07-27T08:39:16Z" id="235523005">&gt; Why are using defaults more important than ease of use?

I don't buy the argument that navigating to `src/test-java` instead of `src/test/java` is going to be easier so I'm -1 on this. It's also going to be a pain to cherry-pick changes to 2.x. Let's just stick with the defaults here.

&gt; We need to have less deep directories structures in general (read: less java packages)

Having less Java packages has nothing to do with this change so don't mix the two concerns.
</comment><comment author="rjernst" created="2016-07-27T08:45:47Z" id="235524558">&gt;  I don't buy the argument that navigating to src/test-java instead of src/test/java

I misspoke when I said lucene uses `src/test-java`, it just uses `src/test`. And saying that you "don't buy the argument" is like saying "I don't believe you are actually hurting". Yet it is a fact that we have developers who use the command line to navigate these files, and they are the ones who support this change. Yet those here who use IDEs for everything and do not even realize where or how deep these file hierarchies go are balking at a change that will literally not affect them? This is a ridiculous stance.

&gt; It's also going to be a pain to cherry-pick changes to 2.x

2.x is dead, or rather it will be very soon. I didn't say we need to do this tomorrow. But certainly doing it before 5.x will produce the least amount of pain.
</comment><comment author="clintongormley" created="2016-07-27T08:47:47Z" id="235525190">I'm ambivalent about the change, although I agree with @ywelsch's comments about difficulties in cherry picking.  One thing is for sure though, I'm not interested in these dramatic bullying comments and will -1 this change based on that alone.  

Calm discussion, or close the issue.
</comment><comment author="javanna" created="2016-07-27T09:14:46Z" id="235531646">Seems like the "too many java packages" problem is the one responsible for too much typing, which should not be confused with this issue. I agree that defaults don't have to be followed all the time, but I am not sure how much typing this change would help saving. Plus I don't see it as a debate between "everybody who uses the IDE" vs "everybody who uses the shell". Most of us use both IDE and shell and what I personally see being annoying on the shell is the number and the depth of java packages, which is a different problem.
</comment><comment author="rmuir" created="2016-07-27T09:47:13Z" id="235538995">Its not "bullying" to clearly point out the codebase is crap, when it is.

It is simply a critical statement: its important we accept that the codebase is a big disorganized mess!

@clintongormley I am sorry if you want to live in a fantasyland where people only make positive comments, but that is not going to happen. Say hello to your reality check.

The problem is, the packages and the layout go hand in hand, and both contribute to making abnormally long file names:
- `src/main/java` can be `src/java`
- `src/main/resources` can be `src/resources`
- `src/test/java` can be `src/test`
- `src/test/resources` can be `src/test-resources`

The packaging starts with the abnormally long `org.elasticsearch` which could easy be `org.elastic` or something better, but it doesnt end there. It sometimes goes to hierarchies 6 or 7 deep, which is totally unnecessary. More than 99% of these hierarchies are undocumented.

That is a disorganized mess, and it makes super long file names which cause problems with e.g. copy-paste, overflowing terminals, all kinds of stuff. Tab completion or fancy IDEs does not fix the problem.

So yea, I'm sorry to have to be the one to say, that this is crap, but its crap. I will repeatedly emphasize this because it is the reality. It is not bullying. It is perfectly acceptable to voice a negative opinion: I will make sure you that recognize this. I reject your world of groupthink where everyone says that the situation is positive. My negative criticism will be amplified to 10x now that you have tried to push back against it. 

Prepare yourself.
</comment><comment author="rjernst" created="2016-07-27T09:48:10Z" id="235539214">&gt; Seems like the "too many java packages" problem is the one responsible for too much typing

Every bit of typing matters. I pointed out the "too many java packages" as an aside to reinforce `this is an easy change that could help with navigating to every single class in the source`

&gt; but I am not sure how much typing this change would help saving

It saves 5 characters, for _every single class_. Note in Robert's comments before, he does not use tab completion. But even with tab completion, this saves on an extra 2 characters (`j`-`&lt;TAB&gt;` once  you are under `src/test`.

&gt; Plus I don't see it as a debate between "everybody who uses the IDE" vs "everybody who uses the shell". Most of us use both IDE and shell and what I personally see being annoying on the shell is the number and the depth of java packages, which is a different problem.

Sure, everybody uses the shell, but not for the same purpose.  And it's not a different problem: depth of directory structure _is_ the problem. Again, this proposal would help with _every single class_.

The cherry picking argument is valid. But we successfully moved the entire src for 2.x, and it did not halt all bugfixes back to 1.7.x. We also have very few patches going back to 2.x even now (and I would bet fewer once 5.0 is out).

I opened this discussion now because we are very close to branching for 5.0. A major version branching is really the only time to do large source moves (again, like we did in 2.0), so if we are going to make this change, it needs to be soon. I would ask that those of you who have -1'd to think if this is actually something that would hurt your development, or if it is worth blocking for those that do feel it would help.
</comment><comment author="clintongormley" created="2016-07-27T12:05:53Z" id="235565708">@rmuir I'm in favour of the change. I'm not in favour of the aggressive style. It's completely unwarranted.  This is a place for discussion, not for childish outbursts.
</comment><comment author="dadoonet" created="2016-07-27T12:30:38Z" id="235570604">I think this thread has split in two separate concerns:
- Move to a simpler directory tree
- Too many packages problem

So I'm going to try to add my comments on both but separated.

## Move to a simpler directory tree

I'm a strong believer in [convention over configuration approach](https://en.wikipedia.org/wiki/Convention_over_configuration).
My experience in the past is that a lot of developers have been educated using de-facto standard.

People now know that on maven if you want to build a project, you run `mvn install` and that's all.
People are starting to know the same commands for Gradle as well as it's more and more adopted.

I'm not saying that this organization of dirs is good or not. That's not my point. The point is that 90% of java devs are using today `src/main/java` and `src/test/java` in their projects.
I know some projects where they are using `ant` and they adopted the exact same organization as it will be easier then to migrate to Maven or Gradle.

Gradle also [adopted those defaults](https://docs.gradle.org/current/userguide/java_plugin.html).

That's why I'm -1 on this change until the convention changes.

## Too many packages

I do agree about this. I wrote more than a year ago in a private issue that we should split the core in modules and make every module more isolated, testable on its own... So it would drastically reduce the number of packages per module. Ideally one module should contain a very limited number of packages.

Although I agree on this, I don't think that we should fix the lack of modularity we have today with a "false fix". Changing the default source dirs will may be mask for a short time the problem but won't fix that.
</comment><comment author="mikemccand" created="2016-07-27T12:31:18Z" id="235570748">On Wed, Jul 27, 2016 at 8:06 AM, Clinton Gormley notifications@github.com
wrote:

&gt; @rmuir https://github.com/rmuir I'm in favour of the change. I'm not in
&gt; favour of the aggressive style. It's completely unwarranted. This is a
&gt; place for discussion, not for childish outbursts.

+1 to keep the discussion/arguments technical.

And big +1 for this change.

Mike McCandless
</comment><comment author="imotov" created="2016-07-27T17:51:43Z" id="235665483">&gt; My negative criticism will be amplified to 10x now that you have tried to push back against it.
&gt; 
&gt; Prepare yourself.

I think it's an important change. However, it might also have unforeseen negative side-effects, and because of it I would like to be able to discuss this change in a calm, respectful manner, without threats of retaliation for sharing one's opinion to make sure we didn't miss something. I think, it should be perfectly acceptable to voice a negative opinion and not been attacked for your choice of IDE in return.

I agree with @clintongormley, I simply cannot +1 any change that was discussed like this. I don't think any change should make it to elasticsearch after this sort of deliberation. So, I am -1 on it until we have a civilized discussion. 

This is literally💩 communication.  Please, let's make it better.
</comment><comment author="clintongormley" created="2016-07-28T16:26:43Z" id="235948506">To pick this up again: I am in favour of this change because I think it is the first step in making our code base easier to navigate.  Shorter is better.  I don't agree with the "convention is better" argument - I find it easier to find stuff in Lucene, which doesn't follow the conventions, than I do in Elasticsearch.  These changes make things more obvious, not less.

Yes, there is more that needs to happen: flattening out packages, reducing abstractions etc.  But those things can be done bit by bit.  This directory rename is a big change that affects the whole code base, and now is a good time to do it before we create the 5.0 branch.

I'm not worried about the cherry picking aspect - cherry picking should (theoretically anyway) work across renames.  Also, the master branch has diverged so far from 2.4 that we seldom cherry pick directly anyway.  The number of changes which go back to 2.4 is pretty limited these days.

So, there are advantages to making this change (perhaps they won't affect you directly) and the disadvantages seem pretty minor.  Anybody who has a strong reason why we shouldn't do this?
</comment><comment author="nik9000" created="2016-07-28T16:46:50Z" id="235954090">I'd be in favor of getting [cloture](https://en.wikipedia.org/wiki/Cloture) at this point and just having an up or down vote on the original issue. The arguments are fairly clear on both sides. Maybe we can give everyone until Monday night to vote? Any objections?
</comment><comment author="imotov" created="2016-07-28T17:33:49Z" id="235967177">So, just to summarize:

Pros:
- simplify typing of paths for developers who are using shell, so instead of something like `core/src/main/java/org/elasticsearch/search/aggregations/pipeline/` we could simply type  `core/src/java/org/elasticsearch/search/aggregations/pipeline/`

Cons:
- painful cherry picking (if 2.x lives for longer than expected). Might not be a big deal - we had to cherry pick only about 50 commits to 1.7 after 2.0.0 was released.
- rebasing or closing 87 PRs that are currently open against master. There is a chance that many of these PRs are dead, but it might be worth reviewing them before the switching the paths.
- non-standard code structure. 

Did I miss something?
</comment><comment author="mikemccand" created="2016-07-29T10:46:09Z" id="236150745">Just to explain my +1 a bit:

I love shorter paths.  Some of us type these paths many, many times
per day.  These paths also get folded into docs, URLs, READMEs,
whatnot.  Random devs clone ES and have to poke through them.

Less typing, easier to read, shorter URLs, etc.  Choosing longer paths
means you choose to add this unnecessary taxation to your life, much
like if your shower and dressing routing consumes 2 hours each
morning.

Shorter paths means less fat and conveys that we value being "lean",
that we value being "different" (non-standard) because it is leaner.
I think conveying that we choose to be lean and fast as a core part of
our design ethic sends an important message.

And "it's a standard" is a poor defense imo: the world would never
improve if all everyone ever did was follow the standards.  Remember
George Bernard Shaw's quote: "The reasonable person adapts themselves
to the conditions that surround them, while the unreasonable person
adapts surrounding conditions to them.  Therefore, all progress
depends on the unreasonable person."

I also dislike the "one person is being a 'bully' so now I veto this
issue" approach: that just amplifies/echo-chambers the effect of one
'bully' and results in a fragile/big-company culture going forward.
You should rather do the opposite (minimize the impact) by calling out
the 'bullying', and then disregarding it and getting back on point
(technical arguments) instead: that's what heals a community, makes it
naturally resilient to 'bullying'.

Painful cherry-picking is a one-time cost, vs an expected loooong
future benefits of less ongoing taxation.  Saying this matters is much
like arguing Lucene can't make a cool change since it would hurt
backwards compatibility, and that used to hold back a great many good
Lucene changes but guess who changed that broken culture in Lucene a
few years back :)
</comment><comment author="ywelsch" created="2016-07-29T11:48:31Z" id="236160723">@mikemccand Nobody argues here that breaking backwards compatibility is always bad or that we should strictly follow standards and not try to be lean. There is no need to get philosophical about this. I agree wholeheartedly that decisions should be made on technical arguments, not bullying / counter-bullying. **I just don't think that "4 less characters to type" vs "non-standard directory layout / painful cherry-picking" is the right tradeoff to do in this case**. I value the standard layout for its internal and external consistency. Regarding external consistency, the argument is simply that most Java projects use exactly this layout. Regarding internal consistency, look at

```
- src
  - java
  - resources
  - test
  - test-resources
```

vs

```
- src
  - main
    - java
    - resources
  - test
    - java
    - resources
```

The former feels inconsistent in the naming (why is `java` and `test` on same directory level? Does that mean that `test` is not only Java code)? The standard layout is clean and consistent.
</comment><comment author="rjernst" created="2016-07-29T15:08:59Z" id="236206504">&gt; I just don't think that "4 less characters to type" vs "non-standard directory layout / painful cherry-picking" is the right tradeoff to do in this case.

How many characters reduction makes breaking from "standards" worthwhile? 7? 10? Minimum 25?  The argument that 4 characters (which by the way, it is 5) less typing isn't meaningful is subjective. To some, it is meaningful. Why continue to cause pain for those developers so that a similar few will be "confused" about the layout for all of 4 seconds (subjective guess!) when navigating the source tree (which most people are going to do from the IDE anyways)?
</comment><comment author="nik9000" created="2016-07-29T15:21:32Z" id="236209838">To me the typing savings is super small. As is the cost in confusion for new developers.

The columns-on-the-screen savings is more compelling to me but not so compelling that I'd bother making the change personally. I just don't suffer much.

I don't think the cherry-pick cost is going to be super high because the files are just moved. If you have to use `patch` you'll need to jump around some extra, and, yeah, that is a bit of a pain but I think ok.

Honestly I voted to do it because it'll make a few folks very happy at fairly low cost. I think the cost is low for the reasons I typed above.
</comment><comment author="dakrone" created="2016-07-29T16:55:13Z" id="236233947">Tangentially, if we wanted to remove/save more typing, we could buy `es.co` as a domain and then replace all the instances of `org.elasticsearch`/`/org/elasticsearch` with `co.es` for package names :)
</comment><comment author="colings86" created="2016-07-29T17:26:28Z" id="236241420">What is the plan for eclipse with this change? At the moment we have `core/main` and `core/test` as separate projects to resolve the fact that Eclipse doesn't separate test and compile/runtime dependencies within a project (at least I think that was the reason). With all four folders being at the same level after this change whats the plan for recreating this separation in Eclipse?
</comment><comment author="rjernst" created="2016-07-29T18:07:27Z" id="236252211">@colings86 It's workable. We already call the gradle file `eclipse-build.gradle`. We can have `eclipse-java-build.gradle` and `eclipse-test-build.gradle` in src, point the root at the java root for each, and then change the src dir for java to `.` and for resources to `../whatever`.
</comment><comment author="colings86" created="2016-07-29T18:20:56Z" id="236255691">IF we decide to go ahead with this change can we please ensure that the above approach works in Eclipse before we merge the change (if we haven't already tested). Eclipse rarely handles relative paths that reference files out of the project well and its important that this change does not adversely affect Eclipse users as, while it is not used as much as IntelliJ in this community, it is still used by a number of this communities members. I am happy to help test that eclipse works.
</comment><comment author="rjernst" created="2016-07-29T18:21:55Z" id="236255959">&gt; can we please ensure that the above approach works in Eclipse before we merge the change

Of course. :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add zero-padding to auto-generated rollover index name increment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19610</link><project id="" key="" /><description>This commit adds a zero-padding length of 6 to the 
increment in the auto-generated rollover index name 
to support sorting generated indices by index name.

closes #19484
</description><key id="167699530">19610</key><summary>Add zero-padding to auto-generated rollover index name increment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T20:09:47Z</created><updated>2016-07-28T15:44:50Z</updated><resolved>2016-07-28T15:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-28T14:38:24Z" id="235914476">LGTM.
</comment><comment author="ywelsch" created="2016-07-28T15:01:41Z" id="235921888">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/RolloverIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverActionTests.java</file></files><comments><comment>Merge pull request #19610 from areek/enhancement/19484</comment></comments></commit></commits></item><item><title>Change foreach processor to use ingest metadata for array element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19609</link><project id="" key="" /><description>Before the `foreach` processor copied the ingest document and inserted the array element being processed under the `_value` key in the source. This caused two issues:
- Modifications made to other fields were silently ignored.
- If the array element contained a field with the name `_value` it would not be accessible.

This PR changes `foreach` processor to now copy the ingest document, but keep using the same ingest document and storing the array element temporarily in the ingest metadata (`_ingest._value` key) instead of the source itself. 

PR for #19592
</description><key id="167684617">19609</key><summary>Change foreach processor to use ingest metadata for array element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T19:00:29Z</created><updated>2016-07-27T07:37:40Z</updated><resolved>2016-07-27T07:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-07-26T19:15:20Z" id="235374513">small doc comment.

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>clarify awkward text</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19608</link><project id="" key="" /><description>clarify awkward text
</description><key id="167673681">19608</key><summary>clarify awkward text</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingrhoton</reporter><labels><label>docs</label><label>v2.3.5</label></labels><created>2016-07-26T18:09:43Z</created><updated>2016-07-27T18:03:32Z</updated><resolved>2016-07-27T18:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-26T18:09:45Z" id="235355130">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-26T18:24:43Z" id="235359776">Looks good. I'll merge in a bit when I have the time to forward port as appropriate.
</comment><comment author="clintongormley" created="2016-07-27T18:03:32Z" id="235669007">thanks @kingrhoton - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>clarify awkward text (#19608)</comment></comments></commit></commits></item><item><title>Deprecate template query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19607</link><project id="" key="" /><description>Both the search template api and the `template` query support templating the search request. However the search template api is able to template the entire search request while the `template` query can template only part of the query. Having both options around doesn't make much sense and since the search template api is more powerful it makes sense to deprecate the `template` query.

PR for #19390
</description><key id="167673332">19607</key><summary>Deprecate template query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Templates</label><label>deprecation</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T18:08:00Z</created><updated>2016-07-29T11:14:33Z</updated><resolved>2016-07-27T07:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-26T18:22:22Z" id="235359040">Can you update the PR description? My understanding is that the PR description is what people usually end up seeing when they watch changes so it helps to restate the "why" there too.
</comment><comment author="nik9000" created="2016-07-26T19:15:01Z" id="235374422">LGTM otherwise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>possessive, not contraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19606</link><project id="" key="" /><description>possessive, not contraction
</description><key id="167667940">19606</key><summary>possessive, not contraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingrhoton</reporter><labels /><created>2016-07-26T17:42:04Z</created><updated>2016-07-26T17:43:15Z</updated><resolved>2016-07-26T17:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-26T17:42:06Z" id="235346643">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-26T17:43:14Z" id="235346981">Duplicate of 19604
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>possessive, not contraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19605</link><project id="" key="" /><description /><key id="167666195">19605</key><summary>possessive, not contraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingrhoton</reporter><labels /><created>2016-07-26T17:33:32Z</created><updated>2016-07-26T17:36:32Z</updated><resolved>2016-07-26T17:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-26T17:33:33Z" id="235344075">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-26T17:36:32Z" id="235344986">Duplicate of #19604
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>possessive, not contraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19604</link><project id="" key="" /><description /><key id="167665125">19604</key><summary>possessive, not contraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingrhoton</reporter><labels><label>docs</label><label>v2.3.5</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T17:28:47Z</created><updated>2016-07-26T18:02:05Z</updated><resolved>2016-07-26T17:59:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-26T17:32:07Z" id="235343680">Looks right. I can't merge it until the CLA checker turns green. If you've already signed the CLA then it should fix itself when I leave this comment. If that doesn't work then there is an email mismatch somewhere.
</comment><comment author="nik9000" created="2016-07-26T17:58:41Z" id="235351707">CLA check passed. Awesome. I'll merge this and try and port it to other branches.
</comment><comment author="nik9000" created="2016-07-26T18:01:53Z" id="235352754">2.4: 4ea10bcf86c61c1b6b7fed5a18218efd1ca54ca2
master: 643ccb8cc189186db993108c51da6b49dc50540a
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add REST _ingest/pipeline to get all pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19603</link><project id="" key="" /><description>This adds an extra REST handler for "_ingest/pipeline" so that users do not need to supply "_ingest/pipeline/*" to get all of them.
- Also adds a teardown section to related REST-tests for ingest.

Relates to #19585
</description><key id="167662775">19603</key><summary>Add REST _ingest/pipeline to get all pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T17:17:35Z</created><updated>2016-07-29T11:14:09Z</updated><resolved>2016-07-26T17:48:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-07-26T17:17:42Z" id="235339328">/cc @martijnvg 
</comment><comment author="martijnvg" created="2016-07-26T17:38:03Z" id="235345422">Thanks for fixing! LGTM
</comment><comment author="javanna" created="2016-07-26T17:45:06Z" id="235347548">Are those teardown sections needed? I don't get how the tests could previously run if they are.
</comment><comment author="pickypg" created="2016-07-26T17:50:00Z" id="235349091">@javanna They are, but only for the test that I added. All of the tests overrode `my_pipeline` before using it, but not all of them removed it (or others for that matter).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mark Foreach Processor as experimental</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19602</link><project id="" key="" /><description>From an offline discussion with @martijnvg and @BigFunger...

The foreach processor breaks many of the conventions of the other processors in Ingest.
It allows for a nested processor to execute in a separate context, and this separate context has been proven to be leaky.

Issues like #19601 and #19592 poke holes in the current interface we have designed. For these reasons, I think that the processor should be marked _experimental_ for the upcoming 5.0 release. I do still think the processor adds a ton of value. We do not have many other tools for working with arrays in our documents, and this is a great tool for that.
</description><key id="167644507">19602</key><summary>Mark Foreach Processor as experimental</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-07-26T15:56:11Z</created><updated>2016-09-30T10:25:11Z</updated><resolved>2016-09-30T10:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-07-26T15:59:30Z" id="235315430">@clintongormley your thoughts on this issue would be great!
</comment><comment author="jpountz" created="2016-07-27T15:36:21Z" id="235625112">+1 to add the `experimental` annotation in case of any doubts about the stability of the API
</comment><comment author="javanna" created="2016-07-27T15:37:58Z" id="235625659">but isn't ingest as a feature already experimental per se? do we want to make foreach super experimental? :)
</comment><comment author="clintongormley" created="2016-07-27T18:52:57Z" id="235683531">i'm good with calling out foreach as experimental.  wondering if we should change the foreach model to avoid the issues (which i don't fully understand).  perhaps tomorrow we can chat on zoom?
</comment><comment author="martijnvg" created="2016-09-30T09:30:02Z" id="250701124">I'll mark the foreach processor as experimental.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file></files><comments><comment>docs: marked `foreach` processor as experimental</comment></comments></commit></commits></item><item><title>Sub Processors within processors like Foreach are not exposed in _simulate?verbose</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19601</link><project id="" key="" /><description>from an offline discussion with @BigFunger around tracing nested processor behavior within a `foreach` processor.

**story of what works:**
The purpose of the `verbose` flag in the simulate API is to expose the behavior of each processor that is executed in the pipeline. This is working correctly for top-level processors as well as nested processors in the `on_failure` block.

**story of what breaks:**
The foreach processor is currently the only processor that has nested processors executed outside of the `on_failure` block. These processors are not tracked in verbose simulate responses and make it difficult to debug and trace the evolution of the document.

**what should be done:**
The Processor API should be updated to make it possible for processors to declare any nested processors that are executed so that the Simulate API can use such an API to keep track of all executed processors and return their results back to the user. Currently, this only affects the Foreach Processor, but further processors wrapping other processors are expected (e.g. a switch-statement-like processor).
</description><key id="167642539">19601</key><summary>Sub Processors within processors like Foreach are not exposed in _simulate?verbose</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>discuss</label></labels><created>2016-07-26T15:50:48Z</created><updated>2016-12-22T16:44:44Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>All of the indices are lost all of a sudden</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19600</link><project id="" key="" /><description>I had elasticsearch service under amazon AWS which had data processed from Amazon AWS S3 and Lambda. And Kibana was used to visualize the incoming streaming data. All of a sudden the indices disappeared and whole data was lost. 

i had about 700 above JSON documents under one index and there were a few more sample indices.

but now when i list all the indices, it returns

```
health status index        pri rep docs.count docs.deleted store.size pri.store.size 
yellow open   esroumaanapp   5   1          6            0     52.5kb         52.5kb 
yellow open   .kibana-4      1   1          1            0      2.4kb          2.4kb 
```

The data is now streaming again, but all the previous data is lost.
Is there a way to recover them?

elasticsearch status

```
{
  "status" : 200,
  "name" : "Golden Girl",
  "cluster_name" : "178367654016:manudomain",
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "20085dbc168df96c59c4be65f2999990762dfc6f",
    "build_timestamp" : "2016-04-20T15:51:59Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="167601775">19600</key><summary>All of the indices are lost all of a sudden</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhishekkunnath</reporter><labels /><created>2016-07-26T13:03:50Z</created><updated>2016-07-26T13:20:19Z</updated><resolved>2016-07-26T13:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-26T13:20:19Z" id="235264766">Please ask for questions on discuss.elastic.co. And describe all the steps and configuration you have (like number of nodes., settings, hardware...)

We can help you there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify Sniffer initialization and automatically create the default HostsSniffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19599</link><project id="" key="" /><description>This PR originated from a suggestion in our forum: https://discuss.elastic.co/t/elastic-5-alpha-why-do-sniffer-and-hostssniffer-have-their-own-restclient/54998 .

The commit takes Sniffer.Builder out to its own top level class. Removes HostsSniffer.Builder and lets SnifferBuilder create the default HostsSniffer. This simplifies the Sniffer initialization as the HostsSniffer is not mandatory anymore. It can still be specified though in case the configuration needs to be changed or a different impl has to be used.
</description><key id="167595609">19599</key><summary>Simplify Sniffer initialization and automatically create the default HostsSniffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T12:31:41Z</created><updated>2016-07-26T15:28:37Z</updated><resolved>2016-07-26T15:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-26T12:47:03Z" id="235256452">@nik9000 can you have a look?
</comment><comment author="nik9000" created="2016-07-26T13:15:18Z" id="235263512">Left some small stuff but LGTM. I think this'll make it less confusing to use.
</comment><comment author="javanna" created="2016-07-26T13:29:04Z" id="235267186">It should be better now, do you want to give it a final look?
</comment><comment author="nik9000" created="2016-07-26T13:40:52Z" id="235270587">I left a comment about the javadoc on a constructor. Otherwise LGTM.
</comment><comment author="nik9000" created="2016-07-26T14:17:46Z" id="235281545">I like that docs changes. Great!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/ElasticsearchHostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/Sniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SnifferBuilder.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/ElasticsearchHostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferBuilderTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/MockHostsSniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SnifferBuilderTests.java</file></files><comments><comment>Simplify Sniffer initialization and automatically create the default HostsSniffer (#19599)</comment></comments></commit></commits></item><item><title>Inefficient FVhighlighting when set many HighlightedField.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19598</link><project id="" key="" /><description>First it may related with Lucene.  but I heard ES guys contribute lucene so.

when each searchDoc's highlightPhase, It calls highlighter's(In this case, FVH) highlight(highlighterContext).
In FastVectorHighlighter.java, loop for each requested 'HighlightedField' and getBestFragments.
getBestFragments method receive parameter[2] hitContext.docId() that uses for getting termVector of doc.
It finally reach org.apache.lucene.search.vectorhighlight.FieldTermStack.java and get termVector of doc.

The problem is 
'every highlightedField's getBestFragments method' call
`
final Fields vectors = reader.getTermVectors(docId);
`  
and it seems Inefficiently slow when search highlight result include (big document &amp;&amp; many highlightedField). read whole doc's termvector with every highlightedField.

my testing machine:
quad 1.87 ghz,
8Gb memory,
spinning disk.
ES-1.5.2 (relevent code not changed, when I saw)

Example,
my query :
`
{
 "from" : 0,
  "size" : 20,

  "query" : {
 "query about highlighted field and more"},
  "explain" : false,
  "fields" : [ "highlight", "fileRevision", "ownerNameUnigram", "ownerName", "ownerId", "timeLastModified", "size" ],
  "sort" : [ {
    "_score" : { }
  } ],
  "highlight" : {
    "pre_tags" : [ "&lt;strong&gt;" ],
    "post_tags" : [ "&lt;/strong&gt;" ],
    "order" : "score",
    "fragment_size" : 128,
    "number_of_fragments" : 10,
    "require_field_match" : true,
    "type" : "fvh",
    "fields" : [ {
      "ownerName" : { }
    }, {
      "fileName_ko" : { }
    }, {
      "fileName_en" : { }
    }, {
      "fileName_id" : { }
    }, {
      "fileName_es" : { }
    }, {
      "fileName_zh" : { }
    }, {
      "fileName_ja" : { }
    }, {
      "fileName_it" : { }
    }, {
      "fileName_ru" : { }
    }, {
      "fileName_pt" : { }
    }, {
      "fileName_hi" : { }
    }, {
      "fileName_etc" : { }
    }, {
      "contents_ko" : { }
    }, {
      "contents_ko.ngram" : { }
    }, {
      "contents_en" : { }
    }, {
      "contents_en.ngram" : { }
    }, {
      "contents_id" : { }
    }, {
      "contents_id.ngram" : { }
    }, {
      "contents_es" : { }
    }, {
      "contents_es.ngram" : { }
    }, {
      "contents_zh" : { }
    }, {
      "contents_zh.ngram" : { }
    }, {
      "contents_ja" : { }
    }, {
      "contents_ja.ngram" : { }
    }, {
      "contents_it" : { }
    }, {
      "contents_it.ngram" : { }
    }, {
      "contents_ru" : { }
    }, {
      "contents_ru.ngram" : { }
    }, {
      "contents_pt" : { }
    }, {
      "contents_pt.ngram" : { }
    }, {
      "contents_hi" : { }
    }, {
      "contents_hi.ngram" : { }
    }, {
      "contents_etc" : { }
    }, {
      "contents_etc.ngram" : { }
    } ]
  }
}
`

Test
[tookTime in millis, getBestFragments]
my doc 12538's every field getBestFragments took about 20ms. 
and total highlight phase tooks 705 ms.

I have a sparse mapping field. that means
 'doc 12538' field fileName_*
[fileName_id , fileName_es, fileName_zh, fileName_ja, fileName_it, fileName_ru, fileName_pt, fileName_hi, fileName_etc]
only one field is filled with data among this array.
It's same to contents_\* field.

dangerous doc 12538 - 
[CONVOCADOS_TALLER_SALUDMENTAL_JULIO2014.xlsx.txt](https://github.com/elastic/elasticsearch/files/383450/CONVOCADOS_TALLER_SALUDMENTAL_JULIO2014.xlsx.txt)

[2016-07-26 16:57:04,043][INFO ][root                     ] [4][FastVectorHighlighter.highlight] [22], filedName : fileName_id, docId : 12538
[23], filedName : fileName_id, docId : 12538
[20], filedName : fileName_es, docId : 12538
[21], filedName : fileName_zh, docId : 12538
[21], filedName : fileName_ja, docId : 12538
[28], filedName : fileName_it, docId : 12538
[26], filedName : fileName_ru, docId : 12538
[24], filedName : fileName_pt, docId : 12538
[22], filedName : fileName_hi, docId : 12538
[22], filedName : fileName_etc, docId : 12538
[22], filedName : contents_ko, docId : 12538
[20], filedName : contents_ko.ngram, docId : 12538
[19], filedName : contents_en, docId : 12538
[19], filedName : contents_en.ngram, docId : 12538
[20], filedName : contents_id, docId : 12538
[20], filedName : contents_id.ngram, docId : 12538
[19], filedName : contents_es, docId : 12538
[18], filedName : contents_es.ngram, docId : 12538
[19], filedName : contents_zh, docId : 12538
[19], filedName : contents_zh.ngram, docId : 12538
[19], filedName : contents_ja, docId : 12538
[19], filedName : contents_ja.ngram, docId : 12538
[18], filedName : contents_it, docId : 12538
[18], filedName : contents_it.ngram, docId : 12538
[18], filedName : contents_ru, docId : 12538
[18], filedName : contents_ru.ngram, docId : 12538
[20], filedName : contents_pt.ngram, docId : 12538
[18], filedName : contents_hi, docId : 12538
[18], filedName : contents_hi.ngram, docId : 12538
[18], filedName : contents_etc, docId : 12538
[19], filedName : contents_etc.ngram, docId : 12538
[2016-07-26 16:57:04,654][INFO ][root                     ] highlight tooks : 705, docId : 12538

and...
reader.getTermVectors(docId) tooks.
I didn't log sync with getBestFragments took. but i can see rough sequence and how it tooks.
I think heavy analyzed doc (have big termvectors) impact my query. (around 20ms sequence.)

long tTime = System.currentTimeMillis();
final Fields vectors = reader.getTermVectors(docId);
termVectorTimeLogging("tVectorTime : "+(System.currentTimeMillis() - tTime));

tVectorTime : 1
tVectorTime : 1
tVectorTime : 24
tVectorTime : 24
tVectorTime : 23
tVectorTime : 21
tVectorTime : 20
tVectorTime : 19
tVectorTime : 19
tVectorTime : 48
tVectorTime : 19
tVectorTime : 18
tVectorTime : 18
tVectorTime : 18
tVectorTime : 18
tVectorTime : 18
tVectorTime : 18
tVectorTime : 18
tVectorTime : 19
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 20
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 1
tVectorTime : 0
tVectorTime : 1
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1
tVectorTime : 0
tVectorTime : 0
tVectorTime : 0
tVectorTime : 1

it may read once and pass through parameter I think.
</description><key id="167556134">19598</key><summary>Inefficient FVhighlighting when set many HighlightedField.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dohykim</reporter><labels /><created>2016-07-26T09:03:50Z</created><updated>2016-07-26T13:38:15Z</updated><resolved>2016-07-26T13:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dohykim" created="2016-07-26T09:09:12Z" id="235206924">I don't know why Insert code doesn't work... 
</comment><comment author="nik9000" created="2016-07-26T13:38:15Z" id="235269816">This is really much more appropriate to file with Lucene I think.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>access doc_count in scripted_metric section.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19597</link><project id="" key="" /><description>Could you please help me on accessing the doc_count or the value_count aggregation result in the scripted metric section?
</description><key id="167551754">19597</key><summary>access doc_count in scripted_metric section.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Shashi-GS</reporter><labels><label>feedback_needed</label></labels><created>2016-07-26T08:44:11Z</created><updated>2016-07-26T09:13:38Z</updated><resolved>2016-07-26T09:13:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-26T08:56:28Z" id="235203647">In future please ask question on the user forum https://discuss.elastic.co. This issues list is reserved for bug reports and feature requests, there is much more help on using Elasticsearch available on the forums.

Could you please give an example of the request you have tried so far to get this working to give some context around your question?
</comment><comment author="Shashi-GS" created="2016-07-26T09:12:03Z" id="235207841">sure. Thank you for the response.   Please find the issue link for your reference.
https://discuss.elastic.co/t/access-doc-count-in-scripted-metric-section/56371

You can close this issue from here. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fs stats can not get the real filesystem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19596</link><project id="" key="" /><description>**Elasticsearch version**:  1.7.1

**JVM version**: 

``` shell
java version "1.8.0_91"
```

**OS version**:

``` shell
cat /etc/SuSE-release 
SUSE Linux Enterprise Server 11 (x86_64)
VERSION = 11
PATCHLEVEL = 3
```

**Description of the problem including expected versus actual behavior**:

``` shell
curl  -XGET 'http://ip:port/_nodes/stats?pretty=true'
...
"data" : [ {
          "path" : "/platform/vsearch/vsindex/data/vs-cluster-vsearch/nodes/0",
          "mount" : "/",
          "dev" : "/dev/sda2",
          "type" : "ext3",
          "total_in_bytes" : 40154628096,
           ...
        } ]
      }
...
```

but in my system:

``` shell
df /platform/vsearch/vsindex/data/vs-cluster-vsearch/nodes/0
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/sdb         1032088 34108    945552   4% /vsindex

```

other info in yml:

``` shell
# Path to directory where to store index data allocated for this node.
#
path.data: ${HOME}/vsindex/data

```

vsindex if a soft link : `vsindex -&gt; /vsindex/`

``` shell
df
Filesystem     1K-blocks     Used Available Use% Mounted on
/dev/sda2       39213504 37565460    612240  99% /
udev             4031520       92   4031428   1% /dev
tmpfs            4031520       72   4031448   1% /dev/shm
/dev/sdb         1032088    34108    945552   4% /vsindex

```
</description><key id="167551685">19596</key><summary>fs stats can not get the real filesystem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fangnoo</reporter><labels><label>:Stats</label><label>feedback_needed</label></labels><created>2016-07-26T08:43:49Z</created><updated>2016-07-28T11:07:59Z</updated><resolved>2016-07-28T11:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T16:50:33Z" id="235647832">Hi @fangnoo 

How we obtain stats changed completely in 2.0.  Could you download Elasticsearch 2.3.4 and see whether it reports your file system stats correctly please?
</comment><comment author="fangnoo" created="2016-07-28T01:27:44Z" id="235771357">Elasticsearch 2.3.4 is normal.
</comment><comment author="clintongormley" created="2016-07-28T11:07:59Z" id="235865624">thanks @fangnoo - we're not doing any more 1.x releases, so i'm going to close this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow empty json object in request body in `_count` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19595</link><project id="" key="" /><description>When the request body is missing, all documents in the target index are counted.
As mentioned in #19422, the same should happen when the request body is an empty
json object. This is also the behaviour for the `_search` endpoint and the two
APIs should behave in the same way.

Closes #19422 
</description><key id="167546177">19595</key><summary>Allow empty json object in request body in `_count` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T08:12:42Z</created><updated>2016-07-29T11:14:18Z</updated><resolved>2016-07-26T16:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-26T08:19:27Z" id="235194822">LGTM would it be possible to test this through a unit test though? 
</comment><comment author="cbuescher" created="2016-07-26T09:06:35Z" id="235206144">@javanna Thanks, will add a unit test that directly tests `parseTopLevelQueryBuilder()`, but I'd like to keep the added rest tests though because I think its good to have a test that goes through the rest layer. Let me know if thats okay for you.
</comment><comment author="javanna" created="2016-07-26T09:12:47Z" id="235208052">sure that's fine, the more the merrier...
</comment><comment author="cbuescher" created="2016-07-26T11:20:01Z" id="235239800">@javanna I added tests for QueryParseContext, including some minor changes in exception types and messages and `parseInnerQuery()` which don't necessarily need to be part of this PR, but I think make sense.
</comment><comment author="javanna" created="2016-07-26T11:46:56Z" id="235244648">nice, thanks for adding those tests, I left one question, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryParseContextTests.java</file></files><comments><comment>Merge pull request #19595 from cbuescher/fix-19422</comment></comments></commit></commits></item><item><title>Update aws sdk to 1.11.20</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19594</link><project id="" key="" /><description>WIP: DO NOT MERGE IT (tests are failing see description)
# AWS Release Notes

From 1.10.69 (see #17784), here are the most important updates:
## Minor 1.10 releases:
- Amazon S3 Added support for a new configuration named BucketAccelerateConfiguration which supports faster uploads/downloads to S3 buckets.
- Adding several missing throttling error codes for API Gateway and S3.
- Amazon S3 Introducing a new version of the ListObjects (ListObjectsV2) API that allows listing objects with a large number of delete markers.
## 1.11:

AWS SDK for Java:
- Improved URL encoding for REST clients.
- Dropped usage of Json.org library in favor of Jackson.
- Updated retry policies to include jitter during backoffs.
- Generate output POJOs for all operations.
- Renamed the aws-java-sdk-flow-build-tools-{sdkversion}.jar to aws-swf-build-tools-1.0.jar. The jar is also available in Maven.

AWS SDK for Java - Amazon S3:
- Added support to return the part count of an object in object metadata. You can also download a part by setting part number in GetObjectRequest.
- TransferManager supports parallel downloads for multipart objects.
- Default to Signature Version 4 signing process in all regions.

Apache HttpClient upgraded to 4.5.2
# Jackson update

Note that [Jackson has been updated at some point](https://github.com/aws/aws-sdk-java/blob/master/pom.xml#L115) but as we don't use dependency management anymore it has never been updated and we did not notice.

So I updated:
- jackson-databind to 2.6.6.
- jackson-annotations to 2.6.0.

That being said, I'm unsure if we should better use `${versions.jackson}` instead.
# Functional changes

For `repository-s3` plugin, this new version of the AWS SDK now sets Throttle Retries (overridable by `use_throttle_retries`) to `true` by default instead of `false` previously.
# Security manager issues

Tests are failing on `repository-s3` plugin with:

```
[2016-07-26 09:36:40,350][WARN ][repositories             ] [AUqzQAb] failed to create repository [s3][test_repo_s3_1]
java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessDeclaredMembers")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.Class.checkMemberAccess(Class.java:2348)
    at java.lang.Class.getDeclaredConstructors(Class.java:2019)
    at com.fasterxml.jackson.databind.introspect.AnnotatedClass.resolveCreators(AnnotatedClass.java:338)
    at com.fasterxml.jackson.databind.introspect.AnnotatedClass.getStaticMethods(AnnotatedClass.java:245)
    at com.fasterxml.jackson.databind.introspect.BasicBeanDescription.getFactoryMethods(BasicBeanDescription.java:461)
    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._addDeserializerFactoryMethods(BasicDeserializerFactory.java:670)
    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._constructDefaultValueInstantiator(BasicDeserializerFactory.java:321)
    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.findValueInstantiator(BasicDeserializerFactory.java:254)
    at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.createCollectionDeserializer(BasicDeserializerFactory.java:1027)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:394)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:352)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)
    at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)
    at com.fasterxml.jackson.databind.DeserializationContext.findContextualValueDeserializer(DeserializationContext.java:428)
    at com.fasterxml.jackson.databind.deser.std.StdDeserializer.findDeserializer(StdDeserializer.java:947)
    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.resolve(BeanDeserializerBase.java:439)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:296)
    at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)
    at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)
    at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:461)
    at com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:3838)
    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3732)
    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2796)
    at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:92)
    at com.amazonaws.partitions.PartitionsLoader.build(PartitionsLoader.java:84)
    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)
    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:66)
    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:54)
    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)
    at com.amazonaws.services.s3.AmazonS3Client.createSigner(AmazonS3Client.java:3288)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3984)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1218)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1175)
    at org.elasticsearch.cloud.aws.blobstore.S3BlobStore.&lt;init&gt;(S3BlobStore.java:88)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:309)
    at org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin.lambda$getRepositories$6(S3RepositoryPlugin.java:73)
    at org.elasticsearch.repositories.RepositoriesService.createRepository(RepositoriesService.java:381)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:354)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:54)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:107)
    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:553)
    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:857)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:450)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Related to #18910.
</description><key id="167542565">19594</key><summary>Update aws sdk to 1.11.20</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>stalled</label><label>upgrade</label></labels><created>2016-07-26T07:51:40Z</created><updated>2016-09-16T15:00:47Z</updated><resolved>2016-09-16T15:00:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-26T08:41:08Z" id="235199622">After talking with @tlrx, we think that we should not update the AWS SDK unless we have a major issue (which is not the case here) until we have real integration tests running.

Let's mark this one as `stalled` for now.
</comment><comment author="dakrone" created="2016-09-14T22:57:11Z" id="247182224">@dadoonet is this still worth keeping open with the other changes we've made in the meantime? (we've already upgrading Jackson separately) If we are not going to upgrade until we have a major issue, maybe we can close this?
</comment><comment author="dadoonet" created="2016-09-16T15:00:28Z" id="247623352">Works for me. I'll just keep my branch somewhere as there have been some efforts already.
I know that my branch is not working correctly though because of the Security manager issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to add header to bulkRequest using BulkProcessor?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19593</link><project id="" key="" /><description>If using BulkRequestBuilder, I can use method `putHeader` to add header, but if I use BulkProcessor, I must `putHeader` to each IndexRequestBuilder. 

Meybe a method of BulkProcessor is necessary, so I could  put header to the BulkRequest.
</description><key id="167541918">19593</key><summary>How to add header to bulkRequest using BulkProcessor?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasper-zhang</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-07-26T07:47:53Z</created><updated>2017-05-05T13:19:15Z</updated><resolved>2017-05-05T13:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-26T08:39:10Z" id="235199171">Please ask your questions on discuss.elastic.co.
You don't have to explicitly add a header. You just have to add IndexRequest or DeleteRequest as explained in the doc: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html
</comment><comment author="dadoonet" created="2016-07-26T08:45:25Z" id="235200618">I'm sorry. I probably misread your demand. Reopening.

Thanks @javanna for the heads up.
</comment><comment author="javanna" created="2017-05-05T13:19:15Z" id="299462371">putHeader is not exposed anymore in requests and request builders. You rather have to do the following which also works with `BulkProcessor` :

```
client.filterWithHeader(Collections.singletonMap("Authorization", token)).prepareSearch().get();
```</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add mutability of rest of document in Ingest-ForeachProcessor sub processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19592</link><project id="" key="" /><description>Spoke with @martijnvg and @BigFunger about some strange behavior around mutated state within sub-processors within ForEach. Realization was that we expose top-level document fields, but silently ignore mutations to them. This is a really confusing behavior.

Currently, the `foreach` processor makes it possible to read all other fields in the ingest document besides `_value`. It does not enable mutability of these fields. 

This test passes that attempts to append to a top-level field passes. The append does not actually do anything since it operates on a copy. This should be fixed so that the original documents reflects modifications outside of `_value`.

```
    public void testExecute() throws Exception {
        List&lt;Object&gt; values = new ArrayList&lt;&gt;();
        values.add("string");
        values.add(1);
        values.add(null);
        IngestDocument ingestDocument = new IngestDocument(
            "_index", "_type", "_id", null, null, null, null, Collections.singletonMap("values", values)
        );

        TemplateService ts = TestTemplateService.instance();

        ForEachProcessor processor = new ForEachProcessor(
            "_tag", "values", new CompoundProcessor(false,
            Collections.singletonList(new UppercaseProcessor("_tag_upper", "_value")),
            Collections.singletonList(new AppendProcessor("_tag", ts.compile("errors"), (model) -&gt; (Collections.singletonList("added"))))
        ));
        processor.execute(ingestDocument);

        List&lt;String&gt; result = ingestDocument.getFieldValue("values", List.class);
        assertThat(result.get(0), equalTo("STRING"));
        assertThat(result.get(1), equalTo(1));
        assertThat(result.get(2), equalTo(null));
    }
```
</description><key id="167526615">19592</key><summary>Add mutability of rest of document in Ingest-ForeachProcessor sub processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-07-26T05:54:33Z</created><updated>2016-07-27T07:37:40Z</updated><resolved>2016-07-27T07:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java</file><file>core/src/main/java/org/elasticsearch/ingest/CompoundProcessor.java</file><file>core/src/main/java/org/elasticsearch/ingest/IngestDocument.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/TrackingResultProcessorTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/CompoundProcessorTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestDocumentTests.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorTests.java</file></files><comments><comment>ingest: Change the `foreach` processor to use the `_ingest._value` ingest metadata attribute to store the current array element being processed.</comment></comments></commit></commits></item><item><title>CorruptedFileIT.testCorruptFileThenSnapshotAndRestore fails occasionally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19591</link><project id="" key="" /><description>`CorruptedFileIT.testCorruptFileThenSnapshotAndRestore` fails occasionally with the following assertion error:

```
   &gt; Throwable #1: java.lang.AssertionError: [test][0], node[kXBrV8JKTCG8ZrjOt_W0vg], [P], s[STARTED], a[id=Zz-bwJLZTkG3yl9D8jMQSw]
   &gt; Expected: &lt;1&gt;
   &gt;      but: was &lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([A0A4AF57F24AD88E:6D7015D6605692ED]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.index.store.CorruptedFileIT.listShardFiles(CorruptedFileIT.java:692)
   &gt;    at org.elasticsearch.index.store.CorruptedFileIT.testCorruptFileThenSnapshotAndRestore(CorruptedFileIT.java:510)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

This means that the node stats for the cluster node cannot be retrieved (or rather that there are no nodes in the corresponding node stats instance).

Latest build where this occurred: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+seq_no+periodic/1606/consoleFull
</description><key id="167525855">19591</key><summary>CorruptedFileIT.testCorruptFileThenSnapshotAndRestore fails occasionally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Snapshot/Restore</label><label>jenkins</label></labels><created>2016-07-26T05:46:48Z</created><updated>2017-06-16T18:36:11Z</updated><resolved>2017-06-16T18:36:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-07-26T19:31:28Z" id="235378905">This test failure seems to be caused by an exception in [FsService](https://github.com/elastic/elasticsearch/blob/8c40b2b54eac3e3ab3c41ece5c758be75173191b/core/src/main/java/org/elasticsearch/monitor/fs/FsService.java#L60) which is getting swallowed. I changed the log levels for this test to see which exception is actually getting logged. 
</comment><comment author="danielmitterdorfer" created="2016-07-27T05:41:03Z" id="235492209">Thanks for digging @imotov.
</comment><comment author="imotov" created="2016-08-03T17:35:42Z" id="237309417">It failed again and @pickypg and I dug a bit more into it and at the moment the only reasonable explanation that we could come up with is some problem in exception handling of stats operation. @pickypg is going to continue digging. 
</comment><comment author="javanna" created="2017-06-16T16:57:52Z" id="309078999">given that this issue had no activity in ~10 months, shall we close it @pickypg @imotov ? </comment><comment author="pickypg" created="2017-06-16T18:36:11Z" id="309102367">With https://github.com/elastic/elasticsearch/pull/25017 we should catch this if it happens again, so yeah.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java</file></files><comments><comment>Tests: add more logging to testCorruptFileThenSnapshotAndRestore</comment></comments></commit></commits></item><item><title>Expand threadpool types that can be listed by /_cat/thread_pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19590</link><project id="" key="" /><description>**Describe the feature**:

Currently `/_cat/thread_pool` seems to be limited to displaying details of a few specific thread pools.  While the default thread pools displayed are great, it would be nice if it could display details on all current thread pools in a cluster, like all listed in the `thread_pool` section of `/_nodes/stats` with the `h` or `v` parameter.  Currently, many of these thread pools cannot be display by this endpoint.
</description><key id="167522472">19590</key><summary>Expand threadpool types that can be listed by /_cat/thread_pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>:CAT API</label></labels><created>2016-07-26T05:13:09Z</created><updated>2016-08-04T03:02:13Z</updated><resolved>2016-08-04T03:02:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-26T07:32:31Z" id="235185330">@joshuar Which thread pools would you like to be added? Currently [these TP](https://github.com/elastic/elasticsearch/blob/0a293fad29311da1a7472e2faf6790fc34f26ae2/core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java#L58) are supported via the `h` parameter.
</comment><comment author="spinscale" created="2016-07-26T08:27:03Z" id="235196469">for starters,  `FETCH_SHARD_STARTED, FORCE_MERGE, FETCH_SHARD_STORE` threadpools are missing, but I'd be awesome if the custom threadpools could be included as well, like in the node stats, so you can just refer to them by their regular name
</comment><comment author="martijnvg" created="2016-07-26T09:01:36Z" id="235204919">+1 to add `FETCH_SHARD_STARTED`, `FORCE_MERGE` and `FETCH_SHARD_STORE`  TPs. 

The reason it is fixed now, is because of how currently for each TP is assigned to an abbreviated name that used in column header. This makes it tricky when it comes to custom TP, since we don't know the name upfront. Perhaps for custom TPs we could just use the full name? 
</comment><comment author="spinscale" created="2016-07-26T09:03:33Z" id="235205370">+1, thats what I meant with `just refer to them by the regular name`.. sorry for being unclear
</comment><comment author="joshuar" created="2016-07-26T22:30:10Z" id="235426125">Possibly `PERCOLATE` on top of @spinscale's list.

I'd say thread pools used by our plugins would be good candidates to be able to view more easily if possible to add here?   
</comment><comment author="martijnvg" created="2016-07-27T06:52:02Z" id="235501837">&gt; Possibly PERCOLATE on top of @spinscale's list.

The percolate TP has been removed since 5.0-alpha1. The percolator uses the search TP since it became a query.

&gt; I'd say thread pools used by our plugins would be good candidates to be able to view more easily if possible to add here?

Agreed. Hopefully this can be done by using their full name.
</comment><comment author="nik9000" created="2016-07-27T13:06:44Z" id="235578840">&gt; Hopefully this can be done by using their full name.

+1. I think we should allow listing all the thread pools.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow license header check to be customized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19589</link><project id="" key="" /><description>This change allows setting which license families are approved, as well
as adding matchers for additional license types.
</description><key id="167493751">19589</key><summary>Allow license header check to be customized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-26T00:09:31Z</created><updated>2016-07-26T00:40:42Z</updated><resolved>2016-07-26T00:40:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-26T00:40:18Z" id="235129246">nice: much simpler!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19589 from rjernst/license_header_generic</comment></comments></commit></commits></item><item><title>Rename client yaml test infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19588</link><project id="" key="" /><description>This makes it obvious that these tests are for running the client yaml
suites. Now that there are other ways of running tests using the REST
client against a running cluster we can't go on calling the shared
client yaml tests "REST tests". They are rest tests, but they aren't
**the** rest tests.
</description><key id="167485947">19588</key><summary>Rename client yaml test infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T23:04:10Z</created><updated>2016-07-26T18:02:35Z</updated><resolved>2016-07-26T17:54:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-26T07:27:33Z" id="235184332">left a few suggestions
</comment><comment author="nik9000" created="2016-07-26T14:25:58Z" id="235284120">@javanna I took your suggestions and pushed b2b1e95.
</comment><comment author="nik9000" created="2016-07-26T14:43:19Z" id="235289833">ci failed! Cool!
</comment><comment author="nik9000" created="2016-07-26T16:21:02Z" id="235322132">I pushed a fix fox the failures.
</comment><comment author="javanna" created="2016-07-26T16:24:04Z" id="235323057">left one tiny comment, LGTM otherwise
</comment><comment author="nik9000" created="2016-07-26T18:02:35Z" id="235352962">Thanks for all the reviewing @javanna ! I merged.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add hook for obtaining the final SearchResponse before sending to client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19587</link><project id="" key="" /><description>Adds a hook which allows plugins to inspect the final SearchResponse (and the SearchRequest that generated it) before it is sent back to the client.  This is distinct from per-shard hooks such as registering a SearchOperationListener, since it runs only on the coordinating node and gives access to the final, merged results.

It's a little more invasive than I had hoped, since I needed to pass the PluginService through the various Search Actions.  But the actual change is pretty minor (I think).
</description><key id="167451842">19587</key><summary>Add hook for obtaining the final SearchResponse before sending to client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Plugins</label><label>:Search</label></labels><created>2016-07-25T19:58:41Z</created><updated>2016-09-13T15:12:58Z</updated><resolved>2016-09-13T15:12:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-08-03T15:47:12Z" id="237278114">@nik9000 Ok, how about now?  Heading in the right direction now?

If this is more on the right track, I'll do some cleanup, comments, add tests, etc.  Related, the naming is awfully clunky (but descriptive at least), and I wasn't sure the best place to put the interface.
</comment><comment author="nik9000" created="2016-08-04T16:04:02Z" id="237600227">I think this is the right direction, yeah!
</comment><comment author="s1monw" created="2016-08-04T18:38:54Z" id="237644212">@polyfractal I left some suggestions to simplify this
</comment><comment author="s1monw" created="2016-08-04T18:56:33Z" id="237649342">@polyfractal maybe we do this very very simple. We a new extension point to `SearchPlugin#getSearchResponseListener()` returning `BiFunction&lt;SearchRequest, SearchResponse&gt;`. Then we pass this on to `TransportSearchAction` and wrap the action listener  if there is at least one response listener and that way we have it integrated in all places and keep the change minimal? WDYT?
</comment><comment author="polyfractal" created="2016-08-18T17:38:33Z" id="240798635">Third time's a charm!  Sorry for the delay, got caught up in some other work.  Refactored to:
- Remove the Guice injection of `SearchPhaseController` and instead instantiate manually in `SearchModule`
- Use a List of BiConsumers for the listeners
- Wrap the listener in `TransportSearchAction` instead of fiddling with each action individually

The de-guicing of `SearchPhaseController` meant the constructor of `SearchModule` had to change, which touched a lot of tests.  I did my best to mock out the new constructor args appropriately -- and the tests seem to pass -- but I admit to largely just stumbling my way through it. :/
</comment><comment author="s1monw" created="2016-08-18T19:08:09Z" id="240824885">i think this looks great. the only think I am missing is a test that actually calls one of those listeners. I think you have to add an Integration test that actually installs a plugin etc.
</comment><comment author="polyfractal" created="2016-09-13T15:12:58Z" id="246715107">Let this sit too long and much of it conflicts with the changes in https://github.com/elastic/elasticsearch/pull/20423

I'll revisit the hook at a future date when I have more time, just juggling too many balls in the air right now.  Sorry for the noise and needless reviews :/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSugar deleteOnExit doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19586</link><project id="" key="" /><description>The temporary directories do not deleted

&gt; sbt my-service/it:test
&gt; 
&gt; scala&gt; System.getProperty("java.io.tmpdir")
&gt; res2: String = /var/folders/y5/lw613j_56sq4n_bkz4l9kr0r0000gn/T/

I can manually delete testNodeHomePath in afterAll method, but it seems it should work with existing  deleteOnExitHook. Do you know why the hook doesn't work?

```
val indexName = getClass.getSimpleName.toLowerCase

  behavior of "index matchers"

  it should "support index document count" in {
    indexName should haveCount(4)
    indexName should not(haveCount(11))
  }
}
```

**Elasticsearch version**:
2.3

**JVM version**:
1.8

**OS version**:
OSX 10.9.5

**Description of the problem including expected versus actual behavior**:
</description><key id="167449143">19586</key><summary>ElasticSugar deleteOnExit doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ssemichev</reporter><labels /><created>2016-07-25T19:44:57Z</created><updated>2016-07-25T19:52:36Z</updated><resolved>2016-07-25T19:52:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-25T19:52:36Z" id="235064500">I think maybe this is an issue with https://github.com/sksamuel/elastic4s instead of Elasticsearch? Reopen if you disagree.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Ingest] GET should match _template behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19585</link><project id="" key="" /><description>While testing some ingest functionality, I ran into this:

``` http
GET /_ingest/pipeline/xyz
```

will always return

``` json
{
  "pipelines": []
}
```

even though `xyz` doesn't exist.
</description><key id="167449055">19585</key><summary>[Ingest] GET should match _template behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T19:44:33Z</created><updated>2016-08-11T12:42:45Z</updated><resolved>2016-07-29T15:59:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-25T19:46:57Z" id="235062956">This is a feature, not a bug :)

I think it is this way because you can get back multiple pipelines with a single request. then what do we do if one is there but another one doesn't exist? we return what we have and the status code is always 200. I don't love this behaviour either, but I think the solution to this would be then to only be able to retrieve one pipeline per request.
</comment><comment author="pickypg" created="2016-07-25T19:48:28Z" id="235063352">Taking `_template` as the example, the normal approach seems to be:

``` http
GET /_template
```

This will return _all_ templates.

``` http
GET /_template/xyz
```

This will return just the one you asked for. So maybe we just need two separate `GET` endpoints?
</comment><comment author="pickypg" created="2016-07-25T19:50:01Z" id="235063777">For what it's worth, `GET _template/xyz` returns `{}` if the template doesn't exist, so that would loosely match what is currently happening.
</comment><comment author="pickypg" created="2016-07-25T19:54:00Z" id="235064877">Also, another subtle difference is that `_template` returns an object whose keys are the template names. While this returns an array of pipelines that contain a field named "id" with the appropriate value.

I honestly prefer the array approach because it avoids any potentially invalid JSON field names, but it is a difference.
</comment><comment author="pickypg" created="2016-07-25T20:12:25Z" id="235069990">Alright, I spoke with @talevy offline about this as well. Although it looks a little different (object keys versus an array), we behave in the same way as

``` http
GET /_template/{id1},{id2}
GET /_ingest/pipeline/{id1},{id2}

GET /_template/prefix*
GET /_ingest/pipeline/prefix*

GET /_template/{unknown_id}
GET /_ingest/pipeline/{unknown_id}
```

(While the failures return an otherwise empty response, they do use HTTP 404)

However, we are lacking:
- [x] Fetch all #19603

``` http
GET /_template
GET /_ingest/pipeline
```

Note: You can get it by just specifying `*`:

``` http
GET /_ingest/pipeline/*
```
</comment><comment author="martijnvg" created="2016-07-26T08:05:11Z" id="235191890">The get pipeline api combines both a get and list in a single api. I'm not sure whether the get template api should change to be more like the get pipeline api or the other way around. Like you I personally like the array response format. 

If we choose to streamline the get template and get pipeline API, we should also consider other APIs that read from the cluster state, like get template and get script APIs.

Regarding missing pipelines, the get pipeline api only returns a 404 if no pipeline could be found. If some of the requested pipelines couldn't be found it isn't immediate visible in the response so maybe we could improve this by including an empty config:

``` json
{
  "id":"missing_id",
  "config": null
}
```
</comment><comment author="javanna" created="2016-07-26T08:16:16Z" id="235194177">I am starting to think that we should do what mget does for all these apis. If we want to return multiple items we cannot have one meaningful status code, so that should always be 200, then each requested item should have its own status in the response. I find this 404 behaviour confusing with the ingest api, that it returns 404 only when nothing is found.
</comment><comment author="pickypg" created="2016-07-26T15:01:41Z" id="235296020">&gt; I find this 404 behaviour confusing with the ingest api, that it returns 404 only when nothing is found.

The downside is that there's no quick "is it there" check without parsing the body since there's _only_ a bulk variant of this get API. `_mget` has the excuse that it's the bulk variant of get, which would return 404s, but `_mget` naturally loses it for the reason you described.
</comment><comment author="pickypg" created="2016-07-26T15:09:41Z" id="235298603">&gt; I'm not sure whether the get template api should change to be more like the get pipeline api or the other way around.

I think this should definitely change to be like Get Index Templates and allow it to return everything without specifying `*` because that's definitely not an obvious feature.
</comment><comment author="martijnvg" created="2016-07-26T15:23:50Z" id="235303268">&gt; I think this should definitely change to be like Get Index Templates and allow it to return everything without specifying \* because that's definitely not an obvious feature.

+1 that makes sense. I was referring to the response format.
</comment><comment author="clintongormley" created="2016-07-27T16:41:15Z" id="235645197">Hmmm returning pipelines in an array is different from what we do everywhere else in Elasticsearch.  Why are we returning this:

```
{
  "pipelines": [
    {
      "id": "my-pipeline-id",
      "config": {
        "description": "describe pipeline",
        "processors": [
          {
            "set": {
              "field": "foo",
              "value": "bar"
            }
          }
        ]
      }
    },
    {
      "id": "my-pipeline-id-2",
      "config": {
        "description": "describe pipeline",
        "processors": [
          {
            "set": {
              "field": "foo",
              "value": "bar"
            }
          }
        ]
      }
    }
  ]
}
```

instead of this:

```
{
  "pipelines": {
    "my-pipeline-id": {
      "description": "describe pipeline",
      "processors": [
        {
          "set": {
            "field": "foo",
            "value": "bar"
          }
        }
      ]
    },
    "my-pipeline-id-2": {
      "description": "describe pipeline",
      "processors": [
        {
          "set": {
            "field": "foo",
            "value": "bar"
          }
        }
      ]
    }
  }
}
```

I'd really prefer consistency here
</comment><comment author="pickypg" created="2016-07-27T17:17:07Z" id="235655390">At that point do we even need to wrap it with the `pipelines` object, which just served to be an array?

``` json
{
  "my-pipeline-id": {
    "description": "describe pipeline",
    "processors": [
      {
        "set": {
          "field": "foo",
          "value": "bar"
        }
      }
    ]
  },
  "my-pipeline-id-2": {
    "description": "describe pipeline",
    "processors": [
      {
        "set": {
          "field": "foo",
          "value": "bar"
        }
      }
    ]
  }
}
```
</comment><comment author="clintongormley" created="2016-07-27T18:59:40Z" id="235685600">&gt; At that point do we even need to wrap it with the pipelines object, which just served to be an array?

True - I was taking this from mappings/settings/etc, but templates don't have a top-level `templates` key and i think pipelines are most similar to those.
</comment><comment author="martijnvg" created="2016-07-28T08:07:33Z" id="235828001">Ok - then lets change to get pipeline API to return pipelines in the style templates API does.

(all thought personally I prefer array based notation as the ordering is more consistent and it is the right element to return multiple objects - but consistency is important too!)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java</file></files><comments><comment>ingest: Made the response format of the get pipeline api match with the response format of the index template api</comment></comments></commit></commits></item><item><title>Elasticsearch continues to attempt writes to an index that has shards on disks that are full</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19584</link><project id="" key="" /><description>Using
1. Elasticsearch: 2.3.1 
2. JVM version: 1.8.0_60-b27
3. OS version: Centos 7 

With 

```
.
.
.
node:
  data: true
  master: false
  name: esd02-data4
  tier: hot
os:
  available_processors: 8
path:
  data:
      - /storage/sdb1
      - /storage/sdx1
.
.
.
```

in `elasticsearch.yml` we have observed twice now where one of the disks (say, `/storage/sdx1`) will fill up to 100% while the other `/storage/sdb1` still has space. The error logs fill up with errors like:

```
[logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -&gt; /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];
Caused by: [logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -&gt; /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];
Caused by: java.nio.file.FileSystemException: /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -&gt; /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device
java.nio.file.FileSystemException: /storage/sdb1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st -&gt; /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st.tmp: No space left on device
```

It's pretty obvious what's happening... `storage/sdx1` owns a shard of `logstash-syslog-2016.07.21` and ES is refusing to "break" the shard across multiple disk paths....

but it's also failing to recover from this on its own, or to stop sending data to the shard whose disk is full.  I would expect either (A) in the event of a full disk, ES would nominate the other shards to accept 100% of the writes to the index or (B) ES would loosen its requirement that shards are not allowed to span multiple write paths. 
</description><key id="167417626">19584</key><summary>Elasticsearch continues to attempt writes to an index that has shards on disks that are full</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">evanv</reporter><labels><label>feedback_needed</label></labels><created>2016-07-25T17:11:39Z</created><updated>2016-07-26T16:57:33Z</updated><resolved>2016-07-26T15:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-25T20:30:15Z" id="235074891">answering in reverse order:

B) we went to a great pain to actually make it so, in order to limit the scope of such a failure (and disk loss) to a few shards on node rather then all of them.

A is our chosen strategy, assuming that by "other shards" you mean that the shard on the full disk will be failed and other other shards will carry the load. Note that all active shards copies are indexing all operations in ES and that each document is mapped to single set of shard copies (and this can not be changed).  

It may however take some time for the master to process the shard failed, remove it and publish to the node in question which in turn will clean it up. 

From you exception it seem you are talking about multiple attempts to assign a primary to that node and it seems you have no other valid copy for ES to use. Is that the case? if so, this is a duplicate of #19446
</comment><comment author="evanv" created="2016-07-25T21:01:30Z" id="235083912">Ah, interesting. So this is a staging cluster where we run without replicas. In retrospect I definitely should have mentioned that. My apologies.

If I follow you correctly, the failover mechanics you are describing require that a promotable replica exists... which I could see leading to some bizarre edge cases like a a node failure on node "Foo" taking down replica copies of indices "A" and "B". The cluster might rightfully attempt to reallocate replica copies of "A" and "B to nodes "Bar" and " Baz." Then if node Bar had the only primary copy of a shard of index A (the replica having been lost when node Foo died, and currently in the process of being streamed from node Bar to node Baz) and one of Bar's write paths filled up, the cluster might enter the sort of deadlock state I observed where the cluster was stuck for hours trying to write data to a disk location that doesn't have any space available on one of its write paths...

which is an edge case that might best be subtitled "If many bad things happen at the same time, Elastic might enter a state from which it can't recover on its own."

The behavior you are describing makes a lot of sense and the scenarios in which I can see that strategy being inadequate are esoteric. This may very well be such an edge case that it's not worth worrying about. 

As far as https://github.com/elastic/elasticsearch/issues/19446, that looks related to me but not a duplicate of this ticket. But I may be overlooking something.  
</comment><comment author="bleskes" created="2016-07-26T15:38:10Z" id="235308075">&gt; If many bad things happen at the same time, Elastic might enter a state from which it can't recover on its own

For what it's worth, if many bad things happen Elasticsearch may very well end up not being able to recover. It's about documenting those scenarios - for example - if you loose all copies of a shard, data will be lost.

&gt;  in which I can see that strategy being inadequate are esoteric.

What are they?

&gt; As far as #19446, that looks related to me but not a duplicate of this ticket. But I may be overlooking something.

As far as I can ES does the only thing it can do which is try to salvage the only copy of the primary it has with the hope that some space was freed (but a user/completed merge). The problem is it never gives up and waits for a human intervention which is what #19446  is about. 

I'm going to close this for now as a duplicate. if it turns out to be something else we can reopen.
</comment><comment author="evanv" created="2016-07-26T16:15:06Z" id="235320372">&gt; in which I can see that strategy being inadequate are esoteric.

The scenarios I can think of basically all boil down to a node failing and a disk containing a primary shard filling up while ES tries to replicate the shards that were lost when a node failed. And with 2 replica copies instead of 1, the odds of these kinds of failures happening seem extremely unlikely to me. 

As an example of what I have in mind, imagine a cluster with one replica of all its indices and a server dies. It seems possible to me that recovery could fill up one of several disks on a node and, assuming no watermarks are exceeded overall (it's worth noting that _cat/allocation was reporting 60% used space on my node that entered a coma), you could have a cluster lock up and freeze. Am I mistaken that this could happen? Suppose you have disks A, B, C, and D. Suppose A is 90% full and the others are 50% full. Is there anything preventing recovery from placing a shard on disk A? 

Inre "For what it's worth, if many bad things happen Elasticsearch may very well end up not being able to recover. It's about documenting those scenarios - for example - if you loose all copies of a shard, data will be lost." 

I agree. But if you loose all copies of a shard, you should expect to lose data. If you have 6 write paths with TBs of free space, node-level disk allocation significantly under the high and low water marks, and a single disk that fills up, I would argue that ES freezing for hours on end until someone clears it up manually is a bit strange to see and probably not what most people would expect. But I could be wrong. 
</comment><comment author="bleskes" created="2016-07-26T16:57:33Z" id="235333196">I'm not 100% sure I understand what you mean exactly, but I think what you mean is whether ES will transfer shard data from one path to another on the same node (remember each shard is always completely on one path), if one is getting full and the other is still empty. Sadly, this is not yet the case - there are existing issues to track this one- for example #16763
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Moved all mustache classes into one package.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19583</link><project id="" key="" /><description>No need for multiple packages inside a small module.
</description><key id="167413754">19583</key><summary>Moved all mustache classes into one package.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-07-25T16:52:05Z</created><updated>2016-07-25T17:29:53Z</updated><resolved>2016-07-25T17:29:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-25T17:04:27Z" id="235016115">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] org.elasticsearch.common.bytes.BytesArrayTests.testSliceArrayOffset fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19582</link><project id="" key="" /><description>This failure reproduces every time for me with:

```
gradle :core:test -Dtests.seed=42EED4DE0EE5FB95 -Dtests.class=org.elasticsearch.common.bytes.BytesArrayTests -Dtests.method="testSliceArrayOffset" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=560m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops" -Dtests.locale=nl-NL -Dtests.timezone=Africa/Lubumbashi
```

Seen here: http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3609/console

```
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=42EED4DE0EE5FB95 -Dtests.class=org.elasticsearch.common.bytes.BytesArrayTests -Dtests.method="testSliceArrayOffset" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=560m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops" -Dtests.locale=nl-NL -Dtests.timezone=Africa/Lubumbashi
FAILURE 0.07s | BytesArrayTests.testSliceArrayOffset &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected:&lt;3663&gt; but was:&lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([42EED4DE0EE5FB95:573929D675033D8B]:0)
   &gt;    at org.elasticsearch.common.bytes.AbstractBytesReferenceTestCase.testSliceArrayOffset(AbstractBytesReferenceTestCase.java:442)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="167408469">19582</key><summary>[CI] org.elasticsearch.common.bytes.BytesArrayTests.testSliceArrayOffset fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-07-25T16:26:06Z</created><updated>2016-07-25T19:24:03Z</updated><resolved>2016-07-25T19:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-25T16:28:29Z" id="235006111">There is also a reproducible failure in `PagedBytesReferenceTests.testSliceArrayOffset` that I believe is related to this (in the same failure on Jenkins):

```
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=42EED4DE0EE5FB95 -Dtests.class=org.elasticsearch.common.bytes.PagedBytesReferenceTests -Dtests.method="testSliceArrayOffset" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=560m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops" -Dtests.locale=vi -Dtests.timezone=Asia/Pyongyang
FAILURE 0.08s | PagedBytesReferenceTests.testSliceArrayOffset &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected:&lt;3663&gt; but was:&lt;0&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([42EED4DE0EE5FB95:573929D675033D8B]:0)
   &gt;    at org.elasticsearch.common.bytes.AbstractBytesReferenceTestCase.testSliceArrayOffset(AbstractBytesReferenceTestCase.java:442)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/common/bytes/AbstractBytesReferenceTestCase.java</file></files><comments><comment>BytesArray tests fix: offsets don't matter on a zero bytes array</comment></comments></commit></commits></item><item><title>Documentation changes for wait_for_active_shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19581</link><project id="" key="" /><description>Documentation changes for the addition of `wait_for_active_shards` in the index creation APIs and the replication action APIs (index, update, etc) and removing write consistency level.  This documentation change addresses the new and breaking changes from #18985 and #19454.  Also documented is the new index-level dynamic setting `index.write.wait_for_active_shards`.

Relates #18985
Relates #19454
</description><key id="167402330">19581</key><summary>Documentation changes for wait_for_active_shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Index APIs</label><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T15:59:00Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-08-02T13:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-25T16:00:07Z" id="234997761">@bleskes @clintongormley The documentation changes for #18985 and #19454 
</comment><comment author="abeyad" created="2016-07-28T14:24:39Z" id="235910062">@clintongormley @nik9000 thanks for the review! I pushed https://github.com/elastic/elasticsearch/pull/19581/commits/5d2bf275f5b7fd7c8a77d9697e4cc44c21ef940d to address the comments
</comment><comment author="clintongormley" created="2016-07-28T18:52:03Z" id="235989858">One minor comment, otherwise LGTM
</comment><comment author="abeyad" created="2016-07-28T18:58:47Z" id="235991876">@clintongormley thanks for the review! I pushed a fix for the one change. I'm just waiting to close #19454, then I'll merge this.
</comment><comment author="bleskes" created="2016-07-29T14:34:17Z" id="236196805">LGTM. Left some very minor comments. Thx @abeyad 
</comment><comment author="abeyad" created="2016-07-29T21:52:45Z" id="236303144">@bleskes I pushed https://github.com/elastic/elasticsearch/pull/19581/commits/86cbdc78e827d4c4c71cbe18fd210e68af264c73 to address your review comments
</comment><comment author="bleskes" created="2016-07-30T07:12:43Z" id="236349852">thx @abeyad 
</comment><comment author="bleskes" created="2016-08-02T12:11:10Z" id="236885366">+1 (again)
</comment><comment author="abeyad" created="2016-08-02T13:14:13Z" id="236900028">@bleskes I added a link in the migration changes doc, thanks for your feedback!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Documentation changes for wait_for_active_shards (#19581)</comment></comments></commit><commit><files /><comments><comment>Documentation changes for wait_for_active_shards (#19581)</comment></comments></commit></commits></item><item><title>Remove duplicate dependency declaration for http client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19580</link><project id="" key="" /><description>We [disable transitive dependencies in our build plugin](https://github.com/elastic/elasticsearch/blob/2d1b0587dd57a74c48022f5456467ab8873a2d0c/buildSrc/src/main/groovy/org/elasticsearch/gradle/BuildPlugin.groovy#L253-L265) 
for all dependencies except for the group org.elasticsearch`. 
However, in the reindex plugin we depend on the REST client 
and declare its dependencies again which is not necessary 
(and led to problems with conflicting versions in #19281. 
After a fair amount of digging / debugging the build I am 
still not sure why though.)

With this PR we remove the duplicate declaration.
</description><key id="167390764">19580</key><summary>Remove duplicate dependency declaration for http client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T15:12:20Z</created><updated>2016-07-26T04:29:05Z</updated><resolved>2016-07-26T04:29:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-25T15:23:24Z" id="234986381">LGTM. @javanna, I think it is important you have a look too.

I didn't know that that is how we handled Elasticsearch's transitive dependencies.
</comment><comment author="javanna" created="2016-07-25T15:29:35Z" id="234988363">this change looks good to me. I am not sure whether long term we should rather disable transitive deps for o.e.client. maybe @rjernst can comment on that?

Note that the conflict problem originates from httpaasyncclient which depends on httpclient 4.5.2 and httpcore 4.4.5, but httpclient 4.5.2 depends on httpcore 4.4.4. Weird situation.

We should also figure out whether there may be problems with commons-codec and commons-logging versions, I think Daniel found problems there too.
</comment><comment author="nik9000" created="2016-07-25T15:31:56Z" id="234989030">&gt; We should also figure out whether there may be problems with commons-codec and commons-logging versions, I think Daniel found problems there too.

So long as we're just removing _duplicate_ deps its ok with me. If we start getting diverging version numbers I think we're in a bad place.
</comment><comment author="rjernst" created="2016-07-25T15:36:48Z" id="234990540">&gt; I am not sure whether long term we should rather disable transitive deps for o.e.client. maybe @rjernst can comment on that?

No, I don't think we should. We should not pull in anything in o.e.client that is not actually needed by the client.
</comment><comment author="nik9000" created="2016-07-25T15:39:20Z" id="234991316">&gt; No, I don't think we should. We should not pull in anything in o.e.client that is not actually needed by the client.

I don't understand your answer. If we depend on o.e.client like we do in reindex should we have to list its deps in reindex or not? We don't now.

I don't think we're talking about allowing o.e.client to skip declaring its own transitive dependencies.
</comment><comment author="rjernst" created="2016-07-25T15:43:38Z" id="234992622">&gt; If we depend on o.e.client like we do in reindex should we have to list its deps in reindex or not? We don't now

No, I don't think we should, for the same reason we do not for any plugin/module that depends on core. One of the main reasons for disabling transitive dependencies is to ensure we vet all dependencies. We have done that for o.e.client, so why would we need to do it again inside reindex?
</comment><comment author="javanna" created="2016-07-25T15:46:05Z" id="234993346">thanks for the explanation Ryan, makes sense. LGTM then
</comment><comment author="danielmitterdorfer" created="2016-07-26T04:28:18Z" id="235159789">@nik9000, @javanna, @rjernst thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove duplicate dependency declaration for http client (#19580)</comment></comments></commit></commits></item><item><title>Rename FieldDataFieldsContext and FieldDataFieldsFetchSubPhase in DocValueFieldsContext and DocValueFieldsFetchSubPhase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19579</link><project id="" key="" /><description>This change renames the package org.elasticsearch.search.fetch.fielddata to org.elasticsearch.search.fetch.docvalues and renames the
FieldData\* classes in DocValue*.
This is a follow up of the renaming that happened in #18943
</description><key id="167378874">19579</key><summary>Rename FieldDataFieldsContext and FieldDataFieldsFetchSubPhase in DocValueFieldsContext and DocValueFieldsFetchSubPhase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T14:24:41Z</created><updated>2016-07-25T15:20:36Z</updated><resolved>2016-07-25T15:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-25T14:44:17Z" id="234974043">LGTM
</comment><comment author="jimczi" created="2016-07-25T15:20:36Z" id="234985478">Thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/docvalues/DocValueFieldsContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/docvalues/DocValueFieldsFetchSubPhase.java</file></files><comments><comment>Merge pull request #19579 from jimferenczi/docvalue_fields_fetch</comment></comments></commit></commits></item><item><title>Remove TODO about Timeout in Azure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19578</link><project id="" key="" /><description>In #15950 #15080 #16084 we added the support of TimeOut for Requests with a default client`setTimeoutIntervalInMs`.
So we can remove this useless todo which was added for only one method.

Closes #18617.
</description><key id="167377387">19578</key><summary>Remove TODO about Timeout in Azure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>non-issue</label><label>review</label></labels><created>2016-07-25T14:18:53Z</created><updated>2016-07-25T14:33:02Z</updated><resolved>2016-07-25T14:33:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-25T14:27:01Z" id="234968723">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CONSOLEify term and phrase suggester docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19577</link><project id="" key="" /><description>This includes a working example of reverse filters to support
correcting prefix errors.

Relates to #18160
</description><key id="167376956">19577</key><summary>CONSOLEify term and phrase suggester docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T14:17:21Z</created><updated>2016-07-26T16:29:37Z</updated><resolved>2016-07-26T16:29:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-25T14:20:22Z" id="234966772">@MaineC, can you look at this one briefly?
</comment><comment author="jpountz" created="2016-07-26T06:46:41Z" id="235177377">LGTM
</comment><comment author="nik9000" created="2016-07-26T16:29:37Z" id="235324748">Thanks for the review @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OOM -&gt; LockObtainFailedException -&gt; All data lost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19576</link><project id="" key="" /><description>Hi,

We experienced an issue today where we saw OOM errors, LockObtainFailedException exceptions and some others (shown in the traces below). We restarted the node (this is a cluster of a single node hosting a number of indices with 5 shards in each index). Following the restart ALL the data was gone from all indices and a check of the file system showed less than 1MB in the data directories of Elastic (previously this would have been Gigs of data).

I've used the title above as it looks very similar to this issue: 
https://github.com/elastic/elasticsearch/issues/12041
but I think its similar to this also.....
http://stackoverflow.com/questions/32494095/all-the-data-suddenly-removed-from-the-elasticsearch-cluster

Please find my responses to your standard questions below.

Thanks in advance for any help!

**Elasticsearch version**: 2.3.2

**JVM version**:
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**: CentOS Linux release 7.2.1511 (Core)

**Description of the problem including expected versus actual behavior**:
After restart due to out of memory all index data directories are missing, the index directories are still present, but contain only _state, no data.

**Provide logs (if relevant)**:
[WARN ][indices.breaker.request  ] [request] New used memory 2179536984 [2gb] for data of [&lt;reused_arrays&gt;] would be larger than configured breaker: 2102132736 [1.9gb], breaking

[DEBUG][action.search            ] [&lt;host&gt;] [&lt;index&gt;][4], node[9wlPuDpBSB6oGedoRiI7-Q], [P], v[4], s[STARTED], a[id=wtkeXgBiRv-o9DVnrbOGnw]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1eb229c6] lastShard [true]
RemoteTransportException[[&lt;host&gt;][x.x.x.x:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: CircuitBreakingException[[request] Data too large, data for [&lt;reused_arrays&gt;] would be larger than limit of [2102132736/1.9gb]];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: CircuitBreakingException[[request] Data too large, data for [&lt;reused_arrays&gt;] would be larger than limit of [2102132736/1.9gb]];
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:366)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:378)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: CircuitBreakingException[[request] Data too large, data for [&lt;reused_arrays&gt;] would be larger than limit of [2102132736/1.9gb]]
    at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.circuitBreak(ChildMemoryCircuitBreaker.java:97)
    at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:147)
    at org.elasticsearch.common.util.BigArrays.adjustBreaker(BigArrays.java:396)
    at org.elasticsearch.common.util.BigArrays.validate(BigArrays.java:433)
    at org.elasticsearch.common.util.BigArrays.newByteArray(BigArrays.java:458)
    at org.elasticsearch.common.util.BigArrays.resize(BigArrays.java:475)
    at org.elasticsearch.common.util.BigArrays.grow(BigArrays.java:489)
    at org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlus.ensureCapacity(HyperLogLogPlusPlus.java:197)
    at org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlus.collect(HyperLogLogPlusPlus.java:230)
    at org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregator$DirectCollector.collect(CardinalityAggregator.java:203)
    at org.elasticsearch.search.aggregations.LeafBucketCollector$3.collect(LeafBucketCollector.java:73)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.range.RangeAggregator$1.collect(RangeAggregator.java:184)
    at org.elasticsearch.search.aggregations.bucket.range.RangeAggregator$1.collect(RangeAggregator.java:138)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator$2.collect(GlobalOrdinalsStringTermsAggregator.java:130)
    at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
    at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:174)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:221)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:172)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
    ... 12 more

[DEBUG][action.search            ] [&lt;host&gt;] [&lt;index&gt;][0], node[9wlPuDpBSB6oGedoRiI7-Q], [P], v[4], s[STARTED], a[id=jsJyGbCqQW677Mcg3OK1fg]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1eb229c6] lastShard [true]
RemoteTransportException[[&lt;host&gt;][x.x.x.x:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: OutOfMemoryError[Java heap space];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: OutOfMemoryError[Java heap space];
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:366)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:378)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.cache.recycler.PageCacheRecycler$1.newInstance(PageCacheRecycler.java:102)
    at org.elasticsearch.cache.recycler.PageCacheRecycler$1.newInstance(PageCacheRecycler.java:99)
    at org.elasticsearch.common.recycler.DequeRecycler.obtain(DequeRecycler.java:53)
    at org.elasticsearch.common.recycler.AbstractRecycler.obtain(AbstractRecycler.java:33)
    at org.elasticsearch.common.recycler.DequeRecycler.obtain(DequeRecycler.java:28)
    at org.elasticsearch.common.recycler.FilterRecycler.obtain(FilterRecycler.java:39)
    at org.elasticsearch.common.recycler.Recyclers$3.obtain(Recyclers.java:119)
    at org.elasticsearch.common.recycler.FilterRecycler.obtain(FilterRecycler.java:39)
    at org.elasticsearch.cache.recycler.PageCacheRecycler.bytePage(PageCacheRecycler.java:150)
    at org.elasticsearch.common.util.AbstractBigArray.newBytePage(AbstractBigArray.java:108)
    at org.elasticsearch.common.util.BigByteArray.&lt;init&gt;(BigByteArray.java:45)
    at org.elasticsearch.common.util.BigArrays.newByteArray(BigArrays.java:451)
    at org.elasticsearch.common.util.BigArrays.resize(BigArrays.java:475)
    at org.elasticsearch.common.util.BigArrays.grow(BigArrays.java:489)
    at org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlus.ensureCapacity(HyperLogLogPlusPlus.java:197)
    at org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlus.collect(HyperLogLogPlusPlus.java:230)
    at org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregator$DirectCollector.collect(CardinalityAggregator.java:203)
    at org.elasticsearch.search.aggregations.LeafBucketCollector$3.collect(LeafBucketCollector.java:73)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.range.RangeAggregator$1.collect(RangeAggregator.java:184)
    at org.elasticsearch.search.aggregations.bucket.range.RangeAggregator$1.collect(RangeAggregator.java:138)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator$2.collect(GlobalOrdinalsStringTermsAggregator.java:130)
    at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
    at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:174)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:221)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:172)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)

[WARN ][index.engine             ] [&lt;host&gt;] [&lt;index&gt;][0] failed engine [refresh failed]
java.lang.OutOfMemoryError: Java heap space

[DEBUG][action.bulk              ] [&lt;host&gt;] failed to execute [BulkShardRequest to [&lt;index&gt;] containing [1] requests] on [[&lt;index&gt;][4]]
[&lt;index&gt;][[&lt;index&gt;][4]] EngineClosedException[CurrentState[CLOSED] Closed]
    at org.elasticsearch.index.engine.Engine.ensureOpen(Engine.java:329)
    at org.elasticsearch.index.engine.InternalEngine.get(InternalEngine.java:317)
    at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:652)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:173)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:86)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:77)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:383)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:191)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

[WARN ][index.shard              ] [&lt;host&gt;] [&lt;index&gt;][4] Failed to perform scheduled engine refresh
[&lt;index&gt;][[&lt;index&gt;][4]] RefreshFailedEngineException[Refresh failed]; nested: OutOfMemoryError[Java heap space];
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:680)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:661)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1349)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space

[WARN ][indices.cluster          ] [&lt;host&gt;] [[&lt;index&gt;][2]] marking and sending shard failed due to [engine failure, reason [refresh failed]]
java.lang.OutOfMemoryError: Java heap space

[WARN ][cluster.action.shard     ] [&lt;host&gt;] [&lt;index&gt;][2] received shard failed for target shard [[&lt;index&gt;][2], node[9wlPuDpBSB6oGedoRiI7-Q], [P], v[4], s[STARTED], a[id=1SRVPoVZTlaB3Qdt2lyCSQ]], indexUUID [qmKpqmQ2RYKaIzlqxXya6w], message [engine failure, reason [refresh failed]], failure [OutOfMemoryError[Java heap space]]
java.lang.OutOfMemoryError: Java heap space

[WARN ][indices.memory           ] [&lt;host&gt;] failed to set shard [&lt;index&gt;][0] index buffer to [62.6mb]
org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:720)
    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:734)
    at org.apache.lucene.index.IndexWriter.ramBytesUsed(IndexWriter.java:475)
    at org.elasticsearch.index.engine.InternalEngine.indexWriterRAMBytesUsed(InternalEngine.java:948)
    at org.elasticsearch.index.shard.IndexShard.updateBufferSize(IndexShard.java:1149)
    at org.elasticsearch.indices.memory.IndexingMemoryController.updateShardBuffers(IndexingMemoryController.java:232)
    at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:286)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:640)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space

[WARN ][indices.cluster          ] [&lt;host&gt;] [[&lt;index&gt;][4]] marking and sending shard failed due to [failed to create shard]
[&lt;index&gt;][[&lt;index&gt;][4]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [&lt;index&gt;][4], timed out after 5000ms];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [lowercasekey-poi-geoshape-1466686817402][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
    ... 10 more

[WARN ][cluster.action.shard     ] [&lt;host&gt;] [&lt;index&gt;][4] received shard failed for target shard [[&lt;index&gt;][4], node[9wlPuDpBSB6oGedoRiI7-Q], [P], v[5], s[INITIALIZING], a[id=oOVK7CO-TsSLK7aWor8q5w], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-07-25T09:26:03.243Z], details[engine failure, reason [refresh failed], failure OutOfMemoryError[Java heap space]]]], indexUUID [qmKpqmQ2RYKaIzlqxXya6w], message [failed to create shard], failure [ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [&lt;index&gt;][4], timed out after 5000ms]; ]
[&lt;index&gt;][[&lt;index&gt;][4]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [&lt;index&gt;][4], timed out after 5000ms];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [&lt;index&gt;][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
    ... 10 more

[WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:146)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:89)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:85)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:356)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:351)
    at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishAsFailed(TransportReplicationAction.java:567)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2.onClusterServiceClose(TransportReplicationAction.java:552)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:222)
    at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:282)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:153)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:98)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:90)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.retry(TransportReplicationAction.java:544)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.retryBecauseUnavailable(TransportReplicationAction.java:596)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:465)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2.onNewClusterState(TransportReplicationAction.java:547)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.postAdded(ClusterStateObserver.java:206)
    at org.elasticsearch.cluster.service.InternalClusterService$1.run(InternalClusterService.java:296)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

[WARN ][rest.suppressed          ] /&lt;index&gt;/_search Params: {index=&lt;index&gt;}
Failed to execute phase [query], all shards failed; shardFailures {[9wlPuDpBSB6oGedoRiI7-Q][&lt;index&gt;][0]: TransportException[transport stopped, action: indices:data/read/search[phase/query]]}{[9wlPuDpBSB6oGedoRiI7-Q][&lt;index&gt;][1]: TransportException[transport stopped, action: indices:data/read/search[phase/query]]}{[9wlPuDpBSB6oGedoRiI7-Q][&lt;index&gt;][2]: TransportException[transport stopped, action: indices:data/read/search[phase/query]]}{[9wlPuDpBSB6oGedoRiI7-Q][&lt;index&gt;][3]: TransportException[transport stopped, action: indices:data/read/search[phase/query]]}{[9wlPuDpBSB6oGedoRiI7-Q][&lt;index&gt;][4]: TransportException[transport stopped, action: indices:data/read/search[phase/query]]}
    at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
    at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: TransportException[transport stopped, action: indices:data/read/search[phase/query]]
    ... 4 more

[INFO ][node                     ] [&lt;host&gt;] stopped
[INFO ][node                     ] [&lt;host&gt;] closing ...
[INFO ][node                     ] [&lt;host&gt;] version[2.3.2], pid[26270], build[b9e4a6a/2016-04-21T16:03:47Z]
[INFO ][node                     ] [&lt;host&gt;] initializing ...

[INFO ][node                     ] [&lt;host&gt;] initialised
[INFO ][node                     ] [&lt;host&gt;] starting ...
[INFO ][transport                ] [&lt;host&gt;] publish_address {10.10.20.10:9300}, bound_addresses {[::]:9300}
[INFO ][discovery                ] [&lt;host&gt;] dev-elastic/RSMtDui2TvS4pzY6vvdJng
[INFO ][node                     ] [&lt;host&gt;] closed
[INFO ][cluster.service          ] [&lt;host&gt;] new_master {&lt;host&gt;}{RSMtDui2TvS4pzY6vvdJng}{x.x.x.x}{x.x.x.x:9300}{master=true}, reason: zen-disco-join(elected_as_master, [0] joins received)
[INFO ][http                     ] [&lt;host&gt;] publish_address {x.x.x.x:9200}, bound_addresses {[::]:9200}
[INFO ][node                     ] [&lt;host&gt;] started
[INFO ][gateway                  ] [&lt;host&gt;] recovered [0] indices into cluster_state
</description><key id="167365740">19576</key><summary>OOM -&gt; LockObtainFailedException -&gt; All data lost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DaveChapman</reporter><labels><label>feedback_needed</label></labels><created>2016-07-25T13:27:44Z</created><updated>2016-07-27T10:41:55Z</updated><resolved>2016-07-27T10:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-25T15:44:14Z" id="234992802">Ouch. I'm not sure at all that the LockObtainFailedException is relevant to the data loss. I do see how it's an artifact of an OOM as things may not be cleared correctly after an OOM , which is why [we recommend restarting the node](https://www.elastic.co/blog/a-heap-of-trouble).

The problem is caused by the fact that one the node has been restarted, it doesn't find any cluster state in it's data folder and thinks all indices are gone:

```
[INFO ][gateway ] [] recovered [0] indices into cluster_state
```

While that is troubling on it's own, it doesn't still explain the data lost as normally ES will reimport that indices from their data folders, with the following message in the logs:

```
dangling index, exists on local file system, but not in cluster metadata
```

&gt; the index directories are still present, but contain only _state, no data.

It's a bit weird also that you do have `_state` folders and files. When ES deletes indices it will delete all the folder as well.

Can you provide a list of the current directory structure, with the files?

If there is anything else that you think is relevant, given the above explanation - please share. 
</comment><comment author="DaveChapman" created="2016-07-25T16:47:52Z" id="235011509">Hi, an update to this: the data seems to still be on disk (I had forgotten that we were writing to a number of directories).

Anyway our configuration is as follows:

`path.data: /data0/data,/data1/data,/data2/data`

And our data directories are sized as:

`924K   /data0/data`
`72G    /data1/data`
`90G    /data2/data`

However, data is not being returned from our searches before the time we restarted Elastic this morning (new items are visible however). The cluster state is yellow.
</comment><comment author="DaveChapman" created="2016-07-26T10:43:38Z" id="235232755">Hi, another update. We have restarted the node again and the indices/data have reappeared, however data loaded since the previous restart is missing. Any ideas as to what may have happened here?
</comment><comment author="bleskes" created="2016-07-26T11:27:43Z" id="235241125">I suspect something is off in your cluster configuration causing the paths within the data folders to change. Either path locking issues or maybe something stupid like the cluster name changes with a restart. That's why I ask for a dump you current directory structure. If you rather keep it private it  you can mail it to my first name at elastic.co
</comment><comment author="DaveChapman" created="2016-07-26T11:41:44Z" id="235243656">Hi, I will need to check if I can send you the directory structure, however this may not be helpful as everything is currently working as expected. We are trying to replicate the issue at the moment.
</comment><comment author="DaveChapman" created="2016-07-26T15:33:32Z" id="235306484">So we have managed to trigger the OOM again, however without the LockObtainFailedException. This time the node was able to recover on it's own.
</comment><comment author="bleskes" created="2016-07-26T15:35:36Z" id="235307225">I strongly suspect it had to do with the node restart not the OOM. It's also very hard to predict/reproduce how an OOM will affect the node.
</comment><comment author="DaveChapman" created="2016-07-26T16:34:12Z" id="235326192">We have replicated the OOM with the LockObtainFailedException and see the following in the logs after a restart:
`[WARN ][rest.suppressed          ] /_bulk Params: {}
ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];]
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:154)
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:144)
    at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:212)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:164)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:71)
    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:436)
    at org.elasticsearch.rest.action.bulk.RestBulkAction.handleRequest(RestBulkAction.java:90)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:205)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:449)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:61)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)`

After the restart the indices were again missing, they came back after a second restart.
</comment><comment author="bleskes" created="2016-07-26T16:50:19Z" id="235330989">@DaveChapman thx, sadly it doesn't add much info. I will either need a complete directory listing or debug logs from before and after the issue. Can you send me those? 
</comment><comment author="bleskes" created="2016-07-27T10:41:54Z" id="235550342">@DaveChapman thx for your e-mail. The problem lies in the fact the after the node restart, the nodes picks up a different data folder lock ordinal then before, causing it not see the previous data. This is setup this way to support multiple nodes on the same server - when a node starts up, it will look for the first ordinal it can lock (others are used by other nodes) and use that. Looking at the logs you sent me, I see what node shutting down and then a new one starting up:

```
[2016-07-25 10:39:57,637][INFO ][node                     ] [dev3_es] stopped
[2016-07-25 10:39:57,637][INFO ][node                     ] [dev3_es] closing ...
[2016-07-25 10:39:57,637][INFO ][node                     ] [dev3_es] stopped
[2016-07-25 10:39:57,637][INFO ][node                     ] [dev3_es] closing ...
[2016-07-25 10:40:00,792][INFO ][node                     ] [dev3_es] version[2.3.2], pid[26270], build[b9e4a6a/2016-04-21T16:03:47Z]
[2016-07-25 10:40:00,793][INFO ][node                     ] [dev3_es] initializing ...
```

the weird thing is that we miss a log message saying `closed` from the first node. See [here](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/node/Node.java#L436). 

Also, the new nodes starts up but somehow disappears and a new one is started again:

```
[2016-07-25 10:40:06,370][INFO ][node                     ] [dev3_es] initialized
[2016-07-25 10:40:06,370][INFO ][node                     ] [dev3_es] starting ...
```

Any idea what's going on?

Also, you can consider using `node.max_local_storage: 1` see [here](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-node.html#max-local-storage-nodes).

I'm closing this now as it seems to be environment related. If not we can reopen. Just a heads up - when if/you re-open, it will be nice to get debug logging of the node startup, with the addition of setting the `env` logger to `TRACE`, it will tell some more of the paths it ended using  and why.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>B/cloud azure arm 2.3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19575</link><project id="" key="" /><description>updated azure plugin to support cloud-azure-arm #19146

( As we need it in 2.3. we created the PR against this branch, a PR against master will follow )
</description><key id="167365454">19575</key><summary>B/cloud azure arm 2.3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bogensberger</reporter><labels /><created>2016-07-25T13:26:17Z</created><updated>2016-09-13T15:39:07Z</updated><resolved>2016-09-13T15:39:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2016-07-25T13:26:17Z" id="234952142">Hi @bogensberger, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/19575.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?
</comment><comment author="elasticmachine" created="2016-07-25T13:26:18Z" id="234952146">Can one of the admins verify this patch?
</comment><comment author="bogensberger" created="2016-07-25T13:29:17Z" id="234952880">Hi @karmi, i think the CLA check is failing beacause @aslanbekirov didn't sign the CLA yet. Will do it immediately.
</comment><comment author="aslanbekirov" created="2016-07-25T13:29:55Z" id="234953023">Hi @karmi, I signed it. 
</comment><comment author="dadoonet" created="2016-07-25T13:32:29Z" id="234953664">Hey @bogensberger. Thanks for the effort.

I did not read the code in details for now. But the idea with #19146 is to create a new plugin named `discovery-azure-arm` instead of changing the legacy plugin.

Also, I don't think we will add this to 2.4 but I might be wrong.

That being said, as I started a branch for 5.0 (not public yet - baby steps), I'll most likely reuse your efforts there unless you have a plan for doing so yourself in the mid term?
</comment><comment author="dadoonet" created="2016-07-26T10:33:46Z" id="235230844">@bogensberger Do you think you will come with a branch based on master branch (5.0)?
</comment><comment author="bogensberger" created="2016-08-02T09:24:38Z" id="236851451">@dadoonet as you planned to create a new plugin and reuse our efforts i didn't plan to create a plugin based on the master branch because the important stuff which you could reuse wouldn't be different on master and the rest will be thrown anyway. 
</comment><comment author="dakrone" created="2016-09-12T22:11:20Z" id="246511467">@dadoonet what's the word on this, are you still planning on reusing the efforts here?
</comment><comment author="dadoonet" created="2016-09-13T00:23:34Z" id="246536693">That's my plan indeed. No idea when I'll be able to do it though. 
Thanks for the heads up!

But as we won't merge this PR as is we can close it I guess?
</comment><comment author="dakrone" created="2016-09-13T15:39:07Z" id="246723942">Okay, I'm going to close this then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to update Marvel related configs via HTTP API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19574</link><project id="" key="" /><description>We're currently using Elasticsearch v2.3.3 and it seems you can't dynamically update Marvel related settings (`marvel.*`) via the HTTP API.  Although it doesn't return an error, I can confirm that the setting doesn't show up in `/_cluster/settings` and Marvel data continues to be generated on all the nodes.

```
user@esclient01:~# curl -XPUT http://10.1.2.3:9200/_cluster/settings -d '{"persistent": {"marvel.enabled": false}}'
{"acknowledged":true,"persistent":{},"transient":{}}
```

```
user@esclient01:~# curl -XGET http://10.1.2.3:9200/_cluster/settings?pretty
{
  "persistent": {
    "indices": {
      "recovery": {
        "concurrent_streams": "3",
        "max_bytes_per_sec": "100mb"
      }
    },
    "cluster": {
      "routing": {
        "allocation": {
          "node_initial_primaries_recoveries": "3",
          "cluster_concurrent_rebalance": "2",
          "node_concurrent_recoveries": "2",
          "disk": {
            "watermark": {
              "low": "90%",
              "high": "93%"
            },
            "threshold_enabled": "true"
          }
        }
      },
      "info": {
        "update": {
          "interval": "30s"
        }
      }
    }
  },
  "transient": {}
}
```

Could this setting be updated to allow dynamic updates or is there a reason it needs to stay static within the elasticsearch.yml config?  Could the [documentation page](https://www.elastic.co/guide/en/marvel/current/configuration.html) be updated also to indicate if these settings are [dynamic or static](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html)?

Thank you
</description><key id="167355314">19574</key><summary>Ability to update Marvel related configs via HTTP API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alefebvre-ls</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-07-25T12:34:06Z</created><updated>2016-07-27T19:01:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-07-27T17:20:06Z" id="235656205">Hi @alefebvre-ls, please ask these types of questions in our [Discuss forums](https://discuss.elastic.co).

Not every setting in ES is dynamically updateable. This will be a bit more obvious in 5.0 thanks to a large amount of work that has gone into the settings part of the code across Elasticsearch. You'll also be able to _unset_ persistent settings in 5.0.

```
user@esclient01:~# curl -XPUT http://10.1.2.3:9200/_cluster/settings -d '{"persistent": {"marvel.enabled": false}}'
{"acknowledged":true,"persistent":{},"transient":{}}
```

This indicates the the setting was not accepted. It was acknowledged that your request was well-formed, but the persistent and transient blocks were empty because nothing you passed in was accepted. This too will improve a bit in 5.0.

`marvel.enabled` is a setting that controls whether the plugin should initialize. If `false`, then it will not load them. The only way to set this usefully is within the `elasticsearch.yml` (or command line) so that it reads the value at startup.

The [setting you're looking](https://www.elastic.co/guide/en/marvel/current/configuration.html#stats-export) for is `marvel.agent.interval`, which controls the polling interval from the internal agent. You want to shut this down until you decide otherwise to effectively stop Marvel:

``` http
PUT /_cluster/settings
{
  "persistent": {
    "marvel.agent.interval": -1
  }
}
```

Note the value is traditionally a time value (e.g., `"10s"`), but to disable it you currently set it to `-1`. In ES 5.0, this setting will change to be more obvious.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't swallow the original IOException thrown at JSON parsing in slowlogs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19573</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**JVM version**: 1.8.0_77

**Description of the problem including expected versus actual behavior**:

The slowlogs are hidding and not logging the original IOException thrown in the [slowlog indexing](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/index/indexing/slowlog/ShardSlowLogIndexingService.java#L160-L168) and [searching](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/index/search/slowlog/ShardSlowLogSearchService.java#L203-L220) utility classes. The same seems to be happening for 2.3.x.

As such in slowlogs entries like the following are logged and there are no clues as to what the original request was and why it failed:

```
[2016-07-25 09:10:38,310][INFO ][index.indexing.slowlog.index] [some_node] [some_index][3] took[7.2s], took_millis[7200], type[some_type], id[12345], routing[12345], source[source[_failed_to_convert_]
```
</description><key id="167349565">19573</key><summary>Don't swallow the original IOException thrown at JSON parsing in slowlogs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:Exceptions</label><label>bug</label><label>discuss</label></labels><created>2016-07-25T11:59:41Z</created><updated>2016-07-27T16:25:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-25T15:48:18Z" id="234994053">This is a typical case of exception swallowing. For master, the code is in `IndexingSlowLog` line 193.  We could easily change this catch to rethrow as an `UncheckedIOException`. However, that would propagate up and cause the indexing request to fail, even though the actual indexing already succeeded. So we could either do that, or log the exception in the slow log itself. I don't know which is better, I've marked this issue for discussion.
</comment><comment author="nik9000" created="2016-07-25T15:55:12Z" id="234996258">Maybe we should log it in the main log file (not the slow log) at warn level? Then we can keep the slow log the way it is. I don't expect this to be especially common so warn level ought to be fine. If it become common we can bump that log level to error as a work around and we can fix the issue that makes it common in the mean time.
</comment><comment author="javanna" created="2016-07-25T15:55:54Z" id="234996460">I don't think slowlog should cause a request to fail, but we should totally not lose what the problem was and log all its details. The odd part of polluting the slowlog file would be that it may break parsers out there that rely on a fixed format? So I think I agree with Nik to try and log it in the main log file if possible.
</comment><comment author="rjernst" created="2016-07-25T15:58:15Z" id="234997261">Well, one problem here is rethrowing and putting into the general log means we will not have this indexing request in the slow log. I think that is kind of bogus? Propagating to fail the indexing request would have been painful yes, but we would know what the issue is and could fix much faster than it just being hidden away in another log file.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update gateway.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19572</link><project id="" key="" /><description>Added a note to clarify that, in cases where nodes in a cluster may have different settings, the settings on the node that is the elected master take precedence over anything else.
</description><key id="167325191">19572</key><summary>Update gateway.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-07-25T09:36:23Z</created><updated>2016-07-28T11:08:36Z</updated><resolved>2016-07-28T11:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-25T09:47:08Z" id="234909197">I think we need to change the opening sentence  too:

```
 The following _static_ settings, which must be set on every data node in the
 cluster, controls how long nodes should wait before they try to recover any
 shards which are stored locally:
```

It is not true (anymore?). It should read - the following _static_ settings, which must be set on every _master_ node... controls how long a freshly elected master should wait before it tries to recover the cluster state and the cluster's data.

WDYT?
</comment><comment author="markwalkom" created="2016-07-25T10:37:36Z" id="234919511">That seems to be a much better way of describing it :)
</comment><comment author="clintongormley" created="2016-07-27T16:10:52Z" id="235636045">@markwalkom you going to update your PR with the changes mentioned?
</comment><comment author="clintongormley" created="2016-07-27T16:11:34Z" id="235636258">Also, I'd role the two `NOTE:` blocks into a single one, otherwise it'll just clutter the page
</comment><comment author="markwalkom" created="2016-07-28T02:49:55Z" id="235786373">Done, per @bleskes's comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update gateway.asciidoc (#19572)</comment></comments></commit></commits></item><item><title>Add _bucket_count option to buckets_path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19571</link><project id="" key="" /><description>This change adds a new special path to the buckets_path syntax
`_bucket_count`. This new option will return the number of buckets for a
multi-bucket aggregation, which can then be used in pipeline
aggregations.

Closes #19553
</description><key id="167322857">19571</key><summary>Add _bucket_count option to buckets_path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-25T09:24:29Z</created><updated>2016-07-26T09:29:27Z</updated><resolved>2016-07-26T09:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-07-25T15:34:45Z" id="234989877">LGTM
</comment><comment author="jpountz" created="2016-07-26T06:41:09Z" id="235176517">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/InternalMultiBucketAggregation.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file></files><comments><comment>#19571 Add _bucket_count option to buckets_path</comment></comments></commit></commits></item><item><title>Upgrade repository type which is not accepted after 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19570</link><project id="" key="" /><description>Reported at https://discuss.elastic.co/t/s3-snapshot-repositories-not-working-after-upgrade-from-1-7-to-2-3/56214
## Reproduction steps:
- Start a new 1.7.5 node:

``` sh
bin/elasticsearch -Des.path.repo=/tmp
```
- Create a repository using type `FS`:

```
curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
    "type": "Fs",
    "settings": {
        "location": "/tmp"
    }
}'
```

Note that we use `Fs` here. So the Class which is Loaded is `FsRepositoryModule`. 
If we use `fs`, same class `FsRepositoryModule` is loaded. If we use `FS` it fails because class `FSRepositoryModule` does not exist.

Why is this important? In the context of S3 repositories, `S3` or `s3` will be allowed in 1.7 as they all try to load `S3RepositoryModule`: https://github.com/elastic/elasticsearch-cloud-aws/blob/es-1.7/src/main/java/org/elasticsearch/repositories/s3/S3RepositoryModule.java.
- Stop 1.7.5 node
- Move data dir to a new empty 2.3.3 node:

``` sh
mv elasticsearch-1.7.5/data elasticsearch-2.3.3
```
- Start 2.3.3 node:

``` sh
bin/elasticsearch -Des.path.repo=/tmp
```

It fails with:

```
[2016-07-25 10:59:09,295][WARN ][repositories             ] [Kleinstocks] failed to create repository [my_backup]
RepositoryException[[my_backup] failed to create repository]; nested: IllegalArgumentException[Unknown [repository] type [Fs]];
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:411)
    at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:299)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unknown [repository] type [Fs]
    at org.elasticsearch.common.util.ExtensionPoint$SelectedType.bindType(ExtensionPoint.java:146)
    at org.elasticsearch.repositories.RepositoryTypesRegistry.bindType(RepositoryTypesRegistry.java:49)
    at org.elasticsearch.repositories.RepositoryModule.configure(RepositoryModule.java:58)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:105)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:143)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:157)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    ... 8 more
```
## Bug source

From 2.x [we compare repository names](https://github.com/elastic/elasticsearch/blob/2.4/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java#L361) with what is defined in https://github.com/elastic/elasticsearch/blob/2.4/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java#L51
## How to solve this

Proposal 1: well, it's just about creating repositories so people are loosing any data if they have to recreate a repository from scratch.

``` sh
curl -XPUT 'http://localhost:9200/_snapshot/my_backup'
# Then create the repository with correct settings
```

We should probably add this to the breaking changes and to the migration plugin.

Proposal 2: more complicated: automatically upgrade an existing repository which fails at startup with its lowercase form and write again this new version of the repository.

Thoughts?
</description><key id="167319814">19570</key><summary>Upgrade repository type which is not accepted after 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2016-07-25T09:10:12Z</created><updated>2016-07-27T20:29:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-25T09:15:41Z" id="234900864">@imotov What do you think?
</comment><comment author="clintongormley" created="2016-07-27T16:07:00Z" id="235634843">&gt;  it's just about creating repositories so people are loosing any data if they have to recreate a repository from scratch.

Is this true?  I thought we left the data in the repo intact.
</comment><comment author="imotov" created="2016-07-27T16:10:03Z" id="235635780">@clintongormley I think David just missed "not" in this sentence. I think it should read "it's just about creating repositories so people are **not** loosing any data"
</comment><comment author="clintongormley" created="2016-07-27T18:56:58Z" id="235684793">Phew, ok :)

In which case I would just go for strict validation.  Bonus points for "did you mean?"
</comment><comment author="dadoonet" created="2016-07-27T20:29:20Z" id="235710585">Ooops. Sorry. Indeed. "Not" is an important missing word here. :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch refresh -1 to disable documents from being searchable doesnt work and updates documents after 5 mins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19569</link><project id="" key="" /><description>**Elasticsearch version**: 2.2

**JVM version**: 1.8.60 

**OS version**: Linux

 I wish to imitate DB transactions in elasticsearch because I want a stream of documents to be available at the same time for searching. In order to do that, thought of disabling refresh until I index all the documents and then do a forcemerge/enable refresh explicitly to make them searchable at the same time. However I noticed , even after disabling refresh, the documents get updated after 5 minutes automitically. 

The way I understand is refresh is equivalent to Lucene flush which would write to segments and make them searchable after 1 second. I also tried a huge interval of transaction logs flush. The settings are as under 
translog": { 
"flush_threshold_size": "1024mb", 
"sync_interval": "60m", 
"flush_threshold_period": "60m", 
"fs": { 
"type": "simple" 
}

Please let me know if this is achievable in Elasticsearch.
</description><key id="167317683">19569</key><summary>Elasticsearch refresh -1 to disable documents from being searchable doesnt work and updates documents after 5 mins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sufficetoentice</reporter><labels /><created>2016-07-25T09:00:35Z</created><updated>2016-07-25T12:56:48Z</updated><resolved>2016-07-25T12:56:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-25T12:56:48Z" id="234945309">It really isn't. See https://github.com/elastic/elasticsearch/pull/16028 for most of the conversation. The short version is that even if it tried fairly hard not to make documents visible unless the user told it to Elasticsearch would still make them visible when migrating a shard copy to another node.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Network: Allow to listen on virtual interfaces.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19568</link><project id="" key="" /><description>Previously when trying to listen on virtual interfaces during
bootstrap the application would stop working - the interface
couldn't be found by the NetworkUtils class.

The NetworkUtils utilize the underlying JDK NetworkInterface
class which, when asked to lookup by name only takes physical
interfaces into account, failing at virtual (or subinterfaces)
ones (returning null).

Note that when interating over all interfaces, both physical and
virtual ones are taken into account.

This changeset asks for all known interfaces, iterates over them
and matches on the given name as part of the loop, allowing it
to catch both physical and virtual interfaces.

As a result, elasticsearch can now also serve on virtual
interfaces.

A test case has been added which  makes sure that all
iterable interfaces can be found by their respective name.

Note that this PR is a second iteration over the previously
merged but later reverted #19537 because it causes tests
to fail when interfaces are down. The test has been modified
to take this into account now.

Closes #17473
Relates #19537
</description><key id="167289464">19568</key><summary>Network: Allow to listen on virtual interfaces.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">daschl</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v5.0.0-beta1</label><label>v6.0.0-alpha1</label></labels><created>2016-07-25T06:12:42Z</created><updated>2016-12-08T10:45:51Z</updated><resolved>2016-09-13T11:41:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-25T06:12:44Z" id="234848639">Can one of the admins verify this patch?
</comment><comment author="daschl" created="2016-07-25T06:14:03Z" id="234848796">@rmuir @jasontedor here's the updated PR with the test case being more restrictive to avoid failures on downed interfaces (and to that regard interfaces with no addresses assigned to them, since this is another case where the elastic code throws an exception)
</comment><comment author="daschl" created="2016-08-01T14:12:42Z" id="236591881">Just a quick note: I'm out the next two weeks so I can either fix it up when I'm back or feel free to amend and push if you want to get it in early :)
</comment><comment author="daschl" created="2016-08-22T07:47:47Z" id="241337269">@spinscale are there any other changes (other than the stream() conversion) that I should make in the next batch?
</comment><comment author="daschl" created="2016-09-13T13:52:15Z" id="246687982">👍 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkUtilsTests.java</file></files><comments><comment>Network: Allow to listen on virtual interfaces.</comment></comments></commit></commits></item><item><title>provide code example for processors setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19567</link><project id="" key="" /><description>This example is simple but was missing.
</description><key id="167269382">19567</key><summary>provide code example for processors setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels><label>docs</label></labels><created>2016-07-25T02:11:11Z</created><updated>2016-07-27T15:55:19Z</updated><resolved>2016-07-27T15:55:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T15:55:18Z" id="235631154">fixed and merged - thanks @geekpete 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>provide code example for processors setting</comment></comments></commit></commits></item><item><title>Adding _operation field to index, update, delete response.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19566</link><project id="" key="" /><description>Revival of #9736
Closes #9642
Closes #19267

I attempted to follow the steps suggested by @nik9000 in #9736. 
I was unsure of whether the IndexResponse#isCreated method and "created": field should be changed, as the version of ES is way past 2.0.0, when the original comment was written, so I left them.

Performing the bulk request shown in #19267 now results in the following:
`{"_index":"test","_type":"test","_id":"1","_version":1,"_operation":"create","forced_refresh":false,"_shards":{"total":2,"successful":1,"failed":0},"status":201}`
`{"_index":"test","_type":"test","_id":"1","_version":1,"_operation":"noop","forced_refresh":false,"_shards":{"total":2,"successful":1,"failed":0},"status":200}`
</description><key id="167210781">19566</key><summary>Adding _operation field to index, update, delete response.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">a2lin</reporter><labels><label>:CRUD</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-24T00:42:52Z</created><updated>2017-05-09T08:15:27Z</updated><resolved>2016-07-26T15:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-24T00:42:54Z" id="234749517">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-24T03:10:09Z" id="234754363">I've assigned it to myself because I plan on having a look, probably my Monday morning. If anyone else wants to review before me feel free.
</comment><comment author="nik9000" created="2016-07-25T13:56:38Z" id="234960087">@a2lin I left a lot of feedback, all of it minor. I'm happy to talk about any of the points I left - a few of them are "can you try making this mostly related change?" I'd personally try and start those in a separate commit and give up if the change starts to get out of hand. Those are more nice-to-haves.
</comment><comment author="nik9000" created="2016-07-25T13:57:08Z" id="234960237">@elasticmachine , test this.
</comment><comment author="a2lin" created="2016-07-26T07:38:54Z" id="235186517">@nik9000 Thanks so much for your detailed feedback! 

I've made the easier fixes to make (and fixed the broken unit test; oops). I think it's better to nuke the isFound / isCreated in a following PR, but let me know if you think I should include those commits into this one. Same thing for the UpdateHelper.Operation change.
</comment><comment author="nik9000" created="2016-07-26T13:03:43Z" id="235260552">&gt; I think it's better to nuke the isFound / isCreated in a following PR, but let me know if you think I should include those commits into this one. Same thing for the UpdateHelper.Operation change.

Sounds good to me.

I reviewed it one more time and it all looks great. I'm I'll have CI run on it one more time and if it passes I'll merge it.

@elasticmachine, test this again.
</comment><comment author="nik9000" created="2016-07-26T15:17:56Z" id="235301265">OK! All tests passed so I've merged! This is the first time I merged only after having run tests using the robot. Exciting!

Thanks for this! @a2lin!
</comment><comment author="nik9000" created="2016-07-26T15:34:14Z" id="235306752">And the elasticsearch robot didn't catch a build failure. I'm not sure why yet but I'll push a fix for it. Likely it is a failure caused by something changes since you forked this. I'll dig.
</comment><comment author="nik9000" created="2016-07-26T15:40:33Z" id="235308895">Yeah, looks like the robot didn't test your change on master, instead testing against you branch point. I think. I'll bring it up to the robots minders.

I pushed a182e356d3e49f35e1197ea4f8a84be2747ebf2c.
</comment><comment author="nik9000" created="2016-07-26T15:42:10Z" id="235309416">Maybe it was just a race. That test that it didn't catch was something I merged fairly recently. So maybe it just hadn't seen it.
</comment><comment author="a2lin" created="2016-07-26T15:44:31Z" id="235310217">Oh, oops :( I should have rebased against tip of master. Thanks!
</comment><comment author="nik9000" created="2016-07-26T15:46:16Z" id="235310816">&gt; Oh, oops :( I should have rebased against tip of master. Thanks!

No big deal. We all merge in different ways so we don't have a policy. And we're trying out the robot, learning about it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/test/java/org/elasticsearch/action/delete/DeleteResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexResponseTests.java</file></files><comments><comment>Remove deprecated created and found from index, delete and bulk (#25516)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>core/src/test/java/org/elasticsearch/action/IndicesRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkWithUpdatesIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/LegacyDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/DateMathIndexExpressionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexPrimaryRelocationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomIOExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/nested/SimpleNestedIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Remove isCreated and isFound from the Java API</comment></comments></commit></commits></item><item><title>Add "max_doc_count" to bucket aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19565</link><project id="" key="" /><description>The idea is to provide the opposite of `min_doc_count` (in some places it's `min_count`) to bucketed aggregations. The reasoning is similarly opposite: to help to find _missing_ data.

The simplest example is when using another data store, you often want to turn around and verify that you have all of the data (consistency check). Looking for _missing_ data is hard otherwise because you need to use a bucket selector, which means you need to use a script.

``` http
GET /test/_search
{
  "size": 0,
  "aggs": {
    "find_missing_ids": {
      "histogram": {
        "field": "numeric_id",
        "interval": 1,
        "min_doc_count": 0
      },
      "aggs": {
        "max_bucket_selector": {
          "bucket_selector": {
            "buckets_path": {
              "count": "_count"
            },
            "script": {
              "inline": "count == 0"
            }
          }
        }
      }
    }
  }
}
```

This does mean that there is a workaround to the feature, but it's much more verbose than `"max_doc_count": 0` and inherently slower. Note: `"min_doc_count": 0` was included just to be explicit -- that is currently the default.
</description><key id="167199093">19565</key><summary>Add "max_doc_count" to bucket aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Aggregations</label><label>discuss</label><label>enhancement</label></labels><created>2016-07-23T19:20:57Z</created><updated>2017-03-21T16:18:33Z</updated><resolved>2016-07-27T17:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T15:47:40Z" id="235628748">&gt; This does mean that there is a workaround to the feature, but it's much more verbose than "max_doc_count": 0 and inherently slower. 

Sure, but how frequent is this use case?  It sounds like a one off check that you might run once in a while.  I don't think we should add extra options for infrequent use cases, especially when the same result can already be achieved.
</comment><comment author="pickypg" created="2016-07-27T17:25:15Z" id="235657755">&gt; I don't think we should add extra options for infrequent use cases, especially when the same result can already be achieved.

It's a fair point, but I always find it weird when we offer a "min" without a "max", or vice versa. I suspect there are other use cases beyond verification, but I'm okay with just leaving this with the workaround above for now. If anyone drives by this with a different use case, then please feel free to add it here.
</comment><comment author="webmstr" created="2017-03-21T16:18:33Z" id="288132470">Here's a use case: I have documents that should appear in pairs.  I want to find when they *don't* appear in pairs.  So, to be able to use "max_doc_count" of 1 would give me just what I need with no additional processing required.

Scripting it might work in your sandbox, and might even work securely in ES5, but not everyone is there and it shouldn't be an excuse to ignore features (especially ones comparable to existing features).</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed QueryParsingException in multi match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19564</link><project id="" key="" /><description>Fixed QueryParsingException in multi match query when a wildcard expression results in no fields.
The query will now return 0 hits (null query) instead of throwing an exception. This matches the behavior if a nonexistent field is specified.
These changes were backported from latest master (mostly from #13405).

All test pass when I ran `mvn clean verify`

Closes #16098 
</description><key id="167171734">19564</key><summary>Fixed QueryParsingException in multi match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">qwerty4030</reporter><labels><label>:Query DSL</label><label>:Search</label><label>enhancement</label><label>review</label><label>v2.4.0</label></labels><created>2016-07-23T07:00:08Z</created><updated>2016-08-01T10:00:43Z</updated><resolved>2016-07-28T09:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-23T07:00:09Z" id="234703644">Can one of the admins verify this patch?
</comment><comment author="clintongormley" created="2016-07-27T15:43:35Z" id="235627421">@cbuescher could you review this please?
</comment><comment author="cbuescher" created="2016-07-28T08:44:24Z" id="235835728">@qwerty4030 thanks for backporting this fix, LGTM. As you mentioned, this is very close to how wildcards in fieldnames are handled on master currently.
I will run the ci checks one more time an merge the PR.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default transport and HTTP to Netty 4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19563</link><project id="" key="" /><description>This commit sets the default transport and HTTP implementation to use
transport-netty4.

Closes #19527
</description><key id="167164343">19563</key><summary>Default transport and HTTP to Netty 4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label></labels><created>2016-07-23T03:17:23Z</created><updated>2016-08-02T16:19:19Z</updated><resolved>2016-08-02T16:19:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-08-01T21:32:32Z" id="236714659">This is probably the right time to switch.
</comment><comment author="spinscale" created="2016-08-02T07:21:09Z" id="236822595">+1
</comment><comment author="s1monw" created="2016-08-02T07:53:54Z" id="236829725">I agree, I think we should do this for the next alpha or at least for the beta. +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/transport/src/main/java/org/elasticsearch/transport/client/PreBuiltTransportClient.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/Netty3Plugin.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/Netty4Plugin.java</file></files><comments><comment>Default transport and HTTP to Netty 4</comment></comments></commit></commits></item><item><title>Netty 4 improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19562</link><project id="" key="" /><description>This is a meta issue to track some known issues with the Netty 4 implementation:
- [x] port #19522 to Netty 4 #19874
- [ ] enable advanced leak detection in tests and fail the build if there is a leak #20398
- [x] upgrade to a non-snapshot build of 4.1.4.Final+ of Netty 4 #19689
- [x] set `io.netty.noUnsafe` to `true` by default? (probably) #19786
- [x] investigate performance
</description><key id="167163477">19562</key><summary>Netty 4 improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>Meta</label></labels><created>2016-07-23T02:54:31Z</created><updated>2016-09-17T09:33:19Z</updated><resolved>2016-09-09T09:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-08-03T19:30:51Z" id="237345752">&gt; investigate performance

I did some simple benchmarking with Rally, and the performance numbers are basically the same between Netty 3 and Netty 4. Now that Netty 4 is the default, we can also watch for this in the nightly benchmarks.
</comment><comment author="Scottmitch" created="2016-08-04T07:00:34Z" id="237469520">just curious are you using the [PooledByteBufAllocator](https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PooledByteBufAllocator.java)? Do you notice any differences in memory / GC profile?
</comment><comment author="s1monw" created="2016-09-09T08:59:09Z" id="245858547">@jasontedor can we have a dedicated issue for that last checkbox open here?
</comment><comment author="jasontedor" created="2016-09-09T09:11:24Z" id="245861268">&gt; can we have a dedicated issue for that last checkbox open here?

@s1monw I opened #20398.
</comment><comment author="s1monw" created="2016-09-09T09:11:42Z" id="245861331">w00t
</comment><comment author="jasontedor" created="2016-09-09T09:17:42Z" id="245862617">Closing since the last remaining item has a dedicated issue, #20398.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use `DefaultAWSCredentialsProviderChain` AWS SDK class for credentials</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19561</link><project id="" key="" /><description>Follow up discussion at https://github.com/elastic/elasticsearch/pull/18690#issuecomment-234505083

Reading the best practices [recommended by AWS](http://docs.aws.amazon.com/java-sdk/latest/developer-guide/credentials.html), we should use `DefaultAWSCredentialsProviderChain` instead of providing the detail of the chain ourselves.

For now, we read credentials (if not provided in `elasticsearch.yml`) using:

``` java
credentials = new AWSCredentialsProviderChain(
   new SystemPropertiesCredentialsProvider(),
   new EnvironmentVariableCredentialsProvider(),
   new InstanceProfileCredentialsProvider()
);
```

Which means that we read from:
- Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` (only recognized by Java SDK)
- Java System Properties - `aws.accessKeyId` and `aws.secretKey`
- Instance profile credentials delivered through the Amazon EC2 metadata service

Using instead:

``` java
credentials = new DefaultAWSCredentialsProviderChain();
```

Will give us two new more methods out of the box:

&gt; - Credential profiles file at the default location (`~/.aws/credentials`) shared by all AWS SDKs and the AWS CLI
&gt; - Credentials delivered through the Amazon EC2 container service if `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` environment variable is set and security manager has permission to access the variable

Cherry on the cake: as soon as AWS SDK will propose a new implementation, we will benefit from it without any modification (just updating the SDK).

We also simplify

```
new AWSCredentialsProviderChain(new StaticCredentialsProvider(new BasicAWSCredentials(key, secret)));
```

As there is no need to wrap StaticCredentialsProvider in AWSCredentialsProviderChain.

Closes #19556.
</description><key id="167151398">19561</key><summary>Use `DefaultAWSCredentialsProviderChain` AWS SDK class for credentials</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-22T23:30:01Z</created><updated>2016-07-28T15:56:23Z</updated><resolved>2016-07-28T15:55:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-23T10:21:08Z" id="234711045">@rjernst I think I find a better solution with this PR to test that we use the right part of the `if/else` code:

``` java
        if (key.isEmpty() &amp;&amp; secret.isEmpty()) {
            credentials = new DefaultAWSCredentialsProviderChain();
        } else {
            credentials = new StaticCredentialsProvider(new BasicAWSCredentials(key, secret));
        }
```

I'm basically testing here that we are getting same credentials as `DefaultAWSCredentialsProviderChain` would return.
I'm not testing the internal behavior of `DefaultAWSCredentialsProviderChain` as it's not the goal here.

WDYT?
</comment><comment author="dadoonet" created="2016-07-23T11:56:18Z" id="234714445">Well. Actually I found a much more easier way to achieve the goal without needing providing sys props...
</comment><comment author="ywelsch" created="2016-07-28T15:18:34Z" id="235927422">LGTM on the change. We should (not directly related to the change in this PR) also add tests that check the various credential providers and show what the user needs to do to make them work with the security manager.
</comment><comment author="dadoonet" created="2016-07-28T15:56:23Z" id="235939595">Thank you @ywelsch!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Extract AWS Key from KeyChain instead of using potential null value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19560</link><project id="" key="" /><description>While I was working on #18703, I discovered a bad behavior when people don't provide AWS key/secret as part as their `elasticsearch.yml` but rely on SysProps or env. variables...

In [`InternalAwsS3Service#getClient(...)`](https://github.com/elastic/elasticsearch/blob/d4366f8493ac8d2f7091404ffd346e4f3c0f9af9/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L76-L141), we have:

``` java
        Tuple&lt;String, String&gt; clientDescriptor = new Tuple&lt;&gt;(endpoint, account);
        AmazonS3Client client = clients.get(clientDescriptor);
```

But if people don't provide credentials, `account` is `null`.

Even if it actually could work, I think that we should use the `AWSCredentialsProvider` we create later on and extract from it the `AWS key` and then use it as the second value of the tuple.

Closes #19557.
</description><key id="167150156">19560</key><summary>Extract AWS Key from KeyChain instead of using potential null value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>enhancement</label><label>review</label><label>v5.0.0-beta1</label></labels><created>2016-07-22T23:18:16Z</created><updated>2016-08-11T12:31:17Z</updated><resolved>2016-08-04T15:58:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-22T23:18:40Z" id="234679627">@rjernst Could you look at this one please?
</comment><comment author="dadoonet" created="2016-08-04T14:27:31Z" id="237569752">If nobody objects, I'll merge this PR next week.
</comment><comment author="ywelsch" created="2016-08-04T15:27:32Z" id="237588794">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Fix reproducible HistogramTests.testEmptyWithExtendedBounds failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19559</link><project id="" key="" /><description>This fixes a reproducible failure in `HistogramTests.testEmptyWithExtendedBounds`` unit test. An unused array size is inconsistent with the computed expected result. The fix removes the unused array but maybe we can remove the whole test? I couldn't exactly follow everything it was trying to do and it looks like there might be other unused variables. Maybe it just needs to be fixed up?

closes #19558
</description><key id="167141909">19559</key><summary>[TEST] Fix reproducible HistogramTests.testEmptyWithExtendedBounds failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>review</label><label>test</label><label>v2.4.0</label></labels><created>2016-07-22T22:08:59Z</created><updated>2016-07-22T22:25:15Z</updated><resolved>2016-07-22T22:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-22T22:12:36Z" id="234669761">Removing the unused array LGTM. I'm not sure about removing the whole test because I'm not that familiar with that code.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reproducible HistogramTests.testEmptyWithExtendedBounds jenkins failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19558</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.4

**JVM version**: OpenJDK 

**OS version**: Ubuntu 16.04 (Jenkins Fedora)

**Description of the problem including expected versus actual behavior**:

This test failure reproduces. It looks like an inconsistency between computed array sizes and expected results.

**Steps to reproduce**:
1. Run maven command:

```
mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=837BA5E6FB749C36 -Dtests.class=org.elasticsearch.messy.tests.HistogramTests -Dtests.method="testEmptyWithExtendedBounds" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=cs-CZ -Dtests.timezone=UTC
```

**Provide logs (if relevant)**:

```
2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=837BA5E6FB749C36 -Dtests.class=org.elasticsearch.messy.tests.HistogramTests -Dtests.method="testEmptyWithExtendedBounds" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=cs-CZ -Dtests.timezone=UTC
ERROR   0.05s J2 | HistogramTests.testEmptyWithExtendedBounds &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.ArrayIndexOutOfBoundsException
   &gt;    at __randomizedtesting.SeedInfo.seed([837BA5E6FB749C36:9A82DD869F535877]:0)
   &gt;    at java.lang.System.arraycopy(Native Method)
   &gt;    at org.elasticsearch.messy.tests.HistogramTests.testEmptyWithExtendedBounds(HistogramTests.java:1005)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="167140223">19558</key><summary>Reproducible HistogramTests.testEmptyWithExtendedBounds jenkins failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>test</label></labels><created>2016-07-22T21:56:49Z</created><updated>2016-07-25T09:42:30Z</updated><resolved>2016-07-25T09:42:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-25T09:42:30Z" id="234908163">Closed by #19559
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Extract AWS Key from KeyChain instead of using potential null value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19557</link><project id="" key="" /><description>While I was working on #18703, I discovered a bad behavior when people don't provide AWS key/secret as part as their `elasticsearch.yml` but rely on SysProps or env. variables...

In [`InternalAwsS3Service#getClient(...)`](https://github.com/elastic/elasticsearch/blob/d4366f8493ac8d2f7091404ffd346e4f3c0f9af9/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L76-L141), we have:

``` java
        Tuple&lt;String, String&gt; clientDescriptor = new Tuple&lt;&gt;(endpoint, account);
        AmazonS3Client client = clients.get(clientDescriptor);
```

But if people don't provide credentials, `account` is `null`.

Even if it actually could work, I think that we should use the `AWSCredentialsProvider` we create later on and extract from it the `account` (AWS KEY actually) and then use it as the second value of the tuple.
</description><key id="167139555">19557</key><summary>Extract AWS Key from KeyChain instead of using potential null value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>non-issue</label></labels><created>2016-07-22T21:51:52Z</created><updated>2016-08-04T15:58:42Z</updated><resolved>2016-08-04T15:58:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AwsS3ServiceImplTests.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3RepositoryTests.java</file></files><comments><comment>Extract AWS Key from KeyChain instead of using potential null value</comment></comments></commit></commits></item><item><title>Use `DefaultAWSCredentialsProviderChain` AWS SDK class for credentials</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19556</link><project id="" key="" /><description>Follow up discussion at https://github.com/elastic/elasticsearch/pull/18690#issuecomment-234505083

Reading the best practices [recommended by AWS](http://docs.aws.amazon.com/java-sdk/latest/developer-guide/credentials.html), we should use `DefaultAWSCredentialsProviderChain` instead of providing the detail of the chain ourselves.

For now, we read credentials (if not provided in `elasticsearch.yml`) using:

``` java
credentials = new AWSCredentialsProviderChain(
   new SystemPropertiesCredentialsProvider(),
   new EnvironmentVariableCredentialsProvider(),
   new InstanceProfileCredentialsProvider()
);
```

Which means that we read from:
- Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` (only recognized by Java SDK)
- Java System Properties - `aws.accessKeyId` and `aws.secretKey`
- Instance profile credentials delivered through the Amazon EC2 metadata service

Using instead:

``` java
credentials = new DefaultAWSCredentialsProviderChain();
```

Will give us two new more methods out of the box:

&gt; - Credential profiles file at the default location (`~/.aws/credentials`) shared by all AWS SDKs and the AWS CLI
&gt; - Credentials delivered through the Amazon EC2 container service if `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` environment variable is set and security manager has permission to access the variable

Cherry on the cake: as soon as AWS SDK will propose a new implementation, we will benefit from it without any modification (just updating the SDK).
</description><key id="167133993">19556</key><summary>Use `DefaultAWSCredentialsProviderChain` AWS SDK class for credentials</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>enhancement</label></labels><created>2016-07-22T21:14:29Z</created><updated>2016-07-28T15:55:53Z</updated><resolved>2016-07-28T15:55:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file></files><comments><comment>Use `DefaultAWSCredentialsProviderChain` AWS SDK class for credentials</comment></comments></commit></commits></item><item><title>Should we move things into :client:transport?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19555</link><project id="" key="" /><description>Now that we have the :client:transport project there are two things I'd personally love to do:
1. Move the Builders into it. That way it is super obvious that they are part of the client and not the core. That was something that confused me as a new contributor.
2. Improve way you call modules from the Client. Right now you have to do funky stuff like `ReindexAction.INSTANCE.newRequestBuilder(client())`. It'd be nice if this were as easy as any other action. I think the dependencies are set up so we could do something like that.

Are these good things?
</description><key id="167131351">19555</key><summary>Should we move things into :client:transport?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-07-22T20:58:23Z</created><updated>2016-11-28T09:04:14Z</updated><resolved>2016-11-28T09:04:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-22T22:09:54Z" id="234669288">Now that I think about it moving the builders would be a reasonably large change too because NodeClient uses them. Though it looks like for the most part we don't actually use them in Elasticsearch, more in the tests.
</comment><comment author="javanna" created="2016-07-29T12:24:11Z" id="236166949">I think those are good things. This relates to #19055 as we want to try and reuse these classes in the high-level java REST client too.
</comment><comment author="javanna" created="2016-11-28T09:04:14Z" id="263217370">We recently discussed this as part of the planned high level REST client effort. Factoring out the java API builders into a common library is an option, though we are not yet sure how long it is going to take, so we should experiment with that a bit. The main point is that we want to get the high level client out there as soon as possible, hence we might just start with making it depend on core, which is not worse than what the transport client already does, and already much better than in the past as it doesn't depend on guice anymore. The bigger part is probably taking out the `toQuery` method of query builders, as that one depends on lucene. Closing for now as we don't need a separate issue for this, but we will take it into account as part of #19055.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Guard against negative result from FileStore.getUsableSpace when picking data path for a new shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19554</link><project id="" key="" /><description>A user in https://discuss.elastic.co/t/elasticsearch-with-amazon-elastic-file-system/55867/5 hit a `NullPointerException` in `ShardPath.selectNewPathForShard`, I think because `FileStore.getUsableSpace` returned `Long.MIN_VALUE`.

So I fixed `ShardPath` to `Math.max` the returned result with 0, and added a test case showing the NPE.
</description><key id="167111508">19554</key><summary>Guard against negative result from FileStore.getUsableSpace when picking data path for a new shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-22T19:06:24Z</created><updated>2016-09-15T22:30:13Z</updated><resolved>2016-07-30T00:26:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-22T19:15:27Z" id="234631451">LGTM
</comment><comment author="rmuir" created="2016-07-23T18:04:48Z" id="234731824">are we sure its correct to treat them as 0?

FileStore computes this method as: `return attrs.blockSize() * attrs.availableBlocks()`

both of those attributes are java `long`, but the values coming from statvfs() are `unsigned long` types on linux. So how can they be negative :)
</comment><comment author="rmuir" created="2016-07-23T18:08:24Z" id="234732001">Also i notice there are talks on the internet about EFS with names such as "how to store 8 exabytes and look good doing it". Perhaps it is a simple case of overflow?
</comment><comment author="mikemccand" created="2016-07-25T14:24:01Z" id="234967842">&gt; are we sure its correct to treat them as 0?

Well, for this `ShardPath` method in particular, since we are just using it to pick which `path.data` to use for this shard, I think using `0` instead is fine.

But in general, e.g. in `ESFileStore`, I don't think we should do that.

I could also fix this w/o futzing with the invalid usable space, by initializing `bestPath` to `paths[0]` or skip the loop entirely if there's only one `path.data` ... that would avoid this NPE.

Really this is all best-effort: the fact is, there is a bug-in-something (your JVM, your filesystem), if it's returning negative values from `FileStore.getUsableSpace`.
</comment><comment author="rmuir" created="2016-07-25T14:36:13Z" id="234971583">&gt; Well, for this ShardPath method in particular, since we are just using it to pick which path.data to use for this shard, I think using 0 instead is fine.
&gt; 
&gt; But in general, e.g. in ESFileStore, I don't think we should do that.
&gt; 
&gt; I could also fix this w/o futzing with the invalid usable space, by initializing bestPath to paths[0] or skip the loop entirely if there's only one path.data ... that would avoid this NPE.
&gt; 
&gt; Really this is all best-effort: the fact is, there is a bug-in-something (your JVM, your filesystem), if it's returning negative values from FileStore.getUsableSpace.

I don't think it works here. It does not fix the problem and it will only be encountered again, in other parts of the code. Setting it to 0 doesn't seem right, besides that, if free space can overflow, then other things like total space probably overflow too, and all the crazy math here (which we should not be doing, unless we use BigInteger) is probably going insane too. Looks like a big pile of bugs in ES here, too, to me!

Instead, we should fix the bugs. The first thing to do is to get the output of `df` for this filesystem so we can discuss it with the jdk developers, and figure out how they plan to fix the multiplication in FileStore. For example, if they plan to expose new methods that return BigInteger, versus attributes returning block size and number of blocks that should be treated as unsigned longs, or whatever. Depending on what they are doing, any workaround (if we should even have one), should be done in a way that will be easy to remove when its no longer necessary.

Until then, I am fine with NFS NPE'ing, kind of a bonus feature.
</comment><comment author="rmuir" created="2016-07-25T14:39:15Z" id="234972485">By the way, on every OS we support (linux, os X, and windows) the underlying OS values are unsigned longs. Negative values are not possible. So to me its no real mystery what happens, but the `df` output will just make the conversation easier so we can move forward to get it fixed.
</comment><comment author="mikemccand" created="2016-07-25T19:05:11Z" id="235052056">OK I've asked the user for `df` output in their case: https://discuss.elastic.co/t/elasticsearch-with-amazon-elastic-file-system/55867/8?u=mikemccand
</comment><comment author="mikemccand" created="2016-07-29T16:41:49Z" id="236230825">I removed the `feedback_needed` label: the user replied and indeed this is a bug in the JDK: https://bugs.openjdk.java.net/browse/JDK-8162520

In that issue they are leaning to having the `FileStore` APIs returning `long` just return `Long.MAX_VALUE` if they would otherwise overflow ... so I did that in this PR, when a negative value is returned.

I also upgraded the `ShardPath` logic to use `BigInteger` and made it a bit more defensive.

And I changed the test case to be even more unit-y, testing `ESFileStore` directly.

I think it's ready.
</comment><comment author="rmuir" created="2016-07-29T18:17:37Z" id="236254860">+1, this is the most reasonable solution for now.
</comment><comment author="nik9000" created="2016-07-29T18:20:27Z" id="236255556">Makes sense to me as well. Sorry for jumping the gun on this one.
</comment><comment author="nik9000" created="2016-07-29T21:15:01Z" id="236295403">LGTM
</comment><comment author="olegmyrk" created="2016-09-15T18:43:37Z" id="247415142">Do You plan to backport this fix also to 2.4? 
5.x is alpha and breaks plugin support so it is not an option atm.
</comment><comment author="mikemccand" created="2016-09-15T22:30:13Z" id="247473332">I agree it makes sense to backport this for a future 2.4.x release @olegmyrk; I'll open a PR.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/ESFileStore.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>core/src/test/java/org/elasticsearch/env/ESFileStoreTests.java</file></files><comments><comment>Merge pull request #19554 from mikemccand/negative_usable_space</comment></comments></commit></commits></item><item><title>[Bucket Selector] Add _bucket_count Special Path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19553</link><project id="" key="" /><description>It would be great if the `bucket_selector` could be used to exclude based on bucketed aggregation _bucket_ counts. For example, imagine this trivial situation where you don't want to show numbers without a total:

``` http
GET /test/_search
{
  "size": 0,
  "aggs": {
    "group_by_number": {
      "terms": {
        "field": "number"
      },
      "aggs": {
        "group_by_total": {
          "terms": {
            "field": "total"
          }
        },
        "min_bucket_selector": {
          "bucket_selector": {
            "buckets_path": {
              "count": "group_by_total._bucket_count"
            },
            "script": {
              "inline": "count != 0"
            }
          }
        }
      }
    }
  }
}
```

If `total` were a metric aggregation (e.g., `sum`), then it would be possible to use it. But as a bucketed aggregation, it makes it impossible to use to ignore parent buckets.

/cc @colings86
</description><key id="167102591">19553</key><summary>[Bucket Selector] Add _bucket_count Special Path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-22T18:17:41Z</created><updated>2016-07-26T09:29:27Z</updated><resolved>2016-07-26T09:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/InternalMultiBucketAggregation.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file></files><comments><comment>Add _bucket_count option to buckets_path</comment></comments></commit></commits></item><item><title>Should we rework strict parsing?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19552</link><project id="" key="" /><description>I was looking into failing the build when we accidentally use deprecated syntax in the docs (a fairly annoying problem to users) and I noticed (again) that we have `index.query.parse.strict`. It is supposed to fail any request that uses deprecated syntax. But we don't really test it so far as I can tell. We test the ParseField supports strict matching but not the setting that enables it from the outside.

So I played with the setting and I'm pretty sure it doesn't work at all now. At least, not in any way I expect it to work. You can't set the setting in elasticsearch.yaml. You can't set it in index settings. You can't set it on a url. This seems bad.

I'm not really sure what we should do about it too. Personally, I'd love to be able to control the setting on a request level and the cluster level. That'd be super useful in the docs because you could turn it on in elasticsearch.yaml and then fail all the deprecated requests unless they were marked as "deprecated" somehow. No more deprecated syntax sneaking into the docs.

But there is also the index level - we use it for things like "is it ok to use `off` instead of `false`?" Should the setting live at the index level too? If it does does it only affect mappings and settings on that level or does it affect all interactions with the index? What happens if you do a search across multiple indexes that have different values for the setting? This all feels like overkill for this setting.

To top it all off the way this setting flows around the code is kind of odd. We have `ParseFieldMatcher` which holds the setting and extracts it from a `Settings` instance. We aren't particularly careful with which `Settings` instance we build the `ParseFieldMatcher` using. Usually it is the global one but because we build `ParseFieldMatcher` a **ton** of times in the code it is difficult to tell for sure.
</description><key id="167101881">19552</key><summary>Should we rework strict parsing?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label></labels><created>2016-07-22T18:13:58Z</created><updated>2017-01-12T14:25:35Z</updated><resolved>2017-01-12T13:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-22T18:15:33Z" id="234616870">Relates to #17512 and #8963.
</comment><comment author="javanna" created="2016-07-27T15:14:20Z" id="235617753">I am afraid that we forgot to migrate `index.query.parse.strict` to the new settings infra. I think we should make it a blocker to fix that so that the setting can be provided. Renaming it to make it more generic should be considered too I think.
</comment><comment author="clintongormley" created="2016-07-27T15:17:46Z" id="235618881">@javanna should we even have this setting anymore? what does it do?

it looks like it was first added here https://github.com/elastic/elasticsearch/commit/bc5a9ca34290c871cdcfa92c39da70372c1d284a#diff-bb1674dc350eba2cee9dbef9525e22c5R120
</comment><comment author="clintongormley" created="2016-07-27T15:20:40Z" id="235619864">It seems to have a number of different uses, some of which should just be defaults (eg queries must end with `}`) but we could use it as a flat to upgrade deprecation warnings to exceptions.
</comment><comment author="javanna" created="2016-07-27T15:21:53Z" id="235620284">I thought that is the only chance we have to fail requests with deprecated syntax, or at least what `ParseField` uses. Unfortunately what it does is very different from what it should do at the moment (it got lost during the settings migration I believe).
</comment><comment author="javanna" created="2016-07-27T15:23:39Z" id="235620908">and the point in favour of renaming it is that we use `ParseField` in a lot more places than just query parsing. We should then discuss whether we want to be able to enable failing requests with deprecated syntax per request, like Nik proposed in this issue. But a setting seems the way to go to control it at the cluster level?
</comment><comment author="nik9000" created="2016-07-27T15:55:45Z" id="235631302">I think we should have _some_ setting that people can set to true in development that fails requests that have deprecated syntax. They can override it on the request level. They can set it to false in production so it never fails anything and they just catch these problems in dev (hopefully). I'd certainly have used it.

If this setting, or some version of it is the way to go then great. I'm a bit skeptical because I see us doing stuff like calling `DEPRECATION_LOGGER.deprecated` directly in places and this settings wouldn't catch it. I'd love to be able to set something in the thread context that just fails the request if you call `DEPRECATION_LOGGER.deprecated`. Or add something to the respond processing to turn the status code to 400 if you have any `Warning` headers. Or something. These are my first instincts though.

I don't know that it is that simple though as part of the problem with deprecated syntax is deprecated mappings and settings. The whole "override on the request" is much harder to do there.
</comment><comment author="nik9000" created="2016-07-28T16:23:36Z" id="235947594">Somewhat related discussion: https://github.com/elastic/elasticsearch/issues/19504#issuecomment-235941923
</comment><comment author="javanna" created="2016-08-04T11:14:50Z" id="237524578">Taking here some of the discussion we are having in #19504 , now that we always return deprecation warnings as response headers I tend to agree that we should consider removing the `index.query.parse.strict` setting and either remove the notion of failing a request if it contains deprecated syntax, or find another way to achieve that.  At the moment the strict parsing and deprecation warnings are too decoupled, one can easily use one without the other.  Strict parsing requires to use the `ParseFieldMatcher` which is a construct that I introduced but I never really liked, it is confusing and I would be more than happy to get rid of it.

I am really tempted to just clean stuff up first, then we can see whether we still need strict parsing and how we can implement it differently with the infra that we have now. Maybe we can simply piggy back on the deprecation logger as the central place for doing anything around deprecated syntax.

Good news is that it is not such a breaking change given that we never documented the setting, plus users cannot even set the setting I think at the moment :)
</comment><comment author="javanna" created="2016-08-12T10:22:34Z" id="239412616">We discussed this in FixItFriday. Everybody agreed that we should remove the `index.query.parse.strict` setting. This setting is undocumented and not settable at the moment within a node; the only place where it has some effect is in our unit tests (e.g. AbstractQueryTestCase) to make sure that we don't use any deprecated syntax in our tests and that we test deprecations separately. This part needs some replacement in our test infra but we don't need a setting for that.

Removing the setting would remove the need for `ParseFieldMatcher` (as well as `ParseFieldMatcherSupplier`) in our codebase and simplify our parsing code.

A separate discussion will need to happen later on whether we want to introduce a new setting to make a request fail in case deprecated syntax is used. Clients could already implement that based on warning response headers. We have to double check if there are deprecation warnings that don't get returned as response headers but only logged, and what should happen for those if any.
</comment><comment author="nik9000" created="2017-01-12T14:25:35Z" id="272175910">Hurray! Thanks for all the work @javanna!</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/ParseFieldMatcher.java</file><file>core/src/main/java/org/elasticsearch/common/ParseFieldMatcherSupplier.java</file></files><comments><comment>Remove ParseFieldMatcher and ParseFieldMatcherSupplier</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ip/IpRangeAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestionBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/LinearInterpolation.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/support/IncludeExcludeTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestUpdateByQueryAction.java</file></files><comments><comment>Remove some usages of ParseFieldMatcher in favour of using ParseField directly</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/ParseFieldRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/subphase/FetchSourceContext.java</file><file>core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/SmoothingModel.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>modules/aggs-matrix-stats/src/main/java/org/elasticsearch/search/aggregations/matrix/stats/MatrixStatsParser.java</file><file>modules/aggs-matrix-stats/src/main/java/org/elasticsearch/search/aggregations/support/MultiValuesSourceParser.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestUpdateByQueryAction.java</file></files><comments><comment>Remove some more usages of ParseFieldMatcher in favour of using ParseField directly</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchPhrasePrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchPhraseQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ParentIdQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/ScriptScoreFunctionBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/Laplace.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/StupidBackoff.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolateQueryBuilder.java</file></files><comments><comment>Remove some more usages of ParseFieldMatcher in favour of using ParseField directly</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketselector/BucketSelectorPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>modules/aggs-matrix-stats/src/main/java/org/elasticsearch/search/aggregations/support/MultiValuesSourceParser.java</file></files><comments><comment>Remove some usages of ParseFieldMatcher in favour of using ParseField directly</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/ParseField.java</file><file>core/src/main/java/org/elasticsearch/common/ParseFieldMatcher.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>core/src/test/java/org/elasticsearch/common/ParseFieldTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/ObjectParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/sort/GeoDistanceSortBuilderTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/TemplateQueryBuilderTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file></files><comments><comment>Remove support for strict parsing mode</comment></comments></commit></commits></item><item><title>Split regular histograms from date histograms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19551</link><project id="" key="" /><description>Currently both aggregations really share the same implementation. This commit
splits the implementations so that regular histograms can support decimal
intervals/offsets and compute correct buckets for negative decimal values.

However the response API is still the same. So for intance both regular
histograms and date histograms will produce an
`org.elasticsearch.search.aggregations.bucket.histogram.Histogram`
aggregation.

The optimization to compute an identifier of the rounded value and the
rounded value itself has been removed since it was only used by regular
histograms, which now do the rounding themselves instead of relying on the
Rounding abstraction.

Closes #8082
Closes #4847
</description><key id="167075099">19551</key><summary>Split regular histograms from date histograms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>v5.0.0-alpha5</label></labels><created>2016-07-22T15:47:16Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-08-03T06:43:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-26T16:15:24Z" id="235320469">I left a bunch of minor stuff. I love that InternalHistogram.Factory is going. And the new factory is well documented.
</comment><comment author="jpountz" created="2016-07-27T07:07:41Z" id="235504261">Thanks @nik9000 for having a look to this huge PR! Why don't you like AssertionError? I actually like it for branches that should never get visited, as a way to mean that an invariant has been violated and that the code is buggy. Also the fact that it is an error and not an exception makes it less likely to be swallowed since errors are not supposed to be caught. Finally we seem to already have a number of places where we already use assertion errors.
</comment><comment author="jpountz" created="2016-08-01T09:39:01Z" id="236534959">I pushed more commits to address comments. The only one I did not address is the naming convention for setters since we use it in all aggregations and it would probably need to be a bigger discussion.
</comment><comment author="nik9000" created="2016-08-02T12:52:22Z" id="236894516">LGTM.
</comment><comment author="jpountz" created="2016-08-03T06:43:48Z" id="237155911">Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardCount.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilders.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteableRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/LocalShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/SearchPlugin.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestTasksAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/Histogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/package-info.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TaskManagerTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/builders/AbstractShapeBuilderTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/NamedWriteableRegistryTests.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/IndexFolderUpgraderTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingDisabledTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryParseContextTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/search/DocValueFormatTests.java</file><file>core/src/test/java/org/elasticsearch/search/SearchModuleTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBoundsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/package-info.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/profile/aggregation/AggregationProfilerIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/searchafter/SearchAfterBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/slice/SliceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SortBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/AbstractSuggestionBuilderTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/SuggestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/WritableTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/phrase/SmoothingModelTestCase.java</file><file>core/src/test/java/org/elasticsearch/tasks/PersistedTaskInfoTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TransportServiceHandshakeTests.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/Netty3SizeHeaderFrameDecoderTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3ScheduledPingTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/SimpleNetty3TransportTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4ScheduledPingTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4SizeHeaderFrameDecoderTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/NettyTransportMultiPortTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/SimpleNetty4TransportTests.java</file><file>test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/OldIndexUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>test/framework/src/test/java/org/elasticsearch/transport/MockTcpTransportTests.java</file></files><comments><comment>Merge branch 'master' into netty4</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/Histogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBoundsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java</file><file>core/src/test/java/org/elasticsearch/search/profile/aggregation/AggregationProfilerIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file></files><comments><comment>Split regular histograms from date histograms. #19551</comment></comments></commit></commits></item><item><title>Change how `nested` and `reverse_nested` aggs know about their nested depth level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19550</link><project id="" key="" /><description>Before the aggregation tree was traversed to figure out what the parent level is, this commit
changes that by using `NestedScope` to figure out the nested depth level. 

The big upsides are that this cleans up `NestedAggregator` (it used a hack to lazily figure out the nested parent filter) and this is also what `nested` query uses and therefor the `nested` query can be included inside `nested` aggregation and work correctly.

PR for #11749 and #12410
</description><key id="167070707">19550</key><summary>Change how `nested` and `reverse_nested` aggs know about their nested depth level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-22T15:27:25Z</created><updated>2016-07-26T07:21:31Z</updated><resolved>2016-07-26T07:21:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-25T10:06:36Z" id="234913483">I left a question but otherwise I like the removal of the lazy initialization of these aggs!
</comment><comment author="martijnvg" created="2016-07-25T11:30:49Z" id="234928895">Thanks for checking this out @jpountz!
I have updated the PR.
</comment><comment author="jpountz" created="2016-07-26T06:40:16Z" id="235176390">LGTM. Thanks @martijnvg for cleaning this up!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deleting data directory while elasticsearch is running puts elasticsearch in a frozen state (RED)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19549</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**:  1.7.0_80-b15

**OS version**: ubuntu 16.04 LTS

**Description of the problem including expected versus actual behavior**:
Deleting data directory while elasticsearch is running puts elasticsearch in a frozen state (RED) and even you restart elasticsearch it is still in a frozen state (RED). 
P.S: I know that it is not good to delete data directory. I am using an embedded server and has configured data directory to be deleted on maven clean command. It was by accident that I came across this issue.

**Steps to reproduce**:
1. run elasticsearch 
2. delete elasticsearch data directory while elasticsearch is running
3. restart elasticsearch
</description><key id="167062990">19549</key><summary>Deleting data directory while elasticsearch is running puts elasticsearch in a frozen state (RED)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rajind</reporter><labels /><created>2016-07-22T14:52:34Z</created><updated>2016-08-09T03:23:53Z</updated><resolved>2016-07-22T15:18:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-22T15:18:12Z" id="234572488">Its not clear what you mean by frozen state?  RED means the cluster can't handle any write operations because primary shards are missing.  If you delete your data directory, this will naturally be the case.  

Closing as this is not a bug.
</comment><comment author="imotov" created="2016-07-22T18:32:06Z" id="234621158">@Rajind I wasn't able to reproduce this issue the way you described it. When I wipe out entire data directory and restart the node, I get a clean cluster in GREEN state. You are probably wiping out the index directory not entire data directory, which causes elasticsearch to think that index should exist when it actually doesn't, therefore shards are missing and as a result you get RED state. The solution here would be to delete the entire data directory when server is stopped and then start it again.
</comment><comment author="Rajind" created="2016-07-25T04:14:46Z" id="234832624">@imotov: Thanks for the reply. I assume you tried deleting the data directory while elasticsearch is running. I am deleting the directory which is given to the elasticsearch configuration as "path.data". Is is same as index directory ? If not where would be the index directory when running elasticsearch embedded ? 

The scenario I saw is that first when I delete data directory when elasticsearch is running the folder is empty. but when elasticsearch shuts down it writes some files to that dir and probably those files might be related to the index i suppose. And when I start elasticsearch again the index is there but no data. So recovery fails. 
</comment><comment author="imotov" created="2016-07-28T01:23:23Z" id="235770763">&gt; I am deleting the directory which is given to the elasticsearch configuration as "path.data". Is is same as index directory ?

Yes, it is the same directory. I just tried it with non-default data directory and couldn't reproduce it either.

&gt; The scenario I saw is that first when I delete data directory when elasticsearch is running the folder is empty. but when elasticsearch shuts down it writes some files to that dir and probably those files might be related to the index i suppose. 

When you delete the data directory while elasticsearch is running it doesn't actually recreate this directory. So, when you start elasticsearch again it creates an empty directory and starts fine. You can download elasticsearch and try to reproduce it yourself. You will see what I mean. 
</comment><comment author="Rajind" created="2016-08-01T08:31:41Z" id="236520235">@imotov I still experience the same scenario. Do you think what I observe has something to do with BulkProcessor trying to publish while the data directory is deleted ? 
</comment><comment author="imotov" created="2016-08-08T00:14:42Z" id="238118316">@Rajind did you try reproducing it outside of your application with non-embedded mode? When you run elasticsearch in an embedded mode a lot of things can happen inside your application that are very hard to reproduce for us unless we know what exactly your application is doing. So, unless you can help us reproducing it without your application code involved, I am not really sure we can do something about it.
</comment><comment author="Rajind" created="2016-08-09T03:23:53Z" id="238443344">@imotov I will try to reproduce it with a sample code with an standalone node and get back to you. Thanks. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to index to an alias pointing to multiple indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19548</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Ability to index to an alias which points to multiple indices.  It would be good to be able to define (either with a flag, or perhaps with a 'priority' field) to which index documents should be written.  This would allow an application to point to an alias for both search and index queries and have the target index to which documents will actually be indexed configurable in the cluster.
</description><key id="167007447">19548</key><summary>Ability to index to an alias pointing to multiple indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>enhancement</label></labels><created>2016-07-22T09:36:43Z</created><updated>2016-07-27T14:33:30Z</updated><resolved>2016-07-27T14:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T14:33:30Z" id="235604085">This has been discussed previously (although I can't find the issue).  Use one alias (which points to one index) for CRUD, and another alias (which can point to one or more indices) for searching.  No need for more complex alias configuration.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multiple fields provided to queries return results for the last one only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19547</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.4

**JVM version**: 1.8

**OS version**: OSX 10.11

**Description of the problem including expected versus actual behavior**:

There is a possibility to provide multiple fields to queries like so:

``` json
{
  "query": {
    "range": {
      "age": {
        "gte": 30,
        "lte": 40
      },
      "price": {
        "gte": 10,
        "lte": 30
      }
    }
  }
}
```

Here I provide two fields to a single query and elasticsearch provides legitimate results for such a query. However, the results returned always only include hits from the last field request, that is, if I were to execute the query given above, I would only get results from the `price` field. Moreover I could not find any mentions of such behaviour in the official docs. 
Analogous behaviour was observed with `prefix`, `regexp`, `geo_distance` queries and more.
Logically this should either provide the hits for both provided fields or not work at all.
</description><key id="166998819">19547</key><summary>Multiple fields provided to queries return results for the last one only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">einorler</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-07-22T08:49:51Z</created><updated>2016-08-05T13:53:35Z</updated><resolved>2016-08-05T13:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-22T10:11:08Z" id="234507315">The range query supports one field, like most of the elasticsearch queries. If you need to execute two range queries you need to combine them using a bool query and two must clauses that contain one range query each.

I don't see the range query supporting multiple fields in the future. What we could do though is throw an error instead of accepting such a query, which is effectively malformed. Otherwise we make users think that we support this syntax although we don't. I am a bit on the fence about this  though cause that would mean adding this "is field already set" type of check potentially in a lot of places. The "keep the last one" behaviour comes with the way we pull parse json and this same situation can happen in many other places in our codebase.
</comment><comment author="jpountz" created="2016-07-22T21:59:40Z" id="234667507">Maybe there are things to do to make it less likely to have this error in the future (we already had a lot of similar bugs reported, and there will probably be other ones), but I think it is important to fail on bad inputs.
</comment><comment author="clintongormley" created="2016-07-27T15:26:02Z" id="235621654">This query doesn't throw an exception in 5.0, and it should.
</comment><comment author="javanna" created="2016-07-27T15:32:46Z" id="235623951">&gt; This query doesn't throw an exception in 5.0, and it should.

Yes it should. But we have this problem in each single query of our DSL. We can fix it here and forget about all the other places where we have this problem, or maybe see what we can do to fix it in more places (preferably everywhere). I have this specific fix in one of my branches. I just felt bad about fixing this single case and wondered if we could do something better other than going through each single query one by one. That is why I had marked discuss, sorry for the confusion.
</comment><comment author="clintongormley" created="2016-07-27T15:34:24Z" id="235624501">+1 for a more comprehensive fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java</file></files><comments><comment>Throw parsing error if range query contains multiple fields</comment></comments></commit></commits></item><item><title>Create a percentile_ranks_bucket pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19546</link><project id="" key="" /><description>We have a [`percentiles_bucket`](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-pipeline-percentiles-bucket-aggregation.html) pipeline aggregation but we currently don't have the reverse operation `percentile_ranks_bucket` which would be the pipeline aggregation equivalent to the `percentile_ranks` aggregation.
</description><key id="166994481">19546</key><summary>Create a percentile_ranks_bucket pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label></labels><created>2016-07-22T08:24:17Z</created><updated>2016-11-17T09:42:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Makthum" created="2016-11-16T20:31:41Z" id="261062935">Hi,

I am aware this functionality is not implemented yet, but is there a way i can accomplish percentile_rank over a bucket aggregation ?
</comment><comment author="colings86" created="2016-11-17T09:42:21Z" id="261200326">Not currently, you would need to do this calculation in your client application for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>No other processors should be executed after on_failure is called</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19545</link><project id="" key="" /><description>**Huge Fix**

After debugging some use-cases with @BigFunger, we realized some unintended behavior leaked into the product.

not sure how this got through, but `on_failure` processors should be the end-all processors. The original pipeline should not be run. This was never the intention.
</description><key id="166918592">19545</key><summary>No other processors should be executed after on_failure is called</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-21T21:10:22Z</created><updated>2016-07-26T11:43:17Z</updated><resolved>2016-07-21T21:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-21T21:17:32Z" id="234386955">Wow, a big side effect of one forgotten `break` statement...
Good catch, too bad we didn't have tests with pipelines have additional processors after on failure and verifying that these processors are not executed.

LGTM!
</comment><comment author="talevy" created="2016-07-21T21:20:31Z" id="234387694">yeah, all our tests only verify that the processors that did run, ran well. We never checked the effects of non-running processors
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/CompoundProcessor.java</file><file>core/src/test/java/org/elasticsearch/ingest/CompoundProcessorTests.java</file></files><comments><comment>fix: no other processors should be executed after on_failure is called in a compound processor (#19545)</comment></comments></commit></commits></item><item><title>Garbage Collector clearing less and less space.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19544</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.3 - 2.3.3

**JVM version**: "1.8.0_72"

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

Running ES on a 5 node cluster cluster where each node has 15GB of RAM. The max heap size is set to 7GB on each server. Using plugins searchguard-ssl, stats, and head.

![elasticsearchgc](https://cloud.githubusercontent.com/assets/1820847/17036297/891bb588-4f40-11e6-85db-17f6cd25bf11.png)

This is a screenshot of the heap usage on each node over a 10 day period. You can see the normal saw-tooth pattern, but the bottom of each cycle is rising. It seems that the garbage collector frees less and less memory every time that it runs. I was having a similar issue on ES 1.7.3 and hoped that bumping to 2.3.3 would help to alleviate, but no such luck.

Once the servers get up to ~6GB of heap used for ES we do a rolling restart of the cluster which restarts the whole cycle.

Let me know if there's any more information that might be useful.

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="166903396">19544</key><summary>Garbage Collector clearing less and less space.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dopey</reporter><labels /><created>2016-07-21T19:53:22Z</created><updated>2016-07-21T22:26:09Z</updated><resolved>2016-07-21T20:26:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-21T20:26:25Z" id="234373251">Have a look at https://www.elastic.co/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html . Usually shapes like this come from fielddata which is a known defficiency in Elasticsearch that we've mostly migrated away from. It isn't the default for indexes created in 2.0 but if you migrated any indexes from 1.x to 2.x you'll still have indexes that use it.

Because this isn't a _new_ bug it is probably best discussed on https://discuss.elastic.co/ . If we can pin it down to a new bug then it makes sense to reopen here.
</comment><comment author="dopey" created="2016-07-21T22:26:09Z" id="234402945">Thanks for the reply! Posted an update discussion topic on the forum. -&gt; https://discuss.elastic.co/t/elasticsearch-garbage-collection-issue/56143
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent handling of constant_score.filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19543</link><project id="" key="" /><description>**Elasticsearch version**:2.3.4

**JVM version**:1.7 latest

**OS version**:Win10

**Description of the problem including expected versus actual behavior**:
While bool query "filter" accepts array of queries and use AND to combine them,
constant_score and filtered query "filter" element also takes array of queries without exception but result seem to be non deterministic - neither "OR" no "AND" not even sure what.

Example - following query should return 0 hits but it returns number of hits one of the queries would produce and not even consistently first or second

```
{
  "query": {
    "constant_score": {
      "filter": [
        {
          "terms": {
            "date.fy": [
              2016
            ]
          }
        },
        {
          "terms": {
            "date.fy": [
              2015
            ]
          }
        }
      ]
    }
  }
}
```

It would be very nice to have it support array of queries like bool.filter does or at least thro exception if it is considered invalid

Also in case of bool.filter (and bool.must), use of array is not clearly documented https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html

</description><key id="166900473">19543</key><summary>Inconsistent handling of constant_score.filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2016-07-21T19:38:13Z</created><updated>2017-06-05T15:23:51Z</updated><resolved>2017-06-05T15:23:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-22T08:44:24Z" id="234488242">This is a bug indeed, `constant_score` should reject arrays.
</comment><comment author="roytmana" created="2016-07-22T12:02:37Z" id="234526198">Filtered query has the same issue.
I wonder if making it treat array same as boolean.filter would be less breaking than growing exception
</comment><comment author="clintongormley" created="2016-07-27T14:38:56Z" id="235605854">Note: filters are deprecated and have been removed in 5.0.  Also, this bug has been fixed in 5.0
</comment><comment author="colings86" created="2017-06-05T15:23:51Z" id="306217045">Closing since this is fixed in 5.0+</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>improve painless instanceof tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19542</link><project id="" key="" /><description>We need to improve ComparisonTests to also test the case where variable is `def` and `null`, and make sure it does the right thing: there may be bugs.

For example: `x instanceof Number`. 
</description><key id="166888779">19542</key><summary>improve painless instanceof tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label></labels><created>2016-07-21T18:38:38Z</created><updated>2017-03-30T03:05:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2017-03-13T06:14:07Z" id="286023446">Sure! Enjoy.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>update_by_query/10_basic/wait_for_completion=false fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19541</link><project id="" key="" /><description>`update_by_query/10_basic/wait_for_completion=false fails` failed quite a few times recently in our 2.4 branch.

Last failure is here: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.4+multijob-os-compatibility/os=sles/43/console .

Seems like the tasks.get call that waits for completion gets back a node failure, which indicates that the node timed out:

```
Suite: org.elasticsearch.index.reindex.ReindexRestIT
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.module:reindex -Dtests.seed=76760A523E13F0FB -Dtests.class=org.elasticsearch.index.reindex.ReindexRestIT -Dtests.method="test {yaml=update_by_query/10_basic/wait_for_completion=false}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.cluster=localhost:9500 -Dtests.rest.suite=reindex,update_by_query -Dtests.locale=el -Dtests.timezone=America/Fortaleza
FAILURE 30.2s | ReindexRestIT.test {yaml=update_by_query/10_basic/wait_for_completion=false} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: field [node_failures] has a true value but it shouldn't
   &gt; Expected: ("" or equalToIgnoringCase("false") or "0")
   &gt;      but: was "[{type=failed_node_exception, reason=Failed node [BNvZfzA9SdCXzLhGhiIclA], caused_by={type=timeout_exception, reason=Timed out waiting for completion of [org.elasticsearch.index.reindex.BulkByScrollTask@265f94f8]}}]"
   &gt;    at __randomizedtesting.SeedInfo.seed([76760A523E13F0FB:FE22358890EF9D03]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.rest.section.IsFalseAssertion.doAssert(IsFalseAssertion.java:51)
   &gt;    at org.elasticsearch.test.rest.section.Assertion.execute(Assertion.java:69)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:375)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="166855339">19541</key><summary>update_by_query/10_basic/wait_for_completion=false fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label></labels><created>2016-07-21T15:55:36Z</created><updated>2016-07-21T18:15:33Z</updated><resolved>2016-07-21T18:14:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-21T15:56:25Z" id="234298899">hey @nik9000 I think you did #19453 while hunting this. The last failure has more info that might interest you.
</comment><comment author="nik9000" created="2016-07-21T15:57:05Z" id="234299092">Indeed. Thanks for pointing me to it!
</comment><comment author="nik9000" created="2016-07-21T17:27:55Z" id="234324572">Luckily I've been able to get this to fail locally with:

```
while mvn verify -Dskip.unit.tests -pl modules/reindex/ -Pdev -Dtests.seed=76760A523E13F0FB -Des.logger.level=info -Dtests.output=always; do echo not yet; done
```
</comment><comment author="nik9000" created="2016-07-21T17:28:10Z" id="234324633">Not every iteration, but after a few minutes.
</comment><comment author="nik9000" created="2016-07-21T18:14:00Z" id="234337513">There is a race condition in 2.x when you rethrottle tasks. Usually speeding up a task will take effect immediately. If you hit it at the right time then speeding up the task will wait until the next batch. This isn't a big deal in real life because you can resend the rethrottle command if you hit the race condition. You probably won't hit it anyway because you'd have to send it in a fairly small window. And it wouldn't matter that much for normal throttle values because you just wait an extra batch.

But in testing we schedule the task to finish 3000 years in the future. And we hit the race condition relatively frequently because we rethrottle immediately after sending the request.

In master this was fixed by 5b94c4a25b8a6322de573d5d9923d5838db748ec in master. Backporting that isn't simple because the implementations have drifted too far apart. So instead I basically silenced the test:
01bf02e56aad670430e0ee808f47eaffad2d0a13 - improve logging
4e2971710dce7a0648f15804bb4ddab8cd5ef09b - tighten the assertions and explain a race condition
</comment><comment author="nik9000" created="2016-07-21T18:15:33Z" id="234337952">I'm going to look at forward porting 01bf02e for nicer logging in master. I'll open a PR about it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes the ActiveShardsObserverIT tests with short timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19540</link><project id="" key="" /><description>Fixes the ActiveShardsObserverIT tests that have a very short index 
creation timeout so they process the index creation cluster state update
before the test finishes and attempts to cleanup. Otherwise, the index
creation cluster state update could be processed after the test finishes
and cleans up, thereby leaking an index in the cluster state that could
cause issues for other tests that wouldn't expect the index to exist.

Closes #19530
</description><key id="166848083">19540</key><summary>Fixes the ActiveShardsObserverIT tests with short timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>review</label><label>test</label></labels><created>2016-07-21T15:26:54Z</created><updated>2016-07-21T15:47:22Z</updated><resolved>2016-07-21T15:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-21T15:44:02Z" id="234294872">LGTM. This kind of simple test fixes don't need to go through review. Just fix and push (and let CI complain if you got things wrong)
</comment><comment author="abeyad" created="2016-07-21T15:46:54Z" id="234295897">thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardsObserverIT.java</file></files><comments><comment>Fixes the ActiveShardsObserverIT tests that have a very short index (#19540)</comment></comments></commit></commits></item><item><title>Bucket aggregation for names in named queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19539</link><project id="" key="" /><description>If naming query clauses is useful for adding context to individual top hits it is arguably also useful context for reviewing larger collections of results in aggregations.

My assumption is that the names of the clauses that matched each doc accompany it as it is collected so that they can be displayed in top hits. If correct, this means a new bucket agg could route docs into buckets based on these query names. I don't think this would require any special parameters in the DSL so would look something like this:

```
GET ofac/_search
{
   "query": {
      "bool": {
         "should": [
            {
               "match": {
                  "_all": {
                     "query": "chicken turkey",
                     "_name": "poultry"
                  }
               }
            },
            {
               "match": {
                  "_all": {
                     "query": "pork beef",
                     "_name": "meat"
                  }
               }
            },
            {
               "match": {
                  "_all": {
                     "query": "spaghetti penne linguine",
                     "_name": "pasta"
                  }
               }
            }

         ]
      }
   },
   "size": 0,
   "aggs": {
      "clauses": {
         "named_queries": {
         },
         "aggs": {
             ....
         }
      }
   }
}
```

The results would show buckets keyed with the names `poultry` and `meat` etc.
An interesting twist is that a named_queries agg could be nested under another named_queries agg and the results effectively represent a weighted graph showing if poultry or meat was a more common ingredient with pasta dishes.
</description><key id="166844988">19539</key><summary>Bucket aggregation for names in named queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2016-07-21T15:14:50Z</created><updated>2016-07-27T14:03:18Z</updated><resolved>2016-07-27T14:03:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-27T12:27:01Z" id="235569863">&gt; My assumption is that the names of the clauses that matched each doc accompany it as it is collected so that they can be displayed in top hits. If correct, this means a new bucket agg could route docs into buckets based on these query names. I don't think this would require any special parameters in the DSL so would look something like this:

The named queries are applied during the fetch phase on the top documents only. They are not extracted during the search phase since it's cheaper to replay the named queries on the top documents as long as the number of top hits is small. 

&gt; An interesting twist is that a named_queries agg could be nested under another named_queries agg and the results effectively represent a weighted graph showing if poultry or meat was a more common ingredient with pasta dishes.

Are `filters` aggs not enough for this use case ?

```
{
   "aggs": {
      "clauses": {
         "filters": {
            "filters": {
               "poultry": {
                  "term": {
                     "body": "chicken turkey"
                  }
               },
               "meat": {
                  "term": {
                     "body": "spaghetti penne linguine"
                  }
               }
            }
         },
         "aggs": {
            "cooccurrence": {
               "filters": {
                  "filters": {
                     "pasta": {
                        "term": {
                           "body": "spaghetti penne linguine"
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```
</comment><comment author="markharwood" created="2016-07-27T14:03:18Z" id="235594723">&gt; The named queries are applied during the fetch phase on the top documents only

Ah - thanks, @jimferenczi that clears up some of the mystery on how the names appear :)

&gt; Are filters aggs not enough for this use case ?

Yes, but I had assumed that the names were context in the search phase so offering them in aggs felt like a cheap by-product of running an existing query - so you could run a search for the top N docs and also get some overview of how your search concepts inter-relate.
I can see now that this isn't query data that is cheaply recycled for use in aggs so will close the issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>On full cluster restart, shards are not recovering and remaining in Unassigned state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19538</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.x

**JVM version**: 1.7 and 1.8

**OS version**: Ubuntu 14.05

**Description of the problem including expected versus actual behavior**:
**TL;DR** If I have more replicas than my data-nodes while restarting the cluster then shards are not recovering. 

I have cluster of 8 nodes ( 3 master + 5 data) and configured replica as 4 for each and every index in order to keep all shards in each and every node(can sustain max of 4 data nodes failure).  Now in process of full cluster restart, I've shutdown all data + master nodes. I started restarting cluster by making master nodes up first then data nodes 1 by one. As I had replica 4, my cluster should have recovered all indices from data node-1 when I restarted it.  But it didn't happened, cluster state was _red_, and all shards were in unassigned state(I didn't find any thing in logs like why shards recovering is not happening). Instead when I changed _replication_ to &lt;=1, all shards are started recovering and finally my cluster state went to _yellow_ .  Can you please let me know if I'm missing any configuration here.

**Steps to reproduce**:
1. Start a cluster of 1 master + 1 data node
2. Create index with 1 shard with replica 1( Now cluster state will become to **Yellow**)
3. Stop elasticsearch in data node - cluster state **Red**
4. Start elasticsearch in data node - cluster state **Yellow** (_Recovered successfully_)
5. Now increase replica to 2 - cluster state **Yellow**
6. Stop elasticsearch in data node - cluster state **Red**
7. Start elasticsearch in data node - cluster state **Red** . Now index is not getting recovered and all shards remaining in state _unassigned_  As replica(2) is more than available data-nodes(1).
8.  Now decrease the replica to 1 or 0 (&lt;= available data nodes in cluster(in this case 1)), and index is recovered immediately.
</description><key id="166837548">19538</key><summary>On full cluster restart, shards are not recovering and remaining in Unassigned state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">venkatreddyamalla</reporter><labels /><created>2016-07-21T14:46:00Z</created><updated>2016-07-22T19:37:21Z</updated><resolved>2016-07-21T14:59:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-21T14:59:04Z" id="234280785">Hi @venkatreddyamalla 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="venkatreddyamalla" created="2016-07-21T18:11:16Z" id="234336702">Hi @clintongormley 
Thanks for your suggestion. I suspected it is a bug in elasticsearch and posted here but I'm wondering like how it was not qualified as bug. Could you please shed some light on this weird behaviour if you are already aware of this?
</comment><comment author="bleskes" created="2016-07-22T19:12:20Z" id="234630764">This is a known  and documented behavior (which doesn't means it's good). It was fixed with https://github.com/elastic/elasticsearch/issues/14739 and was explained in several issues around it, see for example https://github.com/elastic/elasticsearch/issues/18393#issuecomment-219671552
</comment><comment author="venkatreddyamalla" created="2016-07-22T19:37:21Z" id="234636400">Thanks a lot @bleskes for pointers. It is really helpful than simple template message. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to listen on virtual interfaces.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19537</link><project id="" key="" /><description>Previously when trying to listen on virtual interfaces during
bootstrap the application would stop working - the interface
couldn't be found by the NetworkUtils class.

The NetworkUtils utilize the underlying JDK NetworkInterface
class which, when asked to lookup by name only takes physical
interfaces into account, failing at virtual (or subinterfaces)
ones (returning null).

Note that when interating over all interfaces, both physical and
virtual ones are taken into account.

This changeset asks for all known interfaces, iterates over them
and matches on the given name as part of the loop, allowing it
to catch both physical and virtual interfaces.

As a result, elasticsearch can now also serve on virtual
interfaces.

A test case has been added which at least makes sure that all
iterable interfaces can be found by their respective name. (It's
not easily possible in a unit test to "fake" virtual interfaces).

Closes #17473
</description><key id="166829057">19537</key><summary>Allow to listen on virtual interfaces.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daschl</reporter><labels><label>:Network</label><label>bug</label><label>review</label></labels><created>2016-07-21T14:11:25Z</created><updated>2016-07-26T12:56:19Z</updated><resolved>2016-07-22T16:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticmachine" created="2016-07-21T14:11:26Z" id="234265467">Can one of the admins verify this patch?
</comment><comment author="daschl" created="2016-07-21T14:12:02Z" id="234265667">I signed the CLA before sending this PR, maybe it didn't go "through" yet.
</comment><comment author="daschl" created="2016-07-21T14:18:31Z" id="234267686">In case someone wants to verify, you can easily create a virtual interface on linux like:

```
sudo ifconfig eth0:1 10.2.16
```

Now in the config set the `network.host` to `_eth0:1_`. 

Starting 2.3.4 prints

```
Exception in thread "main" java.lang.IllegalArgumentException: No interface named 'eth0:1' found, got [name:lo (lo), name:eth0 (eth0), name:eth0:1 (eth0:1), name:eth1 (eth1)]
    at org.elasticsearch.common.network.NetworkUtils.getAddressesForInterface(NetworkUtils.java:232)
    at org.elasticsearch.common.network.NetworkService.resolveInternal(NetworkService.java:262)
    at org.elasticsearch.common.network.NetworkService.resolveInetAddresses(NetworkService.java:209)
    at org.elasticsearch.common.network.NetworkService.resolveBindHostAddresses(NetworkService.java:122)
    at org.elasticsearch.transport.netty.NettyTransport.bindServerBootstrap(NettyTransport.java:424)
    at org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:321)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
    at org.elasticsearch.transport.TransportService.doStart(TransportService.java:182)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
    at org.elasticsearch.node.Node.start(Node.java:278)
    at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:206)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:272)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

while with the change applied:

```
[2016-07-21 14:08:00,095][INFO ][transport                ] [Murmur II] publish_address {10.2.0.16:9300}, bound_addresses {10.2.0.16:9300}
[2016-07-21 14:08:00,100][INFO ][bootstrap                ] [Murmur II] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks
```
</comment><comment author="jasontedor" created="2016-07-21T14:27:24Z" id="234270498">&gt; It's not easily possible in a unit test to "fake" virtual interfaces

This is not right, testing is always possible. For example, this simple change adds a package-private method that can easily be tested with "fake" virtual interfaces:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java b/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
index e15073e..6f098ef 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
@@ -227,8 +227,12 @@ public abstract class NetworkUtils {

     /** Returns addresses for the given interface (it must be marked up) */
     static InetAddress[] getAddressesForInterface(String name) throws SocketException {
+        return getAddressesForInterface(name, getInterfaces());
+    }
+
+    static InetAddress[] getAddressesForInterface(String name, List&lt;NetworkInterface&gt; interfaces) throws SocketException {
         NetworkInterface intf = null;
-        for (NetworkInterface networkInterface : getInterfaces()) {
+        for (NetworkInterface networkInterface : interfaces) {
             if (name.equals(networkInterface.getName())) {
                 intf = networkInterface;
                 break;
@@ -236,7 +240,7 @@ public abstract class NetworkUtils {
         }

         if (intf == null) {
-            throw new IllegalArgumentException("No interface named '" + name + "' found, got " + getInterfaces());
+            throw new IllegalArgumentException("No interface named '" + name + "' found, got " + interfaces);
         }
         if (!intf.isUp()) {
             throw new IllegalArgumentException("Interface '" + name + "' is not up and running");
```
</comment><comment author="daschl" created="2016-07-21T14:31:34Z" id="234271894">@jasontedor gotcha, I was actually referring to faking an actual virtual interface that would reproduce the scenario when interacting with the original JVM methods. Any idea how that could be done?

Do you think I should make the change you suggested? Not sure it would provide more coverage for the specific case in question.
</comment><comment author="rmuir" created="2016-07-21T14:53:25Z" id="234278893">The change is good. It should have been calling this method all along: it even calls it for error messages (making the bug more confusing, because it shows you the exact one you tried to set in the list).
</comment><comment author="jasontedor" created="2016-07-21T14:57:34Z" id="234280276">&gt; I was actually referring to faking an actual virtual interface that would reproduce the scenario when interacting with the original JVM methods. Any idea how that could be done?

It's fine to just mock the behavior of the JVM method as I suggested.

&gt; Do you think I should make the change you suggested?

Yes, it should be possible to write a test case that would fail without the change and will pass with your suggested change (which is good).
</comment><comment author="daschl" created="2016-07-21T15:16:06Z" id="234286104">@jasontedor sounds good I'll make the changes :)
</comment><comment author="daschl" created="2016-07-22T05:27:59Z" id="234457639">@jasontedor a quick follow up with some question/observations from my digging:

Looks like it's not as easy as I thought because the `NetworkInterface` has no way of setting it to `virtual` - it only has those static initializers which don't provide the semantics we need here I think. Mocking it is also not an option since its a final class, and from the dependencies I can see that powermock is not used in ES right now (apologies if I missed a different way how ES allows to mock final classes, let me know if there is one).

So I propose doing one of the four things (please share your thoughts if there is something I have missed):
1. Stick with the one test I added previously, which makes sure that all the listed interfaces (which includes virtual and physical) can also be looked up by name, this is not the case before the patch and the test case would previously fail on a system where there is a virtual interface defined.
2. If there is a way to mock the final class in ES right now let me know, that is definitely an option.
3. The most complex one I assume, but if there is a way we could setup a virtual interface if the box is a linux machine and then run the test.. but its probably a little out of scope for this level of testing. Is there a higher level test harness that could cover this case?
4. Oh, and we could introduce another layer of indirection over the `NetworkInterface` so it is mockable.
</comment><comment author="rmuir" created="2016-07-22T11:12:18Z" id="234518246">I agree, I think NetworkInterface is mostly initialized via native code, we have no chance.

To be fair to @jasontedor's point, the bug probably happens because we don't test virtual interfaces :) 

But at the same time, we should not try to do something super-elaborate, or compromise the code itself with useless abstractions to do it. We should just test what is reasonable.
</comment><comment author="jasontedor" created="2016-07-22T11:49:02Z" id="234523942">&gt; I agree, I think NetworkInterface is mostly initialized via native code, we have no chance.

Sadly, this is right. It is [possible](https://gist.github.com/jasontedor/a941c2f51393665234e9ff74dc91e42a) to get this tested, but I agree with Robert's point that it's probably not worth it for just this one test. If we were to go over this code and get it all tested, it might be worth doing what I suggest in this [gist](https://gist.github.com/jasontedor/a941c2f51393665234e9ff74dc91e42a), but it's too far here.
</comment><comment author="jasontedor" created="2016-07-22T11:50:04Z" id="234524121">ok to test
</comment><comment author="daschl" created="2016-07-22T12:21:03Z" id="234529390">Looks like the git checkout failed https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+pull-request+multijob-intake/92/console
</comment><comment author="jasontedor" created="2016-07-22T13:36:24Z" id="234545494">test this please
</comment><comment author="jasontedor" created="2016-07-22T16:33:29Z" id="234591973">Thanks @daschl.
</comment><comment author="karlra" created="2016-07-22T16:47:17Z" id="234595135">Thank you for fixing this, @daschl :)
</comment><comment author="dakrone" created="2016-07-22T17:28:08Z" id="234605058">@jasontedor and @daschl - this actually causes test failures if a virtual interface is not up:

```
Suite: org.elasticsearch.common.network.NetworkUtilsTests
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=43BC27A5D082E629 -Dtests.class=org.elasticsearch.common.network.NetworkUtilsTests -Dtests.method="testAddressInterfaceLookup" -Dtests.security.manager=true -Dtests.locale=es-PA -Dtests.timezone=America/Indiana/Indianapolis
ERROR   0.18s | NetworkUtilsTests.testAddressInterfaceLookup &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.IllegalArgumentException: Interface 'virbr0' is not up and running
   &gt;    at __randomizedtesting.SeedInfo.seed([43BC27A5D082E629:C4C740E215C89F5F]:0)
   &gt;    at org.elasticsearch.common.network.NetworkUtils.getAddressesForInterface(NetworkUtils.java:242)
   &gt;    at org.elasticsearch.common.network.NetworkUtilsTests.testAddressInterfaceLookup(NetworkUtilsTests.java:90)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/hinmanm/es/elasticsearch/core/build/testrun/test/J0/temp/org.elasticsearch.common.network.NetworkUtilsTests_43BC27A5D082E629-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=619, maxMBSortInHeap=7.4794493142308305, sim=RandomSimilarity(queryNorm=false,coord=no): {}, locale=es-PA, timezone=America/Indiana/Indianapolis
  2&gt; NOTE: Linux 4.6.3-300.fc24.x86_64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=4,threads=1,free=481288632,total=514850816
  2&gt; NOTE: All tests run in this JVM: [NetworkUtilsTests]
Completed [1/1] in 1.20s, 1 test, 1 error &lt;&lt;&lt; FAILURES!
```

(Reproduces every time for me), `virb0` is a virtual interface used for bridging VirtualBox VMs, but stays down while nothing is running.
</comment><comment author="jasontedor" created="2016-07-22T17:30:26Z" id="234605606">Reverted in c27237be9fc70b077164e22705a17d25ac1e5d9f.
</comment><comment author="jasontedor" created="2016-07-22T17:33:09Z" id="234606216">The test included in the fix here will blow up on any machine that has an interface that is down.
</comment><comment author="daschl" created="2016-07-22T18:05:36Z" id="234614365">Okay good to know, I'll rework the test case on monday and resubmit it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkUtilsTests.java</file></files><comments><comment>Network: Allow to listen on virtual interfaces.</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkUtilsTests.java</file></files><comments><comment>Network: Allow to listen on virtual interfaces.</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkUtilsTests.java</file></files><comments><comment>Allow to listen on virtual interfaces</comment></comments></commit></commits></item><item><title>Add support for returning documents with completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19536</link><project id="" key="" /><description>This is a followup to https://github.com/elastic/elasticsearch/pull/13576#issuecomment-140695698, enriching completion suggestions
 with documents.

This commit enables completion suggester to return documents
associated with suggestions. Now the document source is returned
with every suggestion, which respects [source filtering options](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-source-filtering.html).

In case of suggest queries spanning more than one shard, the
suggest is executed in two phases, where the last phase fetches
the relevant documents from shards, implying executing suggest
requests against a single shard is more performant due to the
document fetch overhead when the suggest spans multiple shards.

Example completion suggest response:

``` bash
{
  "song-suggest": [
    {
      "text": "nev",
      "offset": 0,
      "length": 4,
      "options": [ {
          "text": "Nevermind",
          "_index": "music",
          "_type": "song",
          "_id": "52",
          "_score": 52,
          "_source": {
              "song": "Nevermind",
              "artist": "Nirvana"
          }
        }
      ]
    }
  ]
}
```

NOTE: now that we support returning documents with suggestions, we can remove the `payload` option

relates https://github.com/elastic/elasticsearch/issues/10746
</description><key id="166821611">19536</key><summary>Add support for returning documents with completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-beta1</label></labels><created>2016-07-21T13:39:05Z</created><updated>2016-11-03T16:03:10Z</updated><resolved>2016-08-05T22:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-21T15:08:47Z" id="234283818">@areek thanks so much! It took me a while to figure out what you did with the named suggestions etc. I do think there is an easier way to do what you wanted to do. I think we can, instead of use a `name, scoredoc[]` map we can just use offsets into the `ScoreDoc[]` which would allow us to keep most of the code as is and just resolve the actual documents once we finish up after the fact. ie. query would be implicitly using offset 0 and each of the suggestion will get an offset assigned in the beginning since we know the size of each suggestion and the query ahead of time. 
The code that actually fetches the docs can still simply be a single array so that code can stay! We might need to add some offset, lenght parameters to stuff like `#fillDocIdsToLoad` but I think that is less intrusive than our current path?

I love the tests man thanks for that
</comment><comment author="rmuir" created="2016-07-22T12:39:13Z" id="234532865">Can we improve the abstractions here? For example, the relatively straightforward `ScoreDoc[]` and `IntArrayList` get hidden behind less-obvious layers named `SortedDocs` and `DocIdsToLoad`, and neither of these have any javadocs. It makes it hard to tell what is going on with the code.
</comment><comment author="areek" created="2016-07-22T14:02:51Z" id="234552134">@rmuir I am working on removing the `DocIdsToLoad` abstraction and changing the  `SortedDocs` to store offsets instead of map of `IntArrayList`, as per @s1monw's  comment. Will make sure to add javadocs, to clarify. Thanks for the feedback :)
</comment><comment author="areek" created="2016-07-25T22:14:47Z" id="235102794">@s1monw @rmuir I updated the PR to represent search and suggest score docs array via an offsets array in `SortedDocs` abstraction. Would love to get feedback on the current approach!
</comment><comment author="s1monw" created="2016-08-02T09:04:57Z" id="236846856">@areek this looks pretty awesome already I left some comments
</comment><comment author="areek" created="2016-08-03T21:59:57Z" id="237388302">@s1monw Thanks for the review! I updated the PR, addressing all your feedback. After some thinking, I managed to remove the `SortedDocs` abstraction all together, now we just use `ScoreDoc[]` for both search and suggest docs. Would be awesome if you could review it again?
</comment><comment author="s1monw" created="2016-08-04T08:22:11Z" id="237486000">this looks awesome @areek it's simplified so much from the original patch. All the extra classes and abstractions are gone! good job! I added some comments about sharing more code etc. I also think we should have more tests on the CompletionSuggest end, like simple unittests of filter methods etc. 
What I am really missing is a place where we document how the fetching works, that the order of the docIDs is crucial. I think we should add it where we create the docId array?  I think we are super close here, can you remove the WIP label?
</comment><comment author="s1monw" created="2016-08-04T08:22:45Z" id="237486133">@clintongormley can you please check if we need to work more on docs here?
</comment><comment author="areek" created="2016-08-05T06:51:06Z" id="237770324">Thanks @s1monw for the review! I have added some documentation about how fetching works in `SearchPhaseController#sortedDocs` and what is expected by `SearchPhaseController#merge`, added simple unittests for `Suggest` and `CompletionSuggester` (more tests can be added later?). 
</comment><comment author="s1monw" created="2016-08-05T14:18:11Z" id="237862488">LGTM - I think CI failed so I guess you want to merge master in again and run it again! good job.
</comment><comment author="areek" created="2016-08-05T22:56:16Z" id="237984684">@clintongormley if you any feedback regarding the docs, let me know, will do a separate PR for it.
</comment><comment author="speedplane" created="2016-09-24T08:48:21Z" id="249353675">I'm using ES 1.7 now and the completion suggester is currently one of my largest memory bottlenecks, so thank you, I'm definitely looking forward to this! Is there documentation yet that describes this new feature? 

In my index, to save space, I do not have a _store, so if I want to use this feature, my understanding is that: 
1. I'll have to create a new index that has _store turned on.
2. During indexing, I put the payloads into this new index. Their ID can be keyed off of their value.
3. Also when indexing, I point the completion suggester to the documents in the new index.

Do I have this generally correct?
</comment><comment author="trompx" created="2016-11-03T11:45:20Z" id="258121581">Hello,

Compared to the old completion suggester, do we still need to optimize indices to prevent duplicate results when we update some documents ?

As quoted here : https://discuss.elastic.co/t/autocompletion-suggester-duplicate-record-in-suggestion-return/16950/4

&gt; The main reason, why you see the suggestion twice is, that even though a 
&gt; document is deleted and cannot be found anymore, the suggest data 
&gt; structures are only cleaned up during merges/optimizations. Running 
&gt; optimize should fix this.

Anyway awesome work on this new suggester. By any chance have you run any benchmark to compare suggestion speed 2.2 vs 5.0?
</comment><comment author="areek" created="2016-11-03T16:03:10Z" id="258187846">Hey @trompx,

&gt; Compared to the old completion suggester, do we still need to optimize indices to prevent duplicate results when we update some documents ?

No we don't need to as the new completion suggester is near-real time and is expected to take into account deleted documents (even if the deleted document hasn't been merged away).

&gt; Anyway awesome work on this new suggester. By any chance have you run any benchmark to compare suggestion speed 2.2 vs 5.0?

Thanks :), the benchmark for the new implementation can be found [here](https://github.com/elastic/elasticsearch/issues/10746#issuecomment-157276138)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestion.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java</file><file>core/src/test/java/org/elasticsearch/search/controller/SearchPhaseControllerTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionSuggesterBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionTests.java</file></files><comments><comment>Remove payload option from completion suggester</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/controller/SearchPhaseController.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/ShardFetchSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/Suggest.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestion.java</file><file>core/src/test/java/org/elasticsearch/search/controller/SearchPhaseControllerTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/SuggestTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionTests.java</file></files><comments><comment>Merge pull request #19536 from areek/enhancement/completion_suggester_documents</comment></comments></commit></commits></item><item><title>gradle assemble fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19535</link><project id="" key="" /><description>**Environment info**:

```
dm@io:~ $ gradle --version

------------------------------------------------------------
Gradle 2.13
------------------------------------------------------------

Build time:   2016-04-25 04:10:10 UTC
Build number: none
Revision:     3b427b1481e46232107303c90be7b05079b05b1c

Groovy:       2.4.4
Ant:          Apache Ant(TM) version 1.9.6 compiled on June 29 2015
JVM:          1.8.0_74 (Oracle Corporation 25.74-b02)
OS:           Mac OS X 10.11.5 x86_64
```

**Steps to reproduce**:
1. run `gradle assemble` on the latest master

This produces:

```
:modules:lang-mustache:generatePomFileForJarPublication FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':modules:lang-mustache:generatePomFileForJarPublication'.
&gt; Could not apply withXml() to generated POM
   &gt; Cannot invoke method startsWith() on null object

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED
```

Additionally, I also see:

```
:distribution:deb:generatePomFileForNebulaPublication
Unable to convert null to https form in MavenScmPlugin. Using original value.
[...]
:distribution:integ-test-zip:generatePomFileForNebulaPublication
Unable to convert null to https form in MavenScmPlugin. Using original value.
[...]
:distribution:rpm:generatePomFileForNebulaPublication
Unable to convert null to https form in MavenScmPlugin. Using original value.
[...]
```

before the fatal error occurs, so it seems that some property cannot be resolved? The `lang-mustache` module is maybe just unlucky as it's the first one where a method is invoked on this null object?
</description><key id="166808709">19535</key><summary>gradle assemble fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>build</label></labels><created>2016-07-21T12:38:19Z</created><updated>2016-07-23T00:31:47Z</updated><resolved>2016-07-22T13:54:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-21T17:19:01Z" id="234322026">@danielmitterdorfer Are you trying to run this on a copy of the repo, without git? This works for me locally (with git), and it works in CI (we do a `build`, which includes `assemble` there). The `startsWith` that was added recently looks at the git origin, and creates a url (ie finding the github url for the repo), but if the repo has no git info, I could see this happening.
</comment><comment author="danielmitterdorfer" created="2016-07-21T17:42:47Z" id="234328706">@rjernst Thanks for the pointer. That rang a bell: I have recently set up [git worktree](https://git-scm.com/docs/git-worktree) so I need to checkout the repo only once but have multiple work trees (one for each major version). I'll try tomorrow if that's the root cause (but based on your description I am already pretty sure it is...) and close the ticket then if that's the case.

@javanna has also mentioned that he has experienced this problem or did I misunderstand you?
</comment><comment author="dakrone" created="2016-07-21T17:47:51Z" id="234330055">Should we add a quick check in gradle for the `.git` directory and dump a slightly more useful error message? I could see someone downloading a tarball of the source from github and seeing this without any idea what causes it.
</comment><comment author="javanna" created="2016-07-21T17:55:17Z" id="234332205">I had and still have this out of master. Not sure what is wrong with my local checkout :)
</comment><comment author="nik9000" created="2016-07-21T17:55:34Z" id="234332295">I usually build Elasticsearch without git - though I usually just do `gradle check`. I do this because I usually run the tests on a desktop rather than abuse my silly little Macbook Air. I don't sync the `.git` directory because it is just a bunch of binary stuff and doesn't work well with `unison`.
</comment><comment author="danielmitterdorfer" created="2016-07-22T11:28:24Z" id="234520738">I did a fresh clone of the ES repo without using the git worktree feature and it works fine now. @javanna can you also freshly clone the repo and retry? If that works for you too, can you please close the ticket then?
</comment><comment author="javanna" created="2016-07-22T12:13:11Z" id="234527983">With a fresh checkout of es core it works. Would it be possible to know why I need a fresh checkout? I don't think I was doing anything weird. Maybe some assumptions made in our build should be adjusted? I think we should keep the ticket open till we figured that out.
</comment><comment author="danielmitterdorfer" created="2016-07-22T13:03:32Z" id="234537922">@javanna For my local checkout I am almost certain it relates to git worktree. If you use that feature, `.git` is not a directory as usual but a file that contains (on my machine):

```
gitdir: /Users/dm/Projects/elasticsearch/src/.repo/elasticsearch/.git/worktrees/elasticsearch
```

i.e. a poor man's symlink. :)

Did you keep your old checkout? Maybe your .git/config reveals something interesting.
</comment><comment author="javanna" created="2016-07-22T13:18:46Z" id="234541327">ok it turns out I have this problem because my remote is not named `origin`. I can imagine others rename it too. Not sure what we need that url for, but can we change the build to not make these assumptions about git?
</comment><comment author="nik9000" created="2016-07-22T13:33:21Z" id="234544749">Looks like this comes from https://github.com/nebula-plugins/gradle-info-plugin/blob/master/src/main/groovy/nebula/plugin/info/scm/GitScmProvider.groovy#L55-L64
</comment><comment author="rjernst" created="2016-07-22T13:49:48Z" id="234548774">&gt; can we change the build to not make these assumptions about git?

No,  this only happens when you do an assemble,  because it needs to build poms,  and the poms for maven central require having the URL for the repo. 
</comment><comment author="javanna" created="2016-07-22T13:54:00Z" id="234549843">perfect, then it will just mean a jar/pom/zip/whatever will never come out of my machine.
</comment><comment author="javanna" created="2016-07-22T14:12:53Z" id="234554718">Trying to go back to nice mode :) I don't know what the background is and we may depend on some plugins that don't do the right thing. But why is "no" the only possible answer? Maybe we can evaluate alternatives? Or explain why I'm asking for the moon? 

I don't think making these assumptions is correct. Requiring git is one thing, then assuming `origin` as the right remote is another one. Maybe we can have different paths for when we need this info (CI/making a release) and when we don't. Maybe if I have to build a jar/zip locally I don't need that stuff? Again, just thinking out loud. Let me know if I can help.
</comment><comment author="rjernst" created="2016-07-22T15:58:19Z" id="234583284">When I said "no", I meant that we must have the git info to build the pom. As far as "allowing other names for origin", I don't think we should write a bunch of code just to handle this edge case that is not necessary in 99.99% of cases, especially when our own release process does not need it. As far as being lenient, we could put a bogus url in there if we cannot get the git info, but fail if we are releasing (currently, that is `-Dbuild.snapshot=false`). I'm personally against this b/c I don't think we should have leniency here, but I know we already do in the case of the git info for jar metadata, so if someone wants to add that, I would not stand in the way as long as it is a hard fail for a release.
</comment><comment author="javanna" created="2016-07-22T16:08:32Z" id="234585944">not too sure that renaming remotes is an edge case, that was my main point. happy to be proved wrong if I am the only one doing it. I can always rename back the remote to what it needs to be when I need to generate the jar, or leave origin around by making a copy rather than renaming it. Do others have thoughts?
</comment><comment author="bleskes" created="2016-07-22T18:01:20Z" id="234613294">For what it’s worth, I would like to be able to build local zip/tars to sometime test a build and I have the elasticsearch github remote renamed from origin. If I understand correctly the requirement to have a url comes from maven. Is it maybe possible to:

1) Allow supplying a url as a parameter to the build?
2) Allow only creating a tar/zip/rpm (which don’t go to maven)? Luca, I think this is enough for you? 
3) Hard code the urls? this is the build for a elasticsearch/plugins which have a canonical urls, just like they have canonical artifact id?

&gt; On 22 Jul 2016, at 18:08, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; not too sure that renaming remotes is an edge case, that was my main point. happy to be proved wrong if I am the only one doing it. I can always rename back the remote to what it needs to be when I need to generate the jar, or leave origin around by making a copy rather than renaming it. Do others have thoughts?
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment><comment author="rjernst" created="2016-07-22T18:10:03Z" id="234615508">&gt; 1) Allow supplying a url as a parameter to the build?

I don't think we need this flexibility.

&gt; 2) Allow only creating a tar/zip/rpm (which don’t go to maven)? Luca, I think this is enough for you?

I've already suggested a way to have this leniency above.

&gt; 3) Hard code the urls? this is the build for a elasticsearch/plugins which have a canonical urls, just like they have canonical artifact id?

This is part of the gradle plugin we publish from buildSrc, which allows not only our plugins to be built, but those of community plugins as well.
</comment><comment author="bleskes" created="2016-07-22T18:34:45Z" id="234621848">&gt; &gt; 2) Allow only creating a tar/zip/rpm (which don’t go to maven)? Luca, I think this is enough for you?
&gt; 
&gt; I've already suggested a way to have this leniency above.

I didn't mean building things with a fake url, but rather only building the zip file. Can you elaborate about what you mean requiring a url in the pom files? Looking at a 5.0.0-Alpha4 distro, I don't see a pom.xml file anywhere (not as a file and not in the es-5.0.0.jar). I do see a MANIFEST.MF baked into the jar, which does specify a url as `Module-Origin`. However, if I look at a 2.2.0 distro which I happen to have lying around, it doesn't have a MANIFEST.MF file, but does have a pom.xml which in turn doesn't specify any url.
</comment><comment author="rjernst" created="2016-07-22T18:48:50Z" id="234625204">I'm not talking about the jar, I am talking about what goes into the distributions directory. Running gradle assemble means "assemble everything", and everything includes pom files, because that is part of what we distribute (publishing to maven).  And don't look at 2.x, we did things wrong in maven forever, and only found out about it recently with the unified release work (we were grandfathered into sidestepping the pom checks maven central has). 
</comment><comment author="bleskes" created="2016-07-22T18:53:07Z" id="234626251">&gt; gradle assemble means "assemble everything"

I understand. My question was whether we can have an "assemble zip/tar files" command, which only does that and is part of the assemble everything path, and doesn't require the url (and thus git remotes)

&gt; don't look at 2.x, we did things wrong in maven forever

I'm trying to understand what we should do/are doing. I take it to mean it is not required to have a pom.xml file as part of the jar and that pom files need urls (although it seems maven doesn't enforce it as 2.x is published there). Is that what you mean? 
</comment><comment author="jasontedor" created="2016-07-22T19:17:01Z" id="234631781">You can build a package via `gradle :distribution:(deb|rpm|tar|zip):assemble` and it works whether or not you're in a real repository or a worktree, and whether or not your remote is named elastic/origin/upstream. As a bonanza, it's faster. :smile:

I think the belief that the remote should be named `origin` is too far, this is clearly impacting a lot of people. If you follow the GitHub guidelines for [contributing](https://guides.github.com/activities/contributing-to-open-source/#contributing) you'll [eventually](https://help.github.com/articles/syncing-a-fork/) land on a [page](https://help.github.com/articles/configuring-a-remote-for-a-fork/) where the result is that the upstream remote is named `upstream` and the remote named `origin` points to your fork on GitHub.
</comment><comment author="nik9000" created="2016-07-22T19:19:22Z" id="234632310">That page is how I've always set up my github clones - exception I rename `origin` to `nik9000` because I want to make sure there is never anything that accidentally does something to the "default" name....
</comment><comment author="rjernst" created="2016-07-22T19:39:21Z" id="234636825">&gt; I think the belief that the remote should be named origin is too far

I did not write the code,  it is from a 3rd party gradle plug in as Nike pointed out.  If you want to open a PR there to fix,  please do. But not that I don't know how it would know which of many remotes is the one to grab the URL from.. 
</comment><comment author="jasontedor" created="2016-07-23T00:31:47Z" id="234687426">&gt; I did not write the code, it is from a 3rd party gradle plug in as Nike pointed out.

To be clear, my comments were aimed exactly at Nebula's convention here; sorry for the confusion.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make NetworkPartition disruption scheme configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19534</link><project id="" key="" /><description>We currently have subclasses for the various kind of network disruption schemes combining on one hand failure mode (disconnect/unresponsive/delay) as well as the network links to cut (two partitions / bridge partitioning) into a single class. This PR separates the description of the links in the network that are to be disrupted from the failure that is to be applied to the links (disconnect/unresponsive/delay).
</description><key id="166803995">19534</key><summary>Make NetworkPartition disruption scheme configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>review</label><label>test</label></labels><created>2016-07-21T12:11:52Z</created><updated>2016-08-11T12:55:06Z</updated><resolved>2016-08-11T12:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-22T12:46:16Z" id="234534260">I like how the code consolidates these classes into one place where they can be more easily contained. 

Can we improve the javadocs just a tad more to make it more obvious what is happening?
- add class javadocs to the different subclasses. just a one-liner what it does, to make it obvious.
- add method javadocs to abstract methods (e.g. `applyDisruption()`) as those represent a contract.
</comment><comment author="ywelsch" created="2016-07-22T15:27:25Z" id="234574934">Thanks for the feedback @rmuir. I've pushed c302837 adding more javadocs.
</comment><comment author="jasontedor" created="2016-08-10T20:52:16Z" id="239000398">I like it @ywelsch, it's a great change. I left one comment.
</comment><comment author="jasontedor" created="2016-08-11T11:52:55Z" id="239139977">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/BridgePartition.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkDelaysPartition.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkDisconnectPartition.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkDisruption.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkDisruptionIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkDisruptionTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkPartition.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/NetworkUnresponsivePartition.java</file></files><comments><comment>Make NetworkPartition disruption scheme configurable (#19534)</comment></comments></commit></commits></item><item><title>Accept alternative non-deprecated names in ParseField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19533</link><project id="" key="" /><description>Previously all names except the first passed to the `ParseField` were
considered deprecated. But we have a few cases where a field has more
than one acceptable (non-deprecated) name (e.g. `aggregations`
and `aggs` for declaring aggregations.

This change adds a constructor to `ParseField` which allows alternative
names to be added as well as deprecated names. The change is fully
backwards compatible as The previous constructor still exists.

Note that it is intentionally a little awkward to specify a field with
alternative names since this is something that should only be used in
rare circumstances and should not become the norm.

Closes #19504
</description><key id="166803893">19533</key><summary>Accept alternative non-deprecated names in ParseField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:REST</label><label>blocker</label><label>bug</label><label>review</label></labels><created>2016-07-21T12:11:16Z</created><updated>2016-08-01T10:57:48Z</updated><resolved>2016-07-29T08:05:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-21T12:17:59Z" id="234236688">LGTM
</comment><comment author="colings86" created="2016-07-22T15:04:57Z" id="234568841">@rmuir I've added JavaDocs and a few comments to ParseField. Let me know if there is anything you think isn't explained well.
</comment><comment author="rjernst" created="2016-07-22T15:36:57Z" id="234577424">&gt; But we have a few cases where a field has more than one acceptable (non-deprecated) name (e.g. aggregations and aggs for declaring aggregations)

Do we really need 2 alternatives here? Can we deprecate one or the other? I don't see why we need "aliases". Our docs should all use one form, and we should accept one form.
</comment><comment author="clintongormley" created="2016-07-27T14:44:16Z" id="235607676">&gt; Do we really need 2 alternatives here? Can we deprecate one or the other? I don't see why we need "aliases". Our docs should all use one form, and we should accept one form.

Normally i'd agree with you.  In this case most people write `aggs`, and the response comes back with `aggregations`.  So either we remove `aggs` and break everybody's requests, or we remove `aggregations` and break everybody's response parsing...

The version I'd prefer is `aggs`, but silently changing the response field name could be problematic (esp as we have no way to warn the user about this)
</comment><comment author="rjernst" created="2016-07-28T18:36:08Z" id="235985326">Would there be any other user of this than aggs? Can't we address that by having 2 ParseField for the short and long name? This is only used for parsing on a single line in SearchSourceBuilder. So this:

```
} else if (context.getParseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD)) {
```

would become:

```
} else if (context.getParseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD) ||
               context.getParseFieldMatcher().match(currentFieldName, AGGS_FIELD)) {
```

I think that one off in parsing is better than further complicating ParseField?
</comment><comment author="nik9000" created="2016-07-29T02:27:10Z" id="236079094">I'd be happy having two ParseFields so long as this isn't a common thing. And I don't think it is.
</comment><comment author="colings86" created="2016-07-29T08:05:22Z" id="236119804">I opened https://github.com/elastic/elasticsearch/pull/19674 which adds a second ParseField for `aggs` and replaces this PR
</comment><comment author="javanna" created="2016-07-29T08:12:25Z" id="236121049">@colings86 maybe we should revive the code comments that you had added as part of this change. They were valuable!
</comment><comment author="colings86" created="2016-07-29T08:42:19Z" id="236126647">@javanna I opened https://github.com/elastic/elasticsearch/pull/19676 to retain the comments
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping validation within templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19532</link><project id="" key="" /><description>I know this is PEBKAC (ie my fault), but we should validate this stuff.

Put a template with mappings, note the (mis)use of `"type" : "flot"`;

```
PUT _template/test
{
  "template": "test",
  "order": 0,
  "mappings": {
    "test": {
      "properties": {
        "floatfield": {
          "type" : "flot"
        }
      }
    }
  }
}
```

Response;

```
{
  "acknowledged": true
}
```

Put a doc;

```
PUT test/test/1
{
  "floatfield": 0.1
}
```

Response;

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "No handler for type [flot] declared on field [floatfield]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "Failed to parse mapping [test]: No handler for type [flot] declared on field [floatfield]",
      "caused_by": {
         "type": "mapper_parsing_exception",
         "reason": "No handler for type [flot] declared on field [floatfield]"
      }
   },
   "status": 400
}
```

If you do the same thing with a direct mapping call then we catch it straight up.
</description><key id="166783703">19532</key><summary>Mapping validation within templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Templates</label><label>enhancement</label></labels><created>2016-07-21T10:09:35Z</created><updated>2016-07-21T12:30:21Z</updated><resolved>2016-07-21T12:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-07-21T10:33:48Z" id="234217249">I think this is fixed with https://github.com/elastic/elasticsearch/pull/8802 

In 5.0-alpha4 you will get:

```
{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "No handler for type [flot] declared on field [floatfield]"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "Failed to parse mapping [test]: No handler for type [flot] declared on field [floatfield]",
    "caused_by": {
      "type": "mapper_parsing_exception",
      "reason": "No handler for type [flot] declared on field [floatfield]"
    }
  },
  "status": 400
}
```
</comment><comment author="clintongormley" created="2016-07-21T12:30:21Z" id="234239144">Closed by #8802
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use executor's describeTasks method to log task information in cluster service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19531</link><project id="" key="" /><description>This fixes an issue in ClusterService where in some places we don't use the executor's describeTasks method to log task information.

Broken output:

```
[2016-07-20 20:28:22,103][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [finalize_join ({node_t1}{HPvsEFLhQEKQqwgZZftfeQ}{ab2u1xAJRX6vMIcaOnXKmw}{127.0.0.1}{127.0.0.1:30101})[org.elasticsearch.discovery.zen.ZenDiscovery$4@24c190bd]]
```

Fix by this PR:

```
[2016-07-21 11:56:40,404][TRACE][org.elasticsearch.cluster.service] [node_t2] will process [finalize_join ({node_t1}{4DjC9ebiTLq98JaAyOmwag}{yGrsrg8hQzi4DLedfRtxlQ}{127.0.0.1}{127.0.0.1:30101})]
```
</description><key id="166781914">19531</key><summary>Use executor's describeTasks method to log task information in cluster service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-21T10:00:15Z</created><updated>2016-07-29T11:13:57Z</updated><resolved>2016-07-21T12:32:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-21T10:22:54Z" id="234215127">LGTM, tough I would change the title and description to say we don't use the executor's describeTasks in all logging.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file></files><comments><comment>Use executor's describeTasks method to log task information in cluster service (#19531)</comment></comments></commit></commits></item><item><title>ActiveShardsObserverIT#testCreateIndexNotEnoughActiveShardsTimesOut fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19530</link><project id="" key="" /><description>The test failed only once in CI:

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+java9-periodic/673/

The test itself succeeds actually, the problem is in the after test checks:

```
FAILURE 5.30s J1 | ActiveShardsObserverIT.testCreateIndexNotEnoughActiveShardsTimesOut &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: Shard [test-idx][6] is still locked after 5 sec waiting
   &gt;    at __randomizedtesting.SeedInfo.seed([CAB167E7B91DDDA2:F4E0B8D9B6F84C47]:0)
   &gt;    at org.elasticsearch.test.InternalTestCluster.assertAfterTest(InternalTestCluster.java:1913)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:550)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.after(ESIntegTestCase.java:1980)
   &gt;    at jdk.internal.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
   &gt;    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@9-ea/DelegatingMethodAccessorImpl.java:43)
   &gt;    at java.lang.Thread.run(java.base@9-ea/Thread.java:843)
```

I failed to repro this locally with same seed and many iterations.
</description><key id="166778750">19530</key><summary>ActiveShardsObserverIT#testCreateIndexNotEnoughActiveShardsTimesOut fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label></labels><created>2016-07-21T09:45:02Z</created><updated>2016-07-21T15:47:21Z</updated><resolved>2016-07-21T15:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-21T09:45:45Z" id="234206987">@abeyad I see that this test was recently created by you, I thought you may want to have a look at it, also you must be familiar with it. Assigning you then.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardsObserverIT.java</file></files><comments><comment>Fixes the ActiveShardsObserverIT tests that have a very short index (#19540)</comment></comments></commit></commits></item><item><title>Test: Fixed incorrect YAML indentation in the `indices.put_template/10_basic.yaml` test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19529</link><project id="" key="" /><description>The Ruby YAML parser ignores the `do` actions when they are not indented, making the test suite fail.

Can you please review, @s1monw?

Related: #19506 
</description><key id="166775879">19529</key><summary>Test: Fixed incorrect YAML indentation in the `indices.put_template/10_basic.yaml` test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>non-issue</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-21T09:30:47Z</created><updated>2016-07-29T11:13:22Z</updated><resolved>2016-07-21T12:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-21T09:48:05Z" id="234207536">I wonder if we could make our yaml parser stricter so that we don't end up breaking the ruby build every other day. But I am also not sure what the right behaviour here is. yaml seemed correctly indented to me?
</comment><comment author="karmi" created="2016-07-21T09:53:16Z" id="234208723">@javanna, I'm not sure what is technically correct in the spirit of YAML specification, but this is how the Ruby (2.1.5) parser behaves:

``` ruby
YAML.load &lt;&lt;-YAML
foo:
bar: baz
YAML
# =&gt; {"foo"=&gt;nil, "bar"=&gt;"baz"}

YAML.load &lt;&lt;-YAML
foo:
  bar: baz
YAML
# =&gt; {"foo"=&gt;{"bar"=&gt;"baz"}}
```
</comment><comment author="s1monw" created="2016-07-21T10:20:17Z" id="234214530">man I am sorry this yaml formatting drives me nuts, I think we have to make it fail on the java end if possible. LGTM thanks for fixing it
</comment><comment author="karmi" created="2016-07-21T12:15:57Z" id="234236290">No worries, Simon! I understand it's tricky to catch those if the Java YAML parser takes it just fine! (I have a hunch that the indented form is more "correct" YAML encoding, but I don't wanna spend an hour deciphering the formal grammar in the RFC to verify it :)
</comment><comment author="s1monw" created="2016-07-21T12:26:39Z" id="234238395">why is this closed and not merged @karmi 
</comment><comment author="karmi" created="2016-07-21T12:29:40Z" id="234239007">Was wondering the same, @s1monw! The [commit](https://github.com/elastic/elasticsearch/commit/8c721b10af2b3f300d45dc9f615e71b5c08c958d) has the regular `Closes #19529` line there, which normally puts it to _Merged_ state, I have no idea why it's _Closed_. Either a glitch on the Github side, or I messed up something, but I was doing what I always do...
</comment><comment author="nik9000" created="2016-07-21T12:31:39Z" id="234239414">It was closed by 8c721b10af2b3f300d45dc9f615e71b5c08c958d. I'm guessing you rebased on master but didn't force push to the branch or something.
</comment><comment author="s1monw" created="2016-07-21T12:44:37Z" id="234242136">@karmi just use the merge button
</comment><comment author="karmi" created="2016-07-21T12:55:56Z" id="234244772">Argh, right!, @nik9000 is absolutely correct, this is what I did:

``` bash
git checkout karmi/fix_indendation_in_yaml_tests_put_template
git rebase master
### [!] force push missing here [!]
git checkout master
git merge karmi/fix_indendation_in_yaml_tests_put_template
git push
git push origin :karmi/fix_indendation_in_yaml_tests_put_template
git branch -d karmi/fix_indendation_in_yaml_tests_put_template
```

So I didn't force pushed to the branch at Github, which is what I normally do. Thanks for the debug, Nik!

@s1monw, I'll _never_ use the merge button, you know that... :)
</comment><comment author="s1monw" created="2016-07-21T15:18:07Z" id="234286712">&gt; @s1monw, I'll never use the merge button, you know that... :)

the merge button can squash now I think it will be your best friend!
</comment><comment author="jasontedor" created="2016-07-21T15:20:29Z" id="234287447">&gt; I'll _never_ use the merge button, you know that... :)

@karmi I hope that you consider it now that it's possible to squash on merge, because now the button will not be green unless the build passes.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Test: Fixed incorrect YAML indentation in the `indices.put_template/10_basic.yaml` test</comment></comments></commit></commits></item><item><title>percentiles aggregation return anomaly value when hits too large</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19528</link><project id="" key="" /><description>**Elasticsearch version**:
1.7.3
**JVM version**:
1.7
**OS version**:
CentOS6.5
**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.ingest more than 3 billon event into the index.
 2.send a percentiles aggregation for those event
 3.retry the query for severy times

**Provide logs (if relevant)**:
the right result:
![image](https://cloud.githubusercontent.com/assets/1219655/17017238/1a1e95e0-4f65-11e6-92bc-74651b6bb296.png)
and the error result:
![image](https://cloud.githubusercontent.com/assets/1219655/17017234/159f610c-4f65-11e6-9aa6-6fb13e491268.png)
</description><key id="166770989">19528</key><summary>percentiles aggregation return anomaly value when hits too large</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chenryn</reporter><labels><label>:Aggregations</label><label>stalled</label></labels><created>2016-07-21T09:05:12Z</created><updated>2016-07-28T08:01:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-21T11:31:21Z" id="234228011">HI @chenryn 

Please could you provide text rather than images.
</comment><comment author="colings86" created="2016-07-21T12:41:45Z" id="234241541">@chenryn could you also provide the exact request you are sending to Elasticsearch which causes this issue as well as the request that produces the correct result (again as text rather than an image)?
</comment><comment author="chenryn" created="2016-07-21T13:17:00Z" id="234249818">I had a try on ES2.3.2, got the same problem. The request is:

```
GET /iislog-ctriptravel.com-2016.07.16,iislog-ctriptravel.com-2016.07.17/_search
{
    "aggs": {
        "stats": {
            "percentiles": {
                "field": "time_taken",
                "percents": ["50", "90", "99"]
            }
        }
    },
    "size": 0
}
```

the get the right result:

``` json
{"took":189748, "timed_out":false,"_shards":{"total":20,"successful":20,"failed":0},"hits":{"total":1977427282,"max_score":0,"hits":[]},"aggregations":{"stats":{"values":{"50.0":14,"90.0":61.66360610255921,"99.0":683.5554262723224}}}}
```

Then the same queryDSL, but send to another daily indices which have more log events---- `/iislog-ctriptravel.com-2016.07.20,iislog-ctriptravel.com-2016.07.19/_search`, get:

``` json
{"took":215058, "timed_out":false,"_shards":{"total":20,"successful":20,"failed":0},"hits":{"total":2178141206,"max_score":0,"hits":[]},"aggregations":{"stats":{"values":{"50.0":4299640220090432000,"90.0":5468139946983353000,"99.0":5731052385534261000}}}}
```
</comment><comment author="colings86" created="2016-07-21T13:25:57Z" id="234252113">@chenryn thanks for the info. Could you try running the following requests and post the results back here:

1.

```
GET /iislog-ctriptravel.com-2016.07.16/_search
{
    "aggs": {
        "stats": {
            "stats": {
                "field": "time_taken"
            }
        }
    },
    "size": 0
}
```

2.

```
GET /iislog-ctriptravel.com-2016.07.17/_search
{
    "aggs": {
        "stats": {
            "stats": {
                "field": "time_taken"
            }
        }
    },
    "size": 0
}
```

3.

```
GET /iislog-ctriptravel.com-2016.07.19/_search
{
    "aggs": {
        "stats": {
            "stats": {
                "field": "time_taken"
            }
        }
    },
    "size": 0
}
```

4.

```
GET /iislog-ctriptravel.com-2016.07.20/_search
{
    "aggs": {
        "stats": {
            "stats": {
                "field": "time_taken"
            }
        }
    },
    "size": 0
}
```
1. 

```
GET /iislog-ctriptravel.com-2016.07.16/_mapping
```
1. 

```
GET /iislog-ctriptravel.com-2016.07.17/_mapping
```
1. 

```
GET /iislog-ctriptravel.com-2016.07.19/_mapping
```
1. 

```
GET /iislog-ctriptravel.com-2016.07.20/_mapping
```

Thanks
</comment><comment author="xgwu" created="2016-07-22T03:42:02Z" id="234447462">@colings86 

Here's the information you requested.

Percentiles agg on each individual index:

```
GET /iislog-ctriptravel.com-2016.07.16/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "percents": [
               "50",
               "90",
               "99"
            ]
         }
      }
   },
   "size": 0
}


{
   "took": 188080,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 993579848,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "stats": {
         "values": {
            "50.0": 14,
            "90.0": 63.46756762924877,
            "99.0": 692.673421465423
         }
      }
   }
}


GET /iislog-ctriptravel.com-2016.07.17/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "percents": [
               "50",
               "90",
               "99"
            ]
         }
      }
   },
   "size": 0
}

{
   "took": 199951,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 983847434,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "stats": {
         "values": {
            "50.0": 14.000000000000002,
            "90.0": 59.90929805440135,
            "99.0": 674.0545274019375
         }
      }
   }
}


GET /iislog-ctriptravel.com-2016.07.19/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "percents": [
               "50",
               "90",
               "99"
            ]
         }
      }
   },
   "size": 0
}

{
   "took": 204333,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 1087960178,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "stats": {
         "values": {
            "50.0": 15,
            "90.0": 65.02467183669586,
            "99.0": 760.5187167942532
         }
      }
   }
}

GET /iislog-ctriptravel.com-2016.07.20/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "percents": [
               "50",
               "90",
               "99"
            ]
         }
      }
   },
   "size": 0
}

{
   "took": 216997,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 1090181028,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "stats": {
         "values": {
            "50.0": 14.653717184377456,
            "90.0": 67.1670260754357,
            "99.0": 784.6081364612112
         }
      }
   }
}
```

Mapping:

```
GET /iislog-ctriptravel.com-2016.07.16,iislog-ctriptravel.com-2016.07.17,iislog-ctriptravel.com-2016.07.19,iislog-ctriptravel.com-2016.07.20/_mapping
{
   "iislog-ctriptravel.com-2016.07.19": {
      "mappings": {
         "iislog": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "@version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalIP": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalPool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "c_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_Referer_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_Referer_stem": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_User_Agent_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_bytes": {
                  "type": "integer"
               },
               "cs_host": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_method": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_uri": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_query": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_stem": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_username": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "index_part": {
                  "type": "string",
                  "index": "no"
               },
               "message": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "msg": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "path": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "pool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_computername": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_port": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_sitename": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_bytes": {
                  "type": "long"
               },
               "sc_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_substatus": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_win32_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "site": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "tags": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "time_taken": {
                  "type": "integer"
               }
            }
         },
         "_default_": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               }
            }
         }
      }
   },
   "iislog-ctriptravel.com-2016.07.16": {
      "mappings": {
         "iislog": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "@version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalIP": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalPool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "c_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_Referer_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_Referer_stem": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_User_Agent_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_bytes": {
                  "type": "integer"
               },
               "cs_host": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_method": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_uri": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_query": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_stem": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_username": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "index_part": {
                  "type": "string",
                  "index": "no"
               },
               "message": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "msg": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "path": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "pool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_computername": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_port": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_sitename": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_bytes": {
                  "type": "long"
               },
               "sc_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_substatus": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_win32_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "site": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "tags": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "time_taken": {
                  "type": "integer"
               }
            }
         },
         "_default_": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               }
            }
         }
      }
   },
   "iislog-ctriptravel.com-2016.07.17": {
      "mappings": {
         "iislog": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "@version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalIP": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalPool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "c_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_Referer_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_Referer_stem": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_User_Agent_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_bytes": {
                  "type": "integer"
               },
               "cs_host": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_method": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_uri": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_query": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_stem": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_username": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "index_part": {
                  "type": "string",
                  "index": "no"
               },
               "message": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "msg": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "path": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "pool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_computername": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_port": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_sitename": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_bytes": {
                  "type": "long"
               },
               "sc_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_substatus": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_win32_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "site": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "tags": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "time_taken": {
                  "type": "integer"
               }
            }
         },
         "_default_": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               }
            }
         }
      }
   },
   "iislog-ctriptravel.com-2016.07.20": {
      "mappings": {
         "iislog": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "@version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalIP": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "OriginalPool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "c_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_Referer_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_Referer_stem": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_User_Agent_": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_bytes": {
                  "type": "integer"
               },
               "cs_host": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_method": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_uri": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_query": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_uri_stem": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "cs_username": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "cs_version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "index_part": {
                  "type": "string",
                  "index": "no"
               },
               "message": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "msg": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  }
               },
               "path": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "analyzed": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        }
                     }
                  }
               },
               "pool": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_computername": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_ip": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_port": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "s_sitename": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_bytes": {
                  "type": "long"
               },
               "sc_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_substatus": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "sc_win32_status": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "site": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "tags": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "time_taken": {
                  "type": "integer"
               }
            }
         },
         "_default_": {
            "dynamic": "false",
            "_all": {
               "enabled": false
            },
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "not_analyzed",
                        "type": "string",
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "boolean_fields": {
                     "mapping": {
                        "doc_values": true
                     },
                     "match": "*",
                     "match_mapping_type": "boolean"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               }
            }
         }
      }
   }
}
```
</comment><comment author="colings86" created="2016-07-22T14:35:22Z" id="234560788">@chenryn Thanks for running those requests for us. Your mapping and stats output all look find so that has ruled out it being a mapping issue. I have actually been able to reproduce your problem using the t-digest algorithm (the algorithm we use by default for percentiles aggregations) outside of ES and I have raised a PR to fix the issue on the t-digest repo: https://github.com/tdunning/t-digest/pull/70

On your 2.3.2 you might be able to work around these by using the alternative algorithm `HDRHistogram` in the percentiles aggregation: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-percentile-aggregation.html#_hdr_histogram
</comment><comment author="xgwu" created="2016-07-22T15:06:57Z" id="234569353">@colings86  It was actually me  :) , a friend of @chenryn 's,  running those requests in that I encountered the same issue.

I tried the HDRHistogram as you suggested but the query threw an error:

```
GET /iislog-ctriptravel.com-2016.07.20/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "method" : "hdr", 
            "number_of_significant_value_digits" : 3,
            "percents": [
               "50",
               "90",
               "99"
            ]
         }
      }
   },
   "size": 0
}

{
   "error": {
      "root_cause": [
         {
            "type": "search_parse_exception",
            "reason": "Unexpected token VALUE_STRING in [stats].",
            "line": 5,
            "col": 34
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "iislog-ctriptravel.com-2016.07.20",
            "node": "tBswQpmPQkqjYNmmEg8TvA",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Unexpected token VALUE_STRING in [stats].",
               "line": 5,
               "col": 34
            }
         }
      ]
   },
   "status": 400
}
```

It looks like this option is not supported in 2.3.2? 
</comment><comment author="colings86" created="2016-07-22T15:44:34Z" id="234579526">It should be supported for 2.3.2. Can you try the same request but with the values in the percents array as numbers instead of strings:

```
GET /iislog-ctriptravel.com-2016.07.20/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "method" : "hdr", 
            "number_of_significant_value_digits" : 3,
            "percents": [
               50,
               90,
               99
            ]
         }
      }
   },
   "size": 0
}
```
</comment><comment author="xgwu" created="2016-07-23T01:27:26Z" id="234691253">@colings86  I tried the same request with the values in the percents array as numbers but no luck. The same error in the result.
</comment><comment author="colings86" created="2016-07-25T10:11:26Z" id="234914527">@xgwu I'm not sure whats going on then as this should work. Do you have an exception in your server logs when you try this request?
</comment><comment author="xgwu" created="2016-07-27T06:30:26Z" id="235498525">@colings86 I figured out what is the problem. 

By looking at the error log in server logs, the error was threw out by "AbstractPercentilesParser.java" at line 140 as below:

```
          } else {
                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                        parser.getTokenLocation());
            }
        }
```

Token was **VALUE_STRING** when it errored out.

While at line 91 in the code, token involving method is expected to be an **START_OBJECT**.

```
 } else if (token == XContentParser.Token.START_OBJECT) {
                if (method != null) {
                    throw new SearchParseException(context, "Found multiple methods in [" + aggregationName + "]: [" + currentFieldName
                            + "]. only one of [" + PercentilesMethod.TDIGEST.getName() + "] and [" + PercentilesMethod.HDR.getName()
                            + "] may be used.", parser.getTokenLocation());
                }
                method = PercentilesMethod.resolveFromName(currentFieldName);
                if (method == null) {
                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                            parser.getTokenLocation());
                } else {
                    switch (method) {
                    case TDIGEST:
```

So I changed the percentiles query a bit like below and it returned result successfully.

```
GET /iislog-ctriptravel.com-2016.07.20/_search?pretty
{
   "aggs": {
      "stats": {
         "percentiles": {
            "field": "time_taken",
            "hdr": {
               "number_of_significant_value_digits": 3
            },
            "percents": [
               50,
               90,
               99
            ]
         }
      }
   },
   "size": 0
}
```

So the problem is the documentation for percentiles aggs not been in line with the source code.

Hope this helps!   
</comment><comment author="colings86" created="2016-07-27T08:01:37Z" id="235514556">Good catch, I have fixed the documentation in branches master, 2.x, 2.4 and 2.3 in the following commits respectively:
https://github.com/elastic/elasticsearch/commit/3f344d3154f29d8c873d8002570d65b4e2bdb0ee
https://github.com/elastic/elasticsearch/commit/166c142cf09d90a73c12d9ce2506990170da3bc9
https://github.com/elastic/elasticsearch/commit/eaecb26ec6d14368a5a7d45e2a4cf67d829c1188
https://github.com/elastic/elasticsearch/commit/90f439ff60a3c0f497f91663701e64ccd01edbb4
</comment><comment author="clintongormley" created="2016-07-27T18:07:33Z" id="235670146">I think this issue can be closed now
</comment><comment author="colings86" created="2016-07-28T07:51:34Z" id="235824857">@clintongormley this issue needs to stay open for now. The original issue is due to a bug in the t-digest algorithm. I have a PR waiting to be merged on the t-digest repo and once it is merged and released we will need to update to the latest t-digest version to resolve this. For now I am going to mark this issue as `stalled` since it can't progress until the fix has been merged on t-digest.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Should Netty 4 be the default implementation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19527</link><project id="" key="" /><description>In #19526, a Netty 4-based transport and HTTP implementation was introduced. Currently the Netty 3-based implementation is the default transport. Should the Netty 4-based implementation be the default implementation for future pre-releases of Elasticsearch 5.0.0 to ensure that we get a lot of coverage from the community on the new networking implementation? Resolving this should be a blocker for the next pre-release of Elasticsearch 5.0.0.
</description><key id="166736777">19527</key><summary>Should Netty 4 be the default implementation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>blocker</label></labels><created>2016-07-21T04:49:56Z</created><updated>2016-10-18T08:51:22Z</updated><resolved>2016-08-02T16:19:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-21T09:20:00Z" id="234201117">I don't think we should use unmaintained software. Can we take the netty3 module and move it to a plugin? Maybe that way, if something goes really bad with netty4, we have an escape hatch for the user.
</comment><comment author="s1monw" created="2016-07-21T09:23:08Z" id="234201863">before we make this move (I agree with rob basically) we should have some benchmark numbers, rough ones are fine so we can at least tell we are not fucking things up completely. To make netty4 default we don't need to move netty3 to a plugin, it's a setting basically we can move from netty3 to netty4 to use it as a default. We can consider keeping it as a module for a release or two to make it easy to switch between them...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce Netty 4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19526</link><project id="" key="" /><description>This commit adds transport-netty4, a transport and HTTP implementation
based on Netty 4.

Closes #3226
</description><key id="166736157">19526</key><summary>Introduce Netty 4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha5</label></labels><created>2016-07-21T04:42:24Z</created><updated>2016-07-23T11:54:37Z</updated><resolved>2016-07-23T02:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-21T04:46:41Z" id="234155819">The full top-level `gradle check` passes locally with a custom-built snapshot of Netty 4 but the CI builds are going to fail until a 4.1.4.Final-SNAPSHOT trickles into the repositories that contains some of the latest fixes in Netty. However, the review cycle can start but we should off on a master integration until a 4.1.4.Final release is cut.

No attempts have been made to optimize, this can happen in follow ups. Further, no attempts have been made to eliminate any code duplication between transport-netty3 and transport-netty4. Only one of these implementations is going to survive, we shouldn't bind them together to only unbind them later when we kill one of the implementations.
</comment><comment author="s1monw" created="2016-07-21T09:11:03Z" id="234199093">I did one pass, didn't look too close to the http impls maybe @spinscale can look at pipelining and cors etc.
</comment><comment author="jasontedor" created="2016-07-22T04:09:21Z" id="234450186">@s1monw Thanks. I've responded to your first round of feedback.
</comment><comment author="s1monw" created="2016-07-22T14:24:28Z" id="234557815">with all the comments above I am good with this change to go in. @jasontedor take this as a LGTM once all comments are addresssed
</comment><comment author="spinscale" created="2016-07-22T15:00:57Z" id="234567803">Nice! The PR looks good to me to push as a first version, so it gets into CI and some rally testing, before some more clean up happens like the CORS stuff that we initially copied from netty4 into our branch. Plus that one leak found by the REST tests (I think you said you are about to push a fix)...
</comment><comment author="normanmaurer" created="2016-07-23T05:31:37Z" id="234700682">@jasontedor if you want I can have a look at your netty related code
</comment><comment author="jasontedor" created="2016-07-23T11:54:37Z" id="234714383">@normanmaurer Sure! Note there's still a lot of work left to do here, but any feedback is welcome and appreciated. I'm on vacation starting today until August 1, so I won't read/reply until then.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/transport/src/main/java/org/elasticsearch/transport/client/PreBuiltTransportClient.java</file><file>client/transport/src/test/java/org/elasticsearch/transport/client/PreBuiltTransportClientTests.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ReleasablePagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/io/ReleasableBytesStream.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/ReleasableBytesStreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/lease/Releasable.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServerAdapter.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServerTransport.java</file><file>core/src/main/java/org/elasticsearch/rest/AbstractRestChannel.java</file><file>core/src/main/java/org/elasticsearch/rest/RestChannel.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/RetryTests.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpRequest.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpRequestHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpServerTransport.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3MessageChannelHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3Transport.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/ESNetty3IntegTestCase.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortIntegrationIT.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpChannel.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpRequest.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpRequestHandler.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpServerTransport.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/cors/Netty4CorsConfig.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/cors/Netty4CorsConfigBuilder.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/cors/Netty4CorsHandler.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/pipelining/HttpPipelinedRequest.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/pipelining/HttpPipelinedResponse.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/pipelining/HttpPipeliningHandler.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/Netty4Plugin.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/ByteBufBytesReference.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/ByteBufStreamInput.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4InternalESLogger.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4MessageChannelHandler.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4OpenChannelsHandler.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4SizeHeaderFrameDecoder.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4Transport.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/transport/netty4/Netty4Utils.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/ESNetty4IntegTestCase.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpChannelTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpClient.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpPublishPortTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpRequestSizeLimitIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpServerPipeliningTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpServerTransportTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4PipeliningDisabledIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4PipeliningEnabledIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4RestIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/pipelining/Netty4HttpPipeliningHandlerTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/ByteBufBytesReferenceTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4ScheduledPingTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4SizeHeaderFrameDecoderTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4TransportIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4TransportMultiPortIntegrationIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4TransportPublishAddressIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/Netty4UtilsTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/NettyTransportMultiPortTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/transport/netty4/SimpleNetty4TransportTests.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpSmokeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/FakeRestRequest.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTcpTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTransportClient.java</file></files><comments><comment>Introduce Netty 4</comment></comments></commit></commits></item><item><title>Geodistance - between two points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19525</link><project id="" key="" /><description>It'd be schway if we had a geodistance query that works between two points, ie show me the distance between `33.8688° S, 151.2093°` and `41.3851° N, 2.1734° E`.

Inspired by [this](https://discuss.elastic.co/t/json-array-of-geo-locations-get-distance-of-all-the-location-in-json-array-of-a-document/55974) forum post.
</description><key id="166688165">19525</key><summary>Geodistance - between two points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Geo</label><label>discuss</label><label>enhancement</label></labels><created>2016-07-20T21:36:43Z</created><updated>2016-07-21T12:17:20Z</updated><resolved>2016-07-21T12:17:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-21T08:58:01Z" id="234196072">I'm not sure I understand what this query is supposed to do? If its supposed to score documents based on the distance between a fixed point and a point in the document then we already have this in the `decay` function of the `function_score` query. If not, could you provide more details as to what you would expect the inputs to the query to be and what the query would score against in the document?
</comment><comment author="markwalkom" created="2016-07-21T09:16:06Z" id="234200256">I want to measure how far apart two geopoints are.
No scoring, just "it's X KM from A to B".
</comment><comment author="colings86" created="2016-07-21T09:22:03Z" id="234201608">I might be missing something here but that's not a query? A query needs to make some selection of documents based on some criteria in those documents. Calculating a distance between two points and outputting it is a function. The bit I'm confused about here is how this related to a geo-distance query as mentioned in your original post? Are you just after a convenience method that can be used in scripts/function_score/something else?
</comment><comment author="markwalkom" created="2016-07-21T09:30:25Z" id="234203595">True, it's not a query and it is a simplicity thing. But it'd be useful.
</comment><comment author="clintongormley" created="2016-07-21T12:17:19Z" id="234236565">Duplicate of https://github.com/elastic/elasticsearch/issues/17506
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup the tests in lang-mustache module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19524</link><project id="" key="" /><description>Messy tests with mustache were either moved to core, moved to a rest test or remained untouched if they actually tested mustache. Also removed tests that were redundant.
</description><key id="166665462">19524</key><summary>Cleanup the tests in lang-mustache module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T19:45:22Z</created><updated>2016-07-25T16:01:08Z</updated><resolved>2016-07-25T16:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-22T14:34:24Z" id="234560540">I left a few smallish suggestions. I tried to make sure that you didn't drop any tests accidentally but that is fairly difficult to do in code review so I might have missed something.
</comment><comment author="martijnvg" created="2016-07-25T07:30:45Z" id="234866241">@nik9000 Thanks for checking this out! I've left comments and addressed your feedback.
</comment><comment author="nik9000" created="2016-07-25T12:37:44Z" id="234941205">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wait_for_status param on /_cluster/health endpoint seems to return early, ignoring timeout parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19523</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_92

**OS version**: Ubuntu 14.04 running 3.13.0-86-generic kernel

**Description of the problem including expected versus actual behavior**:

I'm using the `wait_for_status` option on the cluster health API to automate a script that needs to wait for a snapshot to be loaded. The snapshots I'm working with take about 45 minutes to load, so my initial plan was to issue a request like this, which would wait for up to an hour for the snapshot to load successfully:

`curl 'http://myelasticsearchMachine:9200/_cluster/health/myNewIndexLoadedFromSnapshot?wait_for_status=green&amp;timeout=3600s'`

However, no matter what I do, the above request returns after exactly 60 seconds, regardless of whether or not the snapshot is loaded and the index in question is in the green state. I've worked around this for now by looping 100 times: the requests take a minute each until the state is green, after which time they return immediately. However it would be nice to be able to do just one request. 

Here's an example excerpt of the output of said loop which prints out a timestamp, then issues a wait_for_status request like the one above.

All the while this script was running, the overall cluster state was red, as was the state of the index in question. There is another index in the same cluster that is green, if that's somehow relevant.

```
+ curl 'http://pelias-es-dev2.iad.internal.mapzen.com:9200/_cluster/health/pelias-2016.07.06-214540?wait_for_status=green&amp;timeout=300s'
+ for i in '$(seq 100)'
+ date -u
Wed Jul 20 18:06:57 UTC 2016
+ curl 'http://pelias-es-dev2.iad.internal.mapzen.com:9200/_cluster/health/pelias-2016.07.06-214540?wait_for_status=green&amp;timeout=300s'
+ for i in '$(seq 100)'
+ date -u
Wed Jul 20 18:07:57 UTC 2016
+ curl 'http://pelias-es-dev2.iad.internal.mapzen.com:9200/_cluster/health/pelias-2016.07.06-214540?wait_for_status=green&amp;timeout=300s'
+ for i in '$(seq 100)'
+ date -u
Wed Jul 20 18:08:57 UTC 2016
+ curl 'http://pelias-es-dev2.iad.internal.mapzen.com:9200/_cluster/health/pelias-2016.07.06-214540?wait_for_status=green&amp;timeout=300s'
+ for i in '$(seq 100)'
+ date -u
```

This very well could be an issue on my part (I hope it is), but my read of the [cluster health](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params) docs makes it seem like this should work.

**Steps to reproduce**:
1. Load a snapshot that takes a decent amount of time to restore
2. Issue a cluster health API request with `wait_for_status=green` and `timeout=3600s` parameters
3. API request should only return a response when the snapshot is done loading, but it returns after 1 minute regardless of the index status.

**Provide logs (if relevant)**:
</description><key id="166646396">19523</key><summary>wait_for_status param on /_cluster/health endpoint seems to return early, ignoring timeout parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orangejulius</reporter><labels /><created>2016-07-20T18:13:05Z</created><updated>2016-07-20T21:38:53Z</updated><resolved>2016-07-20T21:38:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="orangejulius" created="2016-07-20T21:38:53Z" id="234091689">As I expected this turned out to be a configuration issue on our end, not an Elasticsearch bug. Apologies for the noise!

For any future people reading this with a similar issue: we were routing calls to Elasticsearch through EBS, which killed connections after 60 seconds.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes CORS handling so that it uses the defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19522</link><project id="" key="" /><description>Fixes CORS handling so that it uses the defaults for  for http.cors.allow-methods
and http.cors.allow-headers if none are specified in the config.

Closes #19520
</description><key id="166627500">19522</key><summary>Fixes CORS handling so that it uses the defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T16:41:25Z</created><updated>2016-09-09T08:58:39Z</updated><resolved>2016-07-22T16:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-20T17:46:25Z" id="234025789">@rjernst i pushed https://github.com/elastic/elasticsearch/pull/19522/commits/ec6614760f29d8d0011c1b43adb5905bb2d56b36 
</comment><comment author="abeyad" created="2016-07-21T14:43:22Z" id="234275628">@rjernst did the last commit address your concerns?
</comment><comment author="rjernst" created="2016-07-21T16:49:10Z" id="234313838">I don't think you need the doTrim local,  but other than that yes,  thank you. 
</comment><comment author="abeyad" created="2016-07-21T19:01:39Z" id="234351148">@rjernst I pushed https://github.com/elastic/elasticsearch/pull/19522/commits/8aa52664ee40cb477b1500fdf57a710e75ffaed2
</comment><comment author="rjernst" created="2016-07-22T16:23:16Z" id="234589479">LGTM, thanks!
</comment><comment author="abeyad" created="2016-07-22T16:24:32Z" id="234589792">@rjernst thanks for the review!
</comment><comment author="s1monw" created="2016-09-09T08:40:00Z" id="245854385">@jasontedor @abeyad has this been ported to netty 4?
</comment><comment author="jasontedor" created="2016-09-09T08:57:52Z" id="245858263">&gt; has this been ported to netty 4?

@s1monw I think #19874 is the port?
</comment><comment author="s1monw" created="2016-09-09T08:58:39Z" id="245858449">thx @jasontedor I see you checked the box on the meta issue too ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade Jackson in 2.3, 2.x branch to at least 2.7.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19521</link><project id="" key="" /><description>Any chance Jackson could be upgrade to at least very stable and long time ago released 2.7.5 (if not 2.8)?
Now that you do not shadow it it is conflicting with my other dependencies
</description><key id="166602706">19521</key><summary>Upgrade Jackson in 2.3, 2.x branch to at least 2.7.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2016-07-20T14:57:44Z</created><updated>2016-07-25T09:07:35Z</updated><resolved>2016-07-20T16:14:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-20T15:03:35Z" id="233977459">No changes along these lines will be made in the 2.3 branch. There is #18939 open to upgrade master to 2.8.0, it might be backported to 2.4 but it might not and you should not count on it.
</comment><comment author="roytmana" created="2016-07-20T15:10:57Z" id="233979820">ok, thanks @jasontedor
@tlrx Jackson 2.8 has been released :-)
any rough time frame when back-port to 2.4 (if it is going to happen) and 2.4 release may happen?
</comment><comment author="jasontedor" created="2016-07-20T16:12:23Z" id="233999018">&gt; any rough time frame when back-port to 2.4 (if it is going to happen) and 2.4 release may happen?

Again, a backport here is not guaranteed and we will not give a time frame, even rough. You can track the status on #18939.
</comment><comment author="tlrx" created="2016-07-25T09:07:34Z" id="234897929">&gt; @tlrx Jackson 2.8 has been released :-)

I know, thanks :) We still have to fix the way some `XContent` objects are built and it should be OK to upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cors Access-Request-Control-Headers ignoring defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19520</link><project id="" key="" /><description>According to the docs, the `http.cors.allow-methods` setting should default to `OPTIONS,HEAD,GET,POST,PUT,DELETE`, but it appears to default to no methods at all.

With the default settings:

```
curl -H 'Origin: http://localhost:8000' -H 'Access-Control-Request-Method: PUT' -i -X OPTIONS localhost:9200/t/t/1 -d{}
```

returns:

```
HTTP/1.1 200 OK
Access-Control-Allow-Origin: http://localhost:8000
Vary: Origin
Access-Control-Max-Age: 1728000
date: Wed, 20 Jul 2016 14:37:09 GMT
content-length: 0
```

If I set `http.cors.allow-methods` to `OPTIONS,HEAD,GET,POST,PUT,DELETE`, it returns:

```
HTTP/1.1 200 OK
Access-Control-Allow-Origin: http://localhost:8000
Vary: Origin
Access-Control-Allow-Methods: HEAD
Access-Control-Allow-Methods: DELETE
Access-Control-Allow-Methods: POST
Access-Control-Allow-Methods: GET
Access-Control-Allow-Methods: OPTIONS
Access-Control-Allow-Methods: PUT
Access-Control-Max-Age: 1728000
date: Wed, 20 Jul 2016 14:38:18 GMT
content-length: 0
```
</description><key id="166597722">19520</key><summary>Cors Access-Request-Control-Headers ignoring defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T14:38:33Z</created><updated>2016-07-22T16:25:28Z</updated><resolved>2016-07-22T16:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Strings.java</file><file>core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java</file><file>core/src/test/java/org/elasticsearch/common/StringsTests.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpServerTransport.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/cors/Netty3CorsConfigBuilder.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpServerTransportTests.java</file></files><comments><comment>Fixes CORS handling so that it uses the defaults</comment></comments></commit></commits></item><item><title>Elasticsearch unassigned shards causing ES and Kibana to break.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19519</link><project id="" key="" /><description>I have set up ES and Kibana to monitor some servers. We have roughly 150 servers that need to be monitored using Winlogbeat. I set it up and tested it with 2 fairly active servers, it was working fine so we decided to throw an extra 10 servers into ES using winlogbeat. At this point, ES and Kibana died and is displaying Status Red as seen here: http://pastebin.com/0DDqi0fQ

The output of curl -XGET "http://192.168.60.90:9200/_cluster/health/?level=indices" is: http://www.pastebin.com/y0jLHDkP

I tried using curl -XGET 192.168.60.90:9200/_cat/recovery?v to see what was going on. There are thousands of entries and they are all yellow except for a few reds. Here is the output: http://pastebin.com/TC3b0A9X

One final command I found online and seems useful but I'm unsure is curl -XGET "http://192.168.60.90:9200/_cluster/state/routing_table,routing_node". 
I got the following: http://www.pastebin.com/LQXh747y
This is just a snippet of that output but as you can see there are several unassigned.

So, from what I can understand is that some instances of winlogbeat have not been assigned shards. Whats the fix here? Thanks.
</description><key id="166585912">19519</key><summary>Elasticsearch unassigned shards causing ES and Kibana to break.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BrandonMcGrath</reporter><labels /><created>2016-07-20T13:49:39Z</created><updated>2016-07-20T13:56:26Z</updated><resolved>2016-07-20T13:54:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-20T13:54:45Z" id="233956014">Hi @BrandonMcGrath 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="BrandonMcGrath" created="2016-07-20T13:56:25Z" id="233956501">No help is coming out of there :\
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Verify lower level transport exceptions don't bubble up on disconnects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19518</link><project id="" key="" /><description> #19096 introduced a generic TCPTransport base class so we can have multiple TCP based transport implementation. These implementations can vary in how they respond internally to situations where we concurrently send, receive and handle disconnects and can have different exceptions. However, disconnects are important events for the rest of the code base and should be distinguished from other errors (for example, it signals TransportMasterAction that it needs to retry and wait for the a (new) master to come back).  Therefore, we should make sure that all the implementations do the proper translation from their internal exceptions into ConnectTransportException which is used externally. 

Similarly we should make sure that the transport implementation properly recognize errors that were caused by a disconnect as such and deal with them correctly. This was, for example, the source of a build failure at https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/1080 , where a concurrency issue cause SocketException to bubble out of MockTcpTransport.

This PR adds a tests which concurrently simulates connects, disconnects, sending and receiving and makes sure the above holds. It also fixes anything (not much!) that was found it.
</description><key id="166584359">19518</key><summary>Verify lower level transport exceptions don't bubble up on disconnects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Network</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T13:42:59Z</created><updated>2016-07-22T12:35:47Z</updated><resolved>2016-07-22T12:35:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-20T13:43:14Z" id="233952689">@s1monw can you take a careful look? I'm new to this code.
</comment><comment author="s1monw" created="2016-07-21T09:18:51Z" id="234200861">left some minors LGTM in general
</comment><comment author="bleskes" created="2016-07-21T16:38:44Z" id="234311118">@s1monw thx for the feedback. I updated the PR
</comment><comment author="s1monw" created="2016-07-22T12:33:37Z" id="234531772">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/transport/NetworkExceptionHelper.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file></files><comments><comment>Verify lower level transport exceptions don't bubble up on disconnects (#19518)</comment></comments></commit></commits></item><item><title>Randomize cluster configuration for rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19517</link><project id="" key="" /><description>We're using the REST tests more and more instead of `ESIntegTestCase`. This causes us to lose a few things:
1. REST tests run against a single node so we rarely test serialization. `ESIntegTestCase` tests ran against a randomized number of nodes and usually did serialization between the nodes. As much as we _should_ have unit tests for serialization I think we _should_ have integration tests that use it as well.
2. ESIntegTestCase ran the mock plugins which did things like assert that we didn't leave any contexts open across test cases. We don't do that at all in REST tests.

I'm sure there are more things about the cluster configuration but I can't think of them off the top of my head.
</description><key id="166583761">19517</key><summary>Randomize cluster configuration for rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label><label>enhancement</label><label>test</label></labels><created>2016-07-20T13:40:16Z</created><updated>2017-05-05T13:12:10Z</updated><resolved>2017-05-05T13:12:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-20T14:04:04Z" id="233958833">Good points!

I think we should re-evaluate what we use REST tests for and how we want to move forward, especially now that we have a REST client.

A bit of history: REST tests were first introduced to have all language clients run the same tests. Core elasticsearch started running those same tests as well to test the otherwise untested elasticsearch REST layer (integration tests used solely the java api). Core started to add REST tests too in order to increase coverage for the REST layer too, and those tests made sense for the clients too, as they effectively meant more coverage for the clients too. 

A while ago we decided to move away from testing against an internal cluster, which runs in the same jvm as the tests, as that is a scenario that's far from reality. The closer thing we had at the time to a real integration test relying on an external cluster was the REST tests, so we started using those more and more. We are now in a situation where we use REST tests for a lot of different situations, and clients can run only a subset of those. But REST tests come with limitations, as the infra wasn't made to run full blown integration tests with complicated setups, plus if we have REST tests that are only run by us to test core and integrations, I am wondering if it still make sense to write them in yaml format. Shouldn't we have proper java integration tests for that purpose? Maybe we should invest in some other test infra that programmatically uses the REST client to send requests. I think we are where we are because we didn't have a java REST client, and REST tests were the easier way to write a test that was http based. Maybe other reasons too?
</comment><comment author="karmi" created="2016-07-20T16:18:44Z" id="234000761">Chiming in here with @javanna. I think we should open up a discussion about the REST (YAML) test suite, asking ourselves questions such as "what is exactly the position of these tests within the Elasticsearch core suite", and "what goals should these tests fulfill".

As Luca writes, the REST (YAML) tests were historically a device to ensure all language clients, or "anything that talks to Elasticsearch over HTTP", can be verified in a shared way in its correctness. There's a tremendous value in that, however technically trivial and "small" it ultimately is.

Of course, over time, there are many specific things within the context of Elasticsearch core to test, and these things require a much more flexible and powerful infrastructure to support them. The question is, in my mind, whether we should "bend" the REST (YAML) tests for this purpose, or as Luca suggests, come up with a _different_ infrastructure for these. This other infrastructure could indeed leverage our knowledge and code implementations of the YAML tests, but could be something different, more flexible in terms of Elasticsearch core integration testing.

In other words, there's a big benefit in eg. randomizing the number of nodes the REST tests are run against, but that is more _interesting_ for Elasticsearch itself, than to various language clients we have. 

Even strictly pragmatically, imagine the Ruby test suite fails because the test assumed to be run against a single node, not two (as happened to me [the other day](https://github.com/elastic/elasticsearch/commit/9b1a4771207f21ca275cf10c3a4e8717c9e45311)). Now I'm probably left confused, sometimes the suite runs, sometimes it fails, I have to ask somebody about the problem, ... 

In summary, I think we should ask if what we need aren't _two different REST test suites_:
1. a common REST (YAML) test suite, which tests correctness of language clients and serves as their integration tests, and
2. an Elasticsearch core integration test suite, which might or might not be encoded in YAML, which is run only by Elasticsearch core, to serve as its integration tests suite, with all the nice things like randomized number of nodes, custom configurations, specific plugins installed, and so on.
</comment><comment author="karmi" created="2016-07-20T16:22:15Z" id="234001750">CC: @elastic/es-clients
</comment><comment author="nik9000" created="2016-07-20T17:08:23Z" id="234014885">So, yeah, I just had another conversation about this with @clintongormley. I think we should move away from putting _all_ of the testing of REST things that we do for Elasticsearch into the REST tests. They function very well as smoke tests for Elasticsearch's REST behavior and for the client's interaction but their lack of power, specifically lack of arbitrary loops and methods, means we copy and paste things all over the place. _And_ we keep adding more and more complexity to them and that adds more and more burden on the language clients.

I still think Elasticsearch should randomize the configuration when it runs the yaml rest tests because it can get more value out of that. I don't think the clients should need to worry about that. Their goal isn't to find Elasticsearch bugs, but instead to find bugs they might have in interacting with Elasticsearch.

So I'm going to spend some time this afternoon putting together some REST level tests for Elasticsearch in Java using the its client, probably by reverting the REST changes I made in https://github.com/elastic/elasticsearch/pull/19509 and redoing the checks as other tests.
</comment><comment author="rjernst" created="2016-07-20T17:59:09Z" id="234029608">&gt; I still think Elasticsearch should randomize the configuration when it runs the yaml rest tests because it can get more value out of that

The point of using rest tests for integTest was to test _integration_ of all the componenents, not to test individual behavior. Eg, we should not be testing every variation of a feature, only that that feature is hooked in correctly (the integration of the components). I don't think randomizing is going to help, it will only be then like it was when we had more ESIntegTestCases: sporadic failures that have nothing to do with the feature/test that failed. Let's instead invest in more unit tests, and keep the integration testing to a bare minimum. I don't think randomizing the number of nodes is useful, but if we want to first run the rest tests on a single node cluster, and then run them on a two node cluster, I think that would be fine to ensure eg distributed actions are hooked up correctly.
</comment><comment author="rjernst" created="2016-07-20T18:00:55Z" id="234030172">Also note, we already have a multi node test that runs all the rest tests, but that is only with the core rest tests (not those for plugins/modules). Making it a standard part of rest testing (test on one node cluster, then test on two node cluster) would be good, and we could remove the multi node smoke test.
</comment><comment author="rmuir" created="2016-07-20T18:24:09Z" id="234037018">I don't think integration tests should be randomized. It is a good approach for unit tests.
</comment><comment author="s1monw" created="2016-07-20T20:24:52Z" id="234070820">&gt; I don't think integration tests should be randomized. It is a good approach for unit tests.

++
</comment><comment author="bleskes" created="2016-07-21T13:21:20Z" id="234250945">&gt; I don't think integration tests should be randomized. It is a good approach for unit tests.

We strive to make all our API work in multiple cluster setups - for example, with or without dedicated master nodes. I think it make sense to run the integration with all of those configurations and I don't think we need to test all of them all the time, or is that what you suggest? There is a combinatorial explosion issue here - dedicated coordinating nodes, dedicate ingest ndoes etc. 
</comment><comment author="rmuir" created="2016-07-21T13:26:23Z" id="234252232">I dont recommend using randomization for this purpose, myself. As far as there being too many possibilities, perhaps there shouldnt be so much complexity/so many damn settings :)

I definitely don't think randomization it should be applied to integration tests: nobody wants to debug that. Keep integration tests as simple as possible.
</comment><comment author="nik9000" created="2016-07-21T13:27:45Z" id="234252570">&gt; So I'm going to spend some time this afternoon putting together some REST level tests for Elasticsearch in Java using the its client, probably by reverting the REST changes I made in #19509 and redoing the checks as other tests.

I added a few commits to #19509 that make it relatively simple to use the REST client to test Elasticsearch with java code. It feels like the right tool for the job of testing the new header proposed in that PR, mostly because not all clients expose headers at all. It might be the right tool for other similar jobs. Anyone interested in that (admittedly small portion of this conversation) please review over there.
</comment><comment author="nik9000" created="2016-07-21T13:34:34Z" id="234254351">If we aren't going to randomize while running the REST tests we should certainly run more than one node. That is a much more consistent test that things hook together.

I think some of the point of my opening this issue is lost in the question of randomization. I'd like to point out that the REST tests also have significantly less strong assertions around things like in flight contexts leaking out of the test. `ESIntegTestCase` has a handful of similar assertions that are quite helpful.
</comment><comment author="karmi" created="2016-07-21T14:56:57Z" id="234280047">&gt; If we aren't going to randomize while running the REST tests we should certainly run more than one node. That is a much more consistent test that things hook together.

+1
</comment><comment author="s1monw" created="2016-07-21T15:13:43Z" id="234285364">we do run `smoke-test-multinode` and `backwards-5.0` with multiple nodes I think we are doing ok here?
</comment><comment author="nik9000" created="2016-07-21T15:21:50Z" id="234287839">`smoke-test-multinode` doesn't run _all_ the rest tests, just the ones from core, right? If we ran all rest tests with multiple nodes we could drop `smoke-test-multinode` I think.

I'd like us to think about the search context leaks and whatever other things that `ESIntegTestCase` adds that I can't name off hand as well.
</comment><comment author="s1monw" created="2016-07-21T15:45:24Z" id="234295414">@nik9000 for stuff like this we still have `ESIntegTestCase`? we can still write stress tests in there etc? 
</comment><comment author="nik9000" created="2016-07-21T15:54:29Z" id="234298330">I remember the hanging search context checker catching all kinds of little mistakes when I was working on stuff in the past. I don't recall exactly what, but I'm fairly sure it wasn't stress tests.
</comment><comment author="nik9000" created="2016-07-27T21:50:05Z" id="235732386">Another thing we should think about along these lines: should we run the rest tests with assertions enabled on the cluster we're testing?
</comment><comment author="javanna" created="2016-11-28T07:55:07Z" id="263205076">@nik9000 introduced the ESRestTestCase base class, that allows to send rest requests via java to an external cluster. What else do we want to do/change in our test infra?</comment><comment author="nik9000" created="2016-11-28T15:08:53Z" id="263295045">The two points in the description are still valid, I think. We only use a single node for rest tests by default and we don't have any of the fancy leak detection. If we're fine with losing those as we write more rest tests then we should close the issue.</comment><comment author="javanna" created="2017-05-05T13:12:10Z" id="299460767">I think we are fine with the current limitations of our yaml tests. Closing.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add recovery source to ShardRouting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19516</link><project id="" key="" /><description>The recovery source of a shard is currently determined by information required from `ShardRouting` as well as `IndexMetadata`. The in-sync allocation ids have to be considered for example to determine whether a new shard copy is to be created or existing copy is to be recovered. Similar for shrink index, where we have to check that a sourceShrinkIndex is present. This PR adds an explicit `recoverySource` field to `ShardRouting` that characterizes the type of recovery to perform:
- fresh empty shard copy
- existing local shard copy
- recover from primary
- recover from snapshot
- recover from other local shards on same node (shrink index action)
</description><key id="166542701">19516</key><summary>Add recovery source to ShardRouting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-beta1</label></labels><created>2016-07-20T09:52:45Z</created><updated>2016-08-27T14:12:57Z</updated><resolved>2016-08-27T14:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-20T13:01:33Z" id="233941878">that's a great consolidation left some minors and it LGTM in general, but I can do another deep review if you want.
</comment><comment author="bleskes" created="2016-07-21T11:54:44Z" id="234232297">@ywelsch discussed this and I like the direction and reduced dependency between the meta data and allocation universe. What I wish we didn't have to do is introduce a new concept to ShardRouting/Allocation as it is a complex one already. Since the new RecoverySource is only really relevant to primary shards, I suggested to try and generalize RestoreSource (which has the same property) and make it cover all primary restore cases (Store, Snapshot etc.)
</comment><comment author="ywelsch" created="2016-07-28T10:05:32Z" id="235853847">I've pushed a change that generalizes RestoreSource to RecoverySource, introducing various subclasses for the different recovery types.
</comment><comment author="ywelsch" created="2016-08-23T15:09:24Z" id="241763409">I've rebased this PR due to the many other changes that were made to the allocation world in the last weeks. I have also made the changes that were discussed with @bleskes a few weeks ago (essentially expose the enum type of `RecoverySource` through an abstract `getType` method, and then switch on that at the various call sites). Can you give it another look @s1monw?
</comment><comment author="s1monw" created="2016-08-26T09:30:48Z" id="242679290">@ywelsch I had some minors along the way - this LGTM and you can just fix the things I commented on and push.
</comment><comment author="ywelsch" created="2016-08-27T14:12:57Z" id="242919449">Thanks for the reviews @s1monw and @bleskes!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/TransportClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterIndexHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterShardHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RecoverySource.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RestoreSource.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/IndexMetaDataUpdater.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateEmptyPrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateStalePrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/NodeVersionAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/PeerRecoverySourceService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplanationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RecoverySourceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableGenerator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PriorityComparatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/AbstractIndicesClusterStateServiceTestCase.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryTargetTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/TruncatedRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestIndicesActionTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestRecoveryActionTests.java</file><file>core/src/test/java/org/elasticsearch/search/internal/ShardSearchTransportRequestTests.java</file><file>test/framework/src/main/java/org/elasticsearch/cluster/ESAllocationTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file></files><comments><comment>Add recovery source to ShardRouting (#19516)</comment></comments></commit></commits></item><item><title>Don't register repository settings in S3 plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19515</link><project id="" key="" /><description>Follow up for https://github.com/elastic/elasticsearch/pull/17784#discussion_r64575845

Today we are registering repository settings when `S3RepositoryPlugin` starts:

``` java
        settingsModule.registerSetting(S3Repository.Repository.KEY_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.SECRET_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BUCKET_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.ENDPOINT_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.PROTOCOL_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.REGION_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.SERVER_SIDE_ENCRYPTION_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BUFFER_SIZE_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.MAX_RETRIES_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.CHUNK_SIZE_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.COMPRESS_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.STORAGE_CLASS_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.CANNED_ACL_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BASE_PATH_SETTING);
```

We don't need to register those settings as they are repository level settings and not node level settings.

Closes #18945.
</description><key id="166537900">19515</key><summary>Don't register repository settings in S3 plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T09:28:09Z</created><updated>2016-07-20T10:13:29Z</updated><resolved>2016-07-20T10:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-20T09:59:26Z" id="233906136">Left a small question, LGTM o.w.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restore parameter name auto_generate_phrase_queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19514</link><project id="" key="" /><description>During query refactoring the query string query parameter
'auto_generate_phrase_queries' was accidentally renamed
to 'auto_generated_phrase_queries'.

With this commit we restore the old name.

Closes #19512
</description><key id="166530866">19514</key><summary>Restore parameter name auto_generate_phrase_queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Query DSL</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T08:51:00Z</created><updated>2016-07-20T11:14:01Z</updated><resolved>2016-07-20T11:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-20T08:51:16Z" id="233890731">@MaineC: Could please have a look.
</comment><comment author="danielmitterdorfer" created="2016-07-20T09:42:53Z" id="233902363">jenkins, test it
</comment><comment author="dliappis" created="2016-07-20T10:18:28Z" id="233910244">jenkins, test it
</comment><comment author="MaineC" created="2016-07-20T10:59:42Z" id="233918196">LGTM (provided the triggered Jenkins check (great new feature by the way) succeeds)
</comment><comment author="danielmitterdorfer" created="2016-07-20T11:13:45Z" id="233920850">@MaineC thanks! The failing builds were just a transient Jenkins issue and had nothing to do with my changes. Checks are green now so I'll merge.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java</file></files><comments><comment>Restore parameter name auto_generate_phrase_queries (#19514)</comment></comments></commit></commits></item><item><title>Support specific key/secret for EC2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19513</link><project id="" key="" /><description>We have a bug described in #19445 where a user wants to define specific key/secret for EC2 and S3:

```
cloud:
    aws:
        s3:
            access_key: xxxxxxxxxxxx
            secret_key: XXxxxxxXxxxxXxxxXxXxXxxx
        ec2:
            access_key: yyyyyyyyyyyyy
            secret_key: yyyYyyyyyYyyyyYyyyYyYyY
```

This is documented and should work.

This commit fixes that and adds a test.

BTW, while working on this issue, I discovered that the removal effort made in #12978 has been reintroduced by mistake in fe7421986c71851ac9689b1ab9feaed618a7064a. So I removed this again.

Closes #19445
</description><key id="166528957">19513</key><summary>Support specific key/secret for EC2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>bug</label><label>v2.4.0</label></labels><created>2016-07-20T08:41:37Z</created><updated>2016-09-19T23:25:33Z</updated><resolved>2016-07-20T10:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-20T08:43:39Z" id="233888588">Note that this issue does not exist in 5.0 (master branch) because we fixed it while working on the new setting infrastructure. See https://github.com/elastic/elasticsearch/commit/37b0fc4f1080f7b891dd15f494279c0b2ac328a1#diff-f619b4dc1b259eab1be104920a8d8bd5R86
</comment><comment author="ywelsch" created="2016-07-20T10:25:28Z" id="233911637">LGTM! Thanks @dadoonet 
</comment><comment author="GabrielLaden-minted" created="2016-09-19T21:25:18Z" id="248131804">fyi, I can't create an s3 snapshot repository in es 2.4 but I can in 2.3.5.
</comment><comment author="dadoonet" created="2016-09-19T21:37:17Z" id="248134971">Could you open a new issue and share your settings ?
Of course, hide your key/secret.

Thanks!
</comment><comment author="GabrielLaden-minted" created="2016-09-19T23:25:33Z" id="248158229">Oh, now I have it working in both 2.4 and 2.3.5 (on newly deployed instances), and I hadn't changed not a single thing in my ansible code, just relaunched 2.3.5 and it worked, and then i relauched 2.4.0 and it worked. I don't know why it was failing to "create an s3 repository" for me. The plugin was listed. IDK, perhaps a cluster-wide restart would have helped. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>auto_generate_phrase_queries accidentally renamed to auto_generated_phrase_queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19512</link><project id="" key="" /><description>This one originally came up in https://discuss.elastic.co/t/the-auto-generate-phrase-queries-parameter-of-query-string-query-was-renamed-to-auto-generated-phrase-queries/55934.
</description><key id="166528000">19512</key><summary>auto_generate_phrase_queries accidentally renamed to auto_generated_phrase_queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Query DSL</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T08:36:39Z</created><updated>2016-07-20T11:13:57Z</updated><resolved>2016-07-20T11:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java</file></files><comments><comment>Restore parameter name auto_generate_phrase_queries (#19514)</comment></comments></commit></commits></item><item><title>how to setting the default value of precision_threshold </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19511</link><project id="" key="" /><description> i use elasticsearch-2.2.1,when i cardinality a field for 1 day,and use different  precision_threshold  values got a different cardinality value,in my case ,i use grafana for dash board,and  its unique count request have no feather for precision_threshold , so  i wanna to konw if  es can setting a default value for precision_threshold ,if it can ,where  is setting ,and if i set the value 400000, will it coz some bad results? most of my sence ,it is need to cardinality  operation for a field
the request json like:

```
{
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "inputDate": {
                                    "gte": "1468857600000",
                                    "lte": "1468943999999"
                                }
                            }
                        }
                    ]
                }
            }
        }
    },
     "aggs": {
        "partyId_count": {
            "cardinality": {
                "field": "getterGiftId",
                "precision_threshold": 40000
            }
        }
    }
}'
```
</description><key id="166506950">19511</key><summary>how to setting the default value of precision_threshold </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tonyC2016</reporter><labels /><created>2016-07-20T06:40:11Z</created><updated>2016-07-26T07:27:08Z</updated><resolved>2016-07-20T07:17:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-20T07:17:18Z" id="233862877">There is no way to set the default `precision_threshold`.

&gt;  if i set the value 400000, will it coz some bad results

Yes, counts are approximate: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html#_counts_are_approximate
</comment><comment author="tonyC2016" created="2016-07-20T07:41:27Z" id="233867808">@jpountz  yes i had read the doc
</comment><comment author="tonyC2016" created="2016-07-26T01:14:08Z" id="235134426">@jpountz ，yeah, another question,if there is no a default value for precision_threshold.how can es give back a not excalty metric value ,is es designed just for a real time  and threw the right result of data away?
</comment><comment author="jpountz" created="2016-07-26T07:27:08Z" id="235184234">Indeed elasticsearch wants search requests to be realtime. Computing exact counts in the general case would require shards to exchange the set of values they have locally, which could be gigabytes on high-cardinality fields.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix not_analyzed string fields to error when position_increment_gap is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19510</link><project id="" key="" /><description>Currently if a string field is not_analyzed, but a
position_increment_gap is set, it will lookup the default analyzer and
set it, along with the position_increment_gap, before the code which
handles setting the keyword analyzer for not_analyzed fields has a
chance to run. This change adds a parsing check and test for that case.
</description><key id="166471731">19510</key><summary>Fix not_analyzed string fields to error when position_increment_gap is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-20T00:56:44Z</created><updated>2016-07-21T10:37:11Z</updated><resolved>2016-07-20T06:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-20T01:04:58Z" id="233811035">Analyzer has two gaps: one for positions, and another for offsets.

Confusingly, ES previously named the positions one `positions_offset_gap` (see https://github.com/elastic/elasticsearch/issues/12562), but i've seen separately an `offsetGap` (hopefully its not configurable, its basically going to guarantee broken highlighting...)

Are both the bases covered?
</comment><comment author="rjernst" created="2016-07-20T01:11:40Z" id="233811918">@rmuir Yes, we have tests that ensure position_offset_gap is rejected. And I don't see `offsetGap` anywhere (except in actual Analyzer example code)
</comment><comment author="rmuir" created="2016-07-20T01:16:39Z" id="233812607">i'm not really concerned about `position_offset_gap`, which was simply a confusing name that ES mapped with Analyzer's position increment gap.

Lucene has two gaps, and looking at the source code ES configures both of them (https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzer.java#L47)

So I want to make sure its prevented in both cases.
</comment><comment author="rjernst" created="2016-07-20T01:17:53Z" id="233812779">Mappings do not have a way to set the actual lucene offset gap.
</comment><comment author="rmuir" created="2016-07-20T01:25:03Z" id="233813700">OK, then this change is fine. Separately, exposing that offset gap in any way should be seriously reconsidered: from a practical perspective, all it will accomplish is to break highlighting.
</comment><comment author="rmuir" created="2016-07-20T01:30:53Z" id="233814626">Also i spoke with @rjernst, a more general solution would be this check:

```
// position_increment_gap is senseless unless positions are indexed
if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) &lt; 0) {
  error(...)
}
```
</comment><comment author="rjernst" created="2016-07-20T02:09:23Z" id="233819679">@rmuir I updated to use the check you have above, added another test for disabling positions, and also added this for `text` fields which I realized were prone to this if positions were disabled.
</comment><comment author="jpountz" created="2016-07-20T06:11:46Z" id="233847406">LGTM
</comment><comment author="rjernst" created="2016-07-20T06:37:46Z" id="233852946">2.4: f91e605
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TextFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Merge pull request #19510 from rjernst/mapper_keyword_positions</comment></comments></commit></commits></item><item><title>Add Location header to the index, update, and create APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19509</link><project id="" key="" /><description>This adds a header that looks like `Location: /test/test/1` to the
response for the index/create API. The requirement for the header
comes from https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html

https://tools.ietf.org/html/rfc7231#section-7.1.2 claims that relative
URIs are OK. So we use an absolute path which should resolve to the
appropriate location.

This makes large changes to our rest test infrastructure, allowing us
to write junit tests that test a running cluster via the rest client.
It does this by splitting `ESRestTestCase` into two classes:
- `ESRestTestCase` is the superclass of all tests that use the rest client
  to interact with a running cluster.
- `ESClientYamlSuiteTestCase` is the superclass of all tests that use the
  rest client to run the yaml tests. These tests are shared across all
  official clients, thus the `ClientYamlSuite` part of the name.

Closes #19079
</description><key id="166455826">19509</key><summary>Add Location header to the index, update, and create APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T22:51:02Z</created><updated>2016-07-27T22:51:04Z</updated><resolved>2016-07-25T21:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-20T01:18:29Z" id="233812851">Nice. I left some comments.
</comment><comment author="jasontedor" created="2016-07-20T01:59:41Z" id="233818441">LGTM pending CI. :wink:
</comment><comment author="javanna" created="2016-07-20T11:48:56Z" id="233927072">Has the testing part been discussed with the clients team? These changes to the REST tests infra affect them too, plus when we want to add a new feature they need to know so they have the chance to implement it too. I am personally not sure that stashing is the best way to go here, but the most important part is that clients folks are notified and part of this discussion.
</comment><comment author="javanna" created="2016-07-20T11:55:07Z" id="233928291">About whether stashing is the way to go or not: we introduced stashing the last body as a special case to test cat apis, which return text and we wanted to execute regexes against the whole body as a string. But the last response is always available to assert against anyways, so maybe status and headers should be there too without having to go through the stash. Not sure though, especially cause the `$` prefix is convenient to prevent naming conflicts I guess.
</comment><comment author="nik9000" created="2016-07-20T13:04:29Z" id="233942576">Yeah - getting the status and headers from the stash is really just about that sweet, sweet `$` prefix. I'm happy to do it any way that makes life easy on everyone.
</comment><comment author="nik9000" created="2016-07-20T13:24:46Z" id="233947690">One of the problems with putting this in the stash is that unstashing a missing value throws an exception instead of returning null. So you can't assert that a header isn't sent. Which is lame. So maybe instead of stashing I can add a hack to support evaluating `status` and `headers` against the response? Or maybe `$status` and `$headers` so we can keep the `$`?
</comment><comment author="clintongormley" created="2016-07-21T10:47:56Z" id="234219894">Btw index/create are not the only endpoints that support PUT:

```
bulk.json:    "methods": ["POST", "PUT"],
cluster.put_settings.json:    "methods": ["PUT"],
index.json:    "methods": ["POST", "PUT"],
indices.create.json:    "methods": ["PUT", "POST"],
indices.put_alias.json:    "methods": ["PUT", "POST"],
indices.put_mapping.json:    "methods": ["PUT", "POST"],
indices.put_settings.json:    "methods": ["PUT"],
indices.put_template.json:    "methods": ["PUT", "POST"],
indices.shrink.json:    "methods": ["PUT", "POST"],
ingest.put_pipeline.json:    "methods": [ "PUT" ],
put_script.json:    "methods": ["PUT", "POST"],
put_template.json:    "methods": ["PUT", "POST"],
snapshot.create.json:    "methods": ["PUT", "POST"],
snapshot.create_repository.json:    "methods": ["PUT", "POST"],
```
</comment><comment author="nik9000" created="2016-07-21T12:23:36Z" id="234237772">One at a time I guess.... If we want to do all the others I'll probably convert what I have into something more general when I do the next one.
</comment><comment author="nik9000" created="2016-07-21T13:23:31Z" id="234251502">OK! I pushed some commits that break `ESRestTestCase` into two classes, `ESSharedRestTestCase` does what `ESRestTestCase` used to do - it runs the yaml test suite shared between Elasticsearch and the clients. The new `ESRestTestCase` just sets up the clients.

I then extended `ESRestTestCase` to create `CreatedLocationHeaderIT` in `distribution:integ-test-zip` to test the new headers. I reverted all changes that I made to the rest test infrastructure. That way the clients never test the Location header but that is a good thing since most of them don't expose the header anyway.
</comment><comment author="javanna" created="2016-07-25T13:59:17Z" id="234960858">I left a few comments. I like this a lot, thanks for the work around testing!
</comment><comment author="nik9000" created="2016-07-25T15:57:09Z" id="234996861">@javanna I think this is ready for another round.
</comment><comment author="javanna" created="2016-07-25T16:23:40Z" id="235004654">did another round, few more comments left, I like it even more now :)
</comment><comment author="nik9000" created="2016-07-25T18:27:47Z" id="235040921">@javanna ready for round 3 I think.
</comment><comment author="javanna" created="2016-07-25T19:28:41Z" id="235058263">left couple of small comments, LGTM otherwise, feel free to merge when you have addressed those.
</comment><comment author="nik9000" created="2016-07-25T19:45:01Z" id="235062481">&gt; feel free to merge when you have addressed those.

Will do! Have a good night! I'll try and do the renames and maybe the followup adding more `Location` headers and have them ready for you to review in the morning.
</comment><comment author="nik9000" created="2016-07-27T22:51:03Z" id="235745806">@clintongormley I'm going to use your list to track other actions that need the header.

&gt; bulk.json:    "methods": ["POST", "PUT"],

Returns a `200 OK` instead of a `201 CREATED`. Can't have the header even if it wanted to.

&gt; cluster.put_settings.json:    "methods": ["PUT"],

`200 OK`

&gt; index.json:    "methods": ["POST", "PUT"],

Done.

&gt; indices.create.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; indices.put_alias.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; indices.put_mapping.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; indices.put_settings.json:    "methods": ["PUT"],

`200 OK`

&gt; indices.put_template.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; indices.shrink.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; ingest.put_pipeline.json:    "methods": [ "PUT" ],

`200 OK`

&gt; put_script.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; put_template.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; snapshot.create.json:    "methods": ["PUT", "POST"],

`200 OK`

&gt; snapshot.create_repository.json:    "methods": ["PUT", "POST"],

`200 OK`

---

So, I _think_ I'm actually done. I've opened #19639 to add an assertion that'll help to check if we add any more `201 CREATED` but don't add a `Location` header. It won't be 100% but I think that is ok.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/support/RestStatusToXContentListener.java</file></files><comments><comment>Assert we return Location header with 201 CREATED</comment></comments></commit></commits></item><item><title>EsAbortPolicy violates the RejectedExecutionHandler API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19508</link><project id="" key="" /><description>The interface for `RejectedExecutionHandler` says:

``` java
public interface RejectedExecutionHandler {
    /**
     * Method that may be invoked by a {@link ThreadPoolExecutor} when
     * {@link ThreadPoolExecutor#execute execute} cannot accept a
     * task.  This may occur when no more threads or queue slots are
     * available because their bounds would be exceeded, or upon
     * shutdown of the Executor.
     *
     * &lt;p&gt;In the absence of other alternatives, the method may throw
     * an unchecked {@link RejectedExecutionException}, which will be
     * propagated to the caller of {@code execute}.
     *
     * @param r the runnable task requested to be executed
     * @param executor the executor attempting to execute this task
     * @throws RejectedExecutionException if there is no remedy
     */
    void rejectedExecution(Runnable r, ThreadPoolExecutor executor);
}
```

It's the `@throws` clause that is violated. However, `EsAbortPolicy` throws an `EsRejectedExecutionException` which does not inherit from `RejectedExecutionException`. This leads to situations that commits like 770186f6cfd1a843f6cfca3df791093459b47f4b need to remedy. This should be cleaned up, but it's probably not a small task.
</description><key id="166433776">19508</key><summary>EsAbortPolicy violates the RejectedExecutionHandler API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>PITA</label></labels><created>2016-07-19T20:57:36Z</created><updated>2016-07-19T21:51:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-19T21:51:57Z" id="233777835">`ForceQueuePolicy` violates too, although it's a lesser concern since a rejected execution exception there should never be thrown.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove AggregationStreams</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19507</link><project id="" key="" /><description>Removes `AggregationStreams`, `PipelineAggregatorStreams`, and all the stuff required to support them. Now we only have to support `NamedWriteable`.

Note: `InternalAggregation.Type` is now pretty useless as well but I'll wait to address that in a subsequent PR.
</description><key id="166420140">19507</key><summary>Remove AggregationStreams</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T19:58:08Z</created><updated>2016-07-20T14:34:01Z</updated><resolved>2016-07-20T14:34:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-20T13:33:19Z" id="233949931">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix analyzer alias processing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19506</link><project id="" key="" /><description>In the lack of tests the analyzer.alias feature was pretty much not working
at all on current master. Issues like #19163 showed some serious problems for users
using this feature upgrading to an alpha version.
This change fixes the processing order and allows aliases to be set for
existing analyzers like `default`. This change also ensures that if `default`
is aliased the correct analyzer is used for `default_search` etc.

Closes #19163
</description><key id="166417082">19506</key><summary>Fix analyzer alias processing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Analysis</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T19:44:05Z</created><updated>2016-07-21T07:32:48Z</updated><resolved>2016-07-21T07:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-19T21:52:56Z" id="233778085">I know it has existed for a while, but I never knew about the feature until now. What problem does it solve? It seems to add indirection without any benefit (an alias should not be able to be changed, or it would mean an analyzer could be swapped out, which would completely break existing mappings using it).
</comment><comment author="s1monw" created="2016-07-20T07:15:45Z" id="233862629">&gt; I know it has existed for a while, but I never knew about the feature until now. What problem does it solve? It seems to add indirection without any benefit (an alias should not be able to be changed, or it would mean an analyzer could be swapped out, which would completely break existing mappings using it).

it does - I am going to open an issue to remove it but I first need to find a good way for BWC. Until then I want it at least to work without relying on hash map iteration order
</comment><comment author="s1monw" created="2016-07-20T13:14:25Z" id="233944950">@jpountz I pushed new tests
</comment><comment author="jpountz" created="2016-07-20T14:07:44Z" id="233959904">LGTM
</comment><comment author="s1monw" created="2016-07-20T20:33:55Z" id="234073490">test this please
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Test: Fixed incorrect YAML indentation in the `indices.put_template/10_basic.yaml` test</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TextFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/indices/analysis/AnalysisModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file></files><comments><comment>Fix analyzer alias processing (#19506)</comment></comments></commit></commits></item><item><title>Handle rejected execution exception on reschedule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19505</link><project id="" key="" /><description>A self-rescheduling runnable can hit a rejected execution exception but
this exception goes uncaught. Instead, this exception should be caught
and passed to the onRejected handler. Not catching handling this
rejected execution exception can lead to test failures. Namely, a race
condition can arise between the shutting down of the thread pool and
cancelling of the rescheduling of the task. If another reschedule fires
right as the thread pool is being terminated, the rescheduled task will
be rejected leading to an uncaught exception which will cause a test
failure. This commit addresses these issues.
</description><key id="166414425">19505</key><summary>Handle rejected execution exception on reschedule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T19:30:14Z</created><updated>2016-07-20T08:57:41Z</updated><resolved>2016-07-19T19:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-07-19T19:31:10Z" id="233740585">LGTM
</comment><comment author="bleskes" created="2016-07-19T19:36:04Z" id="233741766">shouldn't we better fix ThreadPool.schedule(interval, executor, Runnable) to understand AbstractRunnable?
</comment><comment author="jasontedor" created="2016-07-19T19:39:25Z" id="233742625">&gt; shouldn't we better fix ThreadPool.schedule(interval, executor, Runnable) to understand AbstractRunnable?

@bleskes I like it, I will open another PR.
</comment><comment author="jasontedor" created="2016-07-19T19:47:13Z" id="233744637">&gt; I like it, I will open another PR.

On second thought, I don't think we should do this. First, it presents problems in terms of what to return when catching the rejected execution exception on an abstract runnable (the idea is we'd do a cast, call `AbstractRunnable#onRejected` but then what? Return `null`? Return a delayed future that has completed exceptionally? Rethrow the exception, but then what's the point?) And then it complicates the understanding of the API because of the difference in handling of abstract runnable versus plain runnable. I'm interested in hearing what you think @bleskes and @jaymode?
</comment><comment author="jaymode" created="2016-07-19T20:50:35Z" id="233761400">&gt; Return a delayed future that has completed exceptionally? Rethrow the exception, but then what's the point?

If we did anything, I'd prefer one of those. But then the delayed future would throw when the caller calls get() so I think we are just moving around where we need to handle the exception and not really adding much. 

Also, this is very likely to only happen in the race where the threadpool is shutting down and something is trying to schedule itself as the queue for the scheduler is limited to `Integer.MAX_VALUE`. I don't see a need to modify the schedule method given this.
</comment><comment author="bleskes" created="2016-07-20T08:57:41Z" id="233892178">Yeah, indeed tricky. The problem we have now is that people are used the standard executor API where you can submit an AbstractRunnable and that will give you a clear path to handle everything (onAfter, onRejected etc.). It's easy to forget that the schedule command doesn't support it. I tend to say we should add AbstractRunnable support because if people use it, they expect the onRejected to be called. I also tend to think the future, if queried, should result in an exception and be marked as done. That said, this is not a clear cut case and I can see the arguments against it. I'm OK with leaving it be for now and see how things turn out.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ScheduleWithFixedDelayTests.java</file></files><comments><comment>Handle rejected execution exception on reschedule</comment></comments></commit></commits></item><item><title>Undeprecate "aggs" in search request bodies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19504</link><project id="" key="" /><description>"aggs" accidentally got deprecated as a field name in requests, we should un-deprecate it since it is a supported parameter in search requests.

```
HTTP/1.1 500 Internal Server Error
Warning: Deprecated field [aggs] used, expected [aggregations] instead
Content-Type: application/json; charset=UTF-8
Content-Encoding: gzip
Content-Length: 556
```
</description><key id="166408668">19504</key><summary>Undeprecate "aggs" in search request bodies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>blocker</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T19:01:33Z</created><updated>2016-07-29T16:02:11Z</updated><resolved>2016-07-29T08:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-20T13:25:19Z" id="233947810">I think it'd be nice to wait until #19509 is merged so we can assert things about headers in the REST tests.
</comment><comment author="colings86" created="2016-07-21T09:08:52Z" id="234198627">Oops, I think this was me sorry. The problem is here: https://github.com/elastic/elasticsearch/blob/dec620c0b08213033dbfcacfc4b37663432a1a5c/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java#L94

The issue is that currently `ParseField` expects there to be only one acceptable name for the field and all other names are taken as deprecated. So the question here is do we want to deprecate either `aggs` or `aggregations` or do we want to add the ability to specify alternative names in `ParseField`.

If it is the latter I would suggest keeping the constructor in ParseField as is so only the first name is taken as an acceptable name and the rest are deprecated, and then add a `addAlternateName(String)` method for the few cases where we have alternative acceptable name.

As for #19509 it would be great if post that PR we could add a check to all calls made in rest tests to ensure they do not contain a warning header and if so fail the test (since most tests should not be using deprecated options). For any tests that do need to test deprecated options we could add a `accept_deprecated:true` to the YAML tests to bypass this warning header check.

wdyt?
</comment><comment author="colings86" created="2016-07-21T10:59:18Z" id="234222006">Actually we should also set the REST tests to have strict parsing anyway so using deprecated functionality causes an exception and the request to be rejected (unless the `accept_deprecated:true` option is set on the YAML test)
</comment><comment author="colings86" created="2016-07-21T12:14:00Z" id="234235912">I opened https://github.com/elastic/elasticsearch/pull/19533 to fix this using the `ParseField` changes I mentioned above. I decided to keep `ParseField` immutable so the alternative names are passed into a constructor rather than in an add method.
</comment><comment author="pickypg" created="2016-07-28T16:04:12Z" id="235941923">&gt; the request to be rejected (unless the accept_deprecated:true option is set on the YAML test)

I think it might be better to call it `expect_deprecated:true` and then have the same semantics, but also fail if it _doesn't_ get a deprecation warning returned.
</comment><comment author="nik9000" created="2016-07-28T16:23:53Z" id="235947674">&gt; I think it might be better to call it expect_deprecated:true and then have the same semantics, but also fail if it doesn't get a deprecation warning returned.

Yes, that sounds good. I'd honestly really like to make that change.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file></files><comments><comment>Undeprecates `aggs` in the search request</comment></comments></commit></commits></item><item><title>Correct Java setting to pass proxy options to plugin install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19503</link><project id="" key="" /><description>It appears that `proxyHost` and `proxyPort` are not valid Java
properties; they should be prefixed with either `http` or `https`. There
is no code that the flags pass through in `plugin` that munges the flags
in any way, so they should be passed through as indicated in the Java
docs.

Note that I don't have an environment set up to test these out, but based upon some [discussion in a related puppet-elasticsearch issue](https://github.com/elastic/puppet-elasticsearch/issues/152), it would seem that this never worked and really should have just been the vanilla Java proxy properties all along.

See:
https://docs.oracle.com/javase/7/docs/technotes/guides/net/proxies.html
https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html
</description><key id="166405406">19503</key><summary>Correct Java setting to pass proxy options to plugin install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">tylerjl</reporter><labels><label>:Plugins</label><label>docs</label><label>review</label></labels><created>2016-07-19T18:45:39Z</created><updated>2016-07-27T16:21:51Z</updated><resolved>2016-07-27T16:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-27T16:21:49Z" id="235639516">thanks @tylerjl - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Document proxy flags explicitly and nuke Windows section</comment></comments></commit></commits></item><item><title>Many index.translog settings are NOT really dynamic, despite the docs </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19502</link><project id="" key="" /><description>**Elasticsearch version**:

2.3.1

**JVM version**:

java version "1.8.0_65"

**OS version**:

CentOS Linux release 7.1.1503 (Core)

**Description of the problem including expected versus actual behavior**:

Unable to change may transog settings dynamically

index.translog.flush_threshold_size
index.translog.flush_threshold_ops
index.translog.flush_threshold_period
index.translog.interval

Get error like:

{"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"Can't update non dynamic settings[[index.persistent.index.translog.flush_threshol_period]] for open indices

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="166377196">19502</key><summary>Many index.translog settings are NOT really dynamic, despite the docs </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashokc</reporter><labels /><created>2016-07-19T16:36:51Z</created><updated>2016-07-19T20:00:09Z</updated><resolved>2016-07-19T17:58:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-19T17:58:02Z" id="233714571">You're not using the right key, you at least have a typo in the key name: `index.translog.flush_threshol_period` is missing the trailing `d` in `threshold` and I'm not sure why you have `index.persistent` there. Once you correct this, you'll see that these settings are in fact updatable:

``` bash
$ curl -XDELETE localhost:9200/_all?pretty=1
{
  "acknowledged" : true
}
$ curl -XPUT localhost:9200/i?pretty=1 -d '
{
  "settings": {
    "index": {
      "number_of_replicas": 0
    }
  }
}'
{
  "acknowledged" : true
}
$ curl -XPUT localhost:9200/i/_settings?pretty=1 -d '
{
  "index.translog.flush_threshold_period": "1m"
}'
{
  "acknowledged" : true
}
$ curl -XPUT localhost:9200/i/_settings?pretty=1 -d '
{
  "index.translog.flush_threshold_ops": 1000
}'
{
  "acknowledged" : true
}
$ curl -XPUT localhost:9200/i/_settings?pretty=1 -d '
{
  "index.translog.flush_threshold_size": "1g"
}'
{
  "acknowledged" : true
}
$
```

which produced the following server logs:

```
[2016-07-19 13:51:59,873][INFO ][cluster.metadata         ] [Paralyzer] [i] creating index, cause [api], templates [], shards [5]/[0], 
mappings []
[2016-07-19 13:51:59,915][INFO ][cluster.routing.allocation] [Paralyzer] Cluster health status changed from [RED] to [GREEN] (reason: [
shards started [[i][4]] ...]).
[2016-07-19 13:52:18,004][INFO ][index.translog           ] [Paralyzer] [i][1] updating flush_threshold_period from [30m] to [1m]
[2016-07-19 13:52:18,004][INFO ][index.translog           ] [Paralyzer] [i][2] updating flush_threshold_period from [30m] to [1m]
[2016-07-19 13:52:18,004][INFO ][index.translog           ] [Paralyzer] [i][3] updating flush_threshold_period from [30m] to [1m]
[2016-07-19 13:52:18,004][INFO ][index.translog           ] [Paralyzer] [i][0] updating flush_threshold_period from [30m] to [1m]
[2016-07-19 13:52:18,004][INFO ][index.translog           ] [Paralyzer] [i][4] updating flush_threshold_period from [30m] to [1m]
[2016-07-19 13:52:33,387][INFO ][index.translog           ] [Paralyzer] [i][1] updating flush_threshold_ops from [2147483647] to [1000]
[2016-07-19 13:52:33,387][INFO ][index.translog           ] [Paralyzer] [i][2] updating flush_threshold_ops from [2147483647] to [1000]
[2016-07-19 13:52:33,387][INFO ][index.translog           ] [Paralyzer] [i][3] updating flush_threshold_ops from [2147483647] to [1000]
[2016-07-19 13:52:33,387][INFO ][index.translog           ] [Paralyzer] [i][0] updating flush_threshold_ops from [2147483647] to [1000]
[2016-07-19 13:52:33,387][INFO ][index.translog           ] [Paralyzer] [i][4] updating flush_threshold_ops from [2147483647] to [1000]
[2016-07-19 13:52:54,830][INFO ][index.translog           ] [Paralyzer] [i][1] updating flush_threshold_size from [512mb] to [1gb]
[2016-07-19 13:52:54,831][INFO ][index.translog           ] [Paralyzer] [i][2] updating flush_threshold_size from [512mb] to [1gb]
[2016-07-19 13:52:54,831][INFO ][index.translog           ] [Paralyzer] [i][3] updating flush_threshold_size from [512mb] to [1gb]
[2016-07-19 13:52:54,831][INFO ][index.translog           ] [Paralyzer] [i][0] updating flush_threshold_size from [512mb] to [1gb]
[2016-07-19 13:52:54,831][INFO ][index.translog           ] [Paralyzer] [i][4] updating flush_threshold_size from [512mb] to [1gb]
```
</comment><comment author="ashokc" created="2016-07-19T20:00:09Z" id="233748007">Sorry, my bad. It works as advertised. Thanks a bunch Jason.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow gradle to run the other client's tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19501</link><project id="" key="" /><description>This adds a hack to the `integTest` task to support running external processes. It looks like:

```
mkdir tmp
cd tmp
git clone https://github.com/nik9000/elasticsearch.git
cd elasticsearch
git checkout gradle_runs_clients
cd ..
git clone https://github.com/elastic/elasticsearch-py.git
cd elasticsearch-py
gradle -p ../elasticsearch distribution:integ-test-zip:integTest -Drest.test.executable=python3 -Drest.test.executable.args='setup.py test'
```

That results in a few failures but that is fine because at least it runs. And most things pass which is cool. Now you can run other plugins yaml tests:

```
gradle -p ../elasticsearch modules:reindex:integTest -Drest.test.executable=python3 -Drest.test.executable.args='setup.py test'
gradle -p ../elasticsearch modules:lang-painless:integTest -Drest.test.executable=python3 -Drest.test.executable.args='setup.py test'
gradle -p ../elasticsearch plugins:ingest-user-agent:integTest -Drest.test.executable=python3 -Drest.test.executable.args='setup.py test'
```

The `python3 setup.py test` part isn't quite right because it runs _all_ the tests, not just the REST ones. But I think that is a thing we can address in the clients at some point.

The neat thing about this is that the clients that we run don't have to know about the fixtures required to run the various rest tests.
</description><key id="166367284">19501</key><summary>Allow gradle to run the other client's tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>discuss</label><label>enhancement</label></labels><created>2016-07-19T15:53:13Z</created><updated>2016-07-26T11:42:32Z</updated><resolved>2016-07-21T14:29:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-19T16:00:42Z" id="233680464">I had a look and the php client doesn't seem to support running the yaml from another directory so I can't test it with this. The ruby client doesn't seem to do so either.
</comment><comment author="karmi" created="2016-07-19T16:44:11Z" id="233693618">@nik9000, the Ruby runner supports the `TEST_REST_API_SPEC` environment variable to point it to a specific folder with tests. I haven't had much time to think about it, but I'm quite undecided about the approach, I think this approaches the problem from a reversed angle.

I would much rather have support for something like `execute` expression in th YAML tests, or in the runners, which would run a command like `gradle test setup whatever`. But again, I haven't had much time to think about it, this is more about describing a "hunch" I have.
</comment><comment author="nik9000" created="2016-07-19T16:48:50Z" id="233694902">Yeah, what I've proposed is very much the path of least resistance. It assumes the clients can adapt to Elasticsearch's build mechanisms.
</comment><comment author="nik9000" created="2016-07-19T16:56:30Z" id="233697066">This came from trouble around #19310. I want to test reindex-from-remote with authentication. But it needs some kind of proxy or a special testing elasticsearch plugin or both. I get bogged down in "how will the clients run these tests?" Something like this will make whatever we do for Elasticsearch just work for the clients too. If we don't do something like this I'm fairly sure the clients just won't be able to run those tests. I don't know if that is ok or not.
</comment><comment author="javanna" created="2016-07-19T16:58:53Z" id="233697753">Can you describe what problem this PR is addressing?  Maybe also what the gradle command that you posted does (runs the rest tests via python, but does some gradle setup beforehand I guess?) and an example of how this feature would be used in practice?
</comment><comment author="javanna" created="2016-07-19T17:00:00Z" id="233698112">Turns out hitting refresh before posting would have made my previous comment almost not needed :)
</comment><comment author="nik9000" created="2016-07-19T17:05:13Z" id="233699669">&gt; Turns out hitting refresh before posting would have made my previous comment almost not needed :)

Sorry! Yeah, it'll setup all the things Elasticsearch's build thinks it needs to run the rest tests for some step and then run _all_ of the python client's tests. The idea is that the client could, in future, change its build to run all of its normal tests and then run something like:

```
gradle -p ../elasticsearch integTest -Drest.test.executable=python3 -Drest.test.executable.args='setup.py test'
```

to run all of the rest tests for all of the modules, plugins, core, qa projects, whatever. And gradle would stand up all the dependencies for it. The client would have to adapt to that sort of two phase testing, but the advantage is you get all the dependencies handled in one spot.
</comment><comment author="rjernst" created="2016-07-19T21:46:32Z" id="233776418">Instead of hacking integTest, can we do this with a new gradle task? It can be setup the same way the rest test task is, but call it `externalTest` or something and make the sysprop required?
</comment><comment author="nik9000" created="2016-07-19T22:07:03Z" id="233781379">&gt; Instead of hacking integTest, can we do this with a new gradle task?

Yes, but I wanted to see if this was a thing we actually wanted to do before I spent the time doing that. Extracting the setup from integTest sounds like it'll be a bit fiddly.

&gt; make the sysprop required?

So, like, declare this new task all the time but don't make `check` depend on it?
</comment><comment author="rjernst" created="2016-07-19T23:14:05Z" id="233794422">&gt; So, like, declare this new task all the time but don't make check depend on it?

Yes, same as we do for packagingTest
</comment><comment author="karmi" created="2016-07-20T15:54:02Z" id="233993542">I've been thinking about it and still think that the solution is approaching the problem from the wrong side. I'd like to use this issue as an opportunity to explain the larger topic of how I view the REST (YAML) test suite in the context of Elasticsearch core and language clients.

## The problem

First, let's describe the problem itself we do indeed need to solve: for some tests, we need to execute additional actions, not currently supported by the YAML test suite. This might be, for example, launching another Elasticsearch node, to be able to test reindexing from a remote cluster. But it also might be launching the node with a specific configuration, or installing a specific plugin into Elasticsearch. All in all, actions which cannot be done by calling an Elasticsearch API (which _is_ supported by the YAML nomenclature), but can be done only by running a command (bash, gradle, ...).

## The solutions

The outlined solution views running the tests as something which can be de facto "embedded" into the Gradle infrastructure of Elasticserch core. It is certainly one of valid technical approaches, but I see it as deficient when compared to other potential solutions.

A different approach would "invert" this view, and see running the tests on another platform (Python, Ruby, ...) still as a standalone thing, but add an ability to _call external action_, which quite well might be implemented as a specific Gradle task.

I see couple of benefits of this "inverted" approach. First, running the tests is being left in the discretion of respective developers, letting them do whatever they like and whatever is appropriate on their platform. This is not a small thing, in my opinion, because it gives language developers more freedom to work with the test suite on _their_ platform.

Second, this opens up interesting posibilities, such as adding an `execute` expression to the YAML nomenclature, which would allow the test runner to call some external command. (Question such as "do we really want to run arbitrary bash scripts or just Gradle tasks" are of course further debatable, having significant security implications.)

But the immediate solution for the problem we're trying to solve here, namely launching another Elasticsearch node with a specific configuration, would be trivial to solve in this way: the runners for different languages would have to call something like `gradle test launch_node_for_reindex`, or a wrapper task like `gradle test setup`, before they run the tests, and `gradle test teardown` after they're done.

Explaining the benefit of this "inverted dependency" (_“Don't call us, we'll call you”_) further, this would allow language developers to run that `gradle test setup` task manually, in case they suspect there are problems, inspect the code of that task, and so on and on. In this way, in other words, all the different parts of the whole integration test platform (YAML tests, language runners, bootstrapping tasks) are nicely separated, yet still integrated.
</comment><comment author="karmi" created="2016-07-20T16:22:02Z" id="234001697">CC: @elastic/es-clients 
</comment><comment author="nik9000" created="2016-07-21T14:29:14Z" id="234271127">I talked to @clintongormley about this yesterday. Whatever we do, it won't be what I've proposed in this PR.

For now we're reasonably comfortable making the assertion that "there is on command you can use to start Elasticsearch and the yaml tests will only need that as a dependency". That means that some of Elasticsearch's REST testing can't be done with the YAML tests because it'll need dependencies. For example, say discovery-ec2 needs a mock ec2 endpoint to test discovery. Those tests won't be run by the clients. They aren't run now and won't be run in the future.

In addition, I've started adding REST level testing that isn't shared yaml tests to Elasticsearch's test suite in https://github.com/elastic/elasticsearch/pull/19509. That gives us another out for REST based testing - we don't have to involve the clients at all.

Given all that I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>minor documentation improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19500</link><project id="" key="" /><description /><key id="166364160">19500</key><summary>minor documentation improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefourtheye</reporter><labels><label>docs</label></labels><created>2016-07-19T15:40:24Z</created><updated>2016-07-21T12:25:19Z</updated><resolved>2016-07-21T12:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-21T10:25:06Z" id="234215548">Hi @thefourtheye 

Thanks for the PR.  I've added some corrections.  If you could update the PR with the changes then I'll be happy to merge it. 

thanks
</comment><comment author="elasticmachine" created="2016-07-21T10:25:08Z" id="234215557">Can one of the admins verify this patch?
</comment><comment author="thefourtheye" created="2016-07-21T10:27:59Z" id="234216124">@clintongormley Thanks for the review. I updated the PR now.
</comment><comment author="clintongormley" created="2016-07-21T12:25:19Z" id="234238116">thanks @thefourtheye - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>minor documentation improvements (#19500)</comment></comments></commit></commits></item><item><title>corrected the use of two synonymous words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19499</link><project id="" key="" /><description>Two synonyms were jointly used in the sentence(i.e "problems" and "issues"), so I deleted one of them.
</description><key id="166354503">19499</key><summary>corrected the use of two synonymous words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Maviza101</reporter><labels /><created>2016-07-19T15:01:56Z</created><updated>2016-07-21T10:21:54Z</updated><resolved>2016-07-21T10:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-21T10:21:54Z" id="234214885">Closed by #19498
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>corrected the use of two synonymous words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19498</link><project id="" key="" /><description>Two synonyms were jointly used in the sentence(i.e "problems" and "issues"), so I deleted one of them.
</description><key id="166351325">19498</key><summary>corrected the use of two synonymous words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Maviza101</reporter><labels><label>docs</label></labels><created>2016-07-19T14:51:05Z</created><updated>2016-07-21T20:03:52Z</updated><resolved>2016-07-21T10:20:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-21T10:20:31Z" id="234214589">thanks @Maviza101 - merged
</comment><comment author="Maviza101" created="2016-07-21T20:03:52Z" id="234367229">The pleasure is mine. @clintongormley . Glad to be of help.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>corrected the use of two synonymous words (#19498)</comment></comments></commit></commits></item><item><title>Add support for path_style_access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19497</link><project id="" key="" /><description>Add a new option `path_style_access` for S3 buckets. It adds support for path style access for [virtual hosting of buckets](http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html).

The default behaviour is to detect which access style to use based on the configured endpoint (an IP will result in path-style access) and the bucket being accessed (some buckets are not valid DNS names).

Backport of #15114 in 2.4 branch.

See also:
- https://github.com/elastic/elasticsearch-cloud-aws/issues/124
- https://github.com/elastic/elasticsearch-cloud-aws/pull/159
</description><key id="166339325">19497</key><summary>Add support for path_style_access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>feature</label><label>v2.4.0</label></labels><created>2016-07-19T14:02:30Z</created><updated>2016-07-20T08:51:29Z</updated><resolved>2016-07-19T14:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-07-19T14:08:02Z" id="233644151">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade lucene to 5.5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19496</link><project id="" key="" /><description /><key id="166337448">19496</key><summary>Upgrade lucene to 5.5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dpursehouse</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.4.0</label></labels><created>2016-07-19T13:55:07Z</created><updated>2016-08-26T13:26:28Z</updated><resolved>2016-07-28T15:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-19T14:17:31Z" id="233646896">Thanks @dpursehouse, this looks good. I'll merge.
</comment><comment author="jpountz" created="2016-07-19T14:36:09Z" id="233652768">Changing the Lucene version on a patch release is a bit scary so  I will only merge it to the 2.4 branch (which should be released "soon".
</comment><comment author="rmuir" created="2016-07-19T14:54:15Z" id="233658606">I have two concerns about upgrading 2.x to this release:

https://github.com/apache/lucene-solr/commits/branch_6x/lucene/core/src/java/org/apache/lucene/util/Version.java

So 5.5.2 does not even exist until _after_ the 6.2 release. All it takes is a bug around version handling to create a big problem. This is not unheard of. 

More concerning is, its unclear to me if the situation has ever been tested (which would easily alleviate both my concerns!). For that we have two mechanisms: TestBackwardsCompatibility (junit test) and smoketester (python). But the latter just uses the test indexes from the former, and doesn't have any additional version coverage, only the test indexes committed to the tree are tested:

https://github.com/apache/lucene-solr/blob/releases/lucene-solr/6.1.0/dev-tools/scripts/smokeTestRelease.py#L1292-L1294

So if previous version M was released after newer version N, we can be sure its never ever been tested (N reading index of M)?
</comment><comment author="jpountz" created="2016-07-19T16:07:27Z" id="233682634">This is valid concern indeed. 5.5.2 indices are already on the 6.0/6.1 branches and TestBackwardsCompatibility passes on both branches so I think we're good?
</comment><comment author="rmuir" created="2016-07-19T16:19:28Z" id="233686317">I will relent but i still insist its not the same thing. Those changes were done after the release, along with other changes which have been backported to those branches, such as addition of version constants :)

I know its been changed to no longer be an enum, and really many of these changes are just helpful and minor, I'm just saying this is how bugs can happen.
</comment><comment author="jpountz" created="2016-07-27T22:39:56Z" id="235743645">I played with TestBackwardCompatibility and the actual 6.0/6.1 releases and all seemed fine. There are some good fixes in this bugfix release (eg. LUCENE-7132) so I will merge this PR tomorrow if there are no objections.
</comment><comment author="elasticmachine" created="2016-07-27T22:39:58Z" id="235743652">Can one of the admins verify this patch?
</comment><comment author="jpountz" created="2016-07-28T15:33:11Z" id="235932185">Merged via 39869b54f5a2b83af2d94e573670d7bf287d7672
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add highlight fragment scoring mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19495</link><project id="" key="" /><description>when highlight, "order" : "score", the returned fragments are not match with that I expected.
In lately Lucene, WeightedFieldFragList is added and It seems work better.
I think WeightedFieldFragList has more chances to return various term fragments. it considers distinct term weight and terms per fragment norm.
It's hard and unefficient to take it in my application. because of number of fragment limit.

package org.elasticsearch.search.highlight;
In line 117 to 122 WeightedFieldFragList than SimpleFieldFragList. or may has ES more highlight order option that use WeightedFieldFragList?
</description><key id="166318154">19495</key><summary>add highlight fragment scoring mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">dohykim</reporter><labels><label>:Highlighting</label><label>enhancement</label></labels><created>2016-07-19T12:25:04Z</created><updated>2017-05-12T07:16:31Z</updated><resolved>2017-05-12T07:16:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-22T09:19:31Z" id="234496188">can you provide an example of what is not in the order you expect and why and preferably what you would it expect to be?
</comment><comment author="dohykim" created="2016-07-25T02:41:09Z" id="234824301">[11933505226509.hwp.txt](https://github.com/elastic/elasticsearch/files/380664/11933505226509.hwp.txt)

I use elasticsearch to service fulltext search on office documents.
Users upload their office document (excel, pdf ... etc) and search with multi keyword AND operator.
I expected the more distinct highlighted term in fragments get better score.
fragment scoring version Query [Coordination](https://www.elastic.co/guide/en/elasticsearch/guide/2.x/practical-scoring-function.html#coord)

The problem rise when execute 'high TF term + low TF term multi keyword search'. 
in example, I search on linked file by keyword "test acceptable".

I tested in ES-1.5.2, but I see use of SimpleFragListBuilder not changed in current version.

Query : 

&gt; post english/typeMain/_search
&gt; {
&gt;    "size": 1,
&gt;    "fields": [
&gt;       "highlight",
&gt;       "fileName"
&gt;    ],
&gt;    "query": {
&gt;       "match": {
&gt;          "contents": {
&gt;             "query": "test acceptable",
&gt;             "operator": "and"
&gt;          }
&gt;       }
&gt;    },
&gt;    "highlight": {
&gt;       "order": "score",
&gt;       "fields": {
&gt;          "contents": {
&gt;             "fragment_size": 128,
&gt;             "number_of_fragments": 1000
&gt;          }
&gt;       }
&gt;    }
&gt; }

Ordering result, Original =======

 "hits": {
      "total": 16,
      "max_score": 0.3831197,
      "hits": [
         {
            "_index": "english",
            "_type": "typeMain",
            "_id": "12419",
            "_score": 0.3831197,
            "highlight": {
               "contents": [
                  "OPIc rubrics and **test** items, OPIc criteria, OPIc **test** preparation, OPIc **tests** in use, and English speaking **tests**. It was found that",
                  "Bradshaw, J. “**Test**-takers  reactions to a placement **test**.” Language **Testing**, 7 (1990): 13-30.  Brown, A. “The role of **test**-taker feedback",
                  "from the **test** institution. (Chung-Ang University)  Key Words: ACTFL proficiency guidelines, oral proficiency **testing**, **test**-takers ",
                  "of Foreign Languages **Testing** Committee - Korea (ATCK). It is now considered by **test** administrators and **test** takers that a good OPIc",
                  "overviews the OPIc **testing** procedure, he or she performs tasks in an actual **test**. During this process, the **test**-taker can listen to",
                  "about the **test** (Kenyon &amp; Malabonga, 2001).  Last, the computer technology facilitates **test** results and ratings. Once OPIc **test**-taking",
                  "considered under real **test** conditions (Bachman, 1990). **Test** validation had been left to professionals concerned with **test** development and",

_"grown regarding how examinees perceive **tests**, including whether a given **test** is **acceptable** to its users or not (Davies et al., 1999)",_

"several considerations as perceived by **test**-takers.   First, **test**-takers  perceptions include **test** validity. It has been suggested that",
                  "difficulty are now **accepted** by **test**-developers when undertaking task revision (Alderson, 1998).  Second, **test**-takers  reactions are",
                  "Examinees  perceptions may assist in **test** revision and overall **acceptability** as well as **test** validity from their perspectives.   III",
                  "response time. In the following parts, **test**-takers  perceptions on OPIc **test** format, preparation and **test** results in use will be presented",
                  "differences between **test**-takers  attitudes with regard to **test** preparation and the intent of OPIc.   Table 7. OPIc **Test** Preparation   Statement",
                  "8 40.\u001f\u001fAn OPIc **test** can be a substitute for       \u001fthe TOEIC **test**.  3.29 1.06 47.6 22.0 4.3 English Speaking **Tests** Table 9 shows general",
                  "discussed OPIc **test**-takers  perceptions regarding ACTFL and ACTFL proficiency guidelines, OPIc **test** format, **test** preparation, and",
                  "Korea, English speaking **tests** are frequently used, although **test**-specific strategies are used mainly to raise **test** scores rather than",
                  " Hence there is a major gap between **test** preparation and the intent of the **test** itself. **Test**-takers should be provided with sufficient",
                  "develop proficiency **testing** materials, to use oral proficiency **test** scores appropriately, and to teach to the **test**. As noted in Table",

# 

And I switch SimpleFragListBuilder to WeightedFragListBuilder and build. (org.elasticsearch.search.highlight.FastVectorHighlighter.java)

` 
if (field.fieldOptions().numberOfFragments() == 0) {
                    fragListBuilder = new SingleFragListBuilder();

```
                if (!forceSource &amp;&amp; mapper.fieldType().stored()) {
                    fragmentsBuilder = new SimpleFragmentsBuilder(mapper, field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                } else {
                    fragmentsBuilder = new SourceSimpleFragmentsBuilder(mapper, context, hitContext, field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                }
            } else {
                fragListBuilder = field.fieldOptions().fragmentOffset() == -1 ? new WeightedFragListBuilder() : new WeightedFragListBuilder(field.fieldOptions().fragmentOffset());
                if (field.fieldOptions().scoreOrdered()) {
                    if (!forceSource &amp;&amp; mapper.fieldType().stored()) {
                        fragmentsBuilder = new ScoreOrderFragmentsBuilder(field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                    } else {
                        fragmentsBuilder = new SourceScoreOrderFragmentsBuilder(mapper, context, hitContext, field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                    }
                } else {
                    if (!forceSource &amp;&amp; mapper.fieldType().stored()) {
                        fragmentsBuilder = new SimpleFragmentsBuilder(mapper, field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                    } else {
                        fragmentsBuilder = new SourceSimpleFragmentsBuilder(mapper, context, hitContext, field.fieldOptions().preTags(), field.fieldOptions().postTags(), boundaryScanner);
                    }
                }
```

}
`

Result : 
  "hits": {
      "total": 16,
      "max_score": 0.3831197,
      "hits": [
         {
            "_index": "english",
            "_type": "typeMain",
            "_id": "12419",
            "_score": 0.3831197,
            "highlight": {
               "contents": [
 _"grown regarding how examinees perceive **tests**, including whether a given **test** is **acceptable** to its users or not (Davies et al., 1999)",_
                  "difficulty are now **accepted** by **test**-developers when undertaking task revision (Alderson, 1998).  Second, **test**-takers  reactions are",
                  "Examinees  perceptions may assist in **test** revision and overall **acceptability** as well as **test** validity from their perspectives.   III",
                  "OPIc rubrics and **test** items, OPIc criteria, OPIc **test** preparation, OPIc **tests** in use, and English speaking **tests**. It was found that",
                  "Bradshaw, J. “**Test**-takers  reactions to a placement **test**.” Language **Testing**, 7 (1990): 13-30.  Brown, A. “The role of **test**-taker feedback",
                  "from the **test** institution. (Chung-Ang University)  Key Words: ACTFL proficiency guidelines, oral proficiency **testing**, **test**-takers ",
                  "of Foreign Languages **Testing** Committee - Korea (ATCK). It is now considered by **test** administrators and **test** takers that a good OPIc",
                  "overviews the OPIc **testing** procedure, he or she performs tasks in an actual **test**. During this process, the **test**-taker can listen to",
                  "about the **test** (Kenyon &amp; Malabonga, 2001).  Last, the computer technology facilitates **test** results and ratings. Once OPIc **test**-taking",
                  "considered under real **test** conditions (Bachman, 1990). **Test** validation had been left to professionals concerned with **test** development and",
                  "several considerations as perceived by **test**-takers.   First, **test**-takers  perceptions include **test** validity. It has been suggested that",
                  "response time. In the following parts, **test**-takers  perceptions on OPIc **test** format, preparation and **test** results in use will be presented",
                  "differences between **test**-takers  attitudes with regard to **test** preparation and the intent of OPIc.   Table 7. OPIc **Test** Preparation   Statement",
                  "8 40.\u001f\u001fAn OPIc **test** can be a substitute for       \u001fthe TOEIC **test**.  3.29 1.06 47.6 22.0 4.3 English Speaking **Tests** Table 9 shows general",
                  "discussed OPIc **test**-takers  perceptions regarding ACTFL and ACTFL proficiency guidelines, OPIc **test** format, **test** preparation, and",
                  "Korea, English speaking **tests** are frequently used, although **test**-specific strategies are used mainly to raise **test** scores rather than",
                  " Hence there is a major gap between **test** preparation and the intent of the **test** itself. **Test**-takers should be provided with sufficient",
                  "develop proficiency **testing** materials, to use oral proficiency **test** scores appropriately, and to teach to the **test**. As noted in Table",
                  "informed of appropriate and ethical **test** content and **test** procedures. This study has shown how Korean **test**-takers perceive ACTFL and OPIc",
                  "feedback in the **test** development process: **test**-takers  reactions to a tape-mediated **test** of proficiency in spoken Japanese.” Language",
                  "preparation and **test** results currently in use. The source of data for this research is a questionnaire filled in by **test**-takers. Results",
                  "commercialized **tests** such as Oral Proficiency Interview-computer (OPIc), TOEIC Speaking and TOEFL-IBT. As in other **tests** of speaking",
                  "speaking assessment, the number of OPIc **test**-takers has been increasing every year since the **test** was first administrated in 2007 by American",
</comment><comment author="clintongormley" created="2016-11-25T16:23:50Z" id="262991024">Tested on 6.0 with unified highlighting - the unified highlighter is not returning the result matching the most different words with lowest IDF first:

&lt;details&gt;

```
PUT t/t/1
{
  "text": "(Chung-Ang University)\n\nShin, Dong-il․Kim, Na-hee․Kang, Suk-joo. “Korean Examinees Perceptions of Testing Spoken English: A Move Toward Proficiency.” Studies in English Language &amp; Literature. 36.2(2010): 263-288. The purpose of this study is to investigate Korean test-takers’ perceptions on ACTFL Proficiency Guidelines and Oral Proficiency Interview-computer rubrics, and to understand Korean examinees’ beliefs regarding OPIc preparation and test results currently in use. The source of data for this research is a questionnaire filled in by test-takers. Results are presented in the following categories: ACTFL and ACTFL proficiency guidelines, OPIc in general, OPIc rubrics and test items, OPIc criteria, OPIc test preparation, OPIc tests in use, and English speaking tests. It was found that Korean examinees did not have enough information on ACTFL, proficiency levels, OPI and proficiency movement, although they had positive perceptions regarding OPIc rubrics. It is suggested that test-takers need to acquire sufficient information on ACTFL Proficiency Guidelines and OPIc preparation and speaking strategies from the test institution. (Chung-Ang University)\n\nKey Words: ACTFL proficiency guidelines, oral proficiency testing, test-takers’ perception, English speaking tests, proficiency movement\n\n\nI. Introduction\n\nEnglish educators in Korea are currently facing critical issues when assessing speaking proficiency. Face-to-face interviews, and semi-direct speaking test formats, are challenging conventional but indirect methods of assessing speaking proficiency. English has often been used to communicate with foreign speakers in work places as well as in academic contexts, and schools and companies have come to evaluate employees’ speaking proficiency through commercialized tests such as Oral Proficiency Interview-computer (OPIc), TOEIC Speaking and TOEFL-IBT. As in other tests of speaking assessment, the number of OPIc test-takers has been increasing every year since the test was first administrated in 2007 by American Council on the Teaching of Foreign Languages Testing Committee - Korea (ATCK). It is now considered by test administrators and test takers that a good OPIc score is necessary to get a good or better job. \nWhat makes OPIc distinct from conventional norm-referenced tests is that it measures performance by criteria described in ACTFL Proficiency Guidelines. In other words, OPIc does not compare a speaker’s performance with others’ performance. A speech sample is rated only according to criteria on the rating scale outlined in the ACTFL Proficiency Guidelines (ACTFL, 2008). Thus, it is helpful to understand these guidelines in order to prepare for OPIc. In current journals, books or academic papers, however, there has been no systematic investigation concerning how correctly Korean test-takers have recognized instructions within ACTFL Proficiency Guidelines or OPIc, and how well they understand guidelines in OPIc test-preparation activities. \nThe purpose of this study is to investigate Korean test-takers’ perceptions of ACTFL proficiency guidelines and OPIc. Also, it is intended to evaluate test-takers’ thoughts concerning OPIc preparation and the OPIc test results in use. The source of data in this study is questionnaires completed by OPIc test-takers. \n \nII. Literature Review\n \n2.1 ACTFL Proficiency Guidelines\nIn the United States, defining and assessing language proficiency was initiated in the 1950s by government agencies including the Foreign Service Institute (FSI), in order to ascertain their employees’ language proficiency and place them in foreign service positions based on their proficiency levels (Hadley, 2001). Along with the Interagency Language Roundtable (ILR), which serves to describe language proficiency levels for government contexts, FSI designed an oral proficiency test based on a proficiency scale. In 1979, a U.S. President’s Commission report on Foreign Language and International Studies recommended that language proficiency, especially oral proficiency, goals and guidelines, be formulated to structure and measure the results of foreign language instruction. Based on its findings, it was determined that standardized evaluation instruments help foreign language educators develop methodologies and curricula based on defined outcomes, and also enable foreign language students to measure their own progress. \nAs demands for proficiency standards grew, institutions like Educational Testing Service (ETS) and ACTFL undertook projects to define language learners’ level of language proficiency for the purpose of developing an analogous scale used by ILR of FSI in academic settings (Lowe &amp; Stansfield, 1988). With grants and support from U.S. government, ACTFL Provisional Guidelines were published in 1982, and revised ACTFL Proficiency Guidelines for Speaking, Reading, Listening, and Writing were disseminated in specific detail in 1986. Also, based on ACTFL Proficiency Guidelines, the ACTFL OPI (Oral Proficiency Interview) was developed by ACTFL. \nIn 1999, ACTFL Proficiency Guidelines-Speaking were revised in accord with the development of oral proficiency testing. These guidelines describe language proficiency in spoken language, from Superior to Novice Low, within a hierarchical order. Except for Superior, each major level is further divided into three sublevels of High, Mid, and Low. Picture 1 shows the ACTFL Rating Scale, which comprises four major levels of language performance. Each description represents a range of ability, while sublevels represent a more specific definition in terms of quality and quantity of control of the speaker for the functions required at each level. \n\nFigure 1. ACTFL Rating Scale (ACTFL, 1999, p. 12)\n\nThe introduction to ACTFL Proficiency Guidelines and the ACTFL OPI formed the foundation of the ACTFL Proficiency Movement. In other words, it was created by testing approaches, which later influenced teaching practice (Hadley, 2001). Swender (2009) states that “it was shift from what teachers do in the classroom, to what students are able to do with the language they have learned. Also, the discussion is no longer about what is the best method for teaching languages, but rather, what does the data on proficiency outcomes tell us about effective instructional and curricular design? Thus, it has dramatically changed the way in which languages are taught and tested in the United States” (p. 6). \nThe impact of the ACTFL proficiency movement has implications for English education in Korea. Although the demand for oral proficiency testing is increasing, test-preparation still focuses on memorizing language forms and pronunciation. It appears there is a big gap between the purpose of testing (to assess language proficiency for real world applications) and test preparation. However, ACTFL may have a positive impact in Korea in that it provides the language profession and educators with new insights for assessing speaking proficiency. \n2.2 The ACTFL OPIc \nThe ACTFL OPI is a one-to-one telephonic or face-to-face interactive medium conducted by an interviewer in the target language. Since its development, it has been used worldwide for testing oral language proficiency. In addition, with increased usage in schools, commercial enterprises, and government agencies, OPIc has been developed to incorporate computer technology (ACTFL, 2008). \nThe goal of OPIc is the same as the OPI: to measure how well an examinee speaks a language by comparing the speaker’s performance with criteria in ACTFL Proficiency Guidelines – Speaking (ACTFL, 1999). Table 1 shows OPIc assessment criteria. \n \nTable 1. Assessment Criteria (ACTFL, 2008, p. 4)\nProficiency\nLevel\nGlobal tasks\nand functions\nContext/\nContent\nAccuracy\nText Type\nAdvanced\nNarrate and describe in major time frames and   deal effectively with an unanticipated complication.\nMost informal and some formal settings/Topics of personal and general interest.\nUnderstood without difficulty by speakers   unaccustomed to dealing with nonnative speakers.\nParagraphs\nIntermediate\nCreate with language   initiate, maintain, and bring to close simple conversations by asking and   responding to simple questions.\nSome informal settings and a limited number   of transactional situations/Predictable, familiar, topics related to daily   activities.\nUnderstood, with some repetition, by speakers   accustomed to dealing with nonnative speakers.\nDiscrete\nsentences\nNovice\nCommunicate minimally with formulaic and rote   utterance, lists and phrases.\nMost common informal settings/Most common   aspects of daily life.\nMay be difficult to understand, even for   speakers accustomed to dealing with nonnative speakers.\nIndividual words and phrases\n\nOPIc is criterion-referenced assessment consisting of four major categories: \n\nthe global tasks or functions performed with the language (ex. asking and answering simple questions, narrating, describing, etc.); the social contexts (ex. in a restaurant in Mexico) and the content areas (ex. ordering a meal) in which the language can be used; the accuracy with which the tasks are performed (ex. The grammar, vocabulary, pronunciation, fluency, sociolinguistic appropriateness or acceptability of what is being said with in a certain setting, and the use of appropriate strategies for discourse management); the oral text types that result from the performance of the tasks (ex. discrete words and phrases, sentences, paragraphs, or extended discourse). Each speaker performance is rated on four major categories by trained and certified raters (Swender, 1999, p. 2).  \n\nOPIc measures oral proficiency up to the Advanced Low level on the ACTFL scale. For example, picture 2 shows the difference between the OPI and the OPIc using the scale. As indicated, OPIc has seven levels including three major levels.\n \nFigure 2. OPI and OPIc on the ACTFL Scale (ACTFL, 2008)\n\n \nIn testing, OPIc takes advantages of computer technology in semi-direct procedures based on ACTFL Proficiency Guidelines. First, OPIc allows examinees to choose topics based on their own interests and experience. Here OPIc has five phases: selecting a language, background survey, self-assessment, overview of OPIc, and the actual test. The procedure begins with one’s language selection. In phase 2, the background survey is implemented based on the candidate’s interests, occupation and experience. It shows how OPIc takes advantage of computer technology to be adaptive by presenting a more learner-centered assessment. \nSecond, OPIc allows examinees to self-assess their proficiency levels through a series of questions. Before the actual tasks begin, an examinee chooses his or her estimated proficiency level (Novice, Intermediate and Advanced) for a sample task. This helps examinees perform in a more flexible and less threatening environment. It also may reduce test-takers’ anxiety, which can inhibit their best performance (Malabonga, Kenyon &amp; Carpenter, 2005).  \nThird, OPIc allows examinees to control the time they need to respond to tasks. After an examinee overviews the OPIc testing procedure, he or she performs tasks in an actual test. During this process, the test-taker can listen to questions twice and even control their response time. Even though there is a maximum time limit, it is sufficient to prepare for and respond to each task. Through this procedure, examinees have more positive feelings about the test (Kenyon &amp; Malabonga, 2001). \nLast, the computer technology facilitates test results and ratings. Once OPIc test-taking is complete, speaking samples are sent to ACTFL certified OPIc raters using the Internet. These raters listen to each sample and find the best match between a sample and the assessment criteria of the rating scale. In addition, OPIc employs a reliable assessment conducted by ACTFL-certified OPIc raters. \n \n2.3 Test-taker Perceptions \nFrom a traditional academic perspective, examinees’ perceptions of tests were regarded in terms of face validity and were not considered under real test conditions (Bachman, 1990). Test validation had been left to professionals concerned with test development and administration. However, interest has grown regarding how examinees perceive tests, including whether a given test is acceptable to its users or not (Davies et al., 1999). In this regard, there are several considerations as perceived by test-takers.  \nFirst, test-takers’ perceptions include test validity. It has been suggested that if examinees have negative perceptions regarding a test, they do not perform well (Elder, Iwashita &amp; McNamara, 2002). Consequently, if attitudes interfere with performance, test scores may be invalid (Elder &amp; Lynch, 1996; Nevo, 1985; Spolsky, 1995). Messick (1989) stated that test-taker perceptions are therefore necessary as a crucial source of evidence for construct validity. Because of their importance, perceptions of task difficulty are now accepted by test-developers when undertaking task revision (Alderson, 1998). \nSecond, test-takers’ reactions are important when developing test formats or task types. Many researchers have suggested that feedback is useful in test development or revision (Bachman &amp; Palmer, 1996; Brown, 1993; Shohamy, 1982; Zeidner, 1990). Findings indicate that test-takers have preferences for certain types of tasks: they tend to perceive easier tasks as those to be more interesting. Since examinee reaction to task difficulty is related to actual task performance (Bradshaw, 1990; Scott, 1986; Zeidner, 1990; Zeidner &amp; Bensoussan, 1988), this information may be useful for test revision (Alderson, 1988; Kenyon &amp; Stansfield, 1991).\nThird, test-takers’ perceptions are important for assessment. Related research indicates their involvement can enhance assessment criteria (Birenbaum, 1996; Wolf, Bixby, Glenn &amp; Gardner, 1991), while Bensoussan and Kreindler (1990) suggest that scoring considerations obtained from feedback are valuable. Also, Cohen (1993) revealed that making a rating key based on feedback was useful, while involving examinees in test design and development promotes later positive impact (Bachman &amp; Palmer, 1996). \nWhile a rich literature exists concerning test-takers’ perceptions in language testing generally, there is little research regarding the newly-developed OPIc. Accordingly, one objective of this current study is to examine feedback to ACTFL proficiency and OPIc. Examinees’ perceptions may assist in test revision and overall acceptability as well as test validity from their perspectives.\n \nIII. Method\n \nThis section addresses three aspects of the study: developing the survey instrument, administering the questionnaire to OPIc examinees, and analyzing data. \n \n3.1 Procedure\n After taking the OPIc test, examinees were asked to fill out the questionnaire. The instrument of the study was a survey questionnaire about examinees’ perception on ACTFL Proficiency Guidelines, the OPIc test, and English speaking tests. There were 45 items divided into three sub sections. The first section, ACTFL Proficiency Guidelines were concerned with how well Korean test-takers recognize and understand ACTFL Proficiency Guidelines. Then, in the second section regarding OPIc, the respondents were asked to indicate how they perceive the OPIc test in overall impression, test format, task type, OPIc preparation, and score use. Finally, the respondents were given questions about whether they perceive English speaking tests reliable and helpful on teaching and learning.  \nThe questionnaire was presented in Korean in order to avoid any possible misinterpretation and misunderstanding from the English terms. Examinees were asked to indicate the degree to which they agreed with each statement using a five point scale: 5 = Strongly Agree, 4 = Agree, 3 = Moderate, 2 = Disagree, and 1 = Strongly Disagree. The questionnaire was administered to 82 OPIc examinees at a computer based test center in Seoul, Korea. They were not allowed to carry the questionnaire into the test-taking center, and all responded after they finished the test and came out of the test room. \n\n3.2 Participants \n37 (54.88%) of 82 examinees were office workers, while the remainder consisted of 16 college students (19.5%) and 21 job applicants (25.6%). The response rate by age group was 24% in the group aged 20 to 25, 26.7% in the group aged 26 to 30, 21.3% in the group aged 31 to 35, 28% in the group aged 36-40. The response rate by gender was 64.6% for males and 13.4% for females. On the average, 56.1% of the respondents responded to take OPIc to check his/her English speaking proficiency in the basic. Among the rest, 41.5 % answered that they took OPIc simply to use its score in job applications or for promotion materials. \n \nTable 2. Respondents Information\n\n \nQuestionnaire Respondents\nTotal Number\n82\nGender\n \nMale\n53 (64.6%)\nFemale\n11 (13.4%)\nNot marked \n18 (22%) \nReason to Take OPIc Test\n \nTo check his/her English speaking proficiency\n46 (56.1%)\nTo use in job application or promotion\n34 (41.5%)\nOthers\n2 (2.4%)\nIV. Results\n \n4.1 ACTFL and Proficiency Guidelines\nThe ACTFL Proficiency Guidelines and the Proficiency Movement have played an important role in the history of foreign language education (Rifkin, 2003). As mentioned, the proficiency guidelines were the basis for development of the ACTFL Proficiency Movement, which has been a shift in focus from memorizing language forms to using languages for real world purposes. Also, based on the ACTFL Proficiency Guidelines-Speaking, the OPI(c) measures how well a person speaks in general. \nDespite their usefulness, Korean examinees did not have enough information overall on ACTFL, the Proficiency Guidelines, Proficiency Movement, and OPI (Table 3). For example, 52.4% of respondents either disagreed or strongly disagreed that they “have heard about ACTFL”, while only 24.4% agreed or strongly agreed. Also, 75.6% of respondents disagreed or strongly disagreed that they “have heard about ACTFL Proficiency Movement, and only 18.3% believed that they “have heard about ACTFL Proficiency Guidelines”. \nSince 2006, there has been rapid growth regarding the OPIc test in Korea. However, it is not directly related to awareness-raising of  ACTFL-driven guidelines or the movement. Few test-takers were aware who developed the OPIc, where it came from in the mandate of the Proficiency Movement, or what ACTFL Proficiency Guidelines or the OPI look like. They simply understood more about OPIc testing generally.\n \n"
}
```
&lt;/details&gt;

```
GET t/_search
{
  "size": 1,
  "_source": false,
  "query": {
    "match": {
      "text": {
        "query": "test acceptable",
        "operator": "and"
      }
    }
  },
  "highlight": {
    "type": "unified",
    "order": "score",
    "fields": {
      "text": {
        "fragment_size": 128,
        "number_of_fragments": 1000
      }
    }
  }
}
```

Returns:

* "First, &lt;b&gt;test&lt;/b&gt;-takers’ perceptions include &lt;b&gt;test&lt;/b&gt; validity.",
* "Second, &lt;b&gt;test&lt;/b&gt;-takers’ reactions are important when developing &lt;b&gt;test&lt;/b&gt; formats or task types.",
* "&lt;b&gt;Test&lt;/b&gt; validation had been left to professionals concerned with &lt;b&gt;test&lt;/b&gt; development and administration.",
* "Also, it is intended to evaluate &lt;b&gt;test&lt;/b&gt;-takers’ thoughts concerning OPIc preparation and the OPIc &lt;b&gt;test&lt;/b&gt; results in use.",
* "It is now considered by &lt;b&gt;test&lt;/b&gt; administrators and &lt;b&gt;test&lt;/b&gt; takers that a good OPIc score is necessary to get a good or better job.",
* "Examinees’ perceptions may assist in &lt;b&gt;test&lt;/b&gt; revision and overall acceptability as well as &lt;b&gt;test&lt;/b&gt; validity from their perspectives.",
* "Third, &lt;b&gt;test&lt;/b&gt;-takers’ perceptions are important for assessment.",
* **FIRST MENTION OF ACCEPTABLE** "However, interest has grown regarding how examinees perceive tests, including whether a given &lt;b&gt;test&lt;/b&gt; is &lt;b&gt;acceptable&lt;/b&gt; to its users or not (Davies et al., 1999).",
* "It is suggested that &lt;b&gt;test&lt;/b&gt;-takers need to acquire sufficient information on ACTFL Proficiency Guidelines and OPIc preparation and speaking strategies from the &lt;b&gt;test&lt;/b&gt; institution.",
* etc</comment><comment author="jimczi" created="2016-11-25T18:50:52Z" id="263010543">&gt; Tested on 6.0 with unified highlighting - the unified highlighter is not returning the result matching the most different words with lowest IDF first:

When using the `plain` mode of the unified highlighter the IDF of the terms is missing. When using the `unified_postings` mode of the highlighter, the IDF is taking in account and the snippet with the term "acceptable" is ranked first. I'll open a ticket in Lucene since it should be feasible to retrieve the IDF of the terms even if we're using the plain highlighter (as long as the field is indexed).</comment><comment author="jimczi" created="2016-11-25T19:06:32Z" id="263011990">And just to be clear, the `unified` highlighter treats each document as a corpus and each passage as a document. The IDF in this context is the number of times the term appears in the document and the TF is the number of times the term appears inside the passage that we're trying to score.</comment><comment author="dohykim" created="2016-11-26T12:43:09Z" id="263061517">I just got there's new unified highlighter. what I used is fvh. and I see fvh as a one mode in unified highlighter. 

I want to say one thing more about fvh highlighter. That is a method [getBestFragments] in lucene. ES use this in FastVectorHighlighter.java line 143 and 146. It's lucene's method that get one field's highlight result. It loads a termvector of doc in segment file in storage maybe. It loads [number of highlight field] time and very slow when query has many highlight field and even doc is big it worse. Even don't care about the field is matched.(I guess Fetch phase is independent form Query phase). I think it's speacial use case. I sets many highlight fields because of requirement that our business service multi-lingual environment.

I suffered a performance issue. I have suggested this but some guy says to me "go to lucene".
I think this issue also concern with ES. If yours improve highlighter things this time, This inefficiency should take account of. I had solved this issue in my service with load termvector once in ES code and lucene [getBestFragments] method receive termvector object. So just "one load termvector operation" per "one document". 
I also created in lucene JIRA. 
https://issues.apache.org/jira/browse/LUCENE-7397?filter=-2
</comment><comment author="jimczi" created="2017-05-12T07:16:31Z" id="301002566">Closing this, the `unified` highlighter returns the expected snippet. </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix serialization bug in allocation explain API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19494</link><project id="" key="" /><description>The allocation explain API would fail if the request was executed via a non elected master node. This triggers the request to be redirected to the elected master node. The internal response serialization would fail because the read code would use regular ints, while the write code uses  vints.
</description><key id="166312143">19494</key><summary>Fix serialization bug in allocation explain API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T11:50:28Z</created><updated>2016-07-26T07:43:04Z</updated><resolved>2016-07-20T08:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-19T16:26:51Z" id="233688536">Thanks for fixing this @martijnvg!
</comment><comment author="jpountz" created="2016-07-25T09:19:06Z" id="234901938">@martijnvg can you update the description of this PR to explain what was broken?
</comment><comment author="martijnvg" created="2016-07-25T11:35:42Z" id="234929769">@jpountz Done, does this look better?
</comment><comment author="jpountz" created="2016-07-26T07:43:04Z" id="235187390">Thanks @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow Completion/Context Suggester to consume each token as an Input</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19493</link><project id="" key="" /><description>**2.x/5.x**:

Currently if configuring the completion suggester to utilise a field value for input, the text is tokenised using the configured analyzer.   The output tokens are subsequently concatenated to form a suggestion.  Only one value can therefore be loaded by this process. In order to load multiple input values for a suggestion, users are currently required to list multiple inputs.

This ticket proposes allowing a configuration option to cause each token to be consumed as a separate suggestion - rather than concatenating to form a single output.  Whilst this would need to be used with caution, as it may lead to a "bloated" FST structure, it would mean users could utilises the full capability of the analyzers chain - splitting out multiple possible inputs for matching.
</description><key id="166299378">19493</key><summary>Allow Completion/Context Suggester to consume each token as an Input</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Suggesters</label><label>discuss</label><label>feature</label></labels><created>2016-07-19T10:35:12Z</created><updated>2016-07-19T13:46:55Z</updated><resolved>2016-07-19T11:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-19T10:49:43Z" id="233596878">I have trouble to understand what you are exactly proposing but it sounds like an infix suggester. The completions suggester is a prefix suggester and we can't simply add an option to make it something else. It doesn't have any special options for the reason you mentioned since it's risky and won't really work in most of the cases either. How would you provide weights for each of the terms?
</comment><comment author="gingerwizard" created="2016-07-19T11:13:31Z" id="233601944">@s1monw correct in infix observation - but it could potentially be used for more than this.  Currently we do the following to achieve infix matches:

```
PUT test/documents/1
{
  "name": "Boeing Jets are super cool",
  "suggest_field": {
    "input": [
      {
        "input": "Boeing Jets are super cool"
      },
      {
        "input": "Jets are super cool"
      },
      {
        "input": "super cool"
      }
    ],
    "output": "Boeing Jets"
  },
  "tags": "engines"
}
```

As discussed, however, this would lead to potential abuse and is not a good generic addition. Closing.
</comment><comment author="clintongormley" created="2016-07-19T13:46:55Z" id="233637695">For future readers, a better approach would be to use the completion suggester for the full title match case (or a couple of alternatives), and to fall back to ordinary search if no results are returned.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allocation explain: Also serialize `includeDiskInfo` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19492</link><project id="" key="" /><description /><key id="166291259">19492</key><summary>Allocation explain: Also serialize `includeDiskInfo` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T09:51:08Z</created><updated>2016-07-26T11:27:23Z</updated><resolved>2016-07-19T09:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-19T09:53:40Z" id="233584834">Good catch @martijnvg. LGTM!
</comment><comment author="dakrone" created="2016-07-19T13:22:58Z" id="233630532">Thanks for fixing this Martijn!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix NPE when initializing replica shard has no UnassignedInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19491</link><project id="" key="" /><description>An initializing replica shard might not have an UnassignedInfo object, for example when it is a relocation target.  The method `allocatedPostIndexCreate` does not account for this situation.

Closes #19488
</description><key id="166282662">19491</key><summary>Fix NPE when initializing replica shard has no UnassignedInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T09:08:23Z</created><updated>2016-07-19T09:32:32Z</updated><resolved>2016-07-19T09:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-19T09:19:30Z" id="233576709">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file></files><comments><comment>Fix NPE when initializing replica shard has no unassignedInfo (#19491)</comment></comments></commit></commits></item><item><title>Make ExtendedBounds immutable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19490</link><project id="" key="" /><description>We used to mutate it as part of building the aggregation. That
caused assertVersionSerializable to fail because it assumes that
requests aren't mutated after they are sent.

Closes #19481
</description><key id="166224135">19490</key><summary>Make ExtendedBounds immutable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-19T00:38:56Z</created><updated>2016-07-19T13:09:49Z</updated><resolved>2016-07-19T13:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-19T00:39:11Z" id="233499903">Leaving this marked WIP for now because I haven't finished running the tests.
</comment><comment author="nik9000" created="2016-07-19T01:31:31Z" id="233506634">Tests passing, removed WIP.
</comment><comment author="nik9000" created="2016-07-19T02:09:40Z" id="233511623">@cbuescher, can you review this? Some of your changes revealed the issue and it is in your "time zone" area.
</comment><comment author="ywelsch" created="2016-07-19T12:04:48Z" id="233611829">@nik9000 left smaller suggestions to add assertion here and there. Feel free to push once addressed. LGTM.
</comment><comment author="nik9000" created="2016-07-19T12:34:41Z" id="233618117">Thanks @ywelsch!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Version bump AWS SDK 1.11.18</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19489</link><project id="" key="" /><description>- bump version
- fixed issue with double md5 signature generation
</description><key id="166201537">19489</key><summary>Version bump AWS SDK 1.11.18</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nl5887</reporter><labels><label>:Plugin Repository S3</label><label>:Upgrade API</label><label>review</label></labels><created>2016-07-18T21:48:04Z</created><updated>2016-07-28T09:56:38Z</updated><resolved>2016-07-27T16:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nl5887" created="2016-07-18T21:51:19Z" id="233470081">the repository-s3 plugin is throwing an error in the unit tests. This has probably to do with the security policy, suggestions on how to solve are appreciated.

&gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [snapshot.create_repository] returned [500 Internal Server Error] [{"error":{"root_cause":[{"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository"}],"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository","caused_by":{"type":"access_control_exception","reason":"access denied (\"java.lang.RuntimePermission\" \"accessDeclaredMembers\")"}},"status":500}]
</comment><comment author="nik9000" created="2016-07-19T12:33:45Z" id="233617897">Yeah, that looks like a security exception. The plugin already has that permission but, I expect, there is so new access pattern for it now. You'll have to track that down and wrap it in `AccessController.doPrivileged`.
</comment><comment author="nik9000" created="2016-07-19T12:34:30Z" id="233618084">Maybe @tlrx should review this? I don't know that AWS stuff well.
</comment><comment author="dadoonet" created="2016-07-26T08:30:08Z" id="235197175">Hi @nl5887.

I'm sorry I did not see your PR before I started working on #19594 (upgrade to 1.11.20).

So I just cherry-picked your commit https://github.com/elastic/elasticsearch/pull/19489/commits/9fd0d62e2b4088088d58a5a2802c82876fbe0185 in my PR.

The PR I started has also some modifications in docs and tests as a default value of the AWS SDK has changed (throttle retries).
Also in 1.11.20 some (a lot of) methods now returns an Object instead of `void`.

I think we should close your PR and work in #19594 now.
</comment><comment author="elasticmachine" created="2016-07-26T08:30:10Z" id="235197180">Can one of the admins verify this patch?
</comment><comment author="clintongormley" created="2016-07-27T16:48:36Z" id="235647246">Closing in favor of #19594
</comment><comment author="nl5887" created="2016-07-28T09:56:38Z" id="235851825">@dadoonet  no worries. Thx for cherry picking the commit, that will solve some issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainIT#testDelayShards fails due to NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19488</link><project id="" key="" /><description>So the test failure is in explaining the allocation of shards, but I believe the actual problem is an NPE in `ShardRouting`, where `unassignedInfo` is somehow null: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java#L243

CI failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1584/console

I can _usually_ recreate with this, although it appears to only happen 1 in 50 or so tests:

```
gradle :core:integTest -Dtests.seed=A82D8E5D38BF0A20 -Dtests.class=org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainIT -Dtests.method="testDelayShards" -Dtests.security.manager=true -Dtests.locale=nn-NO -Dtests.timezone=Europe/London -Dtests.iters=100
```

The test kills a random node, which usually goes smoothly.  But sometimes it NPE's out and causes all the test to drop all the other nodes:

```
1&gt; [2016-07-18 15:52:11,081][INFO ][org.elasticsearch.action.admin.cluster.allocation] --&gt; stopping a random node
  1&gt; [2016-07-18 15:52:11,081][INFO ][org.elasticsearch.test   ] Closing random node [node_t2] 
  1&gt; [2016-07-18 15:52:11,081][INFO ][org.elasticsearch.node   ] [node_t2] stopping ...
  1&gt; [2016-07-18 15:52:11,090][INFO ][org.elasticsearch.node   ] [node_t2] stopped
  1&gt; [2016-07-18 15:52:11,090][ERROR][org.elasticsearch.discovery.local] [node_t2] unexpected failure during [local-disco-update]
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.cluster.routing.ShardRouting.allocatedPostIndexCreate(ShardRouting.java:243)
  1&gt;    at org.elasticsearch.gateway.ReplicaShardAllocator.processExistingRecoveries(ReplicaShardAllocator.java:81)
  1&gt;    at org.elasticsearch.gateway.GatewayAllocator.allocateUnassigned(GatewayAllocator.java:139)
  1&gt;    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:356)
  1&gt;    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:330)
  1&gt;    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:315)
  1&gt;    at org.elasticsearch.discovery.local.LocalDiscovery$3.execute(LocalDiscovery.java:234)
  1&gt;    at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:552)
  1&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:855)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:450)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-07-18 15:52:11,090][INFO ][org.elasticsearch.node   ] [node_t2] closing ...
  1&gt; [2016-07-18 15:52:11,092][INFO ][org.elasticsearch.node   ] [node_t2] closed
  1&gt; [2016-07-18 15:52:11,092][INFO ][org.elasticsearch.plugins] [transport_client_node_t0] no modules loaded
  1&gt; [2016-07-18 15:52:11,092][INFO ][org.elasticsearch.plugins] [transport_client_node_t0] loaded plugin [org.elasticsearch.test.transport.AssertingLocalTransport$TestPlugin]
  1&gt; [2016-07-18 15:52:11,096][INFO ][org.elasticsearch.transport] [transport_client_node_t0] publish_address {local[60]}, bound_addresses {local[60]}
  1&gt; [2016-07-18 15:52:11,103][INFO ][org.elasticsearch.action.admin.cluster.allocation] [ClusterAllocationExplainIT#testDelayShards { seed=[A82D8E5D38BF0A20:5A6033F2A8FE37B]}]: finished test
  1&gt; [2016-07-18 15:52:11,103][INFO ][org.elasticsearch.action.admin.cluster.allocation] [ClusterAllocationExplainIT#testDelayShards { seed=[A82D8E5D38BF0A20:5A6033F2A8FE37B]}]: cleaning up after test
  1&gt; [2016-07-18 15:52:20,900][WARN ][org.elasticsearch.cluster] [node_t0] failed to connect to node {node_t2}{2gVawe0zRDqmYcUcdUZBAw}{Dw9SY5WaQ8-ewtmirmJgrA}{local}{local[59]} (tried [1] times)
  1&gt; ConnectTransportException[[node_t2][local[59]] Failed to connect]
  1&gt;    at org.elasticsearch.transport.local.LocalTransport.connectToNode(LocalTransport.java:184)
  1&gt;    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:272)
  1&gt;    at org.elasticsearch.cluster.NodeConnectionsService.validateNodeConnected(NodeConnectionsService.java:108)
  1&gt;    at org.elasticsearch.cluster.NodeConnectionsService$ConnectionChecker.doRun(NodeConnectionsService.java:133)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-07-18 15:52:20,927][WARN ][org.elasticsearch.cluster] [node_t1] failed to connect to node {node_t2}{2gVawe0zRDqmYcUcdUZBAw}{Dw9SY5WaQ8-ewtmirmJgrA}{local}{local[59]} (tried [1] times)
  1&gt; ConnectTransportException[[node_t2][local[59]] Failed to connect]
  1&gt;    at org.elasticsearch.transport.local.LocalTransport.connectToNode(LocalTransport.java:184)
  1&gt;    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:272)
  1&gt;    at org.elasticsearch.cluster.NodeConnectionsService.validateNodeConnected(NodeConnectionsService.java:108)
  1&gt;    at org.elasticsearch.cluster.NodeConnectionsService$ConnectionChecker.doRun(NodeConnectionsService.java:133)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
HEARTBEAT J0 PID(25998@DualDesk): 2016-07-18T15:52:22, stalled for 11.5s at: ClusterAllocationExplainIT.testDelayShards { seed=[A82D8E5D38BF0A20:5A6033F2A8FE37B]}
HEARTBEAT J0 PID(25998@DualDesk): 2016-07-18T15:52:32, stalled for 21.5s at: ClusterAllocationExplainIT.testDelayShards { seed=[A82D8E5D38BF0A20:5A6033F2A8FE37B]}
  1&gt; [2016-07-18 15:52:40,905][WARN ][org.elasticsearch.action.admin.cluster.node.stats] [node_t0] ignoring unexpected response [null] of type [null], expected [NodeStats] or [FailedNodeException]
  1&gt; [2016-07-18 15:52:41,115][INFO ][org.elasticsearch.node   ] [node_t0] stopping ...
  1&gt; [2016-07-18 15:52:41,120][INFO ][org.elasticsearch.cluster.routing.allocation] [node_t1] Cluster health status changed from [GREEN] to [RED] (reason: [elected as master]).
  1&gt; [2016-07-18 15:52:41,120][INFO ][org.elasticsearch.cluster.service] [node_t1] master {new {node_t1}{lnZrFKUNTdS1JxgsR2k_-Q}{zdgmZzecTwKFnyvQ7g4DOA}{local}{local[58]}, previous {node_t0}{v36fo6K7Qz-E2KyaZUxZ4Q}{zhtJYDKQTzGmIQUjL9-kWA}{local}{local[57]}}, removed {{node_t0}{v36fo6K7Qz-E2KyaZUxZ4Q}{zhtJYDKQTzGmIQUjL9-kWA}{local}{local[57]},{node_t2}{2gVawe0zRDqmYcUcdUZBAw}{Dw9SY5WaQ8-ewtmirmJgrA}{local}{local[59]},}, reason: local-disco-update
  1&gt; [2016-07-18 15:52:41,121][INFO ][org.elasticsearch.cluster.routing] [node_t1] scheduling reroute for delayed shards in [59.9s] (6 delayed shards)
  1&gt; [2016-07-18 15:52:41,123][INFO ][org.elasticsearch.node   ] [node_t0] stopped
  1&gt; [2016-07-18 15:52:41,123][INFO ][org.elasticsearch.node   ] [node_t0] closing ...
  1&gt; [2016-07-18 15:52:41,126][INFO ][org.elasticsearch.node   ] [node_t0] closed
  1&gt; [2016-07-18 15:52:41,126][INFO ][org.elasticsearch.node   ] [node_t1] stopping ...
  1&gt; [2016-07-18 15:52:41,140][INFO ][org.elasticsearch.node   ] [node_t1] stopped
  1&gt; [2016-07-18 15:52:41,140][INFO ][org.elasticsearch.node   ] [node_t1] closing ...
  1&gt; [2016-07-18 15:52:41,142][INFO ][org.elasticsearch.node   ] [node_t1] closed
```

Which ultimately makes the test fail with:

```
&gt; Throwable #1: ElasticsearchException[unable to find any shards to explain [ClusterAllocationExplainRequest[useAnyUnassignedShard=true,includeYesDecisions?=false] in the routing table]
   &gt;    at org.elasticsearch.action.admin.cluster.allocation.TransportClusterAllocationExplainAction.masterOperation(TransportClusterAllocationExplainAction.java:310)
   &gt;    at org.elasticsearch.action.admin.cluster.allocation.TransportClusterAllocationExplainAction.masterOperation(TransportClusterAllocationExplainAction.java:68)
   &gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:85)
   &gt;    at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:169)
   &gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)Throwable #2: java.lang.AssertionError: ClusterHealthResponse has timed out - returned: [{
   &gt;   "cluster_name" : "TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7292795637849911491]-HASH=[127918AC5CC6]-cluster",
   &gt;   "status" : "green",
   &gt;   "timed_out" : true,
   &gt;   "number_of_nodes" : 3,
   &gt;   "number_of_data_nodes" : 3,
   &gt;   "active_primary_shards" : 5,
   &gt;   "active_shards" : 10,
   &gt;   "relocating_shards" : 0,
   &gt;   "initializing_shards" : 0,
   &gt;   "unassigned_shards" : 0,
   &gt;   "delayed_unassigned_shards" : 0,
   &gt;   "number_of_pending_tasks" : 0,
   &gt;   "number_of_in_flight_fetch" : 0,
   &gt;   "task_max_waiting_in_queue_millis" : 0,
   &gt;   "active_shards_percent_as_number" : 100.0
   &gt; }]
   &gt; Expected: is &lt;false&gt;
   &gt;      but: was &lt;true&gt;
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoTimeout(ElasticsearchAssertions.java:111)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.ensureClusterSizeConsistency(ESIntegTestCase.java:1031)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:528)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.after(ESIntegTestCase.java:1980)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Assigning to @ywelsch since it looks like you're intimately familiar with this class, but /cc'ing @dakrone  in case it's I misjudged and it's related to cluster explanations somehow.
</description><key id="166183982">19488</key><summary>[CI] org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainIT#testDelayShards fails due to NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>jenkins</label></labels><created>2016-07-18T20:20:34Z</created><updated>2016-07-19T09:30:57Z</updated><resolved>2016-07-19T09:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-19T09:16:54Z" id="233576058">this comes from the fact that we might sometimes don't have unassinged info if we are initializing. For instance, this particular failure comes from here:

```
at org.elasticsearch.cluster.routing.ShardRouting.&lt;init&gt;(ShardRouting.java:80)
  2&gt;        at org.elasticsearch.cluster.routing.ShardRouting.removeRelocationSource(ShardRouting.java:391)
  2&gt;        at org.elasticsearch.cluster.routing.RoutingNodes.removeRelocationSource(RoutingNodes.java:500)
  2&gt;        at org.elasticsearch.cluster.routing.allocation.AllocationService.cancelShard(AllocationService.java:570)
  2&gt;        at org.elasticsearch.cluster.routing.allocation.AllocationService.applyFailedShard(AllocationService.java:534)
  2&gt;        at org.elasticsearch.cluster.routing.allocation.AllocationService.deassociateDeadNodes(AllocationService.java:423)
  2&gt;        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:347)
```

I think not having unassigned info here is fine, we just need to check for null in that method.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=search.aggregation/20_terms/Double test} fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19487</link><project id="" key="" /><description>Reliably fails.  I tried to go back and git bisect, but was unable to find a recent non-failing commit.

Example failure: http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3563/

```
ERROR   30.9s | MultiNodeBackwardsIT.test {p0=search.aggregation/20_terms/Double test} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.net.SocketTimeoutException: Read timed out
   &gt;    at __randomizedtesting.SeedInfo.seed([4A8B947DEF21C44D:C2DFABA741DDA9B5]:0)
   &gt;    at java.net.SocketInputStream.socketRead0(Native Method)
   &gt;    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
   &gt;    at java.net.SocketInputStream.read(SocketInputStream.java:170)
   &gt;    at java.net.SocketInputStream.read(SocketInputStream.java:141)
   &gt;    at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:139)
   &gt;    at org.apache.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:155)
   &gt;    at org.apache.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:284)
   &gt;    at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:140)
   &gt;    at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)
   &gt;    at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)
   &gt;    at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:165)
   &gt;    at org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:167)
   &gt;    at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)
   &gt;    at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)
   &gt;    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:271)
   &gt;    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)
   &gt;    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)
   &gt;    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
   &gt;    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)
   &gt;    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:117)
   &gt;    at org.elasticsearch.client.RestClient.performRequest(RestClient.java:194)
   &gt;    at org.elasticsearch.test.rest.client.RestTestClient.callApi(RestTestClient.java:245)
   &gt;    at org.elasticsearch.test.rest.RestTestExecutionContext.callApiInternal(RestTestExecutionContext.java:109)
   &gt;    at org.elasticsearch.test.rest.RestTestExecutionContext.callApi(RestTestExecutionContext.java:77)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:92)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:390)
   &gt;    at java.lang.Thread.run(Thread.java:745)

```

Reliably reproduces with:

```
gradle :qa:backwards-5.0:integTest -Dtests.seed=4A8B947DEF21C44D -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=search.aggregation/20_terms/Double test}" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=en-GB -Dtests.timezone=Etc/GMT-10
```

FWIW, I tested on the entire suite (not just the "Double Test" test) and they all fail with the same timeout:

```
gradle :qa:backwards-5.0:integTest -Dtests.seed=4A8B947DEF21C44D -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT  -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=en-GB -Dtests.timezone=Etc/GMT-10
```

I have no idea how to debug these bwc tests however, sorry :(
</description><key id="166167465">19487</key><summary>[CI] org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=search.aggregation/20_terms/Double test} fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>jenkins</label></labels><created>2016-07-18T19:00:49Z</created><updated>2017-05-19T07:23:03Z</updated><resolved>2017-05-19T07:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-07-18T19:14:40Z" id="233428921">/cc @simonw, dunno who'd be best to handle this since I don't see an obvious candidate commit, and you touched the test last.  Sorry!
</comment><comment author="awislowski" created="2016-09-05T16:46:27Z" id="244784614">Test was renamed. From MultiNodeBackwardsIT to Backwards50ClientYamlTestSuiteIT in the commit: 9270e8b22b2df5ae09726464419c179e18d9a3a7

Now test is successful:

```
gradle :qa:backwards-5.0:integTest -Dtests.seed=4A8B947DEF21C44D -Dtests.class=org.elasticsearch.backwards.Backwards50ClientYamlTestSuiteIT -Dtests.method="test {p0=search.aggregation/20_terms/Double test}" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=en-GB -Dtests.timezone=Etc/GMT-10
```

It could be closed
</comment><comment author="s1monw" created="2017-05-19T07:23:03Z" id="302629389">fixed, closing</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>upgrade cloud aws plugin to 1.11.18</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19486</link><project id="" key="" /><description>- removed hash verify in putObject because this is being done by
  aws putObject as well
- updated mock functions to new version
</description><key id="166161402">19486</key><summary>upgrade cloud aws plugin to 1.11.18</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nl5887</reporter><labels /><created>2016-07-18T18:32:34Z</created><updated>2016-07-18T18:47:01Z</updated><resolved>2016-07-18T18:46:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nl5887" created="2016-07-18T18:33:01Z" id="233417174">If wanted, I can create a PR against master as well
</comment><comment author="jasontedor" created="2016-07-18T18:46:23Z" id="233421017">Thanks for the contribution and work here @nl5887. Can you please reopen this PR against master, this PR will not be accepted into 2.3? Whether or not the PR will be backported to 2.4 is yet to be determined.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Invalid JSON returned when highlight query is not found in escaped content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19485</link><project id="" key="" /><description>Found in 1.7, confirmed in 2.3.3

To repro:

```
POST test-index/shakespeare/1
{
  "play_name": "Henry IV",
  "speech_number": 1,
  "text_entry": "{\"content\":\"March all one way and be no more opposed\"}"
}

GET /test-index/_search
{

 "size" : 10,
 "query" : {
   "match" : {
     "speech_number" : {
       "query" : "1"
     }
   }
 },
 "highlight" : {
   "tags_schema" : "styled",
   "fields" : {
     "text_entry" : {
       "number_of_fragments" : 0,
       "highlight_query" : {
         "match" : {
           "text_entry" : {
             "query" : "textstring"
           }
         }
       },
       "no_match_size" : 100000000
     }
   }
 }
}
```

If the highlight query includes text present in the document, the results will be properly formatted.  If the text is not present, the highlight return block looks like:

```
        "highlight": {
          "text_entry": [
            "{\"content\":\"March all one way and be no more opposed"
          ]
        }
```

Not sure this is an actual JSON formatting error as the braces are contained within a quote, but in any case the closing brace should not be dropped.
</description><key id="166148241">19485</key><summary>Invalid JSON returned when highlight query is not found in escaped content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Highlighting</label><label>won't fix</label></labels><created>2016-07-18T17:30:34Z</created><updated>2016-07-22T09:43:53Z</updated><resolved>2016-07-22T09:24:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2016-07-18T20:54:01Z" id="233455438">Also happens with XML content:

```
POST test-index/shakespeare/2
{
  "play_name": "Henry V",
  "speech_number": 2,
  "text_entry": "&lt;Script&gt;&lt;text&gt;March all one way and be no more opposed&lt;/text&gt;&lt;/Script&gt;"
}

GET /test-index/_search
{

 "size" : 10,
 "query" : {
   "match" : {
     "speech_number" : {
       "query" : "2"
     }
   }
 },
 "highlight" : {
   "tags_schema" : "styled",
   "fields" : {
     "text_entry" : {
       "number_of_fragments" : 0,
       "highlight_query" : {
         "match" : {
           "text_entry" : {
             "query" : "finance"
           }
         }
       },
       "no_match_size" : 100000000
     }
   }
 }
}
```
</comment><comment author="clintongormley" created="2016-07-19T13:38:52Z" id="233635249">You can never rely on valid JSON, XML, HTML, etc being returned from a highlighting request. For example:

```
PUT t/t/1
{
  "text": "&lt;p&gt;one &lt;b&gt;two three four five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten five six seven eight nine ten&lt;/b&gt;&lt;/p&gt;"
}

GET _search
{
  "highlight": {
    "fields": {
      "text": {
        "highlight_query": {
          "match": {
            "text": "one two"
          }
        }
      }
    }
  }
}
```

returns:

```
    "highlight": {
      "text": [
        "&lt;p&gt;&lt;em&gt;one&lt;/em&gt; &lt;b&gt;&lt;em&gt;two&lt;/em&gt; three four five six seven eight nine ten five six seven eight nine ten five six seven"
      ]
    }
```

As for why `no_match_size` is dropping the final `}` or `&gt;`, it's because it is using the character offsets returned by the analyzer to determine the end of the text.  

This example:

```
PUT t/t/1
{
  "text": "one two three!!!!!"
}

PUT t/t/2
{
  "text": "three four five!!!!!a"
}

GET _search
{
  "highlight": {
    "fields": {
      "text": {
        "highlight_query": {
          "match": {
            "text": "foo"
          }
        },
        "no_match_size": 1000
      }
    }
  }
}
```

returns highlights `one two three` and `three four five!!!!!a`

Curiously, searching for `three` DOES include the trailing punctuation:`one two &lt;em&gt;three&lt;/em&gt;!!!!!` and `&lt;em&gt;three&lt;/em&gt; four five!!!!!a`.

Potentially we could make these two cases use the same logic.
</comment><comment author="markharwood" created="2016-07-22T09:24:06Z" id="234497204">Closing, as per @clintongormley comments. Highlighting has never attempted to preserve any valid markup contained in the text it is highlighting because this is too complex and not compatible with the goals of summarising text.
</comment><comment author="markharwood" created="2016-07-22T09:43:52Z" id="234501589">One suggestion though - for structured text consider storing it not as a blob of marked-up text but instead as regular JSON e.g. an array of "chapter" objects each of which have text fields. Then consider indexing using `nested` documents and the `inner_hits` query to select the best chapters that match your text query. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rollover API Numbering not honouring preceding zeroes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19484</link><project id="" key="" /><description>Elasticsearch Rollover API tested on ES 5.0-alpha4

Tested as shown in documentation:
https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-rollover-index.html

```
PUT /roll_me_over-0001
{
  "aliases": {
    "roll_me_over": {}
  }
}

POST roll_me_over/_rollover 
{
  "conditions": {
    "max_age":   "1m"
  }
}
```

Docs state that naming will follow the same pattern:

&gt; If the name of the existing index ends with - and a number — e.g. logs-0001 — then the name of the new index will follow the same pattern, just incrementing the number (logs-0002).

But instead:

```
yellow open   roll_me_over-0001             5   1          0            0       650b           650b
yellow open   roll_me_over-2                5   1          0            0       260b           260b
```
</description><key id="166143054">19484</key><summary>Rollover API Numbering not honouring preceding zeroes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Index APIs</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T17:03:57Z</created><updated>2016-07-28T15:44:50Z</updated><resolved>2016-07-28T15:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-19T12:40:10Z" id="233619376">I think we should respect the zero padding as it makes for easier sorting, but then if we rollover to a number higher than allowed with the padding we should do what we do today (ie just increment)
</comment><comment author="s1monw" created="2016-07-19T12:44:14Z" id="233620419">do we have to auto-detect this? can't we ask the user to provide this with the request and have a default there?
</comment><comment author="areek" created="2016-07-19T14:54:59Z" id="233658822">I agree with @s1monw here, we can already supply a name for the new index vi `POST /roll_me_over-0001/_rollover/roll_me_over-0002` where `roll_me_over-0002` is the new index name. IMO, it would be simpler to just increment the old index name not respecting zero padding (current behaviour) and let the user provide the new index name if a simple increment is not enough?
Though, the example in the docs is misleading here, I will adapt it to ensure the above is clear?
@clintongormley WDYT?
</comment><comment author="s1monw" created="2016-07-19T19:24:26Z" id="233738812">another option is to just have a  padding all the time lets say 6 digits and we are done? `String.format` should do this for us? 
</comment><comment author="areek" created="2016-07-19T22:58:32Z" id="233791667">hmm, IMO having any padding at all seems arbitrary, is there any specific reason to add any padding? 
</comment><comment author="s1monw" created="2016-07-20T07:16:53Z" id="233862806">@areek I think people often sort the index by name for recovery order etc. so that might be the most significant one
</comment><comment author="clintongormley" created="2016-07-21T10:38:14Z" id="234218078">&gt; another option is to just have a padding all the time lets say 6 digits and we are done?

++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/RolloverIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverActionTests.java</file></files><comments><comment>Add zero-padding to auto-generated rollover index name increment</comment></comments></commit></commits></item><item><title>Migrate serial_diff aggregation to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19483</link><project id="" key="" /><description>This is the last migration before AggregationStreams and
PipelineAggregatorStreams can be removed to remove redundant
code.
</description><key id="166142942">19483</key><summary>Migrate serial_diff aggregation to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T17:03:17Z</created><updated>2016-07-18T17:09:19Z</updated><resolved>2016-07-18T17:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed incorrect YAML indentation in the "Rollover" tests again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19482</link><project id="" key="" /><description>As part of changes in d78f40fb1e88c78ce4466fe145d365c205441e43, a fix to the YAML
indentation has been reverted, see location: https://github.com/elastic/elasticsearch/commit/d78f40fb1e88c78ce4466fe145d365c205441e43#diff-eaf129528b571da2cafdfd5490c12453

This patch fixes the YAML notation back.

/cc @abeyad
</description><key id="166140078">19482</key><summary>Fixed incorrect YAML indentation in the "Rollover" tests again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T16:49:03Z</created><updated>2016-07-18T17:34:58Z</updated><resolved>2016-07-18T17:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-18T16:50:18Z" id="233387762">LGTM
</comment><comment author="abeyad" created="2016-07-18T16:51:33Z" id="233388104">LGTM
</comment><comment author="karmi" created="2016-07-18T17:34:58Z" id="233400217">Closed in 5bab65d88623837d0c2f5b91544d7bae332b803c
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fixed incorrect YAML indentation in the "Rollover" tests again</comment></comments></commit></commits></item><item><title>org.elasticsearch.search.aggregations.bucket.DateHistogramIT#testDSTEndTransition fails sporadically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19481</link><project id="" key="" /><description>Reproduces locally sometimes, need to run with a high iteration count to get it to trigger.

Example build: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=oraclelinux/790/console

```
&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=82560A64C52E117E -Dtests.class=org.elasticsearch.search.aggregations.bucket.DateHistogramIT -Dtests.method="testDSTEndTransition { seed=[82560A64C52E117E:58332BC3718193E2]}" -Dtests.security.manager=true -Dtests.locale=pl -Dtests.timezone=Asia/Bahrain
FAILURE 0.06s | DateHistogramIT.testDSTEndTransition { seed=[82560A64C52E117E:58332BC3718193E2]} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: Serialization failed with version [5.0.0-alpha5] bytes should be equal for streamable [org.elasticsearch.search.internal.ShardSearchTransportRequest@77143156]
   &gt; Expected: &lt;org.elasticsearch.common.bytes.PagedBytesReference@87c05c74&gt;
   &gt;      but: was &lt;org.elasticsearch.common.bytes.PagedBytesReference@bf413521&gt;
   &gt;     at __randomizedtesting.SeedInfo.seed([82560A64C52E117E:58332BC3718193E2]:0)
   &gt;     at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;     at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertVersionSerializable(ElasticsearchAssertions.java:667)
   &gt;     at org.elasticsearch.test.transport.AssertingLocalTransport.sendRequest(AssertingLocalTransport.java:93)
   &gt;     at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:470)
   &gt;     at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:438)
   &gt;     at org.elasticsearch.search.action.SearchTransportService.sendExecuteQuery(SearchTransportService.java:141)
   &gt;     at org.elasticsearch.action.search.SearchQueryThenFetchAsyncAction.sendExecuteFirstPhase(SearchQueryThenFetchAsyncAction.java:66)
   &gt;     at org.elasticsearch.action.search.AbstractSearchAsyncAction.performFirstPhase(AbstractSearchAsyncAction.java:162)
   &gt;     at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:143)
   &gt;     at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:96)
   &gt;     at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:45)
   &gt;     at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:158)
   &gt;     at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:182)
   &gt;     at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:80)
   &gt;     at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:180)
   &gt;     at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:153)
   &gt;     at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:87)
   &gt;     at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:75)
   &gt;     at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:64)
   &gt;     at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:403)
   &gt;     at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)
   &gt;     at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)
   &gt;     at org.elasticsearch.search.aggregations.bucket.DateHistogramIT.testDSTEndTransition(DateHistogramIT.java:1196)
   &gt;     at java.lang.Thread.run(Thread.java:745)
```

Reproduce with:

```
gradle :core:integTest -Dtests.seed=82560A64C52E117E -Dtests.class=org.elasticsearch.search.aggregations.bucket.DateHistogramIT -Dtests.method="testDSTEndTransition" -Dtests.security.manager=true -Dtests.locale=pl -Dtests.timezone=Asia/Bahrain
```

(May need `-Dtests.iters=50` or similar)
</description><key id="166140070">19481</key><summary>org.elasticsearch.search.aggregations.bucket.DateHistogramIT#testDSTEndTransition fails sporadically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>jenkins</label></labels><created>2016-07-18T16:49:01Z</created><updated>2016-07-19T13:09:49Z</updated><resolved>2016-07-19T13:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-07-18T19:28:33Z" id="233432579">@nik9000 is this perhaps fallout from the recent NamedWriteable changes (e.g. d14e06ce515862ba941308ff80711c98e0fd4be0)
</comment><comment author="nik9000" created="2016-07-18T19:47:42Z" id="233437559">I'll have a look then.
</comment><comment author="nik9000" created="2016-07-18T22:20:18Z" id="233476720">I've figured it out. I don't think it is NamedWriteable related but I don't know what else might have caused it. Should have a PR up for it in a bit. Maybe in the morning.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBoundsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Make ExtendedBounds immutable</comment></comments></commit></commits></item><item><title>Migrate most remaining pipeline aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19480</link><project id="" key="" /><description>This is fairly close to the last step in removing AggregationStreams and PipelineAggregatorStreams. Removing them will allow us to further standardize on NamedWriteables, removing lots of redundant code.
</description><key id="166139242">19480</key><summary>Migrate most remaining pipeline aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T16:44:56Z</created><updated>2016-07-19T18:43:58Z</updated><resolved>2016-07-19T18:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-19T16:42:34Z" id="233693118">LGTM thanks for making the name change
</comment><comment author="nik9000" created="2016-07-19T16:57:20Z" id="233697306">Thanks for reviewing all of these! I'll merge this soon and then make one last one to remove all the shims.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MacAddressProvider munged mac address is random and unrelated to mac address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19479</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.4

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

I find [TimeBasedUUIDGenerator.java](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java) use mac adress to identify current machine([MacAddressProvide.java](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/MacAddressProvider.java)). But it XOR the mac adress with random bytes . As I know this will make the mac address to really random and increase the probability of conflict. 

I'm not sure how to fix it. If we use mac adress directly, we should start only one process in one machine. Maybe add the process id factor will fix it. Add the process id factor can not solve the situation of start services with threads(Such as Tomcat. Tomcat serve multi webapps with same process? I think this will not happen in ES service ).

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="166125394">19479</key><summary>MacAddressProvider munged mac address is random and unrelated to mac address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hao5ang</reporter><labels /><created>2016-07-18T15:44:02Z</created><updated>2016-07-19T03:00:13Z</updated><resolved>2016-07-18T17:08:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-18T15:51:55Z" id="233370884">Why would it increase the probability of conflict?
</comment><comment author="hao5ang" created="2016-07-18T15:58:52Z" id="233373076">Mac address is unique, but the munged address is random. We have possibility of 1/2^48 to conflict if we deploy 2 nodes.
</comment><comment author="jpountz" created="2016-07-18T16:45:48Z" id="233386504">This is true. We need to xor the mac address so that two nodes that are started on the same physical machine do not generate the same identifiers (since they would see the same mac address). 1/2^48 is a very low probability but it is true that as more nodes get added to the cluster, the probability of collisions increases quickly. For instance, in the case of a 1000 nodes cluster, the probability of a collision is ~2*10^-9.
</comment><comment author="jpountz" created="2016-07-18T17:08:47Z" id="233392952">After discussing it a bit with @mikemccand , the current probability is probably low enough to be acceptable, especially given that ids need to be unique per index, so `min(num_nodes, number_of_replicas)` (the number of distinct nodes that hold a shard of the same index) is more relevant that the number of nodes here. Eg. the probability is ~1.6*10^-13 for 10 shards.
</comment><comment author="hao5ang" created="2016-07-19T03:00:13Z" id="233517621">Yes, the probability is really low. I think we can also just use random bytes directly because the munged mac address has no relationship with the mac address.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Automatically created indices should honor `index.mapper.dynamic`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19478</link><project id="" key="" /><description>Today they don't because the create index request that is implicitly created
adds an empty mapping for the type of the document. So to Elasticsearch it
looks like this type was explicitly created and `index.mapper.dynamic` is not
checked.

Closes #17592
</description><key id="166118259">19478</key><summary>Automatically created indices should honor `index.mapper.dynamic`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T15:15:22Z</created><updated>2017-02-25T21:35:48Z</updated><resolved>2016-07-19T07:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-19T06:59:50Z" id="233548121">LGTM
</comment><comment author="Cidan" created="2017-02-25T21:35:48Z" id="282514191">As of 5.2.1, it seems like this has regressed. I am able to create fields dynamically when using a template to auto-create an index.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/indices/TypeMissingException.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingIT.java</file></files><comments><comment>Automatically created indices should honor `index.mapper.dynamic`. #19478</comment></comments></commit></commits></item><item><title>Deleted index is coming live again </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19477</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2
**JVM version**:
1.8
**OS version**:
debian 8.4
**Description of the problem including expected versus actual behavior**:
Cluster with 3 nodes . Data is being pushed to node 1 in the cluster in bulk =&gt; total count 1 million .
After bulk operation complete shutting down node 1 .
Running query to count docs the number is the same as before node 1 shutdown 1 million.
deleting the index .
Counting the number of docs once again =&gt; no data
starting node 1 =&gt; there are docs but less then 1 million (~800k) .

The expected results is to have none . The join node should get sync with the current master .

**Steps to reproduce**:
1. 3 nodes cluster pushing data in bulk using the current master node IP 
2. stop the master node 
   3.delete the index
   1. start the previous master node 
   2. The index have data again but not as its original count 

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="166114251">19477</key><summary>Deleted index is coming live again </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aloneldi</reporter><labels /><created>2016-07-18T14:58:29Z</created><updated>2016-07-18T19:33:33Z</updated><resolved>2016-07-18T15:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-18T15:06:43Z" id="233356632">fixed by https://github.com/elastic/elasticsearch/issues/18249 - we import data in 2.x as we find it. it's a problem but a known and fixed one. thanks for opening the issue
</comment><comment author="aloneldi" created="2016-07-18T15:14:35Z" id="233359082">in which 2.x is it fixed ? 2.3 ? I'm running 2.2 .

Thanks

Alon
</comment><comment author="bleskes" created="2016-07-18T19:33:33Z" id="233433897">@aloneldi the fix is 5.0 only sadly - the fix is quite tricky and will not be ported to 2.x. 

PS. you can see that reflected by the labels in the #18249 . Also check the corresponding PR, https://github.com/elastic/elasticsearch/pull/18250 for some extra background and discussion (if you want)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not throw AssertionError for expected exceptions in SearchWhileRelocatingIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19476</link><project id="" key="" /><description>The test would previously catch Throwable and then decide if it was a critical exception or not. As the catch block was changed from Throwable to Exception this made the test fail for non-critical exceptions. This commit changes the test so that exceptions are only thrown when they're unexpected.
</description><key id="166105150">19476</key><summary>Do not throw AssertionError for expected exceptions in SearchWhileRelocatingIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>review</label><label>test</label></labels><created>2016-07-18T14:21:19Z</created><updated>2016-07-18T14:45:07Z</updated><resolved>2016-07-18T14:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-18T14:31:14Z" id="233345735">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/basic/SearchWhileRelocatingIT.java</file></files><comments><comment>Do not throw AssertionError for expected exceptions in SearchWhileRelocatingIT (#19476)</comment></comments></commit></commits></item><item><title>Stack overflow in Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19475</link><project id="" key="" /><description>The following produces a stack overflow in Painless:

```
POST t/t/1/_update
{
  "upsert": {},
  "scripted_upsert": true,
  "script": {
    "lang": "painless",
    "inline": "for (def key : params.keySet()) { ctx._source[key] = params[key]}"
  }, 
  "params": {
      "bar": "two",
      "baz": "two"
  }
}
```
</description><key id="166090972">19475</key><summary>Stack overflow in Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.1.1</label></labels><created>2016-07-18T13:16:04Z</created><updated>2016-10-18T08:32:10Z</updated><resolved>2016-10-11T09:41:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-18T13:22:19Z" id="233326494">And related (perhaps?), `params` can't be used with this syntax: `params[var]`:

```
POST t/t/1/_update
{
  "upsert": {},
  "scripted_upsert": true,
  "script": {
    "lang": "painless",
    "inline": "for (def key : ['bar','baz']) { ctx._source[key] = params[key]}"
  }, 
  "params": {
      "bar": "two",
      "baz": "two"
  }
}
```

produces:

```
    "_source": {
      "bar": null,
      "baz": null
    }
```
</comment><comment author="jdconrad" created="2016-07-25T16:30:07Z" id="235006626">Just got back from vacation, so I'll take a look at this issue today.
</comment><comment author="jdconrad" created="2016-07-25T23:39:44Z" id="235119405">@clintongormley 

In the first example, the reason you are seeing stack overflow is because the result ends up being  infinitely recursive.  Params contains 'ctx,' 'bar,' and 'baz' (more than just the user input); not just 'bar' and 'baz.'  This means that _source has 'ctx' -&gt; '_source' -&gt; 'ctx' -&gt; 'source' ... forever :).  This is interesting behavior (not exactly a bug), and maybe something we should exclusively prevent from happening.  

In the second example, I'm not sure why those are null values, when I run the parameters as a local unit test, it seems to work well.  There must be some disconnect between in coming user values somewhere along the line, and this will require some more investigation.
</comment><comment author="clintongormley" created="2016-07-27T16:45:39Z" id="235646427">&gt; Params contains 'ctx,' 'bar,' and 'baz' (more than just the user input); not just 'bar' and 'baz.' This means that _source has 'ctx' -&gt; '_source' -&gt; 'ctx' -&gt; 'source' ... forever :).

Ah ok.

&gt; This is interesting behavior (not exactly a bug), and maybe something we should exclusively prevent from happening.

Just so you know, it crashes the node :)
</comment><comment author="tlrx" created="2016-09-22T12:25:03Z" id="248888880">@clintongormley In your second example from https://github.com/elastic/elasticsearch/issues/19475#issuecomment-233326494, the `params` should be included in the `script` object, not at the same level.
</comment><comment author="clintongormley" created="2016-09-23T15:17:53Z" id="249220815">Doh - thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java</file></files><comments><comment>XContentBuilder: Avoid building self-referencing objects (#20550)</comment></comments></commit></commits></item><item><title>Add option to specify custom data directory with gradle run command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19474</link><project id="" key="" /><description>When developing a feature it is very useful to be able to stop and start Elasticsearch multiple times while maintaining the same data directory. This lets you make changes to the feature whilst still being able to test your changes quickly without having to reload whatever dataset each time.

Currently the `gradle run` command starts up a completely fresh cluster with a blank data directory which I think is absolutely the right default behaviour. However it would be good to have a option such as `--data /absolute/path/to/data/directory` which sets the data directory to your custom path.

Bonus points if you can also specify a custom `elasticsearch.yml` file to use so you can enable options like CORS etc. on the cluster.
</description><key id="166090484">19474</key><summary>Add option to specify custom data directory with gradle run command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>build</label></labels><created>2016-07-18T13:13:29Z</created><updated>2016-07-18T19:36:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-18T13:17:22Z" id="233325255">I guess it wouldn't be possible or maybe it is too much to just let it accept whatever option our startup script accepts? I think @bleskes was looking for something similar just the other day.
</comment><comment author="bleskes" created="2016-07-18T19:36:53Z" id="233434746"> I was looking for an option to run ES from the IDE with full config control via the command line (as one starting ES would normally have). Not the same :) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cancellation of recovery deletes files still held onto by writes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19473</link><project id="" key="" /><description>The RepositoryUpgradabilityIT test fails regularly because indices are deleted before trying to recreate the index for other purposes.  We could add `ensureGreen` calls after the index deletions, but the underlying cause of failure is that the ongoing recoveries get cancelled and this causes shard level data to be deleted.  However, the deletion of these files can happen _while_ other threads are holding onto these files for the purposes of writes, which trips assertions where the write operations expects the temp files to be there (but they have been deleted by the recovery cancellation).

An example stack trace:

``````
ERROR   21.6s J0 | RepositoryUpgradabilityIT.testRepositoryWorksWithCrossVersions &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=11223, name=elasticsearch[node_td1][generic][T#6], state=RUNNABLE, group=TGRP-RepositoryUpgradabilityIT]
   &gt;    at __randomizedtesting.SeedInfo.seed([6C53C46436F84D7A:76F1740152D478D]:0)
   &gt; Caused by: java.lang.AssertionError: expected: [recovery.1468504452288._0.cfe] in []
   &gt;    at __randomizedtesting.SeedInfo.seed([6C53C46436F84D7A]:0)
   &gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.assertTempFileExists(RecoveryTarget.java:407)
   &gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.writeFileChunk(RecoveryTarget.java:397)
   &gt;    at org.elasticsearch.indices.recovery.RecoveryTargetService$FileChunkTransportRequestHandler.messageReceived(RecoveryTargetService.java:417)
   &gt;    at org.elasticsearch.indices.recovery.RecoveryTargetService$FileChunkTransportRequestHandler.messageReceived(RecoveryTargetService.java:390)
   &gt;    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
   &gt;    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69)
   &gt;    at org.elasticsearch.transport.local.LocalTransport$1.doRun(LocalTransport.java:322)
   &gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)```
``````
</description><key id="166090185">19473</key><summary>Cancellation of recovery deletes files still held onto by writes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>:Recovery</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T13:11:56Z</created><updated>2016-07-19T08:23:02Z</updated><resolved>2016-07-19T08:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-18T13:16:45Z" id="233325090">Is this fixed by #19466?
</comment><comment author="abeyad" created="2016-07-18T13:22:18Z" id="233326490">Yes, I just created the issue for it to link to.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file></files><comments><comment>Move `reset recovery` into RecoveriesCollection (#19466)</comment></comments></commit></commits></item><item><title>Enable option to use request cache for size &gt; 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19472</link><project id="" key="" /><description>Previously if the size of the search request was greater than zero we would not cache the request in the request cache.

This change retains the default behaviour of not caching requests with size &gt; 0 but also allows the `request_cache=true` query parameter
to enable the cache for requests with size &gt; 0
</description><key id="166086904">19472</key><summary>Enable option to use request cache for size &gt; 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Cache</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T12:54:52Z</created><updated>2016-07-18T15:28:07Z</updated><resolved>2016-07-18T15:28:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-18T14:47:26Z" id="233350669">LGTM. Left a minor thing that you can ignore, though @abeyad may have another method to hunt down and remove later now that index creation waits.
</comment><comment author="jpountz" created="2016-07-18T15:10:10Z" id="233357696">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesRequestCacheIT.java</file></files><comments><comment>#19472 Enable option to use request cache for size &gt; 0</comment></comments></commit></commits></item><item><title>Mapper-attachments access denied while trying to find windows font directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19471</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.4 (With plugin mapper-attachments)

**JVM version**: 8 update 91 (build 1.8.0_91-b14)

**OS version**: Windows Server 2012R2

**Description of the problem including expected versus actual behavior**:
When indexing the first PDF with the mapper-attachments plugin i get an error when pdfbox tries to find the windows directory witch contains the windows font.
This trows a access denied exeption (tried to run under different users always getting access denied)
Then for every PDF without embedded fonts i get a warning that the font cant be found

**Steps to reproduce**:
1. Start up elasticsearch-service-x64.exe (make sure the mapper-attachments plugin is installed before starting)
2. Index a PDF file without embedded font
3. Check the logging

**Provide logs (if relevant)**:
[2016-07-18 14:05:54,930][ERROR][org.apache.pdfbox.pdmodel.font.PDSimpleFont] Can't determine the width of the space character using 250 as default
java.security.AccessControlException: access denied ("java.io.FilePermission" "&lt;&lt;ALL FILES&gt;&gt;" "execute")
    at java.security.AccessControlContext.checkPermission(Unknown Source)
    at java.security.AccessController.checkPermission(Unknown Source)
    at java.lang.SecurityManager.checkPermission(Unknown Source)
    at java.lang.SecurityManager.checkExec(Unknown Source)
    at java.lang.ProcessBuilder.start(Unknown Source)
    at java.lang.Runtime.exec(Unknown Source)
    at java.lang.Runtime.exec(Unknown Source)
    at java.lang.Runtime.exec(Unknown Source)
    at org.apache.fontbox.util.autodetect.WindowsFontDirFinder.getWinDir(WindowsFontDirFinder.java:50)
    at org.apache.fontbox.util.autodetect.WindowsFontDirFinder.find(WindowsFontDirFinder.java:79)
    at org.apache.fontbox.util.autodetect.FontFileFinder.find(FontFileFinder.java:74)
    at org.apache.fontbox.util.FontManager.loadFonts(FontManager.java:65)
    at org.apache.fontbox.util.FontManager.findTTFontname(FontManager.java:290)
    at org.apache.fontbox.util.FontManager.findTTFont(FontManager.java:326)
    at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getTTFFont(PDTrueTypeFont.java:638)
    at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getFontWidth(PDTrueTypeFont.java:673)
    at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getFontWidth(PDSimpleFont.java:231)
    at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getSpaceWidth(PDSimpleFont.java:533)
    at org.apache.pdfbox.util.PDFStreamEngine.processEncodedText(PDFStreamEngine.java:355)
    at org.apache.pdfbox.util.operator.ShowText.process(ShowText.java:50)
    at org.apache.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:557)
    at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:268)
    at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)
    at org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)
    at org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:458)
    at org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:383)
    at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:342)
    at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:148)
    at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
    at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
    at org.apache.tika.Tika.parseToString(Tika.java:537)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:94)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:91)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.mapper.attachments.TikaImpl.parse(TikaImpl.java:91)
    at org.elasticsearch.mapper.attachments.AttachmentMapper.parse(AttachmentMapper.java:481)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:326)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:252)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:580)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:559)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:212)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:327)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:120)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
[2016-07-18 14:05:57,078][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:57,105][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:57,711][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:57,748][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:57,850][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:57,867][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,040][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,057][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,081][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,131][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,142][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,154][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,557][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,565][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
[2016-07-18 14:05:59,572][WARN ][org.apache.fontbox.util.FontManager] Font not found: CourierNewPSMT
[2016-07-18 14:05:59,579][WARN ][org.apache.fontbox.util.FontManager] Font not found: TimesNewRomanPSMT
</description><key id="166081676">19471</key><summary>Mapper-attachments access denied while trying to find windows font directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Skoucail</reporter><labels><label>:Plugin Ingest Attachment</label><label>:Plugin Mapper Attachment</label><label>adoptme</label></labels><created>2016-07-18T12:25:04Z</created><updated>2016-09-14T15:42:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-18T13:08:12Z" id="233323080">The bug is in fontbox code, which tries to execute a process, but doesn't handle securityexception:

https://github.com/apache/pdfbox/blob/trunk/fontbox/src/main/java/org/apache/fontbox/util/autodetect/WindowsFontDirFinder.java#L81-L84

We could add a hack/workaround, to deliver it the IOException it wants instead of SecurityException instead, but fontbox needs to be fixed, otherwise the hack never goes away, and its unclear how well this would work anyway, looks like if it makes it past this point, the code will just continue to try to do sheisty stuff with system files, likely unsuccessfully :)
</comment><comment author="Skoucail" created="2016-07-18T13:47:01Z" id="233332940">Well i see they released a few new versions too.
I tried the mapper with the latest 1.x version but there the problem is still in it.
I couldn't get the mapper to work with the newest 2.x version. I guess the API changes are to big compared with the 1.x version.

Do you know what the practical impact is of this problem?
Can i still reliably use the plugin in a production environment?
Because for my small testset of data my searches still hit on the correct PDFs, but of course all the errors en warning in the log do not give me a confident feeling...
</comment><comment author="rmuir" created="2016-07-18T14:28:31Z" id="233344869">With system fonts, PDF processing implies font fallbacks and such, because you didn't include the exact font you used, and it may not be available to the receiving end.

Even if you ignore this problem, and tika had access to all the system fonts, you'd always have to be prepared that the font for a particular document may not be installed server-side, and it may have to fall back to an approximation that isn't based on the true mapping, but may map some glyphs to characters incorrectly and give a lower quality extraction.

I'm guessing your tested PDFs might only/mostly contain 7-bit ascii, which is usually 1:1 in mappings for these fonts: so they happen to work correctly/mostly correctly via a fallback anyway, even though "font not found".

The easiest improvement would be to remove the ERROR via the hack I suggest above (+ fixing fontbox). Its still not going to give access to any system fonts, it will simply reduce noise.
</comment><comment author="rmuir" created="2016-07-18T14:33:24Z" id="233346370">It should go without saying: i don't think we should expose system fonts (unrelated to security) because then different nodes could index the same document in different ways: that's just a source of potential confusion or inconsistency.

Allowing users to configure a directory fonts in some way for this purpose? I haven't looked at the mappings that fontbox provides, don't have any clue if its "enough", and i'm unsure if we want the complexity. Most PDFs really use `embedded` or `embedded subset`, otherwise they are not really "portable".
</comment><comment author="rmuir" created="2016-07-18T14:45:52Z" id="233350157">An even simpler improvement might be to silence this error in the logs such as what happened with AWS: https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/config/logging.yml#L14-L16

unfortunately my experience with that is, people will continue to report confusion unless they are installing new from scratch. but its easier.
</comment><comment author="Skoucail" created="2016-07-19T06:32:39Z" id="233544035">Yes i agree that it would be easier with fonts embedded but there isn't always a choice. For example PDFs users upload.

Guess for now i will just have to ignore the errors and hope for a bugfix in fontbox...
</comment><comment author="SKumarMN" created="2016-09-14T15:42:32Z" id="247056657">I too encounter the same issue 

ES 2.3.2 with mapper-attachment plugin

[2016-08-31 08:56:03,867][ERROR][org.apache.pdfbox.pdmodel.font.PDSimpleFont] Can't determine the width of the space character using 250 as default
java.security.AccessControlException: access denied ("java.io.FilePermission" "/opt/weblogic/.fonts" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at java.io.File.exists(File.java:814)
    at org.apache.fontbox.util.autodetect.NativeFontDirFinder.find(NativeFontDirFinder.java:44)
    at org.apache.fontbox.util.autodetect.FontFileFinder.find(FontFileFinder.java:74)
    at org.apache.fontbox.util.FontManager.loadFonts(FontManager.java:65)
    at org.apache.fontbox.util.FontManager.findTTFontname(FontManager.java:290)
    at org.apache.fontbox.util.FontManager.findTTFont(FontManager.java:326)
    at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getTTFFont(PDTrueTypeFont.java:638)
    at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getFontWidth(PDTrueTypeFont.java:673)
    at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getFontWidth(PDSimpleFont.java:231)
    at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getSpaceWidth(PDSimpleFont.java:533)
    at org.apache.pdfbox.util.PDFStreamEngine.processEncodedText(PDFStreamEngine.java:355)
    at org.apache.pdfbox.util.operator.ShowTextGlyph.process(ShowTextGlyph.java:62)
    at org.apache.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:557)
    at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:268)
    at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235)
    at org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215)
    at org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:458)
    at org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:383)
    at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:342)
    at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:148)
    at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148)
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
    at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
    at org.apache.tika.Tika.parseToString(Tika.java:537)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:94)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:91)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.mapper.attachments.TikaImpl.parse(TikaImpl.java:91)
    at org.elasticsearch.mapper.attachments.AttachmentMapper.parse(AttachmentMapper.java:481)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:436)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:580)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:559)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:212)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:119)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Make sure that no `_parent#null` gets introduces as default _parent mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19470</link><project id="" key="" /><description>Instead it should just be `_parent` field.

Also added more tests regarding the join doc values field being added.

Closes #19389
</description><key id="166064893">19470</key><summary> Make sure that no `_parent#null` gets introduces as default _parent mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T10:35:57Z</created><updated>2016-07-18T14:38:56Z</updated><resolved>2016-07-18T14:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-18T12:34:04Z" id="233315775">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix create_bwc_index for 5.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19469</link><project id="" key="" /><description>`create_bwc_index.py` has some cruft in it that only works on 2.x or
even before. This commit make the tool functional, yet there might still
be some bwc relevant things missing here.

Closes #19253
</description><key id="166056990">19469</key><summary>Fix create_bwc_index for 5.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T09:47:01Z</created><updated>2016-07-18T10:23:02Z</updated><resolved>2016-07-18T10:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-18T10:05:11Z" id="233290554">LGTM
</comment><comment author="s1monw" created="2016-07-18T10:17:38Z" id="233292779">@jpountz pushed more changes
</comment><comment author="jpountz" created="2016-07-18T10:19:10Z" id="233293042">LGTM2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fix create_bwc_index for 5.x (#19469)</comment></comments></commit></commits></item><item><title>Is there a security vulnerability associated with "mustache templating"?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19468</link><project id="" key="" /><description>It seems like this is basically a direct substitution mechanism into a preset template. So in theory if somebody knew what they were doing, couldn't they alter the intended purpose of a template through some carefully thought-out JSON-injection?

Not sure if this belongs here but was something that occurred to me as a potential security bug and wanted to get some feedback on it.

thanks
</description><key id="166055221">19468</key><summary>Is there a security vulnerability associated with "mustache templating"?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jack-Collins</reporter><labels><label>:Templates</label><label>discuss</label></labels><created>2016-07-18T09:36:34Z</created><updated>2016-07-19T11:05:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-18T11:45:04Z" id="233307108">probably :)

I do not know why this one is assumed to be safe.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Build: Ensure poms for plugin zips have url</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19467</link><project id="" key="" /><description>Maven central requires a project url. The recent change to make poms for
plugin client jars broke that because we no longer use nebula publishing
for plugin pom generation. This change adds back the url to the pom.
</description><key id="166050253">19467</key><summary>Build: Ensure poms for plugin zips have url</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T09:06:08Z</created><updated>2016-07-18T15:23:32Z</updated><resolved>2016-07-18T15:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-18T09:59:25Z" id="233289436">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19467 from rjernst/pom_url</comment></comments></commit></commits></item><item><title>Move `reset recovery` into RecoveriesCollection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19466</link><project id="" key="" /><description>Today when we reset a recovery because of the source not being
ready or the shard is getting removed on the source (for whatever reason)
we wipe all temp files and reset the recovery without respecting any
reference counting or locking etc. all streams are closed and files are
wiped. Yet, this is problematic since we assert that some files are on disk
etc. when we finish writing a file. These assertions don't hold anymore if we
concurrently wipe the tmp files.

This change moves the logic out of RecoveryTarget into RecoveriesCollection which
basically clones the RecoveryTarget on reset instead which allows in-flight operations
to finish gracefully. This means we now have a single path for cleanups in RecoveryTarget
and can safely use assertions in the class since files won't be removed unless the recovery
is either cancelled, failed or finished.

Closes #19473
</description><key id="166043400">19466</key><summary>Move `reset recovery` into RecoveriesCollection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-18T08:28:31Z</created><updated>2016-07-19T08:23:10Z</updated><resolved>2016-07-19T08:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-18T08:28:54Z" id="233268586">@bleskes can you take a look?
</comment><comment author="bleskes" created="2016-07-18T11:32:39Z" id="233305072">It took a while to convince my self that keeping the same recovery id is the right way to go for now. I like how it's done and left some minor comments. Thx @s1monw 
</comment><comment author="s1monw" created="2016-07-18T14:29:00Z" id="233345022">@bleskes I pushed new commits
</comment><comment author="bleskes" created="2016-07-18T19:27:57Z" id="233432420">LGTM. Left minor suggestions - feel free to accept or reject and push away.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file></files><comments><comment>Move `reset recovery` into RecoveriesCollection (#19466)</comment></comments></commit></commits></item><item><title>the inconsistency of java files and class files in elasticsearch-1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19465</link><project id="" key="" /><description>To whom it may concern,
I downloaded java files and class files of elasticsearch-1.4.4, and found several java files missing, such as files in path org\elasticsearch\common\cli, org\elasticsearch\common\lang3, org\elasticsearch\common\compress and org\elasticsearch\common\annotations. 
Why did that happen?

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.4.4

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="166040623">19465</key><summary>the inconsistency of java files and class files in elasticsearch-1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Abbyli</reporter><labels /><created>2016-07-18T08:12:48Z</created><updated>2016-07-18T08:15:11Z</updated><resolved>2016-07-18T08:15:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-18T08:15:11Z" id="233264255">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Merge pull request #1 from elastic/master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19464</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

updating the fork
</description><key id="165987712">19464</key><summary>Merge pull request #1 from elastic/master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NaveenRaj88</reporter><labels /><created>2016-07-17T17:58:46Z</created><updated>2016-07-17T18:00:09Z</updated><resolved>2016-07-17T17:59:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NaveenRaj88" created="2016-07-17T18:00:09Z" id="233194870">test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>date histogram doc count with interval day is correct, but giving more doc count with hour</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19463</link><project id="" key="" /><description>Hi ,
  I am writing query to find hourly count with some filter: 
date histogram doc count with interval day is correct, but giving more doc count with hour

For example my query with interval day:

```
GET _search
{
  "size": 0,
  "aggs": {
    "eventtime": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "RW.RT": "152"
              }
            },
            {
              "term": {
                "RW.RE": "received"
              }
            }
          ]
        }
      },
      "aggregations": {
        "timeslice": {
          "date_histogram": {
            "field": "RW.DT",
            "interval": "day",
            "format": "YYYY-MM-dd HH:MM:ss",
            "min_doc_count": 1
          }
        }
      }
    }
  }
}
```

Output is : 

```
"hits": {
  "total": 147,
  "max_score": 0,
  "hits": []
},
"aggregations": {
  "eventtime": {
     "doc_count": 6,
     "timeslice": {
        "buckets": [
           {
              "key_as_string": "2016-07-14 00:07:00",
              "key": 1468454400000,
              "doc_count": 6
           }
        ]
     }
  }
  }
```

This is ok..but when i change interval with hour ..
Output is 

```
"hits": {
  "total": 147,
  "max_score": 0,
  "hits": []
},
"aggregations": {
  "eventtime": {
     "doc_count": 6,
     "timeslice": {
        "buckets": [
           {
              "key_as_string": "2016-07-14 07:07:00",
              "key": 1468479600000,
              "doc_count": 1
           },
           {
              "key_as_string": "2016-07-14 08:07:00",
              "key": 1468483200000,
              "doc_count": 2
           },
           {
              "key_as_string": "2016-07-14 09:07:00",
              "key": 1468486800000,
              "doc_count": 4
           },
           {
              "key_as_string": "2016-07-14 12:07:00",
              "key": 1468497600000,
              "doc_count": 1
           },
           {
              "key_as_string": "2016-07-14 13:07:00",
              "key": 1468501200000,
              "doc_count": 1
           }
        ]
     }
  }
 }
```

Here in output doc count is  9, this should be 6 
Please help me 
</description><key id="165968251">19463</key><summary>date histogram doc count with interval day is correct, but giving more doc count with hour</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dks007</reporter><labels /><created>2016-07-17T09:13:12Z</created><updated>2016-07-18T07:52:35Z</updated><resolved>2016-07-17T18:54:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-17T18:54:01Z" id="233197562">Your date format is using `MM` for minutes, instead of `mm`
</comment><comment author="dks007" created="2016-07-18T05:40:13Z" id="233231737">Thank u for comment clintongormley , but format is not an issue.. i did that...format is just for displaying if u need any specific kind of format...No of doc count is difference in in case of hour but in case of day it is ok...Please see above query.
</comment><comment author="clintongormley" created="2016-07-18T07:52:35Z" id="233257452">That's true. In which case, (and you haven't provided example docs) but it looks like you have multiple values per doc in that field.  If you use the [value count](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-valuecount-aggregation.html) agg this will work as you expect.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove script access to term statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19462</link><project id="" key="" /><description>In scripts (at least some of the languages), the terms dictionary and
postings can be access with the special _index variable. This is for
very advanced use cases which want to do their own scoring. The problem
is segment level statistics must be recomputed for every document.
Additionally, this is not friendly to the terms index caching as the
order of looking up terms should be controlled by lucene.

This change removes _index from scripts. Anyone using it can and should
instead write a Similarity plugin, which is explicitly designed to allow
doing the calculations needed for a relevance score.

closes #19359
</description><key id="165886438">19462</key><summary>Remove script access to term statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>breaking</label><label>v6.0.0-alpha2</label></labels><created>2016-07-15T22:05:09Z</created><updated>2017-06-06T09:00:47Z</updated><resolved>2017-05-16T16:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-17T18:44:21Z" id="233197078">You should also remove the docs https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-advanced-scripting.html

Please could you change the title to something more meaningful, such as "Remove script access to term statistics"
</comment><comment author="jpountz" created="2016-07-18T07:22:18Z" id="233253132">The code changes LGTM
</comment><comment author="rjernst" created="2016-07-18T07:28:22Z" id="233253887">@clintongormley I removed those docs and updated the title as you suggested.
</comment><comment author="clintongormley" created="2016-07-18T08:08:58Z" id="233262588">There's also a mention and link which you'll need to remove in this section: https://github.com/elastic/elasticsearch/blob/master/docs/reference/modules/scripting/fields.asciidoc#search-and-aggregation-scripts

Would it be possible to add the appropriate deprecation logging in 2.4.0?
</comment><comment author="rjernst" created="2016-07-18T08:12:45Z" id="233263705">&gt; Would it be possible to add the appropriate deprecation logging in 2.4.0?

I'm not sure how to do that without creating potentially very large logs. We only know this is accessed when a script is being run, and it is called _from_ the script. So eg if a script runs on a million docs you would get a million deprecation messages?
</comment><comment author="clintongormley" created="2016-07-18T08:14:29Z" id="233264029">Deprecation logging is off by default.  But yes, I see what you mean.  I wonder if we should be rate-limiting duplicate messages in the deprecation log infra itself.
</comment><comment author="jpountz" created="2016-07-18T08:17:15Z" id="233264759">Another way would be to do something like

```
if (logged == false) {
  // log message
  logged = true;
}
```

in every method of LeafIndexLookup in order to have one message per segment, which would make the volume lower.
</comment><comment author="clintongormley" created="2016-07-18T13:52:22Z" id="233334396">I think we should rethink this PR given https://github.com/elastic/elasticsearch/issues/19359#issuecomment-233300053
</comment><comment author="astefan" created="2016-08-08T10:54:14Z" id="238203721">I have seen scripts being used for retrieving terms' statistics and re-scoring the documents based on them (or sorting the documents based on them) in our public community. It is true it is not often being used, but I've seen it. Removing this possibility assumes the user will need to get a hold of Java and write code for the same thing that was possible in queries in a much simpler and accessible way.
</comment><comment author="dakrone" created="2016-09-12T22:12:33Z" id="246511752">@rjernst is this PR still needed given Clint's earlier comment about rethinking it?
</comment><comment author="rjernst" created="2016-09-15T00:47:49Z" id="247201246">@dakrone I think this PR still makes sense, and I left a comment on #19359 explaining why.
</comment><comment author="elasticmachine" created="2017-02-23T18:14:40Z" id="282074244">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="rjernst" created="2017-05-16T08:57:26Z" id="301719232">@jpountz I've updated this PR now that index lookup is deprecated in 5.5. Can you take a look again?</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexField.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafIndexLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/PositionIterator.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/TermPosition.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupIT.java</file></files><comments><comment>Remove script access to term statistics (#19462)</comment></comments></commit></commits></item><item><title>Simplify plugin configuration for rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19461</link><project id="" key="" /><description>This change removes the multiple ways that plugins can be added to the
integ test cluster. It also removes the use of the default
configuration, and instead adds a zip configuration to all plugins. This
will enable using project substitutions with plugins, which must be done
with the default configuration.

/cc @s1monw 
</description><key id="165884155">19461</key><summary>Simplify plugin configuration for rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>build</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T21:48:36Z</created><updated>2016-07-18T21:07:15Z</updated><resolved>2016-07-18T21:07:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-15T22:05:30Z" id="233080916">thx ryan
</comment><comment author="nik9000" created="2016-07-16T11:12:30Z" id="233125559">I've added this to my review queue for Monday. If someone else gets to it before me then enjoy! If not I'll look Monday.
</comment><comment author="nik9000" created="2016-07-18T13:51:19Z" id="233334144">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19461 from rjernst/plugin_default_config</comment></comments></commit></commits></item><item><title>Removes yellow health check in REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19460</link><project id="" key="" /><description>Removes waiting for yellow cluster health upon index 
creation in the REST tests, as we no longer need it due
to index creation now waiting for active shard copies
before returning (by default, it waits for the primary of
each shard, which is the same as ensuring yellow health).

Relates #19450
</description><key id="165876820">19460</key><summary>Removes yellow health check in REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>review</label><label>test</label></labels><created>2016-07-15T21:01:23Z</created><updated>2016-07-17T18:41:28Z</updated><resolved>2016-07-15T21:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-15T21:17:31Z" id="233072151">LGTM
</comment><comment author="abeyad" created="2016-07-15T21:18:22Z" id="233072305">Thanks for the review @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Removes waiting for yellow cluster health upon index (#19460)</comment></comments></commit></commits></item><item><title>Change Painless Tree Structure for Variable/Method Chains</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19459</link><project id="" key="" /><description>The following changes have been made to Painless in this PR:

1) Changed the tree structure for variable/method chains.  Previously all loads were required to have a parent of EChain.  ALink nodes and EChain were removed in favor of allowing constants and variables to be expression nodes instead.  Postfix nodes have replaced non-primary pieces (call, field, and brace) with the parent-child relationship of the final postfix at the top with a child prefix until a primary becomes a leaf node.  Example:  x.call()[0] becomes Postfix([0]) --&gt; Postfix(call) --&gt; Primary(x).

2) The grammar has been updated to allow proper postfixes to be attached to all primaries including constants and precedence.  Example:  (x ? map1 : map2).get('string')

3) Instead of possible node rewriting for primaries and postfixes, sub nodes have been added and the primary or postfix will have a sub node as a child in some cases to help break up the analysis and writing of certain logical breaks such.  Example: PCallInvoke --&gt; PSubDefCall/PSubCallInvoke instead of PDefCall/PCallInvoke replacing PCallInvoke in the tree altogether.

4) Clean up of variable permissions in nodes attempting to make variables private when possible.

5) Added a way to prevent picky (ambiguity) tests for testing of actual error message output the user will see when testing for lexer/parser errors.
</description><key id="165876486">19459</key><summary>Change Painless Tree Structure for Variable/Method Chains</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T20:59:20Z</created><updated>2016-07-27T21:03:17Z</updated><resolved>2016-07-27T21:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-07-25T21:41:52Z" id="235094707">@rmuir I removed the branching shortcuts.  Not sure it actually simplified that much, but it does remove the necessity to write a jump in a child node to a label that gets written in a parent node.  Note this does write out multiple branches in the byte code now, but since you checked the actual assembly that gets written is the same, good to go.
</comment><comment author="rmuir" created="2016-07-27T10:30:29Z" id="235548265">this cleanup is good. I left two minor comments.
</comment><comment author="jdconrad" created="2016-07-27T21:03:17Z" id="235720150">@rmuir Thanks for the review!  Contrary to what github believes, this has been merged into master.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/ErrorHandlingLexer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ALink.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ANode.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AStatement.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AStoreable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EAssignment.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBool.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBoolean.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECallLocal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConditional.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConstant.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EDecimal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EExplicit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EInstanceof.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EListInit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EMapInit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENewArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENewObj.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENull.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENumeric.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ERegex.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EStatic.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EString.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EVariable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/IDefLink.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubArrayLength.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubListShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubMapShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/PSubShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBreak.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SCatch.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SContinue.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclaration.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDo.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIf.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIfElse.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SReturn.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSubEachArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSubEachIterable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SThrow.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/STry.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SWhile.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/package-info.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicAPITests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicExpressionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/EqualsTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/PostfixTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/RegexTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ReservedWordTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScoreTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScriptEngineTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScriptTestCase.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/TryCatchTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/WhenThingsGoWrongTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/antlr/ParserTests.java</file></files><comments><comment>Refactored Painless link nodes into expression nodes to simplify</comment><comment>load/store operations of variable/method chains.</comment></comments></commit></commits></item><item><title>Vanilla Elasticsearch 5.0.0alpha4 unable to start in LXD container</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19458</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: [5.0.0alpha4](https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/5.0.0-alpha4/elasticsearch-5.0.0-alpha4.tar.gz)

**JVM version**: [Oracle Java JRE Version 8 Update 91](http://javadl.oracle.com/webapps/download/AutoDL?BundleId=207765)

**OS version**: Ubuntu 16.04 LXD (Ubuntu 16.04 LTS amd64 image fingerprint: f452cda3bccb)

**Description of the problem including expected versus actual behavior**:

Running 5.0.0alpha4 `elasticsearch` fails. 
Running 2.3.4 `elasticsearch` succeeds.

**Steps to reproduce**:
1. Launch LXD container
2. Install [Java ](http://javadl.oracle.com/webapps/download/AutoDL?BundleId=207765)\+ [Elasticsearch 5.0.0alpha4](https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/5.0.0-alpha4/elasticsearch-5.0.0-alpha4.tar.gz) 
3. Run Elasticsearch

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

5.0.0alpha4 elasticsearch.yml:

```
root@base-elasticsearch-5-0-0-alpha4:~# grep -v '#' /opt/elasticsearch/elasticsearch-5.0.0-alpha4/config/elasticsearch.yml
cluster.name: beta
network.host: _site_
```

2.3.4 elasticsearch.yml:

```
root@base-elasticsearch-2-3-4:~# grep -v '#' /opt/elasticsearch/elasticsearch-2.3.4/config/elasticsearch.yml
cluster.name: waldo
network.host: _site_
```

5.0.0alpha4 startup log:

```
elasticsearch@base-elasticsearch-5-0-0-alpha4:~$ elasticsearch 2&gt;&gt;elasticsearch5.0.0.log 1&gt;&gt;elasticsearch5.0.0.log
elasticsearch@base-elasticsearch-5-0-0-alpha4:~$ cat elasticsearch5.0.0.log
[2016-07-15 20:50:14,763][INFO ][node                     ] [Raa of the Caves] version[5.0.0-alpha4], pid[694], build[3f5b994/2016-06-27T16:23:46.861Z], OS[Linux/4.4.0-28-generic/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_91/25.91-b14]
[2016-07-15 20:50:14,763][INFO ][node                     ] [Raa of the Caves] initializing ...
[2016-07-15 20:50:15,833][INFO ][plugins                  ] [Raa of the Caves] modules [percolator, lang-mustache, lang-painless, reindex, aggs-matrix-stats, lang-expression, ingest-common, lang-groovy], plugins []
[2016-07-15 20:50:16,608][INFO ][env                      ] [Raa of the Caves] using [1] data paths, mounts [[/ (turtle/_lxd/containers/base-elasticsearch-5-0-0-alpha4)]], net usable_space [8.7tb], net total_space [8.7tb], spins? [possibly], types [zfs]
[2016-07-15 20:50:16,608][INFO ][env                      ] [Raa of the Caves] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-07-15 20:50:18,062][INFO ][node                     ] [Raa of the Caves] initialized
[2016-07-15 20:50:18,062][INFO ][node                     ] [Raa of the Caves] starting ...
[2016-07-15 20:50:18,231][INFO ][transport                ] [Raa of the Caves] publish_address {10.10.1.67:9300}, bound_addresses {10.10.1.67:9300}
Exception in thread "main" java.lang.RuntimeException: bootstrap checks failed
initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:125)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:85)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:65)
        at org.elasticsearch.bootstrap.Bootstrap$5.validateNodeBeforeAcceptingRequests(Bootstrap.java:178)
        at org.elasticsearch.node.Node.start(Node.java:373)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:193)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:252)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
Refer to the log for complete error details.
[2016-07-15 20:50:18,244][INFO ][node                     ] [Raa of the Caves] stopping ...
[2016-07-15 20:50:18,267][INFO ][node                     ] [Raa of the Caves] stopped
[2016-07-15 20:50:18,267][INFO ][node                     ] [Raa of the Caves] closing ...
[2016-07-15 20:50:18,278][INFO ][node                     ] [Raa of the Caves] closed
elasticsearch@base-elasticsearch-5-0-0-alpha4:~$
```

5.0.0alpha4 vm.max_map_count value is 65530:

```
root@base-elasticsearch-5-0-0-alpha4:~# sysctl -a | grep vm.max_map_count
sysctl: permission denied on key 'fs.protected_hardlinks'
sysctl: permission denied on key 'fs.protected_symlinks'
sysctl: permission denied on key 'kernel.cad_pid'
sysctl: permission denied on key 'kernel.unprivileged_userns_apparmor_policy'
sysctl: permission denied on key 'kernel.usermodehelper.bset'
sysctl: permission denied on key 'kernel.usermodehelper.inheritable'
sysctl: reading key "net.ipv6.conf.all.stable_secret"
sysctl: reading key "net.ipv6.conf.default.stable_secret"
sysctl: reading key "net.ipv6.conf.eth0.stable_secret"
sysctl: reading key "net.ipv6.conf.lo.stable_secret"
vm.max_map_count = 65530
```

2.3.4 vm.max_map_count value is 65530:

```
root@base-elasticsearch-2-3-4:~# sysctl -a | grep vm.max_map_count
sysctl: permission denied on key 'fs.protected_hardlinks'
sysctl: permission denied on key 'fs.protected_symlinks'
sysctl: permission denied on key 'kernel.cad_pid'
sysctl: permission denied on key 'kernel.unprivileged_userns_apparmor_policy'
sysctl: permission denied on key 'kernel.usermodehelper.bset'
sysctl: permission denied on key 'kernel.usermodehelper.inheritable'
sysctl: reading key "net.ipv6.conf.all.stable_secret"
sysctl: reading key "net.ipv6.conf.default.stable_secret"
sysctl: reading key "net.ipv6.conf.eth0.stable_secret"
sysctl: reading key "net.ipv6.conf.lo.stable_secret"
vm.max_map_count = 65530
```
</description><key id="165875903">19458</key><summary>Vanilla Elasticsearch 5.0.0alpha4 unable to start in LXD container</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">naisanza</reporter><labels /><created>2016-07-15T20:55:47Z</created><updated>2017-05-09T08:15:27Z</updated><resolved>2016-07-15T21:01:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-15T21:01:26Z" id="233068872">It told you why it isn't starting:
- initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
- please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
- max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]

It doesn't check these things if you bind to localhost because it assumes that you aren't running it in production. If you are running in production these are the minimum changes that we recommend.
</comment><comment author="jasontedor" created="2016-07-15T21:01:48Z" id="233068949">Since you're binding to an external interface, the Elasticsearch [bootstrap checks](https://www.elastic.co/guide/en/elasticsearch/reference/master/bootstrap-checks.html) are enforced. In particular, you're failing three checks:

```
initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
```

These are all documented in the [bootstrap check](https://www.elastic.co/guide/en/elasticsearch/reference/master/bootstrap-checks.html) docs.

You'll need to set the initial heap size equal to your desired max heap size, configure `discovery.zen.minimum_master_nodes`, and set `vm.max_map_count` to at least 262144.
</comment><comment author="naisanza" created="2016-07-15T21:33:49Z" id="233075365">@nik9000 @jasontedor going through resolving the bootstrap tests one-by-one, with `bootstrap.memory_lock: true` set in the config, should you still be encountering the `initial heap size not equal to maximum heap size` error?

I'm still getting that error
</comment><comment author="jasontedor" created="2016-07-15T21:36:09Z" id="233075775">The setting `bootstrap.memory_lock` has nothing to do with setting the heap size. It's in the docs that I linked to. 
</comment><comment author="naisanza" created="2016-07-16T00:12:08Z" id="233096119">@jasontedor that's where I got it from

https://www.elastic.co/guide/en/elasticsearch/reference/master/_heap_size_check.html

&gt; Heap size check
&gt; If a JVM is started with unequal initial and max heap size, it can be prone to pauses as the JVM heap is resized during system usage. To avoid these resize pauses, it’s best to start the JVM with the initial heap size equal to the maximum heap size. Additionally, if **bootstrap.memory_lock** is **enabled**, the JVM will lock the initial size of the heap on startup. If the initial heap size is not equal to the maximum heap size, after a resize it will not be the case that all of the JVM heap is locked in memory. To pass the heap size check, you must configure the heap size.
</comment><comment author="jasontedor" created="2016-07-16T00:38:44Z" id="233098163">That is telling you the advantage of setting the initial heap size to the desired maximum heap size. If you follow the last link on that page it will take you directly to how to set the heap size:

&gt; To pass the heap size check, you must configure the [heap size](https://www.elastic.co/guide/en/elasticsearch/reference/master/heap-size.html).
</comment><comment author="naisanza" created="2016-07-30T22:48:54Z" id="236394166">Solved the vm.max_map_count https://github.com/lxc/lxd/issues/2206#issuecomment-236393880
</comment><comment author="naisanza" created="2016-08-02T05:49:11Z" id="236805572">Working elasticsearch.yml:

```
cluster.name: beta
network.host: _site_
discovery.zen.minimum_master_nodes: 1
```

Working elasticsearch launch parameters:
`ES_JAVA_OPTS='-Xms2048m -Xmx2048m' elasticsearch`

It works now
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Convert snippets in health docs to CONSOLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19457</link><project id="" key="" /><description>This should make them easier to read and adds them to the test suite
I changed the example from a two node cluster to a single node cluster
because that is what we have running in the integration tests. It is also
what a user just starting out is likely to see so I think that is ok.
</description><key id="165871707">19457</key><summary>Convert snippets in health docs to CONSOLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T20:32:07Z</created><updated>2016-07-29T11:12:45Z</updated><resolved>2016-07-15T20:51:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-15T20:42:57Z" id="233064845">LGTM
</comment><comment author="nik9000" created="2016-07-15T20:51:41Z" id="233066723">Thanks for reviewing @abeyad !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Persistent Node Names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19456</link><project id="" key="" /><description>With #19140 we started persisting the node ID across node restarts. Now that we have a "stable" anchor, we can use it to generate a stable default node name and make it easier to track nodes over a restarts. Sadly, this means we will not have those random fun Marvel characters but we feel this is the right tradeoff.

On the implementation side, this requires a bit of juggling because we now need to read the node id from disk before we can log as the node node is part of each log message. The PR move the initialization of NodeEnvironment as high up in the starting sequence as possible, with only one logging message before it to indicate we are initializing. Things look now like this:

```
[2016-07-15 19:38:39,742][INFO ][node                     ] [_unset_] initializing ...
[2016-07-15 19:38:39,826][INFO ][node                     ] [aAmiW40] node name set to [aAmiW40] by default. set the [node.name] settings to change it
[2016-07-15 19:38:39,829][INFO ][env                      ] [aAmiW40] using [1] data paths, mounts [[ /(/dev/disk1)]], net usable_space [5.5gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-07-15 19:38:39,830][INFO ][env                      ] [aAmiW40] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-07-15 19:38:39,837][INFO ][node                     ] [aAmiW40] version[5.0.0-alpha5-SNAPSHOT], pid[46048], build[473d3c0/2016-07-15T17:38:06.771Z], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_51/25.51-b03]
[2016-07-15 19:38:40,980][INFO ][plugins                  ] [aAmiW40] modules [percolator, lang-mustache, lang-painless, reindex, aggs-matrix-stats, lang-expression, ingest-common, lang-groovy, transport-netty], plugins []
[2016-07-15 19:38:43,218][INFO ][node                     ] [aAmiW40] initialized
```

Needless to say, settings `node.name` explicitly still works as before.

The PR also contains some clean ups to the relationship between Environment, Settings and Plugins. The previous code suggested the path related settings could be changed after the initial Environment was changed. This did not have any effect as the security manager already locked things down.
</description><key id="165867492">19456</key><summary>Persistent Node Names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T20:07:47Z</created><updated>2016-07-23T20:46:49Z</updated><resolved>2016-07-23T20:46:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-19T14:24:59Z" id="233649209">@jasontedor thx. I addressed all comments. Now the logs like so:

```
[elasticsearch] [2016-07-19 16:23:00,401][INFO ][node                     ] [] initializing ...
[elasticsearch] [2016-07-19 16:23:00,479][INFO ][node                     ] [e0VVSNc] node name [e0VVSNc] derived from node ID; set [node.name] to override
[elasticsearch] [2016-07-19 16:23:00,500][INFO ][env                      ] [e0VVSNc] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [6.7gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[elasticsearch] [2016-07-19 16:23:00,500][INFO ][env                      ] [e0VVSNc] heap size [1.9gb], compressed ordinary object pointers [true]
```
</comment><comment author="jasontedor" created="2016-07-20T01:48:33Z" id="233817005">&gt; `[elasticsearch] [2016-07-19 16:23:00,500][INFO ][env                      ] [e0VVSNc] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [6.7gb], net total_space [232.6gb], spins? [unknown], types [hfs]`

Getting little a tight there, be careful, you might run into strange issues with `git`. :wink:
</comment><comment author="jasontedor" created="2016-07-22T13:22:45Z" id="234542255">@bleskes Can you resolve the merge conflicts and I will take a final look?
</comment><comment author="bleskes" created="2016-07-22T18:02:02Z" id="234613471">@jasontedor I updated the pr resolving the merge.
</comment><comment author="jasontedor" created="2016-07-23T02:52:24Z" id="234695087">Punch it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOC] Remove obsolete node names from documentation</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/node/NodeTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/IndexSettingsModule.java</file></files><comments><comment>Persistent Node Names (#19456)</comment></comments></commit></commits></item><item><title>Remove wait_for_status=yellow from the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19455</link><project id="" key="" /><description>It is no longer required after 687e2e12b31ed3c12ef4c411333bff9da58fc808.
</description><key id="165867046">19455</key><summary>Remove wait_for_status=yellow from the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T20:05:28Z</created><updated>2016-07-29T11:12:49Z</updated><resolved>2016-07-15T20:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-15T20:08:25Z" id="233057338">LGTM
</comment><comment author="nik9000" created="2016-07-15T20:11:09Z" id="233057891">Thanks for reviewing @abeyad ! I've merged it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removes write consistency level across replication action APIs in favor of wait_for_active_shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19454</link><project id="" key="" /><description>Removes the notion of write consistency level across replication action APIs 
(index, update, delete, re-index, etc) in favor of waiting for active shard copy 
count (wait_for_active_shards) that was introduced in #18985 for index creation.

The reason for the removal of write consistency for replication operations is that 
it led to a false notion of what guarantees were made by checking the "write 
consistency" level.  The way write consistency checks worked before was to check 
the number of active shard copies as a replication request (e.g. indexing) arrived.  If 
the required number of active shards were present (defaulted to "quorum"),
then the replication operation would proceed.  However, it is entirely possible that
replication would still fail on a majority or even all replicas after the write consistency
check, but the operation still succeeds on the primary.  Also, our use of the terms 
"write consistency" and "quorum" in relation to replication actions did not convey 
the same semantics as found in the literature, as we don't have an equivalent 
notion of read consistency and quorum typically relates to a guarantee in the 
consensus model which we do not have for replication actions.  Hence, we have 
changed the terminology to use `wait_for_active_shards` as it better conveys the fact 
that this check is a best attempt at maintaining the desired level of replication and 
thereby resiliency.

This PR also removes the `index.write_consistency` setting in favor of a new setting: `index.write.wait_for_active_shards`.  `index.write.wait_for_active_shards` is an 
index-level setting that is dynamically updatable, and can be set to either `all` or 
any non-negative value up to the total number of shard copies for a shard in an 
index (number of replicas + 1).  The default value for this setting is 1 (meaning only 
wait for the primary shard).
</description><key id="165863978">19454</key><summary>Removes write consistency level across replication action APIs in favor of wait_for_active_shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Index APIs</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T19:48:46Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-08-02T13:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-15T19:49:38Z" id="233053346">@bleskes @ywelsch FYI, I marked it as a WIP in case we want to change the name from wait_for_active_shards to something else.  Otherwise its ready for review.
</comment><comment author="bleskes" created="2016-07-18T14:46:42Z" id="233350448">Thx @abeyad . Left some comments. I also miss the (migration) doc changes. Are those coming in another PR (and please, let's not forget :))
</comment><comment author="abeyad" created="2016-07-19T21:19:28Z" id="233769536">@bleskes 
https://github.com/elastic/elasticsearch/pull/19454/commits/c23e9da971f07066b6e7c223148784c73041d7f2 addresses the code review in general 
https://github.com/elastic/elasticsearch/pull/19454/commits/536a2dbb81e00f81605b81cf155dd0016ee50ccc removes the checkWriteConsistency() method from transport replication actions in favor of `ActiveShardCount`
https://github.com/elastic/elasticsearch/pull/19454/commits/762301f69a00f5110efb2ce606a647a392e0fed1 makes the `index.write.wait_for_active_shards` setting updatable

I have a couple things to discuss with you regarding documentation, then I will push that up in this PR as well.
</comment><comment author="abeyad" created="2016-07-21T21:07:39Z" id="234384428">@bleskes @clintongormley https://github.com/elastic/elasticsearch/pull/19454/commits/48c1d9d24befe945e133fbce0a40b503ed21691f contains the documentation updates. I was not sure if I should add something to the resiliency page or remove the entry in the resiliency page that talks about us adding write consistency level: https://github.com/elastic/elasticsearch/blob/master/docs/resiliency/index.asciidoc#validate-quorum-before-accepting-a-write-request-status-done-v140
</comment><comment author="bleskes" created="2016-07-29T14:12:47Z" id="236191038">Thx @abeyad . Left some very minor comments.

&gt;  I was not sure if I should add something to the resiliency page or remove the entry in the resiliency page that talks about us adding write consistency level: https://github.com/elastic/elasticsearch/blob/master/docs/resiliency/index.asciidoc#validate-quorum-before-accepting-a-write-request-status-done-v140

I would just removed that paragraph. It never made it into official code - I think it was done on the improve_zen feature branch back in 1.4 but removed before the merge into master.
</comment><comment author="abeyad" created="2016-07-30T22:36:28Z" id="236393640">@bleskes I pushed https://github.com/elastic/elasticsearch/pull/19454/commits/8f7f4bdf5bd51badfd07f89a62d6ca36ab46b01c that addresses your code review comments
</comment><comment author="bleskes" created="2016-08-01T13:08:54Z" id="236575587">@abeyad thx. I left some minor comments.
</comment><comment author="abeyad" created="2016-08-01T17:26:32Z" id="236648199">@bleskes I pushed https://github.com/elastic/elasticsearch/pull/19454/commits/81640413e56769929d7f1a85c50983bd49542bd8
</comment><comment author="bleskes" created="2016-08-01T21:24:57Z" id="236712588">thx @abeyad . Almost there. Left some very minor comments.
</comment><comment author="abeyad" created="2016-08-01T23:16:07Z" id="236736911">@bleskes i pushed https://github.com/elastic/elasticsearch/pull/19454/commits/4923da93c83b0b0708747105f427aab03047168b
</comment><comment author="bleskes" created="2016-08-02T06:56:02Z" id="236817410">LGTM. Thx @abeyad 
</comment><comment author="abeyad" created="2016-08-02T13:00:42Z" id="236896606">Thanks for your review and feedback @bleskes !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/WriteConsistencyLevel.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardCount.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardCountTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardsObserverIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/WaitActiveShardCountIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ReplicationOperationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBaseReindexRestHandler.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequest.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequestBuilder.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/RoundTripTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/yaml/restspec/ClientYamlSuiteRestApiParserTests.java</file></files><comments><comment>Merge pull request #19454 from abeyad/remove-write-consistency-level</comment></comments></commit></commits></item><item><title>Dump the cluster log if integ tests fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19453</link><project id="" key="" /><description>This should help tracking down pesky, inconsistent rest failures.
</description><key id="165862766">19453</key><summary>Dump the cluster log if integ tests fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v2.4.0</label></labels><created>2016-07-15T19:41:18Z</created><updated>2016-07-15T19:44:39Z</updated><resolved>2016-07-15T19:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-15T19:42:51Z" id="233051907">LGTM
</comment><comment author="jasontedor" created="2016-07-15T19:43:19Z" id="233052000">LGTM.
</comment><comment author="nik9000" created="2016-07-15T19:44:38Z" id="233052304">Thanks for reviewing!

Next time it fails I might know why!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removes ensureYellow() after index creation in the integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19452</link><project id="" key="" /><description>Removes ensureYellow() calls after index creation in the integration
tests, as they are no longer needed with index creation now waiting 
for shards to be started before returning from the index creation call 
(by default, it waits for the primary of each shard to be started 
before returning, which is what ensureYellow() was ensuring anyway).

Relates #19450
</description><key id="165859134">19452</key><summary>Removes ensureYellow() after index creation in the integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>:Internal</label><label>review</label><label>test</label></labels><created>2016-07-15T19:19:40Z</created><updated>2016-07-17T18:35:36Z</updated><resolved>2016-07-15T19:37:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-15T19:27:45Z" id="233048714">Left some minor stuff. Otherwise LGTM.
</comment><comment author="abeyad" created="2016-07-15T19:32:36Z" id="233049705">@nik9000 Thanks for the review!  I'll make the changes and merge.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplainTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/TransportSearchIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsIT.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicAnalysisBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexRequestBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/indices/IndicesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/types/TypesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/SimpleRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/EquivalenceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/ExistsIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/GeoDistanceSortBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextCompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RandomScoreFunctionTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file></files><comments><comment>Removes ensureYellow() calls after index creation in the (#19452)</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplainTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/TransportSearchIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsIT.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicAnalysisBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexRequestBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/indices/IndicesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/types/TypesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/SimpleRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/EquivalenceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/ExistsIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/GeoDistanceSortBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextCompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RandomScoreFunctionTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file></files><comments><comment>Removes ensureYellow() calls after index creation in the (#19452)</comment></comments></commit></commits></item><item><title>Add log message about enforcing bootstrap checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19451</link><project id="" key="" /><description>This commit adds a log message when bootstrap checks are enforced
informing the user that they are enforced because they are bound to an
external network interface. We also log if bootstrap checks are being
enforced but system checks are being ignored.
</description><key id="165844454">19451</key><summary>Add log message about enforcing bootstrap checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Logging</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T18:01:11Z</created><updated>2016-07-15T18:56:05Z</updated><resolved>2016-07-15T18:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-15T18:01:31Z" id="233023726">This gives log messages:

```
[2016-07-15 13:59:03,027][INFO ][transport                ] [Sunfire] publish_address {192.168.1.8:9300}, bound_addresses {192.168.1.8:9300}, {[fe80::6a5b:35ff:fe9a:1f99]:9300}
[2016-07-15 13:59:03,030][INFO ][bootstrap                ] [Sunfire] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks
[2016-07-15 13:59:03,030][WARN ][bootstrap                ] [Sunfire] enforcing bootstrap checks but ignoring system bootstrap checks, consider not ignoring system checks
```
</comment><comment author="nik9000" created="2016-07-15T18:03:24Z" id="233024235">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/BootstrapCheck.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapCheckTests.java</file></files><comments><comment>Add log message about enforcing bootstrap checks</comment></comments></commit></commits></item><item><title>Makes index creation more friendly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19450</link><project id="" key="" /><description>This PR contains two major features, described in their respective PRs below:
1. Index Creation does _not_ cause the cluster health to temporarily go `RED` (rather it just goes to `YELLOW`).  This greatly helps administrators of Elasticsearch clusters who get alerted whenever the cluster health goes `RED`.
2. Because index creation causes the cluster health to go `YELLOW` instead of `RED`, this PR also enables waiting for a certain number of shard copies to be started before returning from the index creation operation.

This PR encompasses two other PRs:
1. https://github.com/elastic/elasticsearch/pull/18737
2. https://github.com/elastic/elasticsearch/pull/18985

Closes #9126
</description><key id="165813931">19450</key><summary>Makes index creation more friendly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T15:27:30Z</created><updated>2016-07-15T16:50:14Z</updated><resolved>2016-07-15T15:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-15T16:50:14Z" id="233005631">Very awesome!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Removes waiting for yellow cluster health upon index (#19460)</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplainTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/TransportSearchIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsIT.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicAnalysisBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexRequestBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/indices/IndicesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/exists/types/TypesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/SimpleRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregationsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/EquivalenceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/ExistsIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/GeoDistanceSortBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextCompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RandomScoreFunctionTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file></files><comments><comment>Removes ensureYellow() calls after index creation in the (#19452)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardCount.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardsObserver.java</file><file>core/src/main/java/org/elasticsearch/cluster/ack/CreateIndexClusterStateUpdateResponse.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterIndexHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterShardHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateEmptyPrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java</file><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestRolloverIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestShrinkIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplainIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardCountTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardsObserverIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableGenerator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/DecisionsImpactOnClusterHealthTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java</file><file>core/src/test/java/org/elasticsearch/indices/state/SimpleIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Merge pull request #19450 from elastic/feature/friendly-index-creation</comment></comments></commit></commits></item><item><title>Add combo analysis to core analysis plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19449</link><project id="" key="" /><description>The combo analysis plugin is not maintained anymore and it's quite usefull and necessery for multilingual indexes. This plugin should be moved to core and maintened by elastic.

https://github.com/yakaz/elasticsearch-analysis-combo
https://github.com/yakaz/elasticsearch-analysis-combo/pull/21

Thank you
</description><key id="165780627">19449</key><summary>Add combo analysis to core analysis plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moskiteau</reporter><labels /><created>2016-07-15T12:54:51Z</created><updated>2016-07-18T16:09:13Z</updated><resolved>2016-07-15T12:57:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T12:57:45Z" id="232944256">This was already proposed in https://github.com/elastic/elasticsearch/issues/1169 and we decided not to offer this plugin as it is a much better approach to use multiple fields with different analyzers instead of a single field with a mish-mash of tokens.
</comment><comment author="moskiteau" created="2016-07-18T15:13:52Z" id="233358876">Could you reference some literature? 
</comment><comment author="clintongormley" created="2016-07-18T16:09:13Z" id="233376169">I can't, but I can tell you that Lucene's analysis streams are designed to operate on a single field, with a single analyzer. Mixing up analyzers is likely to give you broken offsets and positions which could prevent indexing or result in run time exceptions with eg the FVH.

On top of that, you're messing with term statistics by creating duplicate terms for tokens which analyse identically, so the relevance calculation will be poor. 

Lastly, you don't actually save much space compared to having a field per analyzer.  And even if you do today, things may well change tomorrow as Lucene adds optimisations for its primary use case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make Priority an enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19448</link><project id="" key="" /><description>Today we have an abstraction Priority for representing
priorities. Ideally, these values are a fixed set of constants with a
well-defined ordering which sounds perfect for an enum. This commit
changes Priority so that it is an enum instead of a class.
</description><key id="165780592">19448</key><summary>Make Priority an enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T12:54:41Z</created><updated>2016-07-15T15:28:01Z</updated><resolved>2016-07-15T13:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-15T13:15:51Z" id="232947808">left one nit  LGTM otherwise thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Priority.java</file><file>core/src/test/java/org/elasticsearch/common/PriorityTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>Merge pull request #19448 from jasontedor/priority-enum</comment></comments></commit></commits></item><item><title>Priority values should be unmodifiable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19447</link><project id="" key="" /><description>In Priority there is a field named values that represents an ordered, by
priority, list of all priorities. Yet, this collection is modifiable and
this collection is exposed via the public API. This means that consumers
can modify this list potentially leading to complete chaos. This commit
modifies this field so that it is unmodifiable, documents that the
returned collection is unmodifiable, and returns total order to the
world. We also punish the bad consumer here by making them make a copy
of the returned collection with which they can do as they please. This
fixes a puzzling test failure which only arises if the two tests
(PrioritizedExecutorsTests#testPriorityQueue and
PriorityTests#testCompareTo run in the same JVM, and run in the right
order).
</description><key id="165776357">19447</key><summary>Priority values should be unmodifiable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T12:28:55Z</created><updated>2016-07-18T08:20:06Z</updated><resolved>2016-07-15T12:36:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-15T12:30:51Z" id="232939246">oh man I wanted to do this by moving from an array to a list and missed it half way... LGTM
</comment><comment author="jpountz" created="2016-07-18T08:20:06Z" id="233265672">Good catch!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Priority.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>Priority values should be unmodifiable</comment></comments></commit></commits></item><item><title>Improve index-allocation retry limitation for primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19446</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/18467 was intended to limit how many times shard allocation was attempted before giving up, instead of constantly retrying. eg if an analyzer requires a local synonyms file, which is missing, then we should stop trying until the situation is resolved.

Unfortunately it doesn't work for unassigned primary shards because of the way allocation works today. For instance, a primary shard might be sitting on a disk which is over the high watermark, or it may have allocation filtering which prevents it from being allocated to the node where it already exists.  Today, we just go ahead and try to assign the primary regardless of the allocation deciders (which means that we also don't limit retries to 5).

Instead, we could add extra logic to the appropriate deciders to say "always return `YES` if the shard in question is a primary which already exists on this node".  The decision returned would be `YES`, but the explanation provided by the reroute or cluster allocation explain could include the reason this decider was ignored.

/cc @ywelsch @dakrone @s1monw 

Related to https://github.com/elastic/elasticsearch/issues/18321
</description><key id="165760131">19446</key><summary>Improve index-allocation retry limitation for primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Allocation</label><label>enhancement</label></labels><created>2016-07-15T10:49:15Z</created><updated>2016-08-16T15:25:45Z</updated><resolved>2016-08-16T15:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-15T11:17:09Z" id="232926114">One thing to add here: The primary shard allocator forces allocation of a primary shard if allocation deciders for ALL nodes with shard copies say NO (*). If there’s a node with a shard copy where deciders says YES, we allocate to that node. If there’s a node with a shard copy where deciders say THROTTLE, and there is no node with YES, we don't allocate the shard in that round.

What this means, however, is that we cannot simply add this override logic "always return YES if the shard in question is a primary which already exists on this node" at the decider level, because the overriding logic only comes into play once we've looked at deciders of ALL nodes with shard copies. 

If we want to reflect this correctly in the cluster allocation explain API, it would also need to check the condition marked by \* above.
</comment><comment author="bleskes" created="2016-07-22T10:00:25Z" id="234505068">We discussed possible solutions on fix it friday. Here's what we came up with to deal with the current primary allocation logic. Instead of how it works today, which @ywelsch explained, we should do the following:

Add a `canForceAssignPrimary` method to the `AllocationDecider` class. That one will say `YES` by default and the `MaxRetryAllocationDecider` will forward it to it's `canAllocate` method. 

Using this new method the PrimaryShardAllocator logic becomes:

1) Iterate on all valid found allocation and ask the deciders for a decision.
2) If there is a node which resulted in `YES` assign the primary there.
3) If there is a node which resulted in `THROTTLE`, wait.
4) If all nodes got a `NO` result, instead of what we do now, we will iterate all nodes again, this time calling the new `canForceAssignPrimary` method. If some node has a `YES` now, we assign there. If some node has `THROTTLE` we wait. On `NO` on all nodes, we stay red.

Note that we will also need to adapt the allocation explain API to reflect this
</comment><comment author="bleskes" created="2016-07-22T10:01:07Z" id="234505214">@abeyad maybe this is something for you?
</comment><comment author="abeyad" created="2016-07-22T13:24:46Z" id="234542771">@bleskes Sure, just assigned it to myself.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/MaxRetryAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/MaxRetryAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file></files><comments><comment>Primary shard allocator observes limits in forcing allocation (#19811)</comment></comments></commit></commits></item><item><title>cloud-aws different keys for EC2 &amp; S3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19445</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.2

**JVM version**: openjdk version "1.8.0_91"
                          OpenJDK Runtime Environment (build 1.8.0_91-b14)
                          OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**: Red Hat Enterprise Linux Server release 7.2 (Maipo)

**Description of the problem including expected versus actual behavior**:
Can't provide two access &amp; secret keys (different for ec2 and s3).
I'd like to use cloud-aws plugin for EC2 Discovery &amp; S3 Repository (compatible service).
I've got separate cloud accounts for Eucalyptus &amp; RiakS3.

**Steps to reproduce**:
1) Add to elasticsearch.yml:

```
cloud:
    aws:
        s3:
            access_key: xxxxxxxxxxxx
            secret_key: XXxxxxxXxxxxXxxxXxXxXxxx
            endpoint: s3_cloud_front.address.here
            protocol: http
            signer: S3SignerType
        ec2:
            access_key: yyyyyyyyyyyyy
            secret_key: yyyYyyyyyYyyyyYyyyYyYyY
            endpoint: ec2_cloud_front.address.here
            protocol: http

discovery:
    type: ec2
    ec2:
        groups: group_name
```

2) # /etc/init.d/elasticsearch restart

**Provide logs (if relevant)**:
`[2016-07-14 15:28:17,690][INFO ][discovery.ec2 ] [Hideko Takata] Exception while retrieving instance list from AWS API: Unable to load AWS credentials from any provider in the chain`
</description><key id="165748163">19445</key><summary>cloud-aws different keys for EC2 &amp; S3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">nowakan</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>bug</label></labels><created>2016-07-15T09:39:17Z</created><updated>2016-07-21T12:03:51Z</updated><resolved>2016-07-21T12:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-15T13:49:13Z" id="232955471">It's a bug to me. I need to reproduce it.
</comment><comment author="dadoonet" created="2016-07-20T08:11:55Z" id="233877885">I read the code today and that's indeed a bug in 2.x series. 

We never read values for [EC2](https://github.com/elastic/elasticsearch/blob/2.4/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java#L27-L27) but we do for [S3](https://github.com/elastic/elasticsearch/blob/2.4/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L65-L68).

I'm pretty sure it's ok for 5.0 (I still need to check it though).
I'm starting working on a fix for 2.x series.

As a workaround, I think you could use something like:

``` yml
cloud:
    aws:
        access_key: yyyyyyyyyyyyy
        secret_key: yyyYyyyyyYyyyyYyyyYyYyY
        s3:
            access_key: xxxxxxxxxxxx
            secret_key: XXxxxxxXxxxxXxxxXxXxXxxx
            endpoint: s3_cloud_front.address.here
            protocol: http
            signer: S3SignerType
        ec2:
            endpoint: ec2_cloud_front.address.here
            protocol: http

discovery:
    type: ec2
    ec2:
        groups: group_name
```

ec2 will use the "global" AWS access key but S3 will use the S3 specific one.
</comment><comment author="dadoonet" created="2016-07-21T12:03:51Z" id="234234025">Closed by #19513 in 2.4 branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Need to validate index.merge.scheduler.max_thread_count input?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19444</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.x - 5.x

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

It looks like if one is to set "bad" values for `index.merge.scheduler.max_thread_count` it can cause exceptions to be thrown continually in the logs 

**Steps to reproduce**:

Set the value to something crazy:

Request:

```
PUT /test/_settings
{
    "index.merge.scheduler.max_thread_count": "500"

}
```

Response:

```
{
  "acknowledged": true
}
```

But  in the logs:

```
es_1 | [2016-07-15 08:56:13,804][INFO ][index.shard              ] [escluster1_es_1] [test][0] updating [index.merge.scheduler.max_thread_count] from [7] to [500]
es_1 | [2016-07-15 08:56:13,805][WARN ][index.settings           ] [escluster1_es_1] [test] failed to refresh settings for [org.elasticsearch.index.shard.IndexShard$ApplyRefreshSettings@5903b92]
es_1 | java.lang.IllegalArgumentException: maxThreadCount should be &lt;= maxMergeCount (= 7)
es_1 |  at org.apache.lucene.index.ConcurrentMergeScheduler.setMaxMergesAndThreads(ConcurrentMergeScheduler.java:154)
es_1 |  at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.refreshConfig(ElasticsearchConcurrentMergeScheduler.java:176)
es_1 |  at org.elasticsearch.index.engine.InternalEngine.onSettingsChanged(InternalEngine.java:1272)
es_1 |  at org.elasticsearch.index.shard.IndexShard$ApplyRefreshSettings.onRefreshSettings(IndexShard.java:1325)
es_1 |  at org.elasticsearch.index.settings.IndexSettingsService.refreshSettings(IndexSettingsService.java:54)
es_1 |  at org.elasticsearch.indices.cluster.IndicesClusterStateService.applySettings(IndicesClusterStateService.java:322)
es_1 |  at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:169)
es_1 |  at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
es_1 |  at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
es_1 |  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
es_1 |  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
es_1 |  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
es_1 |  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
es_1 |  at java.lang.Thread.run(Thread.java:745)
```

Indexing appears to work:

Request:

```
PUT test/t/1
{
  "foo": "bar"
}
```

Response:

```
{
  "_index": "test",
  "_type": "t",
  "_id": "1",
  "_version": 1,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "created": false
}
```

But in the logs:

```
es_1 | [2016-07-15 08:56:23,897][INFO ][cluster.metadata         ] [escluster1_es_1] [test] create_mapping [t]
es_1 | [2016-07-15 08:56:30,799][WARN ][index.shard              ] [escluster1_es_1] [test][0] Failed to perform scheduled engine refresh
es_1 | java.lang.IllegalArgumentException: maxThreadCount should be &lt;= maxMergeCount (= 7)
es_1 |  at org.apache.lucene.index.ConcurrentMergeScheduler.setMaxMergesAndThreads(ConcurrentMergeScheduler.java:154)
es_1 |  at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.refreshConfig(ElasticsearchConcurrentMergeScheduler.java:176)
es_1 |  at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:688)
es_1 |  at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:661)
es_1 |  at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1343)
es_1 |  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
es_1 |  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
es_1 |  at java.lang.Thread.run(Thread.java:745)
```

This looks to be harmless, but do need to test more when under indexing and search load.
</description><key id="165741876">19444</key><summary>Need to validate index.merge.scheduler.max_thread_count input?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label></labels><created>2016-07-15T09:03:28Z</created><updated>2016-09-08T10:25:04Z</updated><resolved>2016-09-08T10:25:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-09-08T10:25:04Z" id="245556679">Superseded by #20380 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow dots in field names for mappings API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19443</link><project id="" key="" /><description>The dots in fieldnames work for 5.0 was for dynamic fields and document parsing. But we still need to add support for specifying dots in fieldnames in mappings apis. This is less troublesome to users as dynamic mappings (where sometimes the source field name cannot be controlled), but still a pain to deal with.
</description><key id="165727723">19443</key><summary>Allow dots in field names for mappings API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>v5.0.0-beta1</label></labels><created>2016-07-15T07:30:33Z</created><updated>2016-09-14T14:43:24Z</updated><resolved>2016-08-10T23:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-18T07:56:09Z" id="233258369">Do you mean that a mapping like this:

```
{
  "properties": {
    "a.b": {
      "type": "keyword"
    }
  }
}
```

should be parsed as:

```
{
  "properties": {
    "a": {
      "type": "object",
      "properties": {
        "b": {
          "type": "keyword"
        }
      }
    }
  }
}
```

?
</comment><comment author="rjernst" created="2016-07-18T08:09:56Z" id="233262811">@jpountz Yes, exactly.
</comment><comment author="jpountz" created="2016-08-02T14:31:27Z" id="236922541">Discussed with @rjernst and @colings86, it seems that we could do it reasonably easily by pre-processing the map that stores the mapping it DocumentMapperParser.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file></files><comments><comment>Mappings: Support dots in field names in mapping parsing</comment></comments></commit></commits></item><item><title>I haven`t found   term payloads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19442</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
  - term payloads (`payloads` : true), as base64 encoded bytes
</description><key id="165719305">19442</key><summary>I haven`t found   term payloads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lvbabc</reporter><labels /><created>2016-07-15T06:19:22Z</created><updated>2016-07-15T08:52:18Z</updated><resolved>2016-07-15T08:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T08:52:18Z" id="232897644">Hi @lvbabc 

Thanks for the PR but you appear to have just deleted a trailing space?  No need for this commit I think

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log one plugin info per line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19441</link><project id="" key="" /><description>Today we log all loaded modules and installed plugins in a single
line. The number of modules has grown, and when plugins are installed a
single log line containing the loaded modules and plugins is
lengthy. With this commit, we log a single module or plugin per line,
log these in sorted order, and also log if no modules or no plugins were
loaded.
</description><key id="165700077">19441</key><summary>Log one plugin info per line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T02:42:30Z</created><updated>2016-07-15T02:46:35Z</updated><resolved>2016-07-15T02:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-15T02:43:38Z" id="232846590">This gives log lines like:

``` bash
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [aggs-matrix-stats]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [ingest-common]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [lang-expression]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [lang-groovy]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [lang-mustache]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [lang-painless]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [percolator]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [reindex]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] loaded module [transport-netty3]
[2016-07-14 22:37:19,211][INFO ][plugins                  ] [Fashima] no plugins loaded
```

instead of

``` bash
[2016-07-14 22:43:24,038][INFO ][plugins                  ] [Spitfire] modules [percolator, lang-mustache, lang-painless, reindex, aggs-matrix-stats, lang-expression, transport-netty3, ingest-common, lang-groovy], plugins []
```
</comment><comment author="abeyad" created="2016-07-15T02:45:25Z" id="232846809">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Log one plugin info per line</comment></comments></commit></commits></item><item><title>Make rest headers registration pull based</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19440</link><project id="" key="" /><description>Currently custom headers that should be passed through rest requests are
registered by depending on the RestController in guice and calling a
registration method. This change moves that registration to a getter for
plugins, and makes the RestController take the set of headers on
construction.
</description><key id="165695108">19440</key><summary>Make rest headers registration pull based</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-15T01:49:09Z</created><updated>2016-07-18T09:13:24Z</updated><resolved>2016-07-15T03:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-15T02:10:23Z" id="232842845">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/plugins/ActionPlugin.java</file><file>core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestControllerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestFilterChainTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestIndicesActionTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestRecoveryActionTests.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file></files><comments><comment>Merge pull request #19440 from rjernst/rest_headers</comment></comments></commit></commits></item><item><title>Rename transport-netty to transport-netty3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19439</link><project id="" key="" /><description>This commit renames the Netty 3 transport module from transport-netty to
transport-netty3. This is to make room for a Netty 4 transport module,
transport-netty4.

Closes #19410
</description><key id="165672959">19439</key><summary>Rename transport-netty to transport-netty3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T22:27:54Z</created><updated>2016-07-15T08:48:43Z</updated><resolved>2016-07-15T02:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-14T22:48:39Z" id="232814791">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/RetryTests.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/ESNetty3HttpResponseEncoder.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpChannel.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpRequest.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpRequestHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/Netty3HttpServerTransport.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/cors/Netty3CorsConfig.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/cors/Netty3CorsConfigBuilder.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/cors/Netty3CorsHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/pipelining/HttpPipeliningHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/pipelining/OrderedDownstreamChannelEvent.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/http/netty3/pipelining/OrderedUpstreamMessageEvent.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/Netty3Plugin.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/ChannelBufferBytesReference.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/ChannelBufferStreamInput.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3InternalESLogger.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3MessageChannelHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3OpenChannelsHandler.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3SizeHeaderFrameDecoder.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3Transport.java</file><file>modules/transport-netty3/src/main/java/org/elasticsearch/transport/netty3/Netty3Utils.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/ESNetty3IntegTestCase.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpChannelTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpClient.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpPublishPortTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpRequestSizeLimitIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpServerPipeliningTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3HttpServerTransportTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3PipeliningDisabledIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3PipeliningEnabledIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/pipelining/HttpPipeliningHandlerTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/Netty3SizeHeaderFrameDecoderTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/ChannelBufferBytesReferenceTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3ScheduledPingTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortIntegrationIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportPublishAddressIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3UtilsTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/SimpleNetty3TransportTests.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpSmokeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file></files><comments><comment>Rename transport-netty to transport-netty3</comment></comments></commit></commits></item><item><title>Should the Elasticsearch packages configure respawn?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19438</link><project id="" key="" /><description>After #19272, Elasticsearch instances will now die after hitting a fatal error (e.g., out of memory error). The breaking changes documentation associated with this change includes the note:

&gt; Operators should consider configuring their Elasticsearch services so that they respawn automatically in the case of such a fatal crash.

The question put forth here is: should the Elasticsearch packages attempt to configure this? The challenge here is that this is not straightforward on System V init-based systems (`/etc/inittab`) but it is straightforward on systemd-based systems.

Marking this as [discuss](https://github.com/elastic/elasticsearch/labels/discuss).
</description><key id="165658797">19438</key><summary>Should the Elasticsearch packages configure respawn?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>adoptme</label></labels><created>2016-07-14T21:05:53Z</created><updated>2017-06-30T15:28:22Z</updated><resolved>2017-06-30T15:28:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-15T09:23:43Z" id="232904081">We discussed this during Fix-it-Friday. We will configure respawn for systemd-based systems. We will investigate providing respawn for the System V init-based systems, but since most Linux systems are migrating to systemd, we will accept inconsistency if providing it for System V init-based systems is onerous. We will also need to investigate the possibilities on Windows, can @elastic/microsoft help here?
</comment><comment author="Mpdreamz" created="2016-07-15T09:34:06Z" id="232906214">Windows allows service to recover from failures quite elaborately:

![image](https://cloud.githubusercontent.com/assets/245275/16869979/ab788e28-4a7f-11e6-830b-e314c35e9d24.png)

Options also available on the command line, I'll cross reference this issue on the windows installer repos so we can ship with this OOB and try to allign with the *nix packages.
</comment><comment author="jasontedor" created="2016-07-15T10:02:05Z" id="232911635">@Mpdreamz Right, the question is whether or not it's possible to build the installer to configure this automatically on installation.
</comment><comment author="Mpdreamz" created="2016-07-15T10:04:04Z" id="232912033">Thats what I meant with shipping with this feature out of the box, yes it should be possible to configure this automatically during installation.
</comment><comment author="jasontedor" created="2016-07-15T10:05:27Z" id="232912290">&gt; Thats what I meant with shipping with this feature out of the box, yes it should be possible to configure this automatically during installation.

+1
</comment><comment author="russcam" created="2016-07-15T10:16:51Z" id="232914880">+1 - can be configured with sc.exe
On 15 Jul 2016 8:06 p.m., "Jason Tedor" notifications@github.com wrote:

&gt; Thats what I meant with shipping with this feature out of the box, yes it
&gt; should be possible to configure this automatically during installation.
&gt; 
&gt; +1
&gt; 
&gt; —
&gt; You are receiving this because you are on a team that was mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19438#issuecomment-232912290,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AAMtZ_9L5O_Zh3Mm3BTJ7nwCg7vOCeUSks5qV1uigaJpZM4JM2aV
&gt; .
</comment><comment author="jasontedor" created="2017-06-30T15:28:22Z" id="312298275">Duplicates #25425</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide access to ThreadContext in ingest plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19437</link><project id="" key="" /><description>Also changed IngestPlugin#getProcessors(...) method to accept a holder class that contains services ingest processor rely on instead of accepting these services directly.
</description><key id="165600425">19437</key><summary>Provide access to ThreadContext in ingest plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T16:13:26Z</created><updated>2016-07-15T06:44:31Z</updated><resolved>2016-07-15T06:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-14T20:03:10Z" id="232775924">Thanks for looking @rjernst, I've updated this PR.
</comment><comment author="rjernst" created="2016-07-14T20:05:13Z" id="232776455">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] add 'yaml' feature for the test runner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19436</link><project id="" key="" /><description>Not all clients that rely on the yaml test suite support yaml serialization so I added another `skip` feature called `yaml` to allow them to skip these tests.

When trying to locate these tests I also came across a misnamed test.

cc @javanna 
</description><key id="165575715">19436</key><summary>[TEST] add 'yaml' feature for the test runner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T14:30:02Z</created><updated>2016-07-14T15:30:44Z</updated><resolved>2016-07-14T15:30:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-14T14:54:36Z" id="232689501">sorry @HonzaKral I kinda assumed that the clients that don't support yaml don't support headers too. Wrong assumption I guess. Can you also add back yaml among the supported features in Features class? I had removed that given that the skip yaml was gone...
</comment><comment author="HonzaKral" created="2016-07-14T14:58:35Z" id="232690746">Had to implement `headers` for `x-pack` work. Commit updated to add `"yaml"` to `Features`
</comment><comment author="javanna" created="2016-07-14T15:17:47Z" id="232696419">thanks @HonzaKral one more thing. Are we sure that the right syntax is to have two skip sections? I thought that we would rather make features an array and have a list of the features there rather than a single value. That is what the java SkipSectionParser expects, with two skips I'm afraid the last one wins :(
</comment><comment author="HonzaKral" created="2016-07-14T15:24:24Z" id="232698407">You are right. Updated the commit again, thanks!
</comment><comment author="javanna" created="2016-07-14T15:30:04Z" id="232700196">LGTM
</comment><comment author="HonzaKral" created="2016-07-14T15:30:44Z" id="232700384">Thanks @javanna!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/rest/support/Features.java</file></files><comments><comment>[TEST] add 'yaml' feature for the test runner (#19436)</comment></comments></commit></commits></item><item><title>Add a dedicated client/transport project for transport-client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19435</link><project id="" key="" /><description>The `client/transport` project adds a new jar build project that
pulls in all dependencies and configures all required modules.

Preinstalled modules are:
- `transport-netty`
- `lang-mustache`
- `reindex`
- `percolator`

The `TransportClient` classes are still in core
while `TransportClient.Builder` has only a protected constructor
such that users are redirected to use the new `TransportClientBuilder`
from the new jar.

Closes #19412
</description><key id="165567937">19435</key><summary>Add a dedicated client/transport project for transport-client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>breaking-java</label><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T13:55:10Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-18T13:42:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-14T15:47:06Z" id="232705538">@rjernst can you take a look?
</comment><comment author="rjernst" created="2016-07-15T03:16:23Z" id="232850324">This looks fine. One thought about the naming: I am concerned about having TransportClientBuilder and TransportClient.Builder, because they will both come up with autocomplete in IDEs. Perhaps we can just remove the latter? Maybe the ctor should be `TransportClient(Settings settings, Plugin... plugins)`
</comment><comment author="s1monw" created="2016-07-15T11:52:44Z" id="232933040">@rjernst I basically spend the entire day with this project substitutions. They unfortunately don't work for this. I am not sure what kind of magic you are trying to apply but if I substitute the dependency I only get the zip file on the classpath which obviously doesn't work.  Also I wonder since you apparently added  a `client` classifier how this is going to work here. 

anyway I removed the builder and made the TransportClient a abstract class...
</comment><comment author="s1monw" created="2016-07-18T08:56:53Z" id="233277479">@rjernst I think this is ready, I will move towards maven coordinates in a followup if that's ok?
</comment><comment author="rjernst" created="2016-07-18T09:15:13Z" id="233280939">LGTM. Yes, we can do the maven coordinates/substitutions as a followup, they are dependent on #19461
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/transport/src/main/java/org/elasticsearch/transport/client/PreBuiltTransportClient.java</file><file>client/transport/src/test/java/org/elasticsearch/transport/client/PreBuiltTransportClientTests.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/ClusterStateBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortIntegrationIT.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTransportClient.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Add a dedicated client/transport project for transport-client (#19435)</comment></comments></commit></commits></item><item><title>Add missing space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19434</link><project id="" key="" /><description /><key id="165565052">19434</key><summary>Add missing space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickel715</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-07-14T13:41:52Z</created><updated>2016-07-15T09:47:21Z</updated><resolved>2016-07-15T09:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-14T14:45:56Z" id="232686895">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose Ukrainian analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19433</link><project id="" key="" /><description>When upgrading to Lucene 6.2, we should expose the newly added Ukrainian analyzer

https://issues.apache.org/jira/browse/LUCENE-7287
</description><key id="165532845">19433</key><summary>Expose Ukrainian analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v5.1.1</label></labels><created>2016-07-14T10:52:04Z</created><updated>2016-10-31T17:20:39Z</updated><resolved>2016-10-31T17:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for `wait_for_events` to the `_cluster/health` REST endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19432</link><project id="" key="" /><description>The Java API supports this while mostly used for tests it can also be useful in
production environments. For instance if something is automated like a settings change
and we execute some health right after it the settings update might have some consequences
like a reroute which hasn't been fully applied since the preconditions are not fulfilled yet.
For instance if not all shards started the settings update is applied but the reroute won't move
currently initializing shards like in the shrink API test. Sure this could be done by waiting for
green before but if the cluster moves shards due to some side-effects waiting for all events is
still useful. I also took the chance to add unittests to Priority.java

Closes #19419
</description><key id="165528704">19432</key><summary>Add support for `wait_for_events` to the `_cluster/health` REST endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T10:27:13Z</created><updated>2016-07-14T10:33:30Z</updated><resolved>2016-07-14T10:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-14T10:29:30Z" id="232627292">LGTM except for one missing option
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Priority.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/PriorityTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>Add support for `wait_for_events` to the `_cluster/health` REST endpoint (#19432)</comment></comments></commit></commits></item><item><title>Add flag for plugins/modules which can be used in the transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19431</link><project id="" key="" /><description>This changes adds a flag which can be set in the esplugin closure in
build.gradle for plugins and modules which contain pieces that must be
published to maven, for use in the transport client. The jar/pom and
source/javadoc jars are moved to a new name that has the suffix
"-client".

I enabled this for the two modules that I know definitely need this;
there may be more. One open question is which groupId to use for the
generated pom.

Currently the modules just sit directly under org.elasticsearch. Perhaps these should be under `org.elasticsearch.client.plugins`?
</description><key id="165513649">19431</key><summary>Add flag for plugins/modules which can be used in the transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T09:08:47Z</created><updated>2016-07-15T08:36:59Z</updated><resolved>2016-07-14T15:56:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-14T09:23:50Z" id="232613344">I think we also want to set this flag in the percolator module? (otherwise the jar will need to be added manually if someone wants to use the PercolateQueryBuilder or the deprecate percolate APIs)

The groupId looks good to me, as the name indicates that it is for the Java client.
</comment><comment author="rjernst" created="2016-07-14T09:41:02Z" id="232617426">&gt; The groupId looks good to me, as the name indicates that it is for the Java client.

@martijnvg Do you mean `org.elasticsearch`? The problem is, as it is now, plugins that have this flag set (will we have any?) would be under `org.elasticsearch.plugin`, while modules would be under `org.elasticsearch`. I think they should be consistent?
</comment><comment author="martijnvg" created="2016-07-14T09:54:53Z" id="232620439">I was referring to use the `org.elasticsearch.client.plugins` as groupId for modules that need to publish their artifacts for the transport client. Not sure if that makes sense...

 Using `org.elasticsearch.plugin` for consistency with plugins make sense to me too. Will that then only apply for modules where `hasClientJar` flag has been set to true?
</comment><comment author="rjernst" created="2016-07-14T09:59:50Z" id="232621439">Ok, I went with the latter because it is much easier in the gradle setup to just match.
</comment><comment author="martijnvg" created="2016-07-14T10:07:50Z" id="232623099">&gt; Ok, I went with the latter because it is much easier in the gradle setup to just match.

+1 

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19431 from rjernst/client_plugin</comment></comments></commit></commits></item><item><title>Broken link at Documentation &gt; Ingest node &gt; Grok processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19430</link><project id="" key="" /><description>Hi

Website documentation for ingest [node-grok processor](https://www.elastic.co/guide/en/elasticsearch/reference/master/grok-processor.html) and [github page](https://github.com/elastic/elasticsearch/blob/master/docs/reference/ingest/ingest-node.asciidoc#grok-processor) aswell have a broken link at "[**120 reusable patterns**](https://github.com/elastic/elasticsearch/tree/master/modules/ingest-grok/src/main/resources/patterns)"
</description><key id="165509405">19430</key><summary>Broken link at Documentation &gt; Ingest node &gt; Grok processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mostolog</reporter><labels><label>docs</label></labels><created>2016-07-14T08:47:16Z</created><updated>2016-07-14T10:13:45Z</updated><resolved>2016-07-14T09:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-14T09:13:42Z" id="232610909">Thanks @mostolog, it should be fixed when the docs have been rebuild.
</comment><comment author="mostolog" created="2016-07-14T10:13:45Z" id="232624320">Thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>docs: fix broken link</comment></comments></commit></commits></item><item><title>Why was setPercolateFiler method removed form percolate request (Java Api)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19429</link><project id="" key="" /><description>I'm migrating form ElasticSearch 1.3 to 2.3.2 I see that Java API was changed for Percolate stuff. I used filter for percolated document like this:

request.setPercolateFilter(filter);
New API doesn't have such method. Filter is not supported for percolator. I see only such stuff

request.setPercolateQuery(filterQuery);
Documentation says that we can use filter, but api doesn't have it.

What was the reason to remove filter from percolator?
Does query have the same performance as filter? 
</description><key id="165509055">19429</key><summary>Why was setPercolateFiler method removed form percolate request (Java Api)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Aleksandr-Filichkin</reporter><labels /><created>2016-07-14T08:45:25Z</created><updated>2016-07-14T08:55:46Z</updated><resolved>2016-07-14T08:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-14T08:55:46Z" id="232606511">Please ask questions on discuss.elastic.co.

Filters and queries have been merged in 2.0. Read 2.0 breaking changes doc
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `node.mode` and `node.local` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19428</link><project id="" key="" /><description>Today `node.mode` and `node.local` serve almost the same purpose, they
are a shortcut for `discovery.type` and `transport.type`. If `node.local: true`
or `node.mode: local` is set elasticsearch will start in _local_ mode which means
only nodes within the same JVM are discovered and a non-network based transport
is used. The _local_ mode it only really used in tests or if nodes are embedded.
For both, embedding and tests explicit configuration via `discovery.type` and `transport.type`
should be preferred.

This change removes all the usage of these settings and by-default doesn't
configure a default transport implementation since netty is now a module. Yet, to make
the user experience flawless, plugins or modules can set a `http.type.default` and
`transport.type.default`. Plugins set this via `PluginService#additionalSettings()`
which enforces _set-once_ which prevents node startup if set multiple times. This means
that our distributions will just startup with netty transport since it's packaged as a
module unless `transport.type` or `http.transport.type` is explicitly set.

This change also found a bunch of bugs since several `NamedWriteables` were not registered if a
transport client is used. Now that we don't rely on the `node.mode` leniency which is inherited
instead of using explicit settings, `TransportClient` uses `AssertingLocalTransport` which detects
these problems since it serializes all messages.
</description><key id="165495734">19428</key><summary>Remove `node.mode` and `node.local` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-14T07:29:54Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-14T11:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-14T07:32:38Z" id="232585995">@nik9000 FYI I had to fix some NamedWriteable regsitration here since they are relevant in the transport client case... now tests are failing too, we should stiff consider some unitttests for this stuff
</comment><comment author="rjernst" created="2016-07-14T09:55:17Z" id="232620513">LGTM. Maybe as a followup we should move local transport/discovery to the test framework?
</comment><comment author="s1monw" created="2016-07-14T09:56:23Z" id="232620735">&gt; LGTM. Maybe as a followup we should move local transport/discovery to the test framework?

I agree I will open a followup
</comment><comment author="javanna" created="2016-07-14T10:05:23Z" id="232622570">Relates to #16234, actually it should close it as far as I understand.
</comment><comment author="s1monw" created="2016-07-14T10:15:49Z" id="232624731">@javanna ack I will close it in the final commit
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/ListTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java</file><file>core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScorePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeIT.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/NettyPlugin.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/AbstractAzureComputeServiceTestCase.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureSimpleTests.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Remove `node.mode` and `node.local` settings (#19428)</comment></comments></commit></commits></item><item><title>Return an error if incompatible plugin installed and service start called</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19427</link><project id="" key="" /><description>Currently if you update ES and have plugins that are not compatible, starting the service returns an ok;

```
root@vagrant-ubuntu-trusty-64:~# service elasticsearch start
 * Starting Elasticsearch Server                                                                                                                                                                                                                                     [ OK ]
```

But the service is not running;

```
root@vagrant-ubuntu-trusty-64:~# ps -ef|grep java
root      2497  2288  0 04:01 pts/1    00:00:00 grep --color=auto
```

And the log reports this;

```
[2016-07-14 04:01:29,277][INFO ][node                     ] [Aurora] version[2.3.4], pid[2412], build[e455fd0/2016-06-30T11:24:31Z]
[2016-07-14 04:01:29,279][INFO ][node                     ] [Aurora] initializing ...
[2016-07-14 04:01:29,752][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: Plugin [graph] is incompatible with Elasticsearch [2.3.4]. Was designed for version [2.3.3]
        at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:118)
        at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:378)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:128)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:158)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

We should be returning a `FAILED`, ideally with a reason why.
</description><key id="165471763">19427</key><summary>Return an error if incompatible plugin installed and service start called</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2016-07-14T04:07:06Z</created><updated>2016-07-14T09:41:49Z</updated><resolved>2016-07-14T09:41:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-14T05:25:35Z" id="232560053">+1. Basically we should wait for "Started" in logs or something? (Unless the user change the log level) or check that we have no ERROR line in logs and print the error otherwise?
</comment><comment author="jasontedor" created="2016-07-14T09:41:49Z" id="232617588">Duplicates #19297 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Each Concurrent shards batch takes more than an hour to get allocated after the node left</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19426</link><project id="" key="" /><description>**Elasticsearch version**: "version" : {
    "number" : "1.4.1",
    "build_hash" : "89d3241d670db65f994242c8e8383b169779e2d4",
    "build_timestamp" : "2014-11-26T15:49:29Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"
  },

**JVM version**:java version "1.8.0_40"
Java(TM) SE Runtime Environment (build 1.8.0_40-b25)
Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)

**OS version**: Linux Ubuntu

**Description of the problem including expected versus actual behavior**:
When the node leaves the cluster, a lot of shards are stuck at initializing state.

**Steps to reproduce**:
1. Disable allocation
2. Stop elasticsearch
3. enable allocation

**Provide logs (if relevant)**:
Here is the response of the _cat/pending_tasks  | head
 01439777 56.5m URGENT shard-started ([error-newsflickss][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNodeNew13][4DtklsTUSd-eRhPG_c3uyw][ip-172-31-37-168][inet[/172.31.37.168:9300]]{master=false}]]  
1439778 56.5m URGENT shard-started ([firstcrytest-notificationclickedmoe][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[ES_r3_xlarge_1TB_Node_new15][sbwLxGR6RpmFHmHv3Vc6ag][ip-172-31-47-74][inet[/172.31.47.74:9300]]{master=false}]]  
1439779 56.5m URGENT shard-started ([cleartrip-eamgc][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[ES_r3_xlarge_1TB_Node_2][xirsEXbZSpqldjylVrwzjw][ip-172-31-41-73][inet[/172.31.41.73:9300]]{master=false}]]  
1439783 56.5m URGENT shard-started ([emt-uat-fundtransfer][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNode11][jpNMJUlwQ7aQKk-hlcWLVQ][ip-172-31-40-207][inet[/172.31.40.207:9300]]{master=false}]]  
1439822 56.4m URGENT shard-started ([firstcrytest-notificationclickedmoe][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  
1439780 56.5m URGENT shard-started ([chillr-requestshowqr][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNode11][jpNMJUlwQ7aQKk-hlcWLVQ][ip-172-31-40-207][inet[/172.31.40.207:9300]]{master=false}]]  
1439799 56.4m URGENT shard-started ([sdsellerzone-catalogpdpback][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  
1439833 56.4m URGENT reroute_after_cluster_update_settings  
1439824 56.4m URGENT shard-started ([cleartripprod-fbtan][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  
1439288  5.9h HIGH   refresh-mapping [cleartripprod-apppl-2016-06-30][[datapoints]]

_cat/health
epoch      timestamp cluster           status node.total node.data shards   pri relo init unassign 
1468455843 00:24:03  DataPointsCluster yellow         22        19  26619 14017    0   20     1395 

_cluster/settings
{"persistent":{"cluster":{"routing":{"allocation":{"cluster_concurrent_rebalance":"10","node_concurrent_recoveries":"14","node_initial_primaries_recoveries":"4","enable":"all"}}},"threadpool":{"bulk":{"keep_alive":"2m","size":"16","queue_size":"2000","type":"fixed"}},"indices":{"recovery":{"concurrent_streams":"6","max_bytes_per_sec":"120mb"}}},"transient":{"cluster":{"routing":{"allocation":{"node_initial_primaries_recoveries":"10","balance":{"index":"0.80f"},"enable":"all","allow_rebalance":"indices_all_active","cluster_concurrent_rebalance":"0","node_concurrent_recoveries":"5","exclude":{"_ip":"172.31.39.58"}}}},"indices":{"recovery":{"concurrent_streams":"10"}}}}

Is this because of too many shards?
</description><key id="165451481">19426</key><summary>Each Concurrent shards batch takes more than an hour to get allocated after the node left</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shashank-moengage</reporter><labels /><created>2016-07-14T00:30:20Z</created><updated>2016-07-14T01:45:04Z</updated><resolved>2016-07-14T01:45:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-14T01:45:04Z" id="232534812">Please ask your questions on discuss.elastic.co. We can help you there.

One suggestion though: upgrade to 1.7 first.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove o.e.script.Template class and move template query to lang-mustache module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19425</link><project id="" key="" /><description>- The `Template` class always felt weird to me, it is a subclass of `Script` that is hard coded to use mustache and overrides parsing logic. There is no actual templating infrastructure, this class and its callers just work with the `lang-mustache` module. So instead I think it is cleaner to remove the `Template` class, unify the scripting parsing logic and callers of `Template` use `Script` with the language set to `mustache`. I think if we [introduce the notion of templates](https://github.com/elastic/elasticsearch/issues/16314) we can have a `Template` class (but that then doesn't extend from `Script`).
- The `template` query is hardcoded to work with the `lang-mustache` module, so its right place is in that module instead of core.
- Remove `Script.ParseException` in favour for `ElasticsearchParseException`.
</description><key id="165433251">19425</key><summary>Remove o.e.script.Template class and move template query to lang-mustache module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>breaking-java</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T22:09:59Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-18T08:16:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-14T01:05:22Z" id="232529771">I like it. I don't like that scripts have an xcontent now. If we could not do that somehow that'd be great. If that isn't possible we need to document it super well because it isn't obvious at first glance what it is for unless you are thinking of templates.
</comment><comment author="martijnvg" created="2016-07-14T07:27:16Z" id="232584440">thanks for checking this out @nik9000! I've updated the PR and added docs why the xcontent is needed.
</comment><comment author="nik9000" created="2016-07-14T13:07:33Z" id="232660057">LGTM
</comment><comment author="nik9000" created="2016-07-14T13:26:58Z" id="232664712">Actually - if you move template query then it'd be hard for the query evaluation stuff to use it, right? Maybe don't move it?
</comment><comment author="martijnvg" created="2016-07-14T13:43:14Z" id="232668996">&gt; Actually - if you move template query then it'd be hard for the query evaluation stuff to use it, right? Maybe don't move it?

I guess this depends how the query evaluation api gets implemented. If it where to use rest client then it could use the `template` query. Or maybe it would use the `ScriptService` directly to template the input variables of the query. I think the best place for the `template` query is in the lang-mustache plugin, with the reason that it is hard coded to work with mustache.
</comment><comment author="nik9000" created="2016-07-15T12:18:29Z" id="232937107">&gt; I think the best place for the template query is in the lang-mustache plugin, with the reason that it is hard coded to work with mustache.

Yeah that is fair. I'm fine with moving it and if it turns out we need to move it back for the query evaluation stuff then we think about the problem some more and move it back. But at least it forces us to think about it.

LGTM again.
</comment><comment author="martijnvg" created="2016-07-15T15:31:03Z" id="232985105">&gt; Yeah that is fair. I'm fine with moving it and if it turns out we need to move it back for the query evaluation stuff then we think about the problem some more and move it back. But at least it forces us to think about it.

Totally agreed, if we don't find a way then we can always move the `template` query back.
</comment><comment author="tlrx" created="2016-07-25T09:44:44Z" id="234908678">I like it too, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Complete load-settings error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19424</link><project id="" key="" /><description /><key id="165428810">19424</key><summary>Complete load-settings error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels><label>:Exceptions</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T21:44:34Z</created><updated>2016-07-14T10:46:58Z</updated><resolved>2016-07-13T21:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-13T21:59:06Z" id="232499495">merged thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file></files><comments><comment>Merge pull request #19424 from mfussenegger/error-msg</comment></comments></commit></commits></item><item><title>Updating HDFS repository plugin documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19423</link><project id="" key="" /><description>Updates to the documentation of the HDFS repository plugin. A lot has changed in this code since its relocation and some of the old configuration parameters no longer do anything. Added an example REST call to bring it more in line with the S3 Repository documentation as well.

This PR #is in response to [this issue](https://github.com/elastic/elasticsearch-hadoop/issues/781).
</description><key id="165416447">19423</key><summary>Updating HDFS repository plugin documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jbaiera/following{/other_user}', u'events_url': u'https://api.github.com/users/jbaiera/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jbaiera/orgs', u'url': u'https://api.github.com/users/jbaiera', u'gists_url': u'https://api.github.com/users/jbaiera/gists{/gist_id}', u'html_url': u'https://github.com/jbaiera', u'subscriptions_url': u'https://api.github.com/users/jbaiera/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/875779?v=4', u'repos_url': u'https://api.github.com/users/jbaiera/repos', u'received_events_url': u'https://api.github.com/users/jbaiera/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jbaiera/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jbaiera', u'type': u'User', u'id': 875779, u'followers_url': u'https://api.github.com/users/jbaiera/followers'}</assignee><reporter username="">jbaiera</reporter><labels><label>:Plugin Repository HDFS</label><label>docs</label></labels><created>2016-07-13T20:41:01Z</created><updated>2016-07-14T20:12:59Z</updated><resolved>2016-07-14T20:12:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-13T20:47:28Z" id="232481668">LGTM! Left one nit comment.
</comment><comment author="clintongormley" created="2016-07-13T20:53:59Z" id="232483421">LGTM
</comment><comment author="clintongormley" created="2016-07-14T10:46:30Z" id="232630576">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Updating HDFS repository plugin documentation (#19423)</comment></comments></commit></commits></item><item><title>_count requires query in body where _search does not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19422</link><project id="" key="" /><description>Best explained through some sense commands:

``` json
POST my-index/my-type
{ "name" : "a" }

POST my-index/my-type
{ "name" : "b" }

#works
POST my-index/my-type/_search
POST my-index/my-type/_count

#works
POST my-index/my-type/_search
{}

#does not work
POST my-index/my-type/_count
{}
```

The latter will fail with:

``` json
{
  "error": {
    "root_cause": [
      {
        "type": "parsing_exception",
        "reason": "Required query is missing",
        "line": 1,
        "col": 2
      }
    ],
    "type": "parsing_exception",
    "reason": "Required query is missing",
    "line": 1,
    "col": 2
  },
  "status": 400
}
```

which feels like a discrepency between the two API's.

Tested against current `master`
</description><key id="165400425">19422</key><summary>_count requires query in body where _search does not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-07-13T19:20:42Z</created><updated>2016-07-26T16:17:52Z</updated><resolved>2016-07-26T16:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-07-26T08:14:34Z" id="235193834">I tested this on 2.3, the behaviour for `_count` and `_search` is the same there. I opened a PR to remove throwing the error when no `query` parameter is specified.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file></files><comments><comment>Allow empty json object in request body in `_count` API</comment></comments></commit></commits></item><item><title>Snapshot UUIDs in blob names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19421</link><project id="" key="" /><description>This PR adds to #18815 to enable safer behavior with respect to snapshots.  #18815 makes blob deletions strict - if the blob doesn't exist, an exception is thrown instead of silently failing.  However, this presents an issue with some of our tests scenarios (and possibly real life occurrences) where the inability to delete a blob because it was deleted by someone else would cause the snapshot deletion itself to fail, potentially leaving other blobs around.  This could even happen if the snap-_.dat blob was successfully deleted but the machine crashed before the other blobs in the snapshot could be deleted.  Our current mechanism uses a listing of the snap-_.dat blobs to determine what the current snapshots are.  If deletions can't be relied upon, then we can't be sure that the existence of `snap-A.dat` in the repository  implies that `A` is a current snapshot.

This PR uses the index generational files from #19002 to retrieve the snapshot UUID for snapshots and name all blobs by the snapshot UUID instead of the snapshot name.  If a snapshot `A` was deleted, then recreated, but not all of `A`'s files were deleted, then we would have to worry about overwriting existing blobs, which is problematic.  By naming blobs with the snapshot UUID, we avoid this issue.

This PR also introduces a unique index ID (a UUID) for indices in the snapshots, so index folders can be named by the UUID and avoid problems such as #7540.

Relates #18156 
</description><key id="165387100">19421</key><summary>Snapshot UUIDs in blob names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Internal</label><label>:Snapshot/Restore</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T18:18:12Z</created><updated>2016-08-04T09:10:18Z</updated><resolved>2016-07-31T04:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-14T00:43:18Z" id="232526885">@imotov your review would be most appreciated

@gfyoung the elastic:enhancement/snapshot-blob-handling contains your commits and this PR is against that branch
</comment><comment author="imotov" created="2016-07-19T20:31:59Z" id="233756511">@abeyad Looks good. I left a few minor comments. 
</comment><comment author="abeyad" created="2016-07-24T19:09:51Z" id="234795873">@imotov I've pushed https://github.com/elastic/elasticsearch/pull/19421/commits/a6f5e0b0fe8f2fced6410ea7c3b1f8f3b7204f8e to address code review comments and https://github.com/elastic/elasticsearch/pull/19421/commits/299b8a7a524d4e4dbb2c9aa850d61b8694f9427e to remove the blobExists check from readBlob methods
</comment><comment author="imotov" created="2016-07-27T21:36:47Z" id="235729058">@abeyad left a couple of comments
</comment><comment author="abeyad" created="2016-07-29T02:11:24Z" id="236077204">@imotov i pushed https://github.com/elastic/elasticsearch/pull/19421/commits/58d6b9dcd1c117f4fcbee0e5d89b1ae13601eb87
</comment><comment author="imotov" created="2016-07-30T19:04:55Z" id="236383711">Left a minor comment. Otherwise, LGTM.
</comment><comment author="abeyad" created="2016-07-31T04:04:43Z" id="236408272">@imotov I pushed https://github.com/elastic/elasticsearch/pull/19421/commits/0f335ac87319fdf35dc6c7334b5dc528085aa5da
</comment><comment author="abeyad" created="2016-07-31T04:18:46Z" id="236408856">@imotov thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java</file><file>core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java</file><file>core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/repositories/IndexId.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryData.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotId.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/RepositoryUpgradabilityIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/repositories/IndexIdTests.java</file><file>core/src/test/java/org/elasticsearch/repositories/RepositoryDataTests.java</file><file>core/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobStoreContainerTests.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobStore.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobContainer.java</file><file>plugins/repository-hdfs/src/test/java/org/elasticsearch/repositories/hdfs/HdfsBlobStoreContainerTests.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreContainerTestCase.java</file></files><comments><comment>Merge pull request #19421 from abeyad/snapshot-uuids-in-blob-names</comment></comments></commit></commits></item><item><title>Migrate moving_avg pipeline aggregation to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19420</link><project id="" key="" /><description>This is the first pipeline aggregation that doesn't have its own
bucket type that needs serializing. It uses InternalHistogram instead.
So that required reworking the new-style `registerAggregation` method
to not require bucket readers. So I built `PipelineAggregationSpec` to
mirror `AggregationSpec`. It allows registering any number of bucket
readers or result readers.
</description><key id="165364949">19420</key><summary>Migrate moving_avg pipeline aggregation to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T16:30:11Z</created><updated>2016-07-18T14:14:39Z</updated><resolved>2016-07-18T14:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-15T14:24:55Z" id="232965077">@nik9000 left a small nit pick comment on a comment in the code but otherwise LGTM if you decide to keep the PipelineAggregationSpec. If you decide not to keep it ping me again and I'll quickly review
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.shrink/10_basic/Shrink index via API} fails when multiple nodes are present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19419</link><project id="" key="" /><description>The REST test for the shrink API fails:

```
Suite: org.elasticsearch.backwards.MultiNodeBackwardsIT
  1&gt; [2016-07-13 08:28:05,799][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:33103], elasticsearch version: [5.0.0-alpha5]
  1&gt; [2016-07-13 08:28:05,807][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:33103], elasticsearch version: [5.0.0-alpha5]
  1&gt; [2016-07-13 08:28:06,346][INFO ][org.elasticsearch.backwards] Stash dump on failure [{
  1&gt;   "stash" : {
  1&gt;     "body" : {
  1&gt;       "cluster_name" : "qa_backwards-5.0_integTest",
  1&gt;       "status" : "green",
  1&gt;       "timed_out" : false,
  1&gt;       "number_of_nodes" : 2,
  1&gt;       "number_of_data_nodes" : 2,
  1&gt;       "active_primary_shards" : 5,
  1&gt;       "active_shards" : 5,
  1&gt;       "relocating_shards" : 0,
  1&gt;       "initializing_shards" : 0,
  1&gt;       "unassigned_shards" : 0,
  1&gt;       "delayed_unassigned_shards" : 0,
  1&gt;       "number_of_pending_tasks" : 0,
  1&gt;       "number_of_in_flight_fetch" : 0,
  1&gt;       "task_max_waiting_in_queue_millis" : 0,
  1&gt;       "active_shards_percent_as_number" : 100.0
  1&gt;     },
  1&gt;     "master" : "sBM60RLuTiuBlZutXBWnGg"
  1&gt;   }
  1&gt; }]
  2&gt; REPRODUCE WITH: gradle :qa:backwards-5.0:integTest -Dtests.seed=529E08C5D51BBDF9 -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.shrink/10_basic/Shrink index via API}" -Dtests.security.manager=true -Dtests.locale=es-MX -Dtests.timezone=Indian/Chagos
FAILURE 0.75s | MultiNodeBackwardsIT.test {p0=indices.shrink/10_basic/Shrink index via API} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [indices.shrink] returned [500 Internal Server Error] [{"error":{"root_cause":[{"type":"illegal_state_exception","reason":"index source must have all shards allocated on the same node to shrink index"}],"type":"illegal_state_exception","reason":"index source must have all shards allocated on the same node to shrink index"},"status":500}]
   &gt;    at __randomizedtesting.SeedInfo.seed([529E08C5D51BBDF9:DACA371F7BE7D001]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:108)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:399)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/hinmanm/es/elasticsearch/qa/backwards-5.0/build/testrun/integTest/J0/temp/org.elasticsearch.backwards.MultiNodeBackwardsIT_529E08C5D51BBDF9-004
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=806, maxMBSortInHeap=6.315786973231347, sim=RandomSimilarity(queryNorm=true,coord=crazy): {}, locale=es-MX, timezone=Indian/Chagos
  2&gt; NOTE: Linux 4.6.3-300.fc24.x86_64 amd64/Oracle Corporation 1.8.0_92 (64-bit)/cpus=8,threads=1,free=417462824,total=514850816
  2&gt; NOTE: All tests run in this JVM: [MultiNodeBackwardsIT]
Completed [1/1] in 0.99s, 1 test, 1 failure &lt;&lt;&lt; FAILURES!
```

Reproduction line (reproduces every time for me):

```
gradle :qa:backwards-5.0:integTest -Dtests.seed=529E08C5D51BBDF9 -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.shrink/10_basic/Shrink index via API}" -Dtests.security.manager=true -Dtests.locale=es-MX -Dtests.timezone=Indian/Chagos
```

I'm guessing this is because the test assumes that there will only be a single node, but there are two present for the backwards compatibility tests.
</description><key id="165336934">19419</key><summary>[CI] org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.shrink/10_basic/Shrink index via API} fails when multiple nodes are present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>bug</label><label>test</label></labels><created>2016-07-13T14:35:48Z</created><updated>2016-07-14T14:41:28Z</updated><resolved>2016-07-14T10:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-14T09:05:14Z" id="232608821">@dakrone where did you see this, are there any node logs etc. this has nothing todo with 2 nodes, it should work as it waits for the relocation
</comment><comment author="s1monw" created="2016-07-14T10:21:09Z" id="232625758">I think I know what the problem is, the test doesn't wait for all shards to be active before it uses allocation filtering so the settings call might return successfully but the wait call that happens next is returning immediately since the next reroute is not fully applied and the shard has not yet been relocated. Allocation filtering ie. canRemain is only called on started shards which might not be the case... 
</comment><comment author="dakrone" created="2016-07-14T14:41:28Z" id="232685516">@s1monw I was seeing this running tests on master on my desktop. It previously reproduced every time but it's fixed now, thanks for fixing!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Priority.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/PriorityTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>Add support for `wait_for_events` to the `_cluster/health` REST endpoint (#19432)</comment></comments></commit></commits></item><item><title>Migrate matrix_stats to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19418</link><project id="" key="" /><description>This is the last consumer of the old style register method so I removed
the method.
</description><key id="165336170">19418</key><summary>Migrate matrix_stats to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T14:32:49Z</created><updated>2016-07-13T16:04:57Z</updated><resolved>2016-07-13T15:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-13T14:36:57Z" id="232375208">LGTM
</comment><comment author="nik9000" created="2016-07-13T16:04:56Z" id="232403371">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update delete.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19417</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

The method setOperationThreaded(boolean) is undefined for the type DeleteRequestBuilder. So need delete it.
</description><key id="165322463">19417</key><summary>Update delete.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">infoplat</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-07-13T13:36:33Z</created><updated>2016-09-12T22:37:03Z</updated><resolved>2016-09-12T22:37:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T19:42:53Z" id="232464739">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-09-12T22:37:03Z" id="246517544">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make static Store access shard lock aware</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19416</link><project id="" key="" /><description>We currently have concurrency issue between the static methods on the Store class and store changes that are done via a valid open store. An example of this is the async shard fetch which can reach out to a node while a local shard copy is shutting down (the fetch does check if we have an open shard and tries to use that first, but if the shard is shutting down, it will not be available from IndexService).

Specifically, async shard fetching tries to read metadata from store, concurrently the shard that shuts down commits to lucene, changing the segments_N file. this causes a file not find exception on the shard fetching side. That one in turns makes the master think the shard is unusable. In tests this can cause the shard assignment to be delayed (up to 1m) which fails tests. See https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+java9-periodic/570 for details.

This is one of the things #18938 caused to bubble up.
</description><key id="165309172">19416</key><summary>Make static Store access shard lock aware</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Store</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T12:33:12Z</created><updated>2016-07-18T09:22:58Z</updated><resolved>2016-07-18T09:22:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-18T08:27:01Z" id="233267877">left on  nit - LGTM otherwise
</comment><comment author="ywelsch" created="2016-07-18T08:30:14Z" id="233268922">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file></files><comments><comment>Allow master to assign primary shard to node that has shard store locked during shard state fetching (#21656)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/LocalShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file></files><comments><comment>Tighten up concurrent store metadata listing and engine writes (#19684)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file></files><comments><comment>Make static Store access shard lock aware (#19416)</comment></comments></commit></commits></item><item><title>Can't use Java client due to NoClassDefFoundError: org/apache/log4j/Priority</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19415</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

I had to add these dependencies to resolve:

```

        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt;
            &lt;version&gt;1.5.11&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;
            &lt;version&gt;1.7.21&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;1.7.21&lt;/version&gt;
        &lt;/dependency&gt;
```

**Elasticsearch version**:
5.0.0-alpha4

**JVM version**:
java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)

**OS version**:
OSX 10.11.5

**Description of the problem including expected versus actual behavior**:
Can't use Java client, due to NoDefFound from ESLogger.

**Steps to reproduce**:
1. Create Maven project
2. Depend on  

```
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;5.0.0-alpha4&lt;/version&gt;
        &lt;/dependency&gt;
```
1. Try to make a client:

```
    private Client client = TransportClient.builder().build().addTransportAddress(
            new InetSocketTransportAddress(new InetSocketAddress("localhost", 9300)));
```

**Provide logs (if relevant)**:

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/log4j/Priority
    at org.elasticsearch.common.logging.ESLoggerFactory.getLogger(ESLoggerFactory.java:42)
    at org.elasticsearch.common.logging.ESLoggerFactory.getLogger(ESLoggerFactory.java:46)
    at org.elasticsearch.common.logging.Loggers.getLogger(Loggers.java:123)
    at org.elasticsearch.common.settings.Setting.&lt;clinit&gt;(Setting.java:110)
    at org.elasticsearch.common.logging.ESLoggerFactory.&lt;clinit&gt;(ESLoggerFactory.java:33)
    at org.elasticsearch.common.logging.Loggers.getLogger(Loggers.java:119)
    at org.elasticsearch.transport.netty.NettyInternalESLoggerFactory.newInstance(NettyInternalESLoggerFactory.java:33)
    at org.elasticsearch.common.netty.NettyUtils$1.newInstance(NettyUtils.java:91)
    at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:67)
    at org.jboss.netty.logging.InternalLoggerFactory.getInstance(InternalLoggerFactory.java:60)
    at org.jboss.netty.util.ThreadRenamingRunnable.&lt;clinit&gt;(ThreadRenamingRunnable.java:32)
    at org.elasticsearch.common.netty.NettyUtils.&lt;clinit&gt;(NettyUtils.java:95)
    at org.elasticsearch.transport.netty.NettyTransport.&lt;clinit&gt;(NettyTransport.java:145)
    at org.elasticsearch.client.transport.TransportClient$Builder.newPluginService(TransportClient.java:108)
    at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:120)
    at edge.model.TestElastic.&lt;clinit&gt;(TestElastic.java:10)
Caused by: java.lang.ClassNotFoundException: org.apache.log4j.Priority
    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 16 more
```
</description><key id="165308360">19415</key><summary>Can't use Java client due to NoClassDefFoundError: org/apache/log4j/Priority</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fakeh</reporter><labels /><created>2016-07-13T12:28:24Z</created><updated>2017-07-30T03:29:42Z</updated><resolved>2016-07-13T12:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T12:35:20Z" id="232341566">Hi @fakeh 

Thanks for trying out the alpha.  What you report is correct, and it is listed in the breaking changes: https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_java_api_changes.html#_elasticsearch_will_no_longer_detect_logging_implementations
</comment><comment author="fakeh" created="2016-07-13T12:43:34Z" id="232343431">Thanks for the swift response. If the libraries are required, why not pull in the dependencies transitively?
</comment><comment author="clintongormley" created="2016-07-13T19:23:33Z" id="232459895">We don't use transitive dependencies at all.
</comment><comment author="cooniur" created="2016-10-31T06:12:05Z" id="257220589">Hi @clintongormley , is there a doc explaining the reason why Elasticsearch 5.0 Java SDK uses Log4j2 instead of Slf4j that is IMHO much less intrusive?
</comment><comment author="nik9000" created="2016-10-31T14:16:29Z" id="257305040">&gt; Hi @clintongormley , is there a doc explaining the reason why Elasticsearch 5.0 Java SDK uses Log4j2 instead of Slf4j that is IMHO much less intrusive?

Because Elasticsearch is a [server](https://www.elastic.co/blog/elasticsearch-the-server) we don't feel the need to talk to a logging abstraction layer. We'd have used it if we liked the API better but we don't.

The transport client is a problem in this case, because it uses Elasticsearch's code base as though it were a library. This isn't a thing we can live with in the long term because it boxes us in in the choices we can make. So the elasticsearch java REST client is becoming a thing, though it honestly is too low level for Java developers to really enjoy using it at this point.
</comment><comment author="davidbilge" created="2016-10-31T14:41:44Z" id="257311810">Unfortunately, the Elasticsearch Java Client is not a server. If we want to use the API Jar in one of our applications, this choice forces us to use log4j.

Unfortunately, there does not seem to be a `log4j2-over-slf4j` library, the `log4j-over-slf4j` library only seems to work with log4j 1.x.

Edit:
I think I found a bridge that allows to continue using logback, see http://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-to-slf4j/2.7
</comment><comment author="nik9000" created="2016-10-31T15:59:11Z" id="257334751">&gt; Unfortunately, the Elasticsearch Java Client is not a server. If we want to use the API Jar in one of our applications, this choice forces us to use log4j.

That is the problem with transport client. It **is** the server. The same bits. You don't **want** it to be the server and neither do we. Thus the REST client which is, sadly, far from ready yet.

&gt; I think I found a bridge that allows to continue using logback, see http://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-to-slf4j/2.7

Yeah, I didn't read to your edit and was about to post https://logging.apache.org/log4j/2.0/log4j-to-slf4j/index.html which looks like the same link.
</comment><comment author="nik9000" created="2016-10-31T15:59:32Z" id="257334854">Er, looks like it is talking about the same artifact, rather.
</comment><comment author="dadoonet" created="2016-10-31T16:00:26Z" id="257335149">@davidbilge May be in the meantime you can shade elasticsearch server when it is used with the transport client?
In that case, this article may help: https://www.elastic.co/blog/to-shade-or-not-to-shade
</comment><comment author="davidbilge" created="2016-11-02T11:25:18Z" id="257838567">@dadoonet Thanks for the idea, I will look into that. For now, I'm perfectly fine using the `log4j-to-slf4j` bridge.
</comment><comment author="dadoonet" created="2016-11-02T11:31:29Z" id="257839757">@davidbilge I wonder if it is worth adding that in the doc. I mean in the Transport Client java documentation. Would you like to describe what you did to help other users? 
Like what was made with this specific page? https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/_deploying_in_jboss_eap6_module.html
</comment><comment author="davidbilge" created="2016-11-02T11:37:56Z" id="257840940">Sure thing. The dependencies in my `pom.xml` basically look like this:

```
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
    &lt;artifactId&gt;transport&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt;
    &lt;version&gt;2.7&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
    &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
    &lt;version&gt;1.1.7&lt;/version&gt;
&lt;/dependency&gt;
```

That solves the missing `java.lang.ClassNotFoundException` if you use a logger that implements `slf4j` (like logback does).
</comment><comment author="mvmn" created="2016-12-15T12:43:51Z" id="267318816">&gt; That is the problem with transport client. It is the server

Client is not server. You can't say "client is server", this is absurd, it makes no sense. Client is client, server is server, period.

I get what you mean though - client reuses some classes from server. 
&gt; it uses Elasticsearch's code base as though it were a library

This is clear. 

But, when you actually spell it this way, e.g. correct way, then the fix becomes obvious - decouple shared classes from server and put it into separate library, not dependent on Log4J. Trivial.</comment><comment author="jasontedor" created="2016-12-16T02:14:04Z" id="267500081">&gt; Client is not server. You can't say "client is server", this is absurd, it makes no sense. Client is client, server is server, period.

&gt; But, when you actually spell it this way, e.g. correct way

These establish an adversarial relationship rather than a collaborative one. We strongly prefer the latter, and this matters to us a lot.

&gt; I get what you mean though - client reuses some classes from server.

It can't be both "absurd" and "make no sense", and you get what he means. You left off a key sentence that elucidates exactly what he meant:

&gt; It **is** the server. The same **bits**.

The two sentences go together.

&gt; then the fix becomes obvious - decouple shared classes from server and put it into separate library [...] Trivial.

The fix is obvious, but obvious is not the same as trivial. There is a lot of entanglement. We know the fix, we want to do the fix, but on this subject, for the client effort, we value pragmatism over idealism (see previous [comments](https://github.com/elastic/elasticsearch/issues/19055#issuecomment-265587044) on this very subject).</comment><comment author="mvmn" created="2016-12-16T09:33:48Z" id="267553791">&gt; It can't be both "absurd" and "make no sense"

Ok. Pick the one you like more.

&gt; The two sentences go together.

You do realize the implications of stating that any two codebases that share common classes are the same ("client and server share classes, therefore client **is** server")? Let's consider an example: Tomcat uses java util logging and Weblogic uses java util logging - therefore Tomcat is Weblogic (but not vice versa)? Absurd or makes no sense - pick one you like.

&gt; The fix is obvious, but obvious is not the same as trivial

You're picking on words instead of figuring out the meaning behind words. Speaking of adversarial - let's not pick on words and start a demagogue contest, shall we?

It would be interesting to know what kind of entanglement prevents one from simply moving shared classes into shared library. The information on the problem - not opinions of the comments on the problem - that would be collaborative instead of adversarial.</comment><comment author="clintongormley" created="2016-12-16T09:52:05Z" id="267557068">Well done @mvmn - way to kill a thread...</comment><comment author="mvmn" created="2016-12-16T09:54:17Z" id="267557509">&gt;  The information on the problem - not opinions of the comments on the problem - that would be collaborative instead of adversarial.</comment><comment author="imod" created="2017-01-17T10:40:10Z" id="273100738">The above dependencies do not help if I run elasticsearch (v5.1.2) embedded (we do this for testing)...

as soon as I create an index, e.g.

```
		CreateIndexRequest createIndexRequest = new CreateIndexRequest("indexname");
		IndicesAdminClient indices = es.getNode().client().admin().indices();
		indices.create(createIndexRequest).actionGet();
```

I get this:

```
11:27:32.397 [main] INFO  org.elasticsearch.http.HttpServer - publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
11:27:32.397 [main] INFO  org.elasticsearch.node.Node - started
Exception in thread "elasticsearch[V3ONMVd][clusterService#updateTask][T#1]" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/config/Configurator
	at org.elasticsearch.common.logging.Loggers.setLevel(Loggers.java:149)
	at org.elasticsearch.common.logging.Loggers.setLevel(Loggers.java:144)
	at org.elasticsearch.index.SearchSlowLog.setLevel(SearchSlowLog.java:111)
	at org.elasticsearch.index.SearchSlowLog.&lt;init&gt;(SearchSlowLog.java:106)
	at org.elasticsearch.index.IndexModule.&lt;init&gt;(IndexModule.java:127)
	at org.elasticsearch.indices.IndicesService.createIndexService(IndicesService.java:421)
	at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:394)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:352)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:581)
	at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:920)
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:458)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:238)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:201)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.config.Configurator
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
```

it seems we are doomed to switch to log4j just because we use elasticsearch embedded within our tests :(</comment><comment author="mvmn" created="2017-01-17T11:17:12Z" id="273112551">&gt; it seems we are doomed to switch to log4j just because we use elasticsearch embedded within our tests :(

Actually this is stated in the thread - you have to have Log4J in classpath if you use ElasticSearch (be it server or client - doesn't matter).

You can still use other logging system - but you must have Log4J stuff in your classpath (which might cause conflicts for some logging systems or classpath-detection using things like Spring Boot).
</comment><comment author="kjussakov" created="2017-01-20T15:55:40Z" id="274106688">Anyone managed to run the 5.* version of the TransportClient in Spring Boot/Spring IO Platform application?
If so, please share how you managed to resolve the dependency conflicts.
For me, it is either `ClassNotFoundException` for `org.apache.logging.log4j.core.config.Configurator` when log4j-core is not added as a dependency or `SLF4JLoggerContext` cannot be cast to `org.apache.logging.log4j.core.LoggerContext` with added log4j-core dependency.
I am using SLF4J with Logback:


        &lt;dependency&gt;
            &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
            &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
            &lt;version&gt;1.1.8&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
            &lt;artifactId&gt;logback-core&lt;/artifactId&gt;
            &lt;version&gt;1.1.8&lt;/version&gt;
        &lt;/dependency&gt;        
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
            &lt;artifactId&gt;transport&lt;/artifactId&gt;
            &lt;version&gt;5.1.2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt;
            &lt;version&gt;2.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;
            &lt;version&gt;2.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;1.7.21&lt;/version&gt;
        &lt;/dependency&gt;

I am trying to avoid the dependency shading overkill which I saw proposed in some places.
</comment><comment author="jasontedor" created="2017-01-20T16:08:01Z" id="274109943">@kjussakov Would you please open a topic on the [forum](https://discuss.elastic.co) and link to it from here? However, when you open that topic, please provide a stack trace, I can not help you without a stack trace. Before you do that though, would you please look at my [example](https://github.com/elastic/elasticsearch/issues/22671#issuecomment-273498813) on #22671 and see if it helps you solve your problems? In particular, I think that you might be able to remove the log4j-api dependency.</comment><comment author="mvmn" created="2017-01-20T16:21:19Z" id="274113397">Try this in your pom.xml
```
	&lt;properties&gt;
		&lt;elasticsearch.version&gt;5.1.2&lt;/elasticsearch.version&gt;
	&lt;/properties&gt;
```</comment><comment author="imod" created="2017-01-23T08:06:04Z" id="274422827">@kjussakov can you please add a comment with the link to the new topic?</comment><comment author="kjussakov" created="2017-01-23T09:29:06Z" id="274438050">Hi all,
Thanks for your prompt reply! I will start with making a minimal possible example which has the issue I described and then open a new topic in the forum. I already started with a simple maven project with the dependencies I mentioned above. This also follows the official 5.* [doc](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/_using_another_logger.html)

Without any Spring related dependencies it seems it works so will try now to add more until I broke it.

Will keep you posted.

Best regards,
Rumen
</comment><comment author="kjussakov" created="2017-01-24T15:47:47Z" id="274842252">And here is the link to the new [topic](https://discuss.elastic.co/t/migrate-elasticsearch-java-api-to-version-5-in-spring-io-platform-application/72661?u=kjussakov)

I describe the workaround which I found to resolve the log4j dependency when running our tests with embedded Elasticsearch node.</comment><comment author="laxika" created="2017-01-24T19:03:31Z" id="274902254">@kjussakov Add 

`compile group: 'org.apache.logging.log4j', name: 'log4j-to-slf4j', version: '2.7'`

It works for me with Spring Boot 1.4.2.</comment><comment author="pritamm" created="2017-02-28T13:04:57Z" id="283033465">Hi,
I am using below configuration with elasticsearch 5.2.1, still getting error java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter.

Can someone please help me resolve this?

my confuguration:
"org.slf4j:slf4j-api:1.7.21",
"org.apache.logging.log4j:log4j-api:2.7",
"org.apache.logging.log4j:log4j-to-slf4j:2.7",

Stacktrace:
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
	at java.lang.Class.getConstructor0(Class.java:3075)
	at java.lang.Class.getConstructor(Class.java:1825)
	at org.springframework.boot.logging.LoggingSystem.get(LoggingSystem.java:134)
	at org.springframework.boot.logging.LoggingSystem.get(LoggingSystem.java:125)
	at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:189)
	at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:173)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:163)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:136)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:119)
	at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:111)
	at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:60)
	at org.springframework.boot.SpringApplicationRunListeners.started(SpringApplicationRunListeners.java:48)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:293)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1112)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1101)
	at com.audiencescience.media.search.Application.main(Application.java:14)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 18 more
</comment><comment author="dadoonet" created="2017-02-28T13:30:43Z" id="283038997">The stacktrace is not related to elasticsearch as far as I can see.</comment><comment author="jasontedor" created="2017-02-28T13:55:58Z" id="283045023">@pritamm If your issue is related to Elasticsearch (it does not look like it), please ask questions on the [forum](https://discuss.elastic.co).</comment><comment author="thaDude" created="2017-04-04T18:14:21Z" id="291586239">@nik9000 
&gt; We'd have used it if we liked the API better but we don't.

Dude...

Hello, the fact that ES 5.x `org.elasticsearch.client` transport artifact explicitly expects log4j2 2.7 made things quite hard for us. I don't understand the exact reason it's not using the slf4j API. Using `log4j-over-slf4j` is not an option since there will very likely be `log4j-slf4j-impl` in the class path to process the logs of "standard" libraries that don't make an explicit choice about the logging framework.

Our particular - not so fancy - use case:
We use the ES 5.x client in the code for a Storm topology. The Storm distribution packs log4j2 2.1 -  which is on the class path of all Storm processes on the cluster. ES client is not content with that version due to some new log4j2 API changes I guess so what are the options:

- replace all log4j2 2.1 jars on the Storm cluster with 2.7
- attempt to roll a shaded *.jar which packs log4j2 2.7 with relocation of `org.apache.logging.log4j`moved to different location, `META-INF/log4j-provider.properties` updated to point at that new location, etc., etc. - spent half a day on it, still not working!

... not knowing the exact log4j2-specific features you rely on (being able to log objects?) it seems just using slf4j would have been more user-friendly :/</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[bug report]client won't throw SearchContextMissingException when scroll timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19414</link><project id="" key="" /><description>1. ES version: 2.1.2
2. JDK version: 1.8.60
3. linux version: 2.6.32

step:
MatchAllQueryBuilder maq = QueryBuilders.matchAllQuery();
        SearchResponse sResponse = client.prepareSearch(indexName)
                .setSearchType(SearchType.SCAN)
                .setQuery(maq)
                .setScroll(new TimeValue(1))
                .setSize(20)
                .execute()
                .actionGet();

while (true) {
            SearchHits hits = sResponse.getHits();
            SearchHit[] hitArray = hits.getHits();
            sResponse = client.prepareSearchScroll(sResponse.getScrollId()).setScroll(new TimeValue(1)).execute().actionGet();
            //Break condition: No hits are returned
            if (sResponse.getHits().getHits().length == 0) {
                break;
            }
        }

when set break point on sResponse = client.prepareSearchScroll and wati for scroll timeout, server throw SearchContextMissingException but client eat this exception silently. 
</description><key id="165286150">19414</key><summary>[bug report]client won't throw SearchContextMissingException when scroll timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Java API</label><label>adoptme</label><label>bug</label><label>v2.4.7</label></labels><created>2016-07-13T10:16:43Z</created><updated>2017-07-21T09:31:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2016-07-14T02:03:14Z" id="232537206">@clintongormley  what does adoptme mean? can I pull a request to fix it on 2.1.2 or what since there won't be any release for 2.1 ?
</comment><comment author="jasontedor" created="2016-07-14T02:09:50Z" id="232538007">@makeyang There will not be a release of the 2.1.x line for addressing issues like this. You're welcome to open a pull request against master.
</comment><comment author="makeyang" created="2016-07-14T02:17:18Z" id="232538994">@jasontedor got it. thanks
</comment><comment author="makeyang" created="2016-07-28T08:39:45Z" id="235834768">@jasontedor  it turns out that master has already fixed this issue. 
Failed to execute phase [query], all shards failed; shardFailures {RemoteTransportException[[192.168.200.191][192.168.200.191:9505][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [1]]; }{RemoteTransportException[[192.168.200.191][192.168.200.191:9505][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [4]]; }{RemoteTransportException[[192.168.200.191][192.168.200.191:9505][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [3]]; }{RemoteTransportException[[192.168.200.191][192.168.200.191:9505][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [5]]; }{RemoteTransportException[[192.168.200.191][192.168.200.191:9505][indices:data/read/search[phase/query/scroll]]]; nested: SearchContextMissingException[No search context found for id [2]]; }
    at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.onQueryPhaseFailure(SearchScrollQueryThenFetchAsyncAction.java:155)
    at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onFailure(SearchScrollQueryThenFetchAsyncAction.java:142)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:51)
    at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:934)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1035)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1013)
    at org.elasticsearch.transport.TransportService$5.onFailure(TransportService.java:528)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:496)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</comment><comment author="clintongormley" created="2016-07-28T12:14:44Z" id="235877260">thanks @makeyang - we will be doing a 2.4 release so if you still want to submit a PR to fix this in 2.4, it would be welcome.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripted_upsert in BULK UPDATE operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19413</link><project id="" key="" /><description>**Describe the feature**:

Is it possible to have the "scripted_upsert" feature in bulk update operations? 

It would be great to propose that through BULK API...

right now, I must put my document in "param" fields of the script part AND in the upsert field.

unless I'm doing something wrong...
</description><key id="165278311">19413</key><summary>Scripted_upsert in BULK UPDATE operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmanu23</reporter><labels /><created>2016-07-13T09:36:53Z</created><updated>2016-07-13T19:14:51Z</updated><resolved>2016-07-13T19:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T19:14:51Z" id="232457671">This works just fine, eg:

```
POST /test/type1/_bulk
{"update":{"_id":1}}
{"scripted_upsert":true,"script":{"inline":"ctx._source.counter += count","params":{"count":4}},"upsert":{"counter":1}}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decide how to make transport client to work with netty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19412</link><project id="" key="" /><description>with #19411 and #19392 the transport client must add the netty module explicitly otherwise networking won't work. Today folks need to add the plugin explicitly on the builder, this might be cumbersome and we might want to add some magic to load it automatically if it's on the classpath. We should at least document it and throw appropriate exceptions.
</description><key id="165273923">19412</key><summary>Decide how to make transport client to work with netty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>:Network</label><label>blocker</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T09:14:38Z</created><updated>2016-07-18T13:42:24Z</updated><resolved>2016-07-18T13:42:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-14T08:35:38Z" id="232602046">one way of doing this would be a stand-alone project called `client/transport-client` that depends on `core` as well as `transport-netty` that adds the plugin by default. I think that would be the best separation here. Another idea is to move the transport client into it's own project entirely that depends on core and netty but is pulled in from `test/framework` to cycle it in tests as we do today. I think we have to keep that feature (testing it with core) otherwise there are too many sources of bugs and I am hesitating in investing too much into it. @rjernst WDYT
</comment><comment author="rjernst" created="2016-07-14T08:39:53Z" id="232602870">Creating `client/transport-client` sounds right to me. Having it just depend on core and transport netty sounds ok as a first cut. We can try moving the actual TransportClient class into there as a followup.
</comment><comment author="s1monw" created="2016-07-14T08:41:09Z" id="232603114">lets do that then
</comment><comment author="dadoonet" created="2016-07-14T08:41:15Z" id="232603128">++ moving the TransportClient to its project is cleaner IMHO.
</comment><comment author="nik9000" created="2016-07-14T13:12:05Z" id="232661097">If you make client/transport-client maybe it should include a few modules
by default: reindex, mustache, percolator, stuff like that. That way users
of those built in features don't need to do special stuff.

On Thu, Jul 14, 2016 at 4:41 AM, David Pilato notifications@github.com
wrote:

&gt; ++ moving the TransportClient to its project is cleaner IMHO.
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19412#issuecomment-232603128,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLogOUDLMxKqcj3i580ZeTg1orZMHDks5qVfYvgaJpZM4JLNzd
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/transport/src/main/java/org/elasticsearch/transport/client/PreBuiltTransportClient.java</file><file>client/transport/src/test/java/org/elasticsearch/transport/client/PreBuiltTransportClientTests.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/ClusterStateBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/transport/netty3/Netty3TransportMultiPortIntegrationIT.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTransportClient.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Add a dedicated client/transport project for transport-client (#19435)</comment></comments></commit></commits></item><item><title>Publish `transport-netty` as a jar file in order to use it in the transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19411</link><project id="" key="" /><description>Today we only publish the zip file which is not enough since netty is the primary transport and otherwise folks won't be able to use the transport client at all. 
</description><key id="165273279">19411</key><summary>Publish `transport-netty` as a jar file in order to use it in the transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>blocker</label><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T09:11:09Z</created><updated>2016-07-14T15:56:57Z</updated><resolved>2016-07-14T15:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T19:06:05Z" id="232455242">@rjernst could you look at this please?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Build: Add flag for plugins/modules which can be used in the transport client</comment></comments></commit></commits></item><item><title>Rename `transport-netty` to `transport-netty3` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19410</link><project id="" key="" /><description>Now that we have the infrastructure (#19392) to add different transport mechanism we can make a safe move towards netty4, yet to have a better distinction between the modules we should rename `transport-netty` to `transport-netty3`.
</description><key id="165272964">19410</key><summary>Rename `transport-netty` to `transport-netty3` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>blocker</label><label>build</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T09:09:28Z</created><updated>2016-07-15T02:03:14Z</updated><resolved>2016-07-15T02:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Can I solve one search request whith multi Groovy scripting file ? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19409</link><project id="" key="" /><description>Like the database functions .. we can use the function like to_number(to_char(...)) ， so the function is **not fixed**.

now we defined the function :
 "def abs(data) {return Math.abs(data);};";
 "def upper(data){if(data!=null){return data.toUpperCase();}else{return null}};";

and use it in script , **inline** ...like this
"script":"def abs(data) {return Math.abs(data);};def upper(data){if(data!=null){return data.toUpperCase();}else{return null}};upper(abs(doc['fieild'].value))"

so this way can make memory leak? (Es 2.3.4)

if use script file ,how can i do it ..
</description><key id="165272301">19409</key><summary>Can I solve one search request whith multi Groovy scripting file ? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zliu789</reporter><labels /><created>2016-07-13T09:05:51Z</created><updated>2016-07-13T19:04:25Z</updated><resolved>2016-07-13T19:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T19:04:25Z" id="232454766">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>install elasticsearch error in centos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19408</link><project id="" key="" /><description>hi there, last time i install elasticsearch in windows, everything work perfectly fine now.
after some development on my application modules, now i decided to move to my server.
which in running on centos.

i have problems when installing it.
after downloading elasticsearch package.
i run this command : 
`bin/elasticsearch`

and then i got this error msg : 

```
Exception in thread "main" java.lang.RuntimeException: don't run elasticsearch as root.
        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35
```

this my environment specification : 

&gt; centos-release-6-6.el6.centos.12.2.x86_64
&gt; java version "1.7.0_101"
&gt; elastic version 2.3.4

i dont know what to do to fix this one.
please help me.
thanks in advance.
</description><key id="165255392">19408</key><summary>install elasticsearch error in centos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gutasaputra</reporter><labels /><created>2016-07-13T07:32:37Z</created><updated>2016-07-13T07:55:52Z</updated><resolved>2016-07-13T07:55:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-13T07:55:52Z" id="232283316">Please ask questions on discuss.elastic.co.

The error message is pretty clear here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate derivative pipeline aggregation to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19407</link><project id="" key="" /><description>This is another step in the effort to remove AggregationStreams and
instead use NamedWriteableRegistry like the rest of the code base.
</description><key id="165217296">19407</key><summary>Migrate derivative pipeline aggregation to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T01:28:07Z</created><updated>2016-07-13T11:42:25Z</updated><resolved>2016-07-13T11:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-13T07:37:57Z" id="232279377">I left a comment about how pipeline aggregator are going to be registered in SearchModule but otherwise it LGTM
</comment><comment author="nik9000" created="2016-07-13T10:57:07Z" id="232322983">&gt; I left a comment about how pipeline aggregator are going to be registered in SearchModule but otherwise it LGTM

I'll change them over in the next one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Linking to an id in the docs hides the title</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19406</link><project id="" key="" /><description>If I link to something like [this](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#_highlighted_fragments) the title ends up under the transparent docs `div` making it hard to see the heading.

This is chrome on OSX.
</description><key id="165216799">19406</key><summary>Linking to an id in the docs hides the title</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2016-07-13T01:22:46Z</created><updated>2016-07-13T18:58:39Z</updated><resolved>2016-07-13T18:58:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T18:58:39Z" id="232453197">@nik9000 - there is already an issue open on the website repo
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix prirep =&gt; pri,rep</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19405</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

fix header
</description><key id="165216738">19405</key><summary>fix prirep =&gt; pri,rep</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jw0201</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-07-13T01:22:07Z</created><updated>2016-09-12T22:36:57Z</updated><resolved>2016-09-12T22:36:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-13T01:38:16Z" id="232231207">Thanks @jw0201, this is correct. Can you sign the CLA? We can merge this into master and backport as necessary.
</comment><comment author="dakrone" created="2016-09-12T22:36:57Z" id="246517514">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Show ignored errors in verbose simulate result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19404</link><project id="" key="" /><description>When returning verbose processor responses in ingest simulate responses... 

Exposes any exceptions that were ignored within a processor that has `ignore_failure: true` set. 

Closes #19319
</description><key id="165216106">19404</key><summary>Show ignored errors in verbose simulate result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-13T01:15:32Z</created><updated>2016-07-13T20:32:11Z</updated><resolved>2016-07-13T20:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-13T20:31:17Z" id="232477391">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/TrackingResultProcessorTests.java</file></files><comments><comment>show ignored errors in verbose simulate result (#19404)</comment></comments></commit></commits></item><item><title>Using script_fields turns off _source retrieval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19403</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master (6861d35)
**JVM version**: 1.8.0_60
**OS version**: OSX 10.11.4

**Description of the problem including expected versus actual behavior**:

I recently discovered in Kibana master that creating a script field prevents other fields from loading on the Discover tab  https://github.com/elastic/kibana/issues/7699. The problem is that when `script_fields` is present in the query body, `_source` is no longer returned for each hit. Nothing has changed in the Kibana query as far as I can tell, so I think this is a change in behavior for ES.

**Steps to reproduce**:
1. Execute a query without `script_fields`, see that `_source` is returned by default.
2. Execute the same query with `script_fields` (any simple script should do, but it can't just be an empty object), see that `_source` is now missing
</description><key id="165198552">19403</key><summary>Using script_fields turns off _source retrieval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels /><created>2016-07-12T22:43:06Z</created><updated>2016-07-14T18:37:56Z</updated><resolved>2016-07-13T18:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T18:57:19Z" id="232452844">Yes, it has always been this way.  The idea is that if you are specifying custom fields, then you don't want the full `_source`.  Turn return the `_source` with script fields, just set `?_source=true`
</comment><comment author="Bargs" created="2016-07-14T18:37:56Z" id="232753950">Thanks @clintongormley, I see what changed in Kibana now. Sorry for the false alarm.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>update foreach processor to only support one applied processor.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19402</link><project id="" key="" /><description>Historically, Foreach Processor supported the application of multiple processors on the elements of the list it was being applied to. To make things a bit more consistent with the "do one thing" mentality of the processors, this PR limits foreach to only apply one processor per field.

Closes #19345.
</description><key id="165196148">19402</key><summary>update foreach processor to only support one applied processor.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T22:26:09Z</created><updated>2016-07-13T20:13:01Z</updated><resolved>2016-07-13T20:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-13T06:08:58Z" id="232265156">This change looks good. I left one question. Additionally also the docs need to be updated.
</comment><comment author="talevy" created="2016-07-13T17:13:00Z" id="232422906">thanks for the review @martijnvg, I'll update accordingly
</comment><comment author="talevy" created="2016-07-13T17:29:06Z" id="232427721">updated!
</comment><comment author="martijnvg" created="2016-07-13T19:19:22Z" id="232458861">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorTests.java</file></files><comments><comment>update foreach processor to only support one applied processor. (#19402)</comment></comments></commit></commits></item><item><title>Add resource watcher to services available for plugin components</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19401</link><project id="" key="" /><description /><key id="165191444">19401</key><summary>Add resource watcher to services available for plugin components</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T22:00:23Z</created><updated>2016-07-13T18:56:20Z</updated><resolved>2016-07-12T22:28:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-07-12T22:27:47Z" id="232201415">LGTM! thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file></files><comments><comment>Merge pull request #19401 from rjernst/more_plugin_services</comment></comments></commit></commits></item><item><title>Introduce async performRequest method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19400</link><project id="" key="" /><description>The new  performRequest method accepts the usual parameters (method, endpoint, params, entity and headers) plus a response listener and an async response consumer. Shortcut methods are also added that don't require params, entity and the async response consumer optional.

The existing sync methods also move to using the async client under the hood, although they block and wait for a response to be returned or an exception to be thrown.

There are a few relevant api changes as a consequence of the move to async client that affect sync methods:
- Response doesn't implement Closeable anymore, responses don't need to be closed
- performRequest throws Exception rather than just IOException, as that is the the exception that we get from the FutureCallback#failed method in the async http client
- ssl configuration is a bit simpler, one only needs to call setSSLStrategy from a custom HttpClientConfigCallback, that doesn't end up overriding any other default around connection pooling (it used to happen with the sync client and make ssl configuration more complex)

As part of this PR also reindex from remote is moved to using the async performRequest method.
</description><key id="165187851">19400</key><summary>Introduce async performRequest method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>feature</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T21:40:08Z</created><updated>2016-07-22T20:36:06Z</updated><resolved>2016-07-22T20:36:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-12T21:40:33Z" id="232190292">@nik9000 can you review this?
</comment><comment author="nik9000" created="2016-07-13T00:45:38Z" id="232224052">&gt; @nik9000 can you review this?

I'll review it in the morning.
</comment><comment author="javanna" created="2016-07-13T12:00:50Z" id="232334462">The build is finally all green, all yours @nik9000  ;)
</comment><comment author="javanna" created="2016-07-19T16:23:13Z" id="233687482">@nik9000 I believe I addressed/replied to all your comments. I also merged master in. Can you have another look please?
</comment><comment author="nik9000" created="2016-07-20T13:06:25Z" id="233942993">Yikes, merging master into this made there be a zillion patches in it....
</comment><comment author="nik9000" created="2016-07-20T13:07:47Z" id="233943315">I'm not sure what is up - I can't really review it using the `Files Changed` tab. I'll see if I can by going through the individual commits.
</comment><comment author="javanna" created="2016-07-20T13:13:01Z" id="233944590">oh oh I must have done something wrong here, I had rebased at first, but then decided to drop that rebased branch in favour of merging to make reviews easier......ops that didn't work out
</comment><comment author="javanna" created="2016-07-20T13:35:47Z" id="233950608">@nik9000 I have fixed the git shenanigans, good news is you can review again, bad news is I had to force push :)
</comment><comment author="nik9000" created="2016-07-20T13:40:55Z" id="233951987">OK - I'll review in a bit!
</comment><comment author="nik9000" created="2016-07-20T14:19:55Z" id="233963528">I left some minor stuff but LGTM.
</comment><comment author="s1monw" created="2016-07-21T11:09:16Z" id="234223949">I left some comments - looks good though
</comment><comment author="javanna" created="2016-07-21T17:49:50Z" id="234330592">I have replied/addressed all comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/HeapBufferedAsyncResponseConsumer.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseListener.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClientBuilder.java</file><file>client/rest/src/main/java/org/elasticsearch/client/SSLSocketFactoryHttpConfigCallback.java</file><file>client/rest/src/test/java/org/elasticsearch/client/FailureTrackingResponseListenerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/HeapBufferedAsyncResponseConsumerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/HostsTrackingFailureListener.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/SyncResponseListenerTests.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SniffOnFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/Sniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportReindexAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteScrollableHitSource.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/remote/RemoteScrollableHitSourceTests.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsNotSetIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsRegexIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DeprecationHttpIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsDisabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsEnabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ResponseHeaderPluginIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/RestTestExecutionContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestResponse.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestResponseException.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/DoSection.java</file></files><comments><comment>Merge pull request #19400 from javanna/feature/async_rest_client</comment></comments></commit></commits></item><item><title>Include other Reference to other ElasticSearch clients in Docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19399</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

Include reference to other ElasitcSearch client in the documentation. Right now only Perl and Python are included. It is a little bit strange to have only just these two, when ElasticSearch does have other official clients. https://www.elastic.co/guide/en/elasticsearch/client/index.html
</description><key id="165187570">19399</key><summary>Include other Reference to other ElasticSearch clients in Docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadepo</reporter><labels><label>docs</label></labels><created>2016-07-12T21:38:32Z</created><updated>2017-04-04T15:13:50Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-13T18:47:51Z" id="232450322">@dadepo anywhere in particular?
</comment><comment author="dadepo" created="2016-07-13T19:07:43Z" id="232455703">I think only these two places:
- https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html
</comment><comment author="clintongormley" created="2016-07-13T20:21:32Z" id="232474850">I believe that Ruby and PHP don't have scroll helpers.  Not sure about .net

@polyfractal @karmi @Mpdreamz ?
</comment><comment author="polyfractal" created="2016-07-13T20:30:58Z" id="232477320">PHP has iterator helpers that wrap scroll:  https://github.com/elastic/elasticsearch-php/tree/master/src/Elasticsearch/Helper/Iterators

However, it was a user contribution and not well documented at the moment.  So not a good page to link to currently.

No bulk helper outside the standard bulk endpoint.
</comment><comment author="Mpdreamz" created="2016-07-13T20:42:37Z" id="232480426">We have a `Reindex()` helper. I'm actually adding helper version of `Bulk` and `Scroll` helpers that return observables (e.g they will return multiple responses vs the single requests to the endpoints)  as I write this :smile:. 
</comment><comment author="karmi" created="2016-07-14T06:30:11Z" id="232569082">The Ruby client has a syntactic sugar for the Bulk API, where you can pass your data as a [`data`](https://github.com/elastic/elasticsearch-ruby/blob/master/elasticsearch-api/lib/elasticsearch/api/actions/bulk.rb#L22-L28) option, so users don't have to perform acrobatics with the action:data pairs in their code. (There is no "reversed" helper, which would take an Array and store it into Elasticsearch in bulks, because it seemed to me trivial to iterate over the Array and send the bulks in user's code.)

There is no special wrapper or sugar for the Scroll API, which would for instance yield batches of documents from the index -- as a remedy, I've included a fair number of [copy&amp;paste ready examples](https://github.com/elastic/elasticsearch-ruby/blob/master/elasticsearch-api/lib/elasticsearch/api/actions/scroll.rb#L31-L35). I might think about adding some kind of sugar for that in the `elasticsearch-extensions` gem, it's mostly a matter of nice naming and API.

Apart from supporting the core Reindex API in Elasticsearch 2, the Ruby client has a fairly complex [reindexing extension](https://github.com/elastic/elasticsearch-ruby/blob/master/elasticsearch-extensions/lib/elasticsearch/extensions/reindex.rb) which has a compatible API, works on old clusters, and allows extra tricks like transforming the documents during the reindexing operation. (The biggest downside is that it is not parallelized right now.)
</comment><comment author="clintongormley" created="2016-07-14T10:55:39Z" id="232632369">Any of you are welcome to add per-client examples to the reference docs mentioned above
</comment><comment author="Mpdreamz" created="2016-07-14T11:04:12Z" id="232634263">Aye Aye @clintongormley should have stated that I will ammed to the docs myself and not dump it on you :smile:
</comment><comment author="colings86" created="2017-03-31T13:38:13Z" id="290714155">@clintongormley @Mpdreamz can this issue be closed? or is there still more to do?</comment><comment author="Mpdreamz" created="2017-04-04T15:13:50Z" id="291531977">Leave it open just a tad we'll be doing a big doc update on our end next week so we have somewhere nice to point people for scroll and bulk helpers. cc @russcam</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove support for properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19398</link><project id="" key="" /><description>This commit removes support for properties syntax and config files:
- removed support for elasticsearch.properties
- removed support for logging.properties
- removed support for properties content detection in REST APIs
- removed support for properties content detection in Java API

Relates #19391, relates #19388
</description><key id="165179867">19398</key><summary>Remove support for properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T21:01:40Z</created><updated>2016-07-13T12:48:17Z</updated><resolved>2016-07-12T21:55:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-12T21:07:04Z" id="232181562">I had no idea :) LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoaderFactory.java</file><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/common/logging/LoggingConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceIT.java</file></files><comments><comment>Remove support for properties</comment></comments></commit></commits></item><item><title>Don't recursively count children profile timings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19397</link><project id="" key="" /><description>The breakdown is already inclusive of children timing. Including the child times will double-count and inflate the final time.

@jpountz mind taking a quick look when you have a sec?

Closes #18693
</description><key id="165162267">19397</key><summary>Don't recursively count children profile timings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T19:39:22Z</created><updated>2016-07-14T13:29:55Z</updated><resolved>2016-07-14T13:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-13T10:32:07Z" id="232318355">LGTM
Good catch!
</comment><comment author="polyfractal" created="2016-07-14T13:28:54Z" id="232665182">Thanks! Merging
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/profile/AbstractInternalProfileTree.java</file><file>core/src/test/java/org/elasticsearch/search/profile/query/QueryProfilerTests.java</file></files><comments><comment>Don't recursively count children profile timings (#19397)</comment></comments></commit></commits></item><item><title>Groovy scripting - possible memory leak ES 2.3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19396</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**:  java version "1.8.0_40"
Java(TM) SE Runtime Environment (build 1.8.0_40-b25)
Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)

**OS version**: centos 6.3 (x86_64) :: linux 2.6.32-279.el6.x86_64

**Description of the problem including expected versus actual behavior**:

We upgraded our cluster from  1.4.4 to 2.3.3 and now experiencing an issue where nodes become unresponsive and get heavily garbage collected. A Node restart helps for about 24 hours until 24Gb of RAM get fully utilized again.

**Provide logs (if relevant)**:

`jmap -histo:live` on the node with 91.1% heap used shows that 4.5Gb is taken by org.codehaus.groovy.runtime.metaclass.MetaMethodIndex

```
num     #instances         #bytes  class name
----------------------------------------------
  1:      86241328     4829514368  org.codehaus.groovy.runtime.metaclass.MetaMethodIndex$Entry
  2:      66209193     1922638888  [Ljava.lang.Object;
  3:      34089692     1636305216  java.lang.invoke.MethodHandleImpl$CountingWrapper
  4:      62450344     1498808256  org.codehaus.groovy.util.FastArray
  5:      33948832     1086362624  java.util.HashMap$Node
  6:        223427     1063930664  [B
  7:       6938969      999212432  [Lorg.codehaus.groovy.util.ComplexKeyHashMap$Entry;
  8:      27089896      866876672  java.lang.invoke.BoundMethodHandle$Species_LL
  9:      17686899      707475960  java.lang.invoke.BoundMethodHandle$Species_L3
 10:       5818726      612509464  [C
 11:         23980      604964856  [I
 12:      16356260      523400320  org.codehaus.groovy.util.SingleKeyHashMap$Entry
 13:        495640      515466624  [Lorg.codehaus.groovy.runtime.metaclass.MetaMethodIndex$Entry;
 14:       7202607      460966848  java.lang.invoke.BoundMethodHandle$Species_L4IL4
 15:       2020760      383997512  [Ljava.util.HashMap$Node;
 16:       7202607      345725136  java.lang.invoke.BoundMethodHandle$Species_L5
 17:       3003762      264331056  java.lang.reflect.Method
 18:       7884148      252292736  java.lang.invoke.BoundMethodHandle$Species_L
 19:       7434592      237906944  groovy.lang.MetaBeanProperty
 20:       4553471      218566608  java.util.HashMap
 21:       2508224      200657920  java.lang.reflect.Constructor
```
</description><key id="165155136">19396</key><summary>Groovy scripting - possible memory leak ES 2.3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">bogdanovich</reporter><labels><label>:Scripting</label></labels><created>2016-07-12T19:03:28Z</created><updated>2016-08-15T14:08:30Z</updated><resolved>2016-08-15T14:08:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-12T20:12:08Z" id="232165156">I believe this may be related to https://github.com/elastic/elasticsearch/issues/18572 , which is resolved in 2.4.0 by https://github.com/elastic/elasticsearch/pull/18975
</comment><comment author="rmuir" created="2016-07-12T21:24:11Z" id="232186135">Its more than that though. Don't compile a new unique script with each request!!! Use `params` !!!!
</comment><comment author="rmuir" created="2016-07-12T21:54:32Z" id="232193733">users that do this, will continue to hit problems, including jvm crapping out with stuff like https://bugs.openjdk.java.net/browse/JDK-8023191

seriously, the root cause is that the scripting api encourages users to send a new unique script with each request. one _symptom_ was that es had a leak of those tons of generated classes. another symptom, will be that the jdk has a code cache leak, not fixed until java 9. Other symptoms will be slow performance, due to compiler thread pressure and other problems.

But we should address the root cause: there needs to be documentation, limits, something to prevent the anti-pattern.
</comment><comment author="jasontedor" created="2016-07-12T22:01:55Z" id="232195473">Relates #8632
</comment><comment author="bogdanovich" created="2016-07-12T22:02:18Z" id="232195574">Would CMSClassUnloadingEnabled flag temporarily fix the problem?
</comment><comment author="rmuir" created="2016-07-12T22:02:27Z" id="232195607">no.
</comment><comment author="rmuir" created="2016-07-12T22:04:45Z" id="232196126">The way to prevent the problem, again, is not to send a new unique script with each request. Instead, let the script take parameters, such as `my_var`, whatever parts are changing for each request, and change those parameters via `params`. See https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html, there are examples doing this.

That means, your script will only be compiled once, and things will be much faster.
</comment><comment author="bogdanovich" created="2016-07-12T22:06:01Z" id="232196400">@rmuir Why then it was working fine for months in ES 1.4.4 ?

PS. Thanks for clarifying the issue. We indeed use unique scripts every request and will fix that.
</comment><comment author="rmuir" created="2016-07-12T22:10:40Z" id="232197445">Who knows, perhaps ES 1.4.4 didnt leak? Perhaps it leaked slower, because it used groovy without invokedynamic. Not particularly interesting to me.

CMSClassUnloading is not relevant, you are using java 8, so its already on anyway by default. But CMS never stands a chance to collect the class, because of https://github.com/elastic/elasticsearch/issues/18572

But forget all about that too: because its going to cause all kinds of other problems for your jvm (see my links and comments above), besides being horribly slow. 

That's why i'd rather us not pass around a bunch of aspirin trying to treat symptoms, but address the root cause, which is that its too easy to compile a brand new slightly-different script with each request. Obviously the api invites this (thats what happens when an API's efficiency relies on caching!), but that is hard to fix, so for now at least we should try to add some kind of system limit + documentation encouraging people to do it the fast way (using `params`).
</comment><comment author="clintongormley" created="2016-07-13T13:00:08Z" id="232347137">The docs for scripting have already been improved (see the `Prefer parameters` block on https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-using.html)

We report the number of compilations and cache evictions in the node stats (`GET _nodes/stats/script`).

The only other thing I can think of doing is to add a [safeguard](https://github.com/elastic/elasticsearch/issues/11511) which limits the number of compilations per minute to e.g. 10

Does this sound reasonable?
</comment><comment author="rmuir" created="2016-07-13T13:05:54Z" id="232348534">Can we make the `Prefer parameters` a linkable thing? This would help I think, to communicate the issue more easily/directly when it happens. The text is great.

As far as compilations safeguard, I have no idea how it should work, but i do think we need something? At least to catch the most extreme cases before the JVM goes belly-up.
</comment><comment author="clintongormley" created="2016-07-13T15:06:09Z" id="232384731">&gt; Can we make the Prefer parameters a linkable thing? This would help I think, to communicate the issue more easily/directly when it happens. The text is great.

Success! https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-using.html#prefer-params

&gt; As far as compilations safeguard, I have no idea how it should work, but i do think we need something? At least to catch the most extreme cases before the JVM goes belly-up.

Is it the frequency of compilations or just the total number of compilations causing the problem here?  If the former, then we could add some rate-limiting. If the latter, then the only way of resolving the issue would be to restart a node (which users have to do at the moment anyway).  Should we just be deferring this until 5.0 and Painless?  We're not going to make a breaking change in 2.x now.
</comment><comment author="rmuir" created="2016-07-13T15:16:42Z" id="232388120">Thanks for making that link, I will use it.

&gt; Is it the frequency of compilations or just the total number of compilations causing the problem here? If the former, then we could add some rate-limiting. If the latter, then the only way of resolving the issue would be to restart a node (which users have to do at the moment anyway). Should we just be deferring this until 5.0 and Painless? We're not going to make a breaking change in 2.x now.

Absolutely we should defer, we don't need to rush anything in, I just want us to think about addressing it. 

Mainly, I think we want to catch the anti-pattern where its a brand-new-script-per-request vs using params. It doesn't have to be a hard limit, it could be a warning in the logs at a start, i mean we should just try to do something. I think the way the caching works is very unintuitive.

In some cases, e.g. expressions, doing this might not be such of a problem. Expressions makes a tiny method without loops, and probably will never cause an issue, even with the java bugs. Expressions was actually discussed around the bug in question:

```
No loops means no OSR compilation of that bytecode but if it's called
millions of times (based on what you said above) then it'll get compiled.
It may be that your bytecode generates really small nmethods (you mentioned
it's simple algebraic expressions) and their generation naturally doesn't
outpace code cache cleaning.
```

On the other hand groovy is more complicated and heavy-duty. Features like groovy closures call out to reflection, in slow ways, and are heavy duty. You can see some of that stuff on the histogram on this issue.

Painless? We don't know yet, someone should benchmark it. It uses java's lambda support, which is faster/more lightweight than groovy closures. Maybe its ok to compile jazillions: maybe its not.

But even if its "ok", it does not give the user good performance. It is faster to avoid doing this stuff on every request, always.
</comment><comment author="dakrone" created="2016-07-13T19:47:23Z" id="232465904">&gt; As far as compilations safeguard, I have no idea how it should work, but i do think we need something? At least to catch the most extreme cases before the JVM goes belly-up.

I have an idea for this for a "rate-limiting" circuit breaker, where we could limit script compilation to something like 10 a minute (or whatever limit) or throw a circuit breaking exception with a helpful message. Maybe that would help in this situation?
</comment><comment author="bogdanovich" created="2016-07-13T20:34:04Z" id="232478173">@dakrone Someone could have 100 different scripts used in their app, and all of them could be compiled during first minute app is running . That would cause circuit beaker exception if it would be implemented the way you are suggesting.

Maybe requests/compilations ratio could be helpful.
</comment><comment author="dakrone" created="2016-07-13T20:49:04Z" id="232482070">&gt; Someone could have 100 different scripts used in their app, and all of them could be compiled during first minute app is running . That would cause circuit beaker exception if it would be implemented the way you are suggesting.

It might be possible to thread a flag through for dynamic scripts versus disk-based scripts, so that disk-based scripts are exempt.

Still, 100 different scripts seems like a lot! Once a user gets to that level they may want to restructure their data so they can avoid using scripting so much.
</comment><comment author="clintongormley" created="2016-07-15T09:18:34Z" id="232903098">Proposal:  For inline scripts which have no `params` specified, apply a unique-compilations-per-minute limit (eg 10 per minute).  If the limit is breached, delay compilation by eg 1 second, and return a `Warning:` header with the response.
</comment><comment author="rmuir" created="2016-07-15T10:03:26Z" id="232911895">Wouldn't that just make things slower?
</comment><comment author="clintongormley" created="2016-07-15T11:03:18Z" id="232923463">Yes - that's the intention here.  To provide an incentive for users to not hardcode params (along with informing them why their query is slow).  The alternative is simply to throw an exception if the limit is breached.  Either way, limiting it to inline scripts without a params clause (even if no params are needed) limits the impact of this change I think
</comment><comment author="rmuir" created="2016-07-15T11:21:50Z" id="232927009">Well, my opinion: I want things to be fast, hence raising the whole discussion about the thing :)

I'm concerned that this warning will not be noticed (which is ok), but then we will make things even slower.

A lot of ES scripting is pretty damn slow, I'm not sure a 1 second delay is going to grab anyone's attention in the way we want.

I think its ok to just log a warning in the system logs and send your warning header too? Just as a start: nothing risky, we are just trying to grab your attention.
</comment><comment author="clintongormley" created="2016-07-15T11:33:03Z" id="232929307">Fair enough.  I have high hopes for these `Warning:` headers... just need to get them exposed in the clients
</comment><comment author="dakrone" created="2016-07-15T15:07:57Z" id="232977763">I agree with @rmuir, I think delaying and returning a warning are likely to be ignored. I'm in favor of a hard limit, throw a helpful exception when it's exceeded that forces a user to take action, whether that action is increasing the breaker limit, or switching the way that scripts are sent.
</comment><comment author="s1monw" created="2016-07-22T09:54:19Z" id="234503830">@dakrone are you working on this? I removed discuss and added adopt me, feel free to change or assign
</comment><comment author="dakrone" created="2016-07-22T18:00:47Z" id="234613160">@s1monw I'm currently working on the breaker for buckets and the translog tool, but I'm planning to work on this after those are done. I'll leave this in "adoptme" until then in case someone wants to work on it before I get to it though.
</comment><comment author="nik9000" created="2016-08-15T14:08:30Z" id="239810906">I think this was solved in #19694 (targeting 5.0). It creates a breaker that'll trip and fail the request quickly if you attempt to compile too many inline scripts. The steady-state limit is 15 scripts a minute. It'll fail if you compile 16 scripts really, really fast but after four seconds it'll let another one through, then another after four seconds, etc. It just won't ever let more than 15 through at a time no matter how many you've "save up" by not compiling a script every four seconds.

Anyway, that should make issues like this _super_ obvious.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Circuit break the number of inline scripts compiled per minute</comment></comments></commit></commits></item><item><title>Reindex from remote support multiple nodes and failover</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19395</link><project id="" key="" /><description>Currently, reindex from remote allows to configure one single host to send http requests to. In case of failure, there are cases (e.g. 429 response code for search) where we schedule a retry for the same request against the same node. We also have a backoff policy so that bulk failures get retried too.

I wonder if it makes sense to allow reindex from remote to either use the Sniffer and send requests to multiple nodes, or allow to configure multiple nodes, or both. In case multiple nodes are available, it makes much more sense to let the `RestClient` do the retries on a different node, then the current custom retry code would be less needed I think.

This issue is to discuss what makes sense to do in reindex from remote and whether we need to change anything there.
</description><key id="165152266">19395</key><summary>Reindex from remote support multiple nodes and failover</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Reindex API</label><label>adoptme</label><label>enhancement</label></labels><created>2016-07-12T18:50:13Z</created><updated>2016-08-01T15:40:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-12T18:50:46Z" id="232143066">@nik9000 can you double check that what I said in the description is true and eventually correct me please?
</comment><comment author="javanna" created="2016-07-12T19:38:59Z" id="232155932">Maybe also @elastic/es-clients would like to have a look. It is the first time that we have something in es that sends http requests, although this is more of a background process than an interactive thing, reason why maybe scheduling retries does make sense, for the sake of not making a long running operation (reindex) fail.
</comment><comment author="polyfractal" created="2016-07-12T19:51:21Z" id="232159249">Hm, I don't see any reason why it _wouldn't_ be a good idea to allow retrying against multiple nodes and/or sniffing the cluster state?  

Seems like it would help make reindex more robust without really introducing much extra complexity, just round-robin between the known nodes like the clients do.  It can keep the same backoff policy per-node, and only kill the job if all nodes are "marked dead" and their backoff timer isn't up?
</comment><comment author="HonzaKral" created="2016-07-12T20:12:24Z" id="232165241">Agreed with Zach - it really should just round robin etc like regular clients, like the documented best practices. Including passing in multiple nodes via initial configuration and specifying custom timeout (indexing across data centers, slow network, weak cluster, ...). Also to follow same retry logic as the clients - retrying on `502`, `503` and `504` which is the current concensus.

We just need to make sure that anything that relies on environment (sniffing that won't work on nodes behind load balancer to give an example) can be turned off and is probably off by default.
</comment><comment author="nik9000" created="2016-07-12T22:33:37Z" id="232202516">From a dependencies perspective I'd prefer not to sniff. I'd like to let
the user specify the hosts in the request.

As far as retries go, reindex only retries on too many requests right now.
We can change it but I don't want reindex from remote to drift too far from
normal reindex.
On Jul 12, 2016 4:12 PM, "Honza Král" notifications@github.com wrote:

&gt; Agreed with Zach - it really should just round robin etc like regular
&gt; clients, like the documented best practices. Including passing in multiple
&gt; nodes via initial configuration and specifying custom timeout (indexing
&gt; across data centers, slow network, weak cluster, ...). Also to follow same
&gt; retry logic as the clients - retrying on 502, 503 and 504 which is the
&gt; current concensus.
&gt; 
&gt; We just need to make sure that anything that relies on environment
&gt; (sniffing that won't work on nodes behind load balancer to give an example)
&gt; can be turned off and is probably off by default.
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19395#issuecomment-232165241,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLogw8EBUzgAxMJrkRygLoyM-mmT-rks5qU_UtgaJpZM4JKtDR
&gt; .
</comment><comment author="Mpdreamz" created="2016-07-13T11:08:08Z" id="232325003">Echoing my agreement with zach :+1: 

I do agree a seperate backoff retry mechanism is also neccessary, whereas the clients are meant to exhaust retries fast (fail fast) in the case of reindex a builtin backoff could really help users.

Why not expose these choices to the user more explicitly?

``` json
hosts: ["x", "y"]
mode: "static|sniff"
backoff_interval_max: "5m",
backoff_interval: "1m"
```

`sniff` being default.
</comment><comment author="javanna" created="2016-07-13T11:58:11Z" id="232333968">I feel like I need to clarify something: what I called backoff in the description of the issue, is to prevent retrying the same request on the same node forever, which is what reindex from remote currently does. Moving to the ordinary rest client behaviour would mean that a request would never be retried on the same node where it already failed (unless reindex schedules a retry like it currently does), but it would use load balancing and fail-over that come for free with the rest client. About configuring the backoff in the rest client, which is what Martijn was referring to , about when to send again a request to a node that previously failed, that configuration is not exposed to users at the moment in the java RestClient. I feel like this last bit would be part of a different discussion, but it seems like everybody agrees that reindex from remote shouldn't do too much magic under the hood.

@nik9000 maybe you can bring up the reasons why reindex currently retries when it gets back 429 rather than failing the reindex as a whole?
</comment><comment author="nik9000" created="2016-07-13T12:19:00Z" id="232338033">&gt; @nik9000 maybe you can bring up the reasons why reindex currently retries when it gets back 429 rather than failing the reindex as a whole?

The idea is to prevent failure when the cluster is overloaded. Reindex can stand to wait around for a bit if the cluster doesn't have room to add it to the queue. It does for bulk as well. It is part of reindex behaving like a management tool more than an interactive tool.

I'd love for users to be able to configure multiple hosts. We'll run them all through the whitelist, fail if they configure one that is forbidden, and then let the client round robin between them. That'll way we'll be able to handle if the node goes down. I mean, it won't be super pretty if one of the nodes holding the scroll context goes down, but at least we can handle the coordinating node going down.

I still don't like the idea of sniffing for reindex-from-remote. I think the nodes should be explicit so we don't surprise anyone. I'm not totally sure how sniffing would work with the whitelist.
</comment><comment author="javanna" created="2016-07-13T12:25:14Z" id="232339378">I guess my last questions is: if we had multiple nodes, could we remove the manual retry on 429? I guess not cause for 4xx errors the client just returns them, no retry attempts on other nodes, and we couldn't manually retry on another host, cause you don't get to decide which host you send a request to.
</comment><comment author="Mpdreamz" created="2016-07-13T12:29:17Z" id="232340265">We could not, thats what I meant. Theres two types of retries the one the client does (to provide failover if the node goes down), and a reindex specific backoff on 429 which makes a lot of sense to me too. 

I can imagine sniffing being tricky with whitelists but we can still allow users to specify multiple hosts using a static connection pool and benefit from the clients failover capabilities.
</comment><comment author="clintongormley" created="2016-07-13T19:20:37Z" id="232459191">&gt; I still don't like the idea of sniffing for reindex-from-remote. I think the nodes should be explicit so we don't surprise anyone. I'm not totally sure how sniffing would work with the whitelist.

I think that we should at least take multiple nodes, like we do with the client.  I also think that sniffing would make this easier to configure than specifying multiple nodes -- you just specify one and get round-robining automatically.  And I'm still not convinced that we need a whitelist for reindex-from-remote.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Circuit break on aggregation bucket numbers with request breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19394</link><project id="" key="" /><description>This adds new circuit breaking with the "request" breaker, which adds
circuit breaks based on the number of buckets created during
aggregations. It consists of incrementing during `AggregatorBase` creation

This also bumps the `REQUEST` breaker to 60% of the JVM heap now.

The output when circuit breaking an aggregation looks like:

``` json
{
  "shard" : 0,
  "index" : "i",
  "node" : "a5AvjUn_TKeTNYl0FyBW2g",
  "reason" : {
    "type" : "exception",
    "reason" : "java.util.concurrent.ExecutionException: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: CircuitBreakingException[[request] Data too large, data for [&lt;agg [myagg]&gt;] would be larger than limit of [104857600/100mb]];",
    "caused_by" : {
      "type" : "execution_exception",
      "reason" : "QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: CircuitBreakingException[[request] Data too large, data for [&lt;agg [myagg]&gt;] would be larger than limit of [104857600/100mb]];",
      "caused_by" : {
        "type" : "circuit_breaking_exception",
        "reason" : "[request] Data too large, data for [&lt;agg [myagg]&gt;] would be larger than limit of [104857600/100mb]",
        "bytes_wanted" : 104860781,
        "bytes_limit" : 104857600
      }
    }
  }
}
```

Relates to #14046
</description><key id="165151642">19394</key><summary>Circuit break on aggregation bucket numbers with request breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>feature</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T18:47:12Z</created><updated>2016-07-25T20:01:09Z</updated><resolved>2016-07-25T20:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-12T18:48:48Z" id="232142518">So I marked this as "discuss" because I need assistance in determining what the "weight" of a bucket should be. Currently I have it set to 256 bytes by default, but I don't know of a way to measure the actual memory overhead for buckets with the JVM. Any help or suggestions would be appreciated!
</comment><comment author="nik9000" created="2016-07-12T19:10:53Z" id="232148651">Iirc there is an Accountable interface in lucene that might make sense for
InternalAggregation.Bucket to implement. I dunno if it does. That doesn't
answer the weight question other than that it might make sense to look at
other implementers of that method.
On Jul 12, 2016 2:48 PM, "Lee Hinman" notifications@github.com wrote:

&gt; So I marked this as "discuss" because I need assistance in determining
&gt; what the "weight" of a bucket should be. Currently I have it set to 256
&gt; bytes by default, but I don't know of a way to measure the actual memory
&gt; overhead for buckets with the JVM. Any help or suggestions would be
&gt; appreciated!
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19394#issuecomment-232142518,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLol_ViwJ4GFcVC7J8GYXhjzPWVp_oks5qU-GVgaJpZM4JKs4Q
&gt; .
</comment><comment author="jpountz" created="2016-07-13T16:00:00Z" id="232401885">I like the part in AggregatorBase which is very contained and would probably help catch the aggregations that cause us most harm these days. However I would vote to drop the changes on BucketsAggregator since it duplicates what we are doing with the big arrays in a less efficient ways since big arrays know about what is exactly allocated while BucketsAggregator only knows about the number of buckets (for instance the data structures might exponentially oversize to be prepared for future growth).
</comment><comment author="dakrone" created="2016-07-13T21:05:12Z" id="232486433">@jpountz thanks for taking a look! I removed the breaking in `BucketsAggregator` in favor of just using the `AggregatorBase` breaking. I also did a bit more testing with this, indexing 5 documents with a `text` field with fielddata enabled containing some lorem ipsum text (see: https://writequit.org/org/es/design/circuit-break-aggs.html#h:0393981a-7bea-46f0-8e7e-f84834d22315), set the bucket breaker to 99% of the JVM's default 2gb heap, and then did [an aggregation like this](https://writequit.org/org/es/design/circuit-break-aggs.html#h:1ffc4bb0-22c9-4531-99b7-41db73961be6):

``` json
POST /test/_search?pretty
{
  "size": 0,
  "aggs": {
    "foo": {
      "terms": {
        "field": "body",
        "size": 2147483647
      },
      "aggs": {
        "bar": {
          "terms": {
            "field": "body",
            "size": 2147483647
          },
          "aggs": {
            "baz": {
              "terms": {
                "field": "body",
                "size": 2147483647
              }
            }
          }
        }
      }
    }
  }
}
```

With a bucket weight of 90kb per bucket, the JVM gets an OOME, when I increased it to 95kb it circuit beaks before going out of memory. So I decided to make the bucket weight an even 100kb for safety.

Let me know what you think now, whether this is an accurate representation of what this should prevent, or whether I should [adjust the tests ](https://writequit.org/org/es/design/circuit-break-aggs.html)accordingly.
</comment><comment author="jpountz" created="2016-07-18T07:44:09Z" id="233256133">That is interesting, I would have expected much less than 90kb per bucket. I am wondering that the memory might go into something different than buckets, maybe the generation of response objects. Maybe a better test to compute the size of buckets would be to run the following:

``` json
POST /test/_search?pretty
{
  "size": 0,
  "aggs": {
    "foo": {
      "terms": {
        "field": "body",
        "size": 1,
        "collect_mode": "depth_first"
      },
      "aggs": {
        "bar": {
          "terms": {
            "field": "body",
            "size": 1,
            "collect_mode": "depth_first"
          },
          "aggs": {
            "baz": {
              "terms": {
                "field": "body",
                "size": 1,
                "collect_mode": "depth_first"
              }
            }
          }
        }
      }
    }
  }
}
```

This way we still force elasticsearch to compute the buckets internally on the shards, but each aggregation object returns a response that only wraps a single bucket.
</comment><comment author="dakrone" created="2016-07-19T15:57:44Z" id="233679550">@jpountz thanks for the info! I'll retest with this and try to get a more accurate count for the size
</comment><comment author="dakrone" created="2016-07-19T22:15:24Z" id="233783180">Thanks for the info @jpountz, I did some more testing with a variant of the query you posted (otherwise it wouldn't OOME):

``` json
POST /test/_search?pretty
{
  "size": 0,
  "aggs": {
    "foo": {
      "terms": {
        "field": "body",
        "size": 1,
        "collect_mode": "depth_first"
      },
      "aggs": {
        "bar": {
          "terms": {
            "field": "body",
            "size": 1,
            "collect_mode": "depth_first"
          },
          "aggs": {
            "baz": {
              "terms": {
                "field": "body",
                "size": 1,
                "collect_mode": "depth_first"
              },
              "aggs": {
                "eggplant": {
                  "terms": {
                    "field": "body",
                    "size": 1,
                    "collect_mode": "depth_first"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

With this, I was able to reduce the bucket weight to 5kb, at 4kb the 2g ES node would go out of memory, and at 5kb it prevented an OOME when the breaker is set to `99.9%`
</comment><comment author="jpountz" created="2016-07-20T07:14:32Z" id="233862407">I still have concerns about using a dedicated breaker for this, otherwise +1.
</comment><comment author="dakrone" created="2016-07-20T16:25:15Z" id="234002601">Thanks @jpountz, I still think it would be better to leave this as a dedicated breaker for now, if only so the `overhead` can be tweaked in the event that the 5kb weight is incorrect for other aggregations. Other than that, I'll add some docs for this and then push (I assume your `+1` is equivalent to a LGTM?)
</comment><comment author="jpountz" created="2016-07-20T23:14:32Z" id="234112357">&gt; the overhead can be tweaked in the event that the 5kb weight is incorrect for other aggregations

These settings are so expert, I don't think any user would think about modifying this parameter to fix the problem. We need simpler escape hatches, something like disabling the breaker that proves buggy, or even disabling breaking entirely.

Another reason why I don't like the new breaker is that the first request you used to compute the bucket size highlighted the fact that we need to account memory that is used for the buckets that are created for the shard responses of aggregations. Should we add a different breaker for it too? I dont think so: this does not scale, we cannot afford to have a breaker for every single way that Elasticsearch may allocate memory when executing search requests.

Sorry but I am -1 on having a dedicated breaker for buckets.
</comment><comment author="dakrone" created="2016-07-20T23:17:56Z" id="234112919">&gt; something like disabling the breaker that proves buggy, or even disabling breaking entirely

That's totally possible:

``` json
PUT /_cluster/settings
{
  "transient": {
    "indices.breaker.bucket.type": "noop"
  }
}
```

&gt; we cannot afford to have a breaker for every single way that Elasticsearch may allocate memory when executing search requests

Okay, but by separating out the breakers into separate breakers, we allow someone (or us) to turn off circuit breaking for a particular area without turning off breaking entirely. If I put this in with the `REQUESTS` breaker, it means that disabling the requests breaker in order to allow certain aggregations also means disabling the breaker used for `BigArrays` for large cardinality aggregations.
</comment><comment author="jpountz" created="2016-07-20T23:31:49Z" id="234115321">&gt; it means that disabling the requests breaker in order to allow certain aggregations also means disabling the breaker used for BigArrays for large cardinality aggregations.

I am fine with that: I would rather like to keep things simple than have the ability to disable individual breakers.
</comment><comment author="dakrone" created="2016-07-20T23:36:02Z" id="234116043">&gt; I am fine with that: I would rather like to keep things simple than have the ability to disable individual breakers.

Okay, should we remove the in-flight and fielddata breakers also and just have a single breaker?
</comment><comment author="jpountz" created="2016-07-21T07:29:25Z" id="234178190">I think these ones are a bit more useful since they have a different lifecycle. So for instance the fact we have different breakers allows to keep running search requests even if the fielddata memory pool is full. But maybe you're right and we should remove them indeed since they force us to make assumptions about how clusters are going to be used. For instance now that fielddata is the exception rather than the norm, is it still right to only give 40% for requests out of the 70% that the global circuit breaker has in total?
</comment><comment author="dakrone" created="2016-07-21T17:41:17Z" id="234328283">@jpountz okay, I pushed a commit to fold this all in to the `REQUEST` breaker (no new bucket breaker) and increase the limit of the request breaker to 60%
</comment><comment author="jpountz" created="2016-07-21T22:02:00Z" id="234397740">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Mark combinatorial explosion in aggs as 'done'</comment></comments></commit></commits></item><item><title>fix a typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19393</link><project id="" key="" /><description>an --&gt; and
</description><key id="165114204">19393</key><summary>fix a typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">speedplane</reporter><labels><label>docs</label></labels><created>2016-07-12T15:47:56Z</created><updated>2016-07-12T20:32:43Z</updated><resolved>2016-07-12T20:32:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-12T20:30:07Z" id="232170528">Thanks @speedplane. I'm going to merge this into master and backport.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fix typo in cluster module docs</comment></comments></commit></commits></item><item><title>Modularize netty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19392</link><project id="" key="" /><description>This PR moves all netty related code into `modules/transport-netty` the module is build as a zip file as well as a JAR to serve as a dependency for transport client. For the time being this is required otherwise we have no network based impl. for transport client users. This might be subject to change given that we move forward http client. 
</description><key id="165097727">19392</key><summary>Modularize netty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>build</label><label>enhancement</label><label>PITA</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T14:41:43Z</created><updated>2016-07-13T07:52:03Z</updated><resolved>2016-07-13T07:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-12T15:00:27Z" id="232075260">I'm good with it. Punch it.
</comment><comment author="rmuir" created="2016-07-12T15:28:44Z" id="232084721">looks great. I left one suggestion, it should be possible to consolidate the security code to a single place (and it may make it more robust since then its not dependent on class initialization order). We can do it as a followup too.
</comment><comment author="rmuir" created="2016-07-12T15:43:38Z" id="232089605">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/transport/TransportServiceHandshakeTests.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeIT.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/RetryTests.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/ESHttpResponseEncoder.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/HttpRequestHandler.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/cors/CorsConfig.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/cors/CorsConfigBuilder.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandler.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/pipelining/OrderedDownstreamChannelEvent.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/http/netty/pipelining/OrderedUpstreamMessageEvent.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/NettyPlugin.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/ChannelBufferBytesReference.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/ChannelBufferStreamInput.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/NettyInternalESLogger.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/NettyMessageChannelHandler.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/NettyUtils.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/OpenChannelsHandler.java</file><file>modules/transport-netty/src/main/java/org/elasticsearch/transport/netty/SizeHeaderFrameDecoder.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/ESNettyIntegTestCase.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpClient.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpPublishPortTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpRequestSizeLimitIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyHttpServerTransportTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/ChannelBufferBytesReferenceTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyTransportPublishAddressIT.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/NettyUtilsTests.java</file><file>modules/transport-netty/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryClusterFormationTests.java</file><file>qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsNotSetIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsRegexIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DeprecationHttpIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsDisabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsEnabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpSmokeTestCase.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ResponseHeaderPluginIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/TestDeprecatedQueryBuilder.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/TestDeprecationHeaderRestAction.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/TestDeprecationPlugin.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/TestResponseHeaderPlugin.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/TestResponseHeaderRestAction.java</file><file>test/framework/src/main/java/org/elasticsearch/common/bytes/AbstractBytesReferenceTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/NodeConfigurationSource.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Merge pull request #19392 from elastic/modularize_netty</comment></comments></commit></commits></item><item><title>Cleanup Elasticsearch configuration files support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19391</link><project id="" key="" /><description>Today Elasticsearch can be configured via YAML, JSON, or properties files. The support for properties files is undocumented and should be immediately removed. The support for JSON files should be deprecated and removed in 6.0.0.
- [x] remove support for .properties #19398
- [ ] deprecate support for .json in 2.4.0
- [ ] remove support for .json in 5.0.0
- [ ] deprecate support .yml in 2.4.0
- [ ] fail hard if .yml exists in 5.0.0
- [ ] remove check for existence of .yml in 6.0.0
</description><key id="165093119">19391</key><summary>Cleanup Elasticsearch configuration files support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>adoptme</label><label>low hanging fruit</label><label>Meta</label></labels><created>2016-07-12T14:23:44Z</created><updated>2017-05-15T00:45:05Z</updated><resolved>2017-05-15T00:45:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-12T15:53:48Z" id="232092669">We also have the weird case of supporting both `elasticsearch.yml` and `elasticsearch.yaml`. The former is the original name, and the latter was added because `.yaml` is the official extension (according to yaml.org).  While I understand that `.yml` has been around much longer, I think we should make our single configuration file name use `.yaml`. In addition to it being the official extension, I have also been bit before when writing rest tests and accidentally using `.yml`, because I had just been editing an `elasticsearch.yml` file.

In all 3 cases (.yml, json and properties), I think we can do this cleanly for the user by looking for the older file names on startup, and throwing an error. In 2.0, we already switched to failing if there was more than one file available. 
</comment><comment author="jasontedor" created="2016-07-12T15:55:04Z" id="232093013">&gt; While I understand that `.yml` has been around much longer, I think we should make our single configuration file name use `.yaml`.

+1
</comment><comment author="jasontedor" created="2016-07-14T12:23:39Z" id="232650428">Relates #9706
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add migration note about .yaml and .json removal (#24689)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/node/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/node/InternalSettingsPreparerTests.java</file></files><comments><comment>Settings: Remove support for yaml and json config files (#24664)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file></files><comments><comment>Settings: Update settings deprecation from yml to yaml (#24663)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/node/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/node/InternalSettingsPreparerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Settings: Deprecate settings in .yml and .json (#24059)</comment></comments></commit></commits></item><item><title>Deprecate and eventually remove template query?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19390</link><project id="" key="" /><description>The search template api can template the entire search request, so I think that should be preferred over the `template` query. I can't think of a good templating use case right now that only the `template` query can be used, so lets deprecate and remove this query?
</description><key id="165091016">19390</key><summary>Deprecate and eventually remove template query?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Query DSL</label></labels><created>2016-07-12T14:14:41Z</created><updated>2016-07-27T07:51:20Z</updated><resolved>2016-07-27T07:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-07-12T19:42:09Z" id="232156804">++

It's also confusing because the syntax is only subtly different from the template search request.  I was just helping a user on the forums who was confused why their `"size"` parameter wasn't being applied.  Turns out they were using a template query instead of template search request on accident.
</comment><comment author="s1monw" created="2016-07-12T21:03:44Z" id="232180615">I wonder if that has an impact on https://github.com/elastic/elasticsearch/issues/19195 which might eventually want to run template queries rather than template requests? for the metrics to evaluate queries we want to be in charge of the search request the user should only specify the query?
</comment><comment author="martijnvg" created="2016-07-13T08:17:28Z" id="232288564">@s1monw I would expect that the ranking evaluation API is supposed to work with actual concrete user queries rather than templated queries? I can imagine that if templating is wanted then maybe also the set of search results should be templatable (or just the entire request)? 

If templating is required for other APIs too then we can think of a way to support this in general? For example maybe if the url path ends with `_template` the requests gets redirected to the template api first, templating happens and then the request is redirected to the target API. 

Today the validate query and the explain APIs are the only APIs that allow specifying a query and these APIs would then lose the ability to template if `template` query were to be removed. I think that this fine, because these are debug APIs. In the case a query needs to be explained that sits in file or stored template then one can use the template render api to resolve that, extract the query bit from the rendered request and then execute the explain api.
</comment><comment author="s1monw" created="2016-07-13T08:24:51Z" id="232290171">&gt; @s1monw I would expect that the ranking evaluation API is supposed to work with actual concrete user queries rather than templated queries? I can imagine that if templating is wanted then maybe also the set of search results should be templatable (or just the entire request)?

that is not true - usually these APIs work with the user input and a template - if you bind your evaluation to a specific way how to query the API is useless. You use that for experiments so you just wanna change the template not the input data for the query. its the same reason why logging queries is useless, all you need is the data to build the query ie. the user input from the application layer
</comment><comment author="martijnvg" created="2016-07-13T08:49:06Z" id="232295653">Thanks for explaining. I didn't exactly know how the evaluation API should work. So just to be sure the ranking evaluation request will hold a template that is used by all the specified queries to be evaluated? 
</comment><comment author="s1monw" created="2016-07-13T09:15:26Z" id="232301623">I want @MaineC and @cbuescher to comment here too before we make a move
</comment><comment author="clintongormley" created="2016-07-13T19:11:03Z" id="232456624">&gt; You use that for experiments so you just wanna change the template not the input data for the query. 

This would still be feasible with a search template rather than a query template.  We could evaluate the search template first, then make any changes we wanted to make on top of that, no?
</comment><comment author="s1monw" created="2016-07-14T09:01:38Z" id="232608034">&gt; This would still be feasible with a search template rather than a query template. We could evaluate the search template first, then make any changes we wanted to make on top of that, no?

I think this is a huge hassle, we would need to embed it ourself. we can try this but I want this to be  tested before we delete this feature.
</comment><comment author="s1monw" created="2016-07-22T09:59:45Z" id="234504927">@martijnvg just explained me that we use could just use a script to evaluate to a string instead of a template query inside a search template, similar to what suggest collate does. I think that makes sense so I think we can just move forward here. +1
</comment><comment author="MaineC" created="2016-07-25T09:25:53Z" id="234904355">Sounds like a good plan to me (with the caveat that I don't know exactly how suggest collate works).
</comment><comment author="martijnvg" created="2016-07-26T14:21:10Z" id="235282576">@MaineC It is basically a query that runs to verify if a candidate suggestions actually yields documents. The collate query can be specified as mustache script and the candidate suggestion is available here as a variable. Makes sense?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/TemplateQueryBuilder.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/SearchTemplateIT.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/TemplateQueryBuilderTests.java</file></files><comments><comment>Removed deprecated template query.</comment></comments></commit><commit><files><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/TemplateQueryBuilder.java</file></files><comments><comment>Deprecate template query.</comment></comments></commit></commits></item><item><title>New type always adds a _parent#null field if no _parent mapping specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19389</link><project id="" key="" /><description>```
PUT test 
{
  "mappings": {
    "type": {

    }
  }
}

GET test/_mapping/field/*
```

results in:

```
...
        "_parent#null": {
          "full_name": "_parent#null",
          "mapping": {}
        }
...
```
</description><key id="165083522">19389</key><summary>New type always adds a _parent#null field if no _parent mapping specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T13:42:47Z</created><updated>2016-07-18T15:41:23Z</updated><resolved>2016-07-18T14:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-07-12T13:50:48Z" id="232053283">Pushed a test for that too since I had one already.
</comment><comment author="martijnvg" created="2016-07-18T08:31:20Z" id="233269280">I was concerned that the ParentFieldMapper also adds a `_parent#null` doc values field in this case. Luckily this is not the case. The `_parent#null` field only exists in the mapping, but it isn't doing anything.
</comment><comment author="jpountz" created="2016-07-18T12:34:54Z" id="233315938">@martijnvg was the `_parent#null` field only visible with the get field mapping API, or also with the mappings API?
</comment><comment author="clintongormley" created="2016-07-18T13:59:35Z" id="233336401">Only with the get-field-mapping API.  Doesn't that mean that it is a real Lucene field?
</comment><comment author="jpountz" created="2016-07-18T14:31:59Z" id="233345968">No, it only means that this field can be looked up by name in the mappings.
</comment><comment author="martijnvg" created="2016-07-18T15:41:23Z" id="233367575">@jpountz Yes, only the get-field-mapping api. Also I verified that no such field was actually added to the Lucene index.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file></files><comments><comment>parent/child: Make sure that no `_parent#null` gets introduces as default _parent mapping.</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file></files><comments><comment>Add test for #19389 new type always creates _parent#null field</comment></comments></commit></commits></item><item><title>Remove content detection mechanism from REST endpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19388</link><project id="" key="" /><description>Today, some Elasticsearch REST endpoints (e.g., update index settings API) attempt to determine the content type (e.g., JSON vs. YAML) of the request body with a [rudimentary detection mechanism](https://github.com/elastic/elasticsearch/blob/f6aec3fdb53d7e3d7693393c25d5c3012efecb07/core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoaderFactory.java#L59-L80). This mechanism breaks down on YAML bodies that contain braces because the content is inappropriately detected as JSON. Instead, this mechanism should not be used, Elasticsearch should default to JSON for all request bodies, and possibly obey the `Content-Type` header for allowing YAML.

Relates #19366
</description><key id="165076519">19388</key><summary>Remove content detection mechanism from REST endpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:REST</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2016-07-12T13:11:48Z</created><updated>2017-04-05T13:38:53Z</updated><resolved>2017-04-05T13:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-12T13:41:16Z" id="232050517">I was under the impression that we already obey to the `Content-Type` header, and it's a bug if some apis don't. Seems like we do things differently only for apis that handle settings. I noticed that we support "properties" format for settings, which may be the reason behind this weird choice... wondering who uses/remembers that we support properties format there.
</comment><comment author="jasontedor" created="2016-07-12T13:52:29Z" id="232053733">&gt; I was under the impression that we already obey to the `Content-Type` header, and it's a bug if some apis don't.

Sadly we do not always. :cry:

&gt; I noticed that we support "properties" format for settings, which may be the reason behind this weird choice... wondering who uses/remembers that we support properties format there.

I think that should be removed as part of closing this issue. :smile:
</comment><comment author="javanna" created="2016-07-12T13:54:37Z" id="232054329">&gt; Sadly we do not.

It seems like we do apart from where we accept settings in the body? Or are there other REST endpoints where we try and detect content type?

&gt; I think that should be removed as part of closing this issue.

Sure, why not. Maybe it should be brought up before it gets removed though :)
</comment><comment author="clintongormley" created="2016-07-12T14:08:24Z" id="232058545">&gt; I noticed that we support "properties" format for settings, which may be the reason behind this weird choice... wondering who uses/remembers that we support properties format there.

Not documented. Please rip it out!
</comment><comment author="javanna" created="2016-07-12T14:16:27Z" id="232060986">&gt; Not documented. Please rip it out!

Perfect that makes it a no-brainer, with properties out of the way it makes no sense to do any auto-detection I think. We should do for settings what we already do for any other request I believe.
</comment><comment author="jasontedor" created="2016-07-12T14:24:26Z" id="232063479">We can completely remove support for properties. I opened #19391.
</comment><comment author="anti-social" created="2016-07-13T12:49:00Z" id="232344674">If you removed `---` requirement YAML parser could parse JSON since YAML is a superset of JSON.
</comment><comment author="clintongormley" created="2016-07-13T19:28:47Z" id="232461221">To quote from https://metacpan.org/pod/JSON::XS#JSON-and-YAML

&gt; YAML has hardcoded limits on (simple) object key lengths that JSON doesn't have and also has different and incompatible unicode character escape syntax, so you should make sure that your hash keys are noticeably shorter than the 1024 "stream characters" YAML allows and that you do not have characters with codepoint values outside the Unicode BMP (basic multilingual page). YAML also does not allow \/ sequences in strings
</comment><comment author="popravich" created="2016-07-13T20:12:03Z" id="232472328">ok, but what about checking `Content-Type` header?
YAML format suits our flow the best -- its easier to read, write, add comments and maintain at all.
 It would be really great if you guys atleast keep current YAML support.
</comment><comment author="jasontedor" created="2016-07-13T20:20:07Z" id="232474449">@popravich That's the plan.
</comment><comment author="javanna" created="2016-07-13T20:38:04Z" id="232479258">I care to highlight that we already take into account the `Content-Type` header. I think the title of this issue is misleading as it makes people think that we guess the content type all the time, while we (used to) have this detection in place only for apis that accept settings in their request body. The general approach is to take `Content-Type` into account and that's always been the case apart from the mentioned exceptions.
</comment><comment author="jasontedor" created="2016-07-13T20:54:17Z" id="232483487">@javanna It's in the [description](https://github.com/elastic/elasticsearch/issues/19388#issue-165076519) of the issue that this is a problem only on certain endpoints.
</comment><comment author="javanna" created="2016-07-14T09:25:37Z" id="232613716">ok it appears that what I cared to highlight above was completely wrong. We don't read the `Content-Type` header, ever, sorry for the confusion. We rather auto-detect based on markers on all REST endpoints. Settings were indeed a special place as we supported also properties format there (now gone).

The reason for this is that we have the java api, which exposes a lot of `source` setters that allow to effectively provide the response body as a byte array. Those methods are also used from REST to transport where we convert from `RestRequest` to java api object, so no content type is ever taken into account.
</comment><comment author="bleskes" created="2016-07-14T09:45:53Z" id="232618446">I think the discussion should be split in two. First for the future where we will rely on http for all out of cluster communication and then second for what to do until then. As Luca noted, we don't do anything with the incoming content type at the moment.

For the future, I would suggest we move over to using content type headers exclusively and use JSON as default. That means we will need to extend our transport layer to carry that information. Maybe move from `ByteReference` to `XContentByteReference` and drop support for setting raw strings.

Until then we have two options - either keep things as is (but improve detection and make it uniform - i.e., not have one detection logic for settings and another for the rest) or change our transport client to require content type information. If we go for the latter, we have to do it on a major version. Since that is quite a big job for us and will case pain for users to upgrade to something that will be going away any way, my vote is to keep things the same for now and just solve the little issues we find. As soon as we can fully rely on http this will become simpler on it's own.
</comment><comment author="javanna" created="2016-07-14T10:03:27Z" id="232622196">One compromise could be to keep the current `source(BytesReference)` etc. methods (maybe deprecate them) and add new methods that accept the content-type as a second argument, so that the REST layer can call those instead and hand over the content-type header. Where we don't know, we would still auto-detect. This is a big job but it doesn't necessarily have to be a pain for users.

Looking at what we are moving towards to (http), having those new methods may actually end up helping users migrating as the content-type will always be required with the rest client when providing a request body. Not too sure about this but maybe worth considering.

Another option we would have is to copy the content-type header from REST layer to transport layer so that the info is accessible when actually parsing the bytes and can be taken into account if available. This is suboptimal as the transport client would not have that info unless the header is explicitly set for each request, which is something that I am not sure users would do. Would work within elasticsearch though without changing methods signatures, probably too sneaky though.
</comment><comment author="clintongormley" created="2016-07-15T09:38:12Z" id="232907000">Discussed in FixitFriday.  To fix the transport client we have to either:
- Require a content header
- Remove the ability to specify bytes
- Remove the transport client

All of these are big changes.  In the meantime, should we just fix the REST layer and require a content header for anything that is not JSON?
</comment><comment author="javanna" created="2016-07-15T11:53:35Z" id="232933175">Fixing the REST layer anyhow requires the transport layer to accept the content-type and carry it around wherever bytes can be provided. We agreed on FixItFriday that we want to fix the REST layer to not do auto-detection and rely on `Content-Type` instead. I think next step is to more closely evaluate what the impact is on the java api, and see if we can at least decrease the places where one can provide bytes in the java api, and add the content-type bit wherever providing bytes is needed.  It is a big job but it is required to fix the REST layer, regardless of future deprecation of transport client. 
</comment><comment author="javanna" created="2017-03-17T20:32:23Z" id="287463615">@jaymode should we close this? We have removed auto-detection from the REST layer, is there anything left to do here?</comment><comment author="jaymode" created="2017-04-05T13:38:52Z" id="291863964">@javanna you are correct. There is nothing left to do here.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest-high-level/src/test/java/org/elasticsearch/client/RestHighLevelClientTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/HeapBufferedAsyncResponseConsumerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/ResponseExceptionTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/storedscripts/PutStoredScriptRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/storedscripts/PutStoredScriptRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/RestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/tasks/TaskResultsService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkWithUpdatesIT.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataMappingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CamelCaseFieldNameTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CopyToMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DoubleIndexingDocTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicTemplatesTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/GenericStoreDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/PathMatchDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/UpdateMappingOnClusterIT.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/indices/state/OpenCloseIndexIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/rest/RestControllerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestRequestTests.java</file><file>core/src/test/java/org/elasticsearch/routing/PartitionedRoutingIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomIOExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/subphase/highlight/HighlighterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java</file><file>core/src/test/java/org/elasticsearch/search/slice/SearchSliceIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>distribution/integ-test-zip/src/test/java/org/elasticsearch/test/rest/WaitForRefreshAndCloseTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportUpdateByQueryAction.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/ReindexParentChildTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/remote/RemoteScrollableHitSourceTests.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/rest/Netty4HeadBodyIsEmptyIT.java</file><file>qa/backwards-5.0/src/test/java/org/elasticsearch/backwards/IndexingIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestClient.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestResponse.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestResponseException.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ESClientYamlSuiteTestCase.java</file></files><comments><comment>Enforce Content-Type requirement on the rest layer and remove deprecated methods (#23146)</comment></comments></commit><commit><files><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/transport/TransportClientBenchmark.java</file><file>client/client-benchmark-noop-api-plugin/src/main/java/org/elasticsearch/plugin/noop/action/bulk/NoopBulkRequestBuilder.java</file><file>client/client-benchmark-noop-api-plugin/src/main/java/org/elasticsearch/plugin/noop/action/bulk/RestNoopBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/storedscripts/PutStoredScriptRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/storedscripts/PutStoredScriptRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/byscroll/ClientScrollableHitSource.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/byscroll/ScrollableHitSource.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoaderFactory.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentType.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexingSlowLog.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SourceToParse.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/TermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java</file><file>core/src/main/java/org/elasticsearch/ingest/PipelineStore.java</file><file>core/src/main/java/org/elasticsearch/rest/AbstractRestChannel.java</file><file>core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java</file><file>core/src/main/java/org/elasticsearch/rest/RestChannel.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/RestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestPutStoredScriptAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestGetSourceAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptMetaData.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/StoredScriptSource.java</file><file>core/src/test/java/org/elasticsearch/action/IndicesRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/ListenerActionIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/storedscripts/PutStoredScriptRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/ShrinkIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/BWCTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorClusterSettingsIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestModifierTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkWithUpdatesIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/TransportBulkActionTookTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/byscroll/AsyncBulkByScrollActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/PutPipelineRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/WaitActiveShardCountIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file><file>core/src/test/java/org/elasticsearch/action/update/UpdateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsFilterTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/JsonSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/YamlSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java</file><file>core/src/test/java/org/elasticsearch/document/ShardInfoIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexRequestBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/index/IndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingDisabledTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/IdFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MultiFieldTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ParentFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/RoutingFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/SourceFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/UpdateMappingOnClusterIT.java</file><file>core/src/test/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/DateMathIndexExpressionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/analysis/AnalysisModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestProcessorNotInstalledOnAllNodesIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/SimpleRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestControllerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestRequestTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/admin/indices/RestAnalyzeActionTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestTableTests.java</file><file>core/src/test/java/org/elasticsearch/routing/AliasRoutingIT.java</file><file>core/src/test/java/org/elasticsearch/routing/SimpleRoutingIT.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptMetaDataTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/StoredScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/StoredScriptsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/child/ParentFieldLoadingIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/subphase/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/msearch/MultiSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/QueryStringIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java</file><file>core/src/test/java/org/elasticsearch/search/scroll/RestClearScrollActionTests.java</file><file>core/src/test/java/org/elasticsearch/search/scroll/RestSearchScrollActionTests.java</file><file>core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java</file><file>core/src/test/java/org/elasticsearch/search/simple/SimpleSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java</file><file>core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateNoopIT.java</file><file>distribution/integ-test-zip/src/test/java/org/elasticsearch/test/rest/CreatedLocationHeaderIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/StoredExpressionTests.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/RestPutSearchTemplateAction.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MultiSearchTemplateRequestTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/SearchTemplateIT.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolateQueryBuilder.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolateQueryBuilderTests.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorFieldMapperTests.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorQuerySearchIT.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportReindexAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteRequestBuilders.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteResponseParsers.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteScrollableHitSource.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/CancelTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/RestReindexActionTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/remote/RemoteRequestBuildersTests.java</file><file>modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpRequest.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpClient.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilterTests.java</file><file>qa/backwards-5.0/src/test/java/org/elasticsearch/backwards/IndexingIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsDisabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsEnabledIT.java</file><file>test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/XContentTestUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/FakeRestChannel.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/FakeRestRequest.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestClient.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestExecutionContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestResponse.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ClientYamlTestResponseException.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/yaml/ESClientYamlSuiteTestCase.java</file></files><comments><comment>Optionally require a valid content type for all rest requests with content (#22691)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file></files><comments><comment>Deprecate XContentType auto detection methods in XContentFactory (#22181)</comment></comments></commit></commits></item><item><title>Remove deprecated 1.x script and template syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19387</link><project id="" key="" /><description>PR for #13729
</description><key id="165056641">19387</key><summary>Remove deprecated 1.x script and template syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>breaking</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T11:30:31Z</created><updated>2016-07-21T14:30:46Z</updated><resolved>2016-07-13T13:32:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-12T12:25:39Z" id="232031828">@martijnvg I left some comments mainly on the tests but its looking good
</comment><comment author="colings86" created="2016-07-12T14:27:28Z" id="232064450">@martijnvg thanks for the quick update to this PR, I left a very minor comment but LGTM now
</comment><comment author="martijnvg" created="2016-07-13T13:32:36Z" id="232355726">Pushed via: https://github.com/elastic/elasticsearch/commit/2c3165d080c066cc11c2d1478df6aefff2700e90
</comment><comment author="Bargs" created="2016-07-21T14:18:21Z" id="234267635">@martijnvg do you know what version of ES this syntax was first deprecated in? This change hits Kibana and I'm trying to figure out how we (Kibana) could be more proactive about recognizing and fixing use of deprecated features. I checked alpha4 and I don't seem to get any deprecation logs for the old syntax there.
</comment><comment author="colings86" created="2016-07-21T14:20:08Z" id="234268210">The old Script API was deprecated from 2.0.0 although I don't think we had the deprecation logger at that point so it might have been in one of the minor version that the logging of the deprecation was enabled. It was in the release notes though, I'll try to find the link to it
</comment><comment author="colings86" created="2016-07-21T14:30:46Z" id="234271642">This is the PR that introduced the new script API in 2.0.0-beta1: https://github.com/elastic/elasticsearch/pull/11164

and this is a link to the relevant section in the breaking changes document:
https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_scripting_changes.html

Hope that helps
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature request] limit the size of scroll search </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19386</link><project id="" key="" /><description>ES 2.1
**Describe the feature**:
below query still get response:
                MatchAllQueryBuilder maq = QueryBuilders.matchAllQuery();
        SearchResponse sResponse = client.prepareSearch(indexName)
                .setSearchType(SearchType.SCAN)
                .setQuery(maq)
                .setSize(20000)
                .execute()
                .actionGet();

better limit size for scroll search type the same way for search with max result window
</description><key id="165029788">19386</key><summary>[feature request] limit the size of scroll search </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-07-12T09:12:38Z</created><updated>2016-07-12T15:04:15Z</updated><resolved>2016-07-12T10:25:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T10:25:39Z" id="231999994">Duplicate of #19249 and fixed by https://github.com/elastic/elasticsearch/pull/19367
</comment><comment author="makeyang" created="2016-07-12T10:29:22Z" id="232000796">won't backport to 2.1?
</comment><comment author="jasontedor" created="2016-07-12T15:04:15Z" id="232076494">&gt; won't backport to 2.1?

Nothing will be backported to 2.1, there will be no additional releases of the 2.1.x line.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make NotMasterException a first class citizen</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19385</link><project id="" key="" /><description>That exception is currently serialized as its current base class IllegalStateException which confuses code supposed to deal with the stepping down of a master. This is an important exception and we should be able to serialize it correctly. This commit fixes it by moving the exception to inherit from ElasticsearchException and properly register it.

As a bonus I adapted CapturingTransport to properly simulate serialized exceptions.

See https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=debian/751 for an example failure
</description><key id="165028072">19385</key><summary>Make NotMasterException a first class citizen</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T09:03:39Z</created><updated>2016-07-12T10:44:41Z</updated><resolved>2016-07-12T10:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-12T09:05:33Z" id="231981032">LGTM
</comment><comment author="jasontedor" created="2016-07-12T09:36:52Z" id="231988700">LGTM. 
</comment><comment author="bleskes" created="2016-07-12T10:10:29Z" id="231996696">@danielmitterdorfer @jasontedor thx. I pushed two minor fixes to tests.
</comment><comment author="jasontedor" created="2016-07-12T10:16:54Z" id="231998058">&gt; thx. I pushed two minor fixes to tests.

Still LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/cluster/NotMasterException.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/CapturingTransport.java</file></files><comments><comment>Make NotMasterException a first class citizen (#19385)</comment></comments></commit></commits></item><item><title>org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT#testIndexRollingUpgrade fails sporadically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19384</link><project id="" key="" /><description>Example build: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.3+multijob-os-compatibility/os=ubuntu/181/

Exception trace:

```
java.lang.AssertionError: There are still 1 in-flight contexts
    at org.elasticsearch.search.MockSearchService.assertNoInFLightContext(MockSearchService.java:65)
    at org.elasticsearch.test.ESTestCase$2.run(ESTestCase.java:188)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:405)
    at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:369)
    at org.elasticsearch.test.ESTestCase.ensureAllSearchContextsReleased(ESTestCase.java:185)
    at org.elasticsearch.test.ESIntegTestCase.after(ESIntegTestCase.java:1959)
[...]
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
    Suppressed: java.lang.AssertionError: There are still 1 in-flight contexts
        at org.elasticsearch.search.MockSearchService.assertNoInFLightContext(MockSearchService.java:65)
        at org.elasticsearch.test.ESTestCase$2.run(ESTestCase.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:394)
        ... 38 more
    Caused by: java.lang.RuntimeException
        at org.elasticsearch.search.MockSearchService.putContext(MockSearchService.java:80)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:623)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:371)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        ... 1 more
    Suppressed: java.lang.AssertionError: There are still 1 in-flight contexts
        at org.elasticsearch.search.MockSearchService.assertNoInFLightContext(MockSearchService.java:65)
        at org.elasticsearch.test.ESTestCase$2.run(ESTestCase.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:394)
        ... 38 more
    [CIRCULAR REFERENCE:java.lang.RuntimeException]
    Suppressed: java.lang.AssertionError: There are still 1 in-flight contexts
        at org.elasticsearch.search.MockSearchService.assertNoInFLightContext(MockSearchService.java:65)
        at org.elasticsearch.test.ESTestCase$2.run(ESTestCase.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:394)
        ... 38 more
    [CIRCULAR REFERENCE:java.lang.RuntimeException]
[...]
```

Reproduces with `mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:2.2 -Dtests.seed=5A271C4709F94E84 -Dtests.class=org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT -Dtests.method="testIndexRollingUpgrade" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=es-CO -Dtests.timezone=Europe/Budapest -Dskip.integ.tests=false -Dtests.iters=1000`

Note: The circular reference is caused by the `ESTestCase.assertBusy` calling the assertion in a loop and `MockSearchService` always returning the same exception instance (this is only secondary though).
</description><key id="165027062">19384</key><summary>org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT#testIndexRollingUpgrade fails sporadically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>jenkins</label></labels><created>2016-07-12T08:58:24Z</created><updated>2017-06-16T16:58:41Z</updated><resolved>2017-06-16T16:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-06-16T16:58:40Z" id="309079218">Given that this issue had no activity in almost a year, I would close it. We can always reopen if we encounter the same failure again.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature request]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19383</link><project id="" key="" /><description>ES 2.1
**Describe the feature**:
below query still get response:
                MatchAllQueryBuilder maq = QueryBuilders.matchAllQuery();
        SearchResponse sResponse = client.prepareSearch(indexName)
                .setSearchType(SearchType.SCAN)
                .setQuery(maq)
                .setSize(20000)
                .execute()
                .actionGet();

better limit size for scroll search type the same way for search with max result window
</description><key id="165024499">19383</key><summary>[feature request]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-07-12T08:45:28Z</created><updated>2016-07-12T09:11:52Z</updated><resolved>2016-07-12T09:11:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>feature request ES 5: Add parameter "copy_to" to fieldtype "percolator"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19382</link><project id="" key="" /><description>**Describe the feature**:

ES 2 indexes the content of a percolator query object to "_all". That is useful for an application to maintain percolators. 

The ES 5 field type "percolator" is not indexed to "_all" neither does it allow the "copy_to" parameter, nor could I copy another object field to a percolator type field.

"query.extracted_terms" did not show any hits for what I tried. (I use "query_string" queries).

So from my perspective the "copy_to" parameter in a percolator field would be the most versatile solution.

Thank you.
</description><key id="165015881">19382</key><summary>feature request ES 5: Add parameter "copy_to" to fieldtype "percolator"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mumpi</reporter><labels><label>:Percolator</label><label>discuss</label></labels><created>2016-07-12T07:54:36Z</created><updated>2016-07-13T13:33:17Z</updated><resolved>2016-07-13T12:17:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T10:22:54Z" id="231999394">As far as i can tell, ES2 doesn't index the contents of the query into the `_all` field which makes sense, as the `query` field is disabled --  it isn't even parsed:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "string"
        }
      }
    }
  }
}

PUT t/.percolator/1
{
  "query": {
    "match": {
      "foo": "bar"
    }
  },
  "one": "two"
}

GET t/.percolator/_search
{
  "aggs": {
    "all_field": {
      "terms": {
        "field": "_all",
        "size": 10
      }
    }
  }
}
```

This returns:

```
"all_field": {
  "doc_count_error_upper_bound": 0,
  "sum_other_doc_count": 0,
  "buckets": [
    {
      "key": "two",
      "doc_count": 1
    }
  ]
}
```

The `copy_to` parameter wouldn't help here either, as you can't copy an object to a `text` field.

In 5.0, the `query.extracted_terms` includes the extractable terms in the format `field\0term`, so eg this works:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "text"
        },
        "query": {
          "type": "percolator"
        }
      }
    }
  }
}

PUT t/t/1
{
  "query": {
    "match": {
      "foo": "bar"
    }
  }
}

GET t/t/_search
{
  "query": {
    "match": {
      "query.extracted_terms": "foo\u0000bar"
    }
  }
}
```

But I realise that this may not be the most useful format for your purposes. Honestly, I think the best solution is for you to extract the terms you want indexed yourself, and to add them as metadata.
</comment><comment author="mumpi" created="2016-07-12T11:21:06Z" id="232012863">There might be another way round: For My EL 2 percolator I set a mapping 
...
       "query": {
            "type": "object",
            "enabled": true
        }
...
This seems responsible for indexing the query. A listing of the actual mapping shows:

`"query": {
            "properties": {
              "query_string": {
                "properties": {
                  "default_field": {
                    "type": "string"
                  },
                  "default_operator": {
                    "type": "string"
                  },
                  "minimum_should_match": {
                    "type": "string"
                  },
                  "query": {
                    "type": "string"
                  }
                }
              }
            }
          },
`

So - instead of applying "copy_to" to the percolator type field why not start from the object type and "copy_to" the percolator type? (I tried; but ES 5 refused).
</comment><comment author="martijnvg" created="2016-07-12T11:24:59Z" id="232013945">&gt; (I tried; but ES 5 refused).

This is because the percolator from 5.0 now only works if a field of type `percolator` has been configured, whereas before an `object` field was required.
</comment><comment author="clintongormley" created="2016-07-12T11:27:22Z" id="232014634">&gt; This seems responsible for indexing the query. A listing of the actual mapping shows:

Yes, but you also end up creating lots and lots of indexed fields, some of which might end up causing mapping conflicts later on.  This is a no-go.
</comment><comment author="mumpi" created="2016-07-12T11:35:00Z" id="232017244">hmm, i see. But instead of defining the query as "object" and "enable" it I could also specify the required properties / subproperties for - in my sample - the "query_string" query.

And then "copy_to" the "percolator" type 
</comment><comment author="clintongormley" created="2016-07-13T12:17:08Z" id="232337641">@mumpi reread my comment https://github.com/elastic/elasticsearch/issues/19382#issuecomment-232014634 where I explain why this is a bad idea (even though it may have worked in your situation).

I don't think there is anything we can do here, so I'm going to close
</comment><comment author="mumpi" created="2016-07-13T13:33:17Z" id="232355900">Thank you for your effort.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to query negative number by query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19381</link><project id="" key="" /><description>ES version:2.3

curl -XGET 'http://localhost:9200/test/_search?pretty' -d '{
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "f1:-123",
                    "default_operator": "AND",
                    "default_field": "_all"
                }
            }
        }
    },
    "size":10
}'

error:
nested: QueryParsingException[[test] Failed to parse query [f1:-123]]; nested: ParseException[Cannot parse 'f1:-123': Encountered " "-" "- "" at line 1, column 3.
</description><key id="165009301">19381</key><summary>How to query negative number by query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gy1227</reporter><labels /><created>2016-07-12T07:13:34Z</created><updated>2016-07-12T09:10:51Z</updated><resolved>2016-07-12T09:10:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T09:10:51Z" id="231982280">You have to escape the `-` otherwise it is considered to be an operator:

```
`"f1:\\-123"`
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add some basic services to createComponents for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19380</link><project id="" key="" /><description>This adds the first few basic services needed for any plugin to create
its own components that interact with the rest of the system.

I also removed createComponents from the transport client as this does not make sense now that we are adding actual services in which do not exist in the transport client, like the cluster service.
</description><key id="165002703">19380</key><summary>Add some basic services to createComponents for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T06:22:43Z</created><updated>2016-07-12T09:08:28Z</updated><resolved>2016-07-12T06:58:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-12T06:58:06Z" id="231953734">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Merge pull request #19380 from rjernst/plugin_components_inputs</comment></comments></commit></commits></item><item><title>[feature request]bound some settings to alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19379</link><project id="" key="" /><description>**Describe the feature**:
let's say I have a index and it has 2 aliases: index_r and index_w, one for read and one for write.
if it can bound some index settings to alise, that would be good.

curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "index",
                 "alias" : "index_r",
                 "settings" : {
                      "index.blocks.read_only" : "true"
                   }  
              },
              "add" : {
                 "index" : "index",
                 "alias" : "index_w",
                 "settings" : {
                      "index.blocks.read" : "true",
                      "index.blocks.metadata" : "true"
                   }  
              }

```
    }
]
```

}'

with this bound, a better data reindex service can be provieded
</description><key id="164996813">19379</key><summary>[feature request]bound some settings to alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-07-12T05:26:43Z</created><updated>2016-07-12T11:25:40Z</updated><resolved>2016-07-12T09:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T09:08:07Z" id="231981632">Aliases belong to an index, as do these settings.  They don't exist outside an index, so there is nowhere to put settings like these which only apply to the alias.  You need a proxy or other security solution which implements these controls
</comment><comment author="makeyang" created="2016-07-12T11:03:48Z" id="232008027">@clintongormley  don't hurry to close issue. 
so my question is: will ES introduce the notation of "view"? alias and settings bound together to form a view?
</comment><comment author="clintongormley" created="2016-07-12T11:25:40Z" id="232014110">no
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove the redundant  ";"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19378</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

remove the redundant  ";"
</description><key id="164988308">19378</key><summary>remove the redundant  ";"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhouchaochao</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-07-12T03:45:31Z</created><updated>2016-09-12T22:38:04Z</updated><resolved>2016-09-12T22:38:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T09:02:29Z" id="231980343">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-09-12T22:38:04Z" id="246517766">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change Deprecated REST Handler prefix to be more granular</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19377</link><project id="" key="" /><description>**Elasticsearch version**: master (v5.0.0)

**JVM version**: n/a

**OS version**: n/a

**Describe the feature**:

Recently a deprecation warning response header was added as part of https://github.com/elastic/elasticsearch/pull/17804 and /elastic/elasticsearch/issues/17687

The [prefix used](https://github.com/elastic/elasticsearch/pull/17804/files#diff-a2f8f23221b079e15cfc451728820f92R40) is `Warning:` however this is not really granular enough to make it useful.

For example to really get the full benefit of this running packetbeat on Elasticsearch nodes indexing the rest requests would be ideal. Then dashboards could be built in Kibana showing the top X URL paths that have deprecations, etc. However since these are grouped under the common/generic `Warning:` header it is less than ideal as one would need to do a query on the word "deprecation" in the text of the header. If something like `X-Elasticsearch-Deprecation` was used it would make it very easy to filter and also future proofs if other non-deprecation warnings are added in the future.
</description><key id="164979463">19377</key><summary>Change Deprecated REST Handler prefix to be more granular</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2016-07-12T02:13:11Z</created><updated>2016-07-12T09:01:40Z</updated><resolved>2016-07-12T09:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-12T09:01:40Z" id="231980143">I think you're misunderstanding the purpose of the deprecation headers.  Deprecation headers are intended to alert the USER that the request that they have just run uses deprecated functionality (ie it provides the context to the user). As such, the official `Warning:` header is perfect for the purpose.

We also have deprecation logs, which provide the SYSADMIN with information that users are using deprecated functionality (but with less context).  If you wanted to make graphs out of deprecations, then consuming the deprecation logs would be the way to go.

However, making graphs out of how many different types of deprecations there are really doesn't help much.  What you want to know is that there is NO use of deprecated functionality, as it will break in the next major version.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate query registration from push to pull</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19376</link><project id="" key="" /><description>Remove `ParseField` constants used for names where there are no deprecated
names and use the `String` version of the registration method
instead.

This is step 2 in cleaning up the plugin interface for extending
search time actions. Aggregations are next.

This is breaking because queries must now be registered using the `SearchPlugin` interface.
</description><key id="164976360">19376</key><summary>Migrate query registration from push to pull</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-12T01:43:53Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-20T16:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-12T01:45:54Z" id="231915596">@rjernst this is the next step after #19238. Aggregations come after this and I think those are the last search extensions.
</comment><comment author="nik9000" created="2016-07-16T11:10:23Z" id="233125494">@rjernst can you review this? It is relatively simple, the same stuff I did in the last plugin migration PR.
</comment><comment author="javanna" created="2016-07-20T12:43:48Z" id="233937772">I noticed this was waiting for review so I had a look, left a couple of comments.
</comment><comment author="s1monw" created="2016-07-20T12:49:35Z" id="233939083">left some comments but LGTM in general
</comment><comment author="nik9000" created="2016-07-20T12:50:53Z" id="233939391">Thanks for looking. @javanna!  Removing the `ParseField`s I think of as a
bonus. It gives us one less thing to think about in the simple case but
still supports the more complex case when we have deprecated names and need
the ParseField. I like leaving the ParseField constant on the class when we
have a deprecated name because it makes the deprecated name more visible. I
think it'd make the deprecated name much more hidden if it only showed up
in SearchModule or in a plugin.

On Jul 20, 2016 8:43 AM, "Luca Cavanna" notifications@github.com wrote:

&gt; I noticed this was waiting for review so I had a look, left a couple of
&gt; comments.
&gt; 
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19376#issuecomment-233937772,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLojQxZXUM8gWsBKLG5J7tfkJTQX2Kks5qXhgJgaJpZM4JJ9Yo
&gt; .
</comment><comment author="nik9000" created="2016-07-20T13:21:58Z" id="233946884">Lets see what this does:

jenkins, test this
</comment><comment author="javanna" created="2016-07-20T14:50:56Z" id="233973262">LGTM as well
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate top_hits, histogram, and ip_range aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19375</link><project id="" key="" /><description>This is just another step towards removing `AggregationStreams` in favor of `NamedWriteable`.
</description><key id="164954335">19375</key><summary>Migrate top_hits, histogram, and ip_range aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T22:29:38Z</created><updated>2016-07-13T03:12:09Z</updated><resolved>2016-07-13T03:12:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-12T08:51:14Z" id="231977629">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate nested, reverse_nested, and children aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19374</link><project id="" key="" /><description>Just another step in removing AggregationStreams in favor of NamedWriteable.
</description><key id="164942903">19374</key><summary>Migrate nested, reverse_nested, and children aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T21:49:40Z</created><updated>2016-07-13T02:52:59Z</updated><resolved>2016-07-13T02:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-12T08:47:39Z" id="231976820">LGTM
</comment><comment author="nik9000" created="2016-07-13T02:52:59Z" id="232240410">Merged! Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add callback to customize http client settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19373</link><project id="" key="" /><description>The callback replaces the ability to fully replace the http client instance. By doing that, one used to lose any default that the RestClient had set for the underlying http client. Given that you'd usually override one or two things only, like a couple of timeout values, the ssl factory or the default credentials providers, it is not user friendly if by doing that users end up replacing the whole http client instance and lose any default set by us.
</description><key id="164937599">19373</key><summary>Add callback to customize http client settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T21:22:45Z</created><updated>2016-07-18T08:46:13Z</updated><resolved>2016-07-12T11:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-11T21:24:52Z" id="231870012">@nik9000 can you have a look please? 

I am still working on improving ssl configuration. If you want to use a pooling connection manager, which we do by default, you need to create a custom registry of connection factories and pass it in. At the moment you would simply customize the http client by setting your own connection manager, but then you'd lose our pooling connection manager default values (maxConnPerRoute etc.). Not sure how much this is important but I would like to make this more user friendly. Let me know if you have thoughts.
</comment><comment author="nik9000" created="2016-07-11T21:35:34Z" id="231872883">I think this is a great improvement. I left a comment about making it easier on java 8 users, but otherwise I'm happy with it.

Regarding ssl - yeah, I'd love to be able to customize that without blowing away the pools too. Sounds finicky. If all we have to do is provide the ssl context then maybe a way to register that'd be cool.

Unfortunately we'd really like to have a proxy to test the SSL.... Maybe I can have a look at adding a fixture for it?
</comment><comment author="nik9000" created="2016-07-11T21:35:57Z" id="231872985">Oh, I'm fine with merging this without a solution to SSL and working on SSL customization next.
</comment><comment author="nik9000" created="2016-07-12T11:07:59Z" id="232008845">Left two minor things. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/main/java/org/elasticsearch/client/SSLSocketFactoryHttpConfigCallback.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file></files><comments><comment>Merge pull request #19373 from javanna/enhancement/rest_client_builder_callback</comment></comments></commit></commits></item><item><title>Migrate geohash_grid and geo_bounds aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19372</link><project id="" key="" /><description>Just another small step in removing Aggregation's custom streams implementation in favor of NamedWriteable.
</description><key id="164933948">19372</key><summary>Migrate geohash_grid and geo_bounds aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T21:03:36Z</created><updated>2016-07-13T02:37:51Z</updated><resolved>2016-07-13T02:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-07-12T08:44:12Z" id="231976084">LGTM
</comment><comment author="nik9000" created="2016-07-13T02:37:51Z" id="232238581">Merged with 06bd896ce04c68e69ddc722b76589dbd2ca9af0c.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add components getter as bridge between guice and new plugin init world</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19371</link><project id="" key="" /><description>This change adds a getComponents() method to Plugin implementations
which they can use to return already constructed componenents/services.
Eventually this should be just services ("components" don't really do
anything), but for now it allows any object so that preconstructed
instances by plugins can still be bound to guice. Over time we should
add basic services as arguments to this method, but for now I have left
it empty so as to not presume what is a necessary service.

Note I also renamed nodeModules and nodeServices to make it clear these are guice related.
</description><key id="164923899">19371</key><summary>Add components getter as bridge between guice and new plugin init world</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T20:14:15Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-11T21:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-11T21:12:47Z" id="231866840">@nik9000 I pushed a new commit. Your comment made me realize our use of "get" for a lot of these should really be "create"...I renamed accordingly for at least the methods I worked on in this PR...
</comment><comment author="rjernst" created="2016-07-11T21:17:07Z" id="231867965">@nik9000 I pushed another commit addressing your concern.
</comment><comment author="nik9000" created="2016-07-11T21:20:53Z" id="231868916">If you move the `@SuppressWarnings` from the big method like I mentioned in https://github.com/elastic/elasticsearch/pull/19371#discussion_r70338846 then LGTM.
</comment><comment author="nik9000" created="2016-07-11T21:20:59Z" id="231868948">It is step in the right direction.
</comment><comment author="nik9000" created="2016-07-11T21:23:36Z" id="231869644">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/index/SettingsListenerIT.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/plugin/discovery/azure/classic/AzureDiscoveryPlugin.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java</file><file>plugins/jvm-example/src/main/java/org/elasticsearch/plugin/example/JvmExamplePlugin.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>test/framework/src/main/java/org/elasticsearch/index/MockEngineFactoryPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/test/MockIndexEventListener.java</file></files><comments><comment>Merge pull request #19371 from rjernst/plugin_components</comment></comments></commit></commits></item><item><title>Thread leak in TribeNode when a cluster is offline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19370</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.4
**JVM version**: 1.8.0_91
**OS version**: RedHat 6.5

We are using the TribeNode feature to enable search across a number of geographically distributed ElasticSearch clusters.  Occasionally when we take one of these clusters completely offline, we find that our TribeNode hits the following exception:

```
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:714)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1368)
        at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
        at org.elasticsearch.threadpool.ThreadPool$ThreadedRunnable.run(ThreadPool.java:676)
        at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:640)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

This exception is thrown because of thread exhaustion due to the TribeNode creating a new thread every couple of seconds.  Below is the stack trace of the leaked threads:

```
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
       java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
       java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
       org.elasticsearch.common.util.concurrent.KeyedLock.acquire(KeyedLock.java:75)
       org.elasticsearch.transport.netty.NettyTransport.disconnectFromNode(NettyTransport.java:1063)
       org.elasticsearch.transport.TransportService.disconnectFromNode(TransportService.java:274)
       org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:258)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
```

**Steps to reproduce**:
 Create TribeNode configuration where one cluster is offline.   Its not enough that the processes are shutdown and the machine is online, the nodes specified in the discovery.zen.ping.unicast.hosts for the offline cluster must be offline and not respond to ping/connection attempts.    Here is a simple configuration I was able to use to reproduce the problem.

```

---
cluster.name: "thread-leak-test"
node.name: "thread-leak-node"
http.port: "9201"
http.host: "127.0.0.1"
tribe:
  online-cluster:
    cluster.name: "online-cluster"
    discovery.zen.ping.unicast.hosts:
    - "localhost"
  offline-cluster:
    cluster.name: "offline-cluster"
    discovery.zen.ping.unicast.hosts:
    - "10.10.10.10"
```

Start the Tribe node.   Observe that the number of threads continue to grow unbounded (`ps -m &lt;pid&gt; | wc -l`) until the OutOfMemoryError: unable to create new native thread exceptions are thrown.

This issue appears similar to the problem described in #8057.
</description><key id="164920372">19370</key><summary>Thread leak in TribeNode when a cluster is offline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">escheie</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2016-07-11T19:56:35Z</created><updated>2016-12-21T14:09:59Z</updated><resolved>2016-12-21T14:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-08-01T11:31:50Z" id="236556589">@escheie Thanks for reporting! I reproduced it and will come back with a potential fix.
</comment><comment author="escheie" created="2016-08-05T00:23:25Z" id="237724350">Thanks tlrx for looking into the issue.   I've found that setting  `discovery.zen.ping.unicast.concurrent_connects` to 1 (default value is 10) limits the number of threads that pile up as the lock gets released to the threads waiting on disconnect every 30s between connection timeouts.   When it was 10, the connect threads could theoretically hold the lock forever, preventing the disconnect threads from ever getting a chance to complete.
</comment><comment author="tlrx" created="2016-08-05T07:14:44Z" id="237774021">@escheie True. But I think the comment made is the #19719 makes sense and changing the connection timeout for pings will help. But I still need to think about it again.
</comment><comment author="escheie" created="2016-08-05T17:49:02Z" id="237916865">With more testing overnight, I found that setting the "discovery.zen.ping.unicast.concurrent_connects" to 1 only works if TRACE logging is enabled for discovery.   Seems that little extra time the connect thread spends logging gives the other threads performing the disconnect a chance to get the lock.  Would a shorter connect timeout help if it is still more than the interval that connects are attempted which appear to be every 1.5 seconds? 

Looks like the KeyedLock used by NettyTransport in 2.x and TcpTransport in 5.x supports a fair option so that threads are able to acquire the lock in the order they request it.  This fair option is currently set to false.  If threads are able to obtain the lock in they order they request it, then that should ensure the disconnect threads get a chance to run between connection attempts.  I suppose enabling the fair option though would result in a significant performance penalty, so probably not an option.
</comment><comment author="escheie" created="2016-08-05T19:18:24Z" id="237939292">I've confirmed that enabling the "fair" flag in the KeyedLock does prevent the number of threads from growing unbounded.   The maximum number of threads that pile up equals (discovery.zen.ping.unicast.concurrent_connects \* connect_timeout)/(disconnect frequency) = 10*30/3 = 100.   This number can be reduced by lowering discovery.zen.ping.unicast.concurrent_connects in the configuration or if the connect_timeout is also lowered as proposed.  

Since it looks like the KeyedLock is only used during connect and disconnect and not for connection lookup, enabling the fair flag may not impact performance as I previously feared.
</comment><comment author="tlrx" created="2016-08-24T10:34:27Z" id="242021791">Thanks @escheie ! Your effort and investigation are great.

I do feel like the issue happens because we try to disconnect from nodes even if we never succeed to connect to them, and `UnicastZenPing` blindly piles up thread for disconnecting them (and these threads try to acquire a lock and slow dows ping threads too).

I proposed a new fix #19719, I'm wondering if it works for you too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/transport/ConnectionProfile.java</file><file>core/src/main/java/org/elasticsearch/transport/Transport.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/UnicastZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TCPTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TransportServiceHandshakeTests.java</file><file>plugins/discovery-file/src/main/java/org/elasticsearch/discovery/file/FileBasedUnicastHostsProvider.java</file><file>plugins/discovery-file/src/test/java/org/elasticsearch/discovery/file/FileBasedUnicastHostsProviderTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/discovery/MockZenPing.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTcpTransport.java</file></files><comments><comment>Simplify Unicast Zen Ping (#22277)</comment></comments></commit></commits></item><item><title>File Dependencies do not work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19369</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha5-SNAPSHOT

**JVM version**:  jdk 1.8.0_91

**OS version**: Ubuntu 14.04 LTS

**Description of the problem including expected versus actual behavior**: File Dependencies do not work as expected

**Steps to reproduce**:
1. If I use the following build script, it works so far:

```
   buildscript {
       repositories {
           mavenCentral()
           maven {
               name 'sonatype-snapshots'
               url "https://oss.sonatype.org/content/repositories/snapshots/"
        }
        jcenter()
    }
    dependencies {
          classpath "org.elasticsearch.gradle:build-tools:5.0.0-alpha5-SNAPSHOT" 
    }
}
apply plugin: 'java'
apply plugin: 'idea'
apply plugin: 'eclipse'
apply plugin: 'elasticsearch.esplugin'
```
1. If instead I manually download the .jar file https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/gradle/build-tools/5.0.0-alpha5-SNAPSHOT/build-tools-5.0.0-20160613.145150-483 and use the file dependencies as follows, it does not work any more (see error log below):

```
   buildscript {
    repositories {
        mavenCentral()
        maven {
            name 'sonatype-snapshots'
            url "https://oss.sonatype.org/content/repositories/snapshots/"
        }
        jcenter()
    }
    dependencies {
          classpath files("/home/anhxa/tmpdir/build-tools-5.0.0-20160613.145150-483.jar") 
    }
}
```
1. What am I missing here? What do I have to do in order to make the file dependency described in step 2 work properly? Many thanks for your help!

```
**Error logs**: * What went wrong:
A problem occurred evaluating root project 'example-plugin'.
\&gt; Failed to apply plugin [id 'carrotsearch.randomized-testing']
   \&gt; Could not create task of type 'RandomizedTestingTask'.
```
</description><key id="164909339">19369</key><summary>File Dependencies do not work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matzischatzi</reporter><labels><label>bug</label><label>build</label><label>feedback_needed</label></labels><created>2016-07-11T18:59:42Z</created><updated>2016-07-12T08:42:58Z</updated><resolved>2016-07-11T20:39:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-11T19:27:16Z" id="231838397">I'm tentatively marking this as a build bug.

I'm not sure what is wrong - could you run gradle with `--stacktrace` and `--debug --stacktrace` and post both as a gist? It is hard to tell what is up from here.
</comment><comment author="matzischatzi" created="2016-07-11T20:34:20Z" id="231856028">Thank you for your prompt reply! 

As you suggested, I already created a gist including stacktrace.txt and debug_stacktrace.txt:

https://gist.github.com/matzischatzi/866c5fe657c16c73b205780752997118
</comment><comment author="nik9000" created="2016-07-11T20:39:37Z" id="231857565">Ah! I should have realized. It says it can't create `com/carrotsearch/ant/tasks/junit4/SuiteBalancer which makes sense because that isn't part of Elasticseach's built - it is a dependency _of_ Elasticsearch's build. And including the project dependency gets you our build's dependencies. But including the file dependency doesn't. So, yeah, file isn't going to work unless you resolve the dependencies yourself. [This](https://github.com/elastic/elasticsearch/blob/master/buildSrc/build.gradle#L86) is the current list of the build dependencies.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Currently available elasticsearch build-tools and elasticsearch binaries do not match the current source code on github</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19368</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha5-SNAPSHOT

**JVM version**: jdk 1.8.0_91

**OS version**: Ubuntu 14.04 LTS

**Description of the problem including expected versus actual behavior**:

The currently available elasticsearch build-tools and elasticsearch binaries do not match the current source code on github

**Steps to reproduce**:
1. I've downloaded the elasticsearch source code from https://github.com/elastic/elasticsearch
2. Then I tried to build a simple elasticsearch plugin using the RestHandler interface. According to the source code, I have to implement the following method: 
    void handleRequest(RestRequest request, RestChannel channel) throws Exception;
3. To build the above elasticsearch plugin, I used the version "5.0.0-alpha5" for the elasticsearch build-tools as well as for elasticsearch itself (see the build.gradle attached below). 
   [build_gradle.txt](https://github.com/elastic/elasticsearch/files/357774/build_gradle.txt)
4.  Regarding the RestHandler interface, there apparently is a difference between the elasticsearch source code on github and in the repository https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/gradle/build-tools/5.0.0-alpha5-SNAPSHOT/
   Therefore, I always get the error message displayed in logs.
5. How do I get elasticsearch build-tools and elasticsearch binaries which match the current source code on github? Many thanks in advance for your help!

**logs**: ...src/main/java/org/elasticsearch/plugin/example/HelloRestHandler.java:8: error: HelloRestHandler is not abstract and does not override abstract method handleRequest(RestRequest,RestChannel,NodeClient) in RestHandler

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="164903099">19368</key><summary>Currently available elasticsearch build-tools and elasticsearch binaries do not match the current source code on github</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matzischatzi</reporter><labels /><created>2016-07-11T18:29:29Z</created><updated>2016-07-11T19:03:36Z</updated><resolved>2016-07-11T19:02:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-11T19:02:46Z" id="231832053">The Elasticsearch codebase is an incredibly fast moving target, and even the SNAPSHOT builds off of master can lag behind by several hours and sometimes longer (from the time the commit moves through the CI pipeline to a successful build to the repository publishing job).

&gt; Then I tried to build a simple elasticsearch plugin using the RestHandler interface. According to the source code, I have to implement the following method: 
&gt; void handleRequest(RestRequest request, RestChannel channel) throws Exception;

That interface [changed on 2016-06-29](https://github.com/elastic/elasticsearch/commit/865b951b7d5ba0acfa51bae2c12416044642338d#diff-60aca5a672f64e77d0f5b721b2da2fc6), I do not think that you're up to date on the latest commits in master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Limit batch size when scrolling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19367</link><project id="" key="" /><description>Limits the batch size from scrolling using the same setting as interactive
search: `index.max_result_window`.

Closes #19249
</description><key id="164882876">19367</key><summary>Limit batch size when scrolling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T16:47:27Z</created><updated>2016-07-11T20:10:45Z</updated><resolved>2016-07-11T20:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-11T17:09:48Z" id="231799452">LGTM
</comment><comment author="nik9000" created="2016-07-11T19:29:23Z" id="231838942">Thanks for reviewing @jimferenczi !
</comment><comment author="nik9000" created="2016-07-11T20:10:45Z" id="231849696">Thanks again for reviewing @jimferenczi !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_settings API fails to determine source format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19366</link><project id="" key="" /><description>**Elasticsearch version**: 2.1.2

**JVM version**: openjdk 8u91-b14-0ubuntu4~15.10.1

**OS version**: ubuntu 15.10

**Description of the problem including expected versus actual behavior**:

Trying to put settings through REST API using YAML format fails with exception `json_parse_exception`.

**Steps to reproduce**:
1. simply run the following

```
curl -XPUT localhost:9200/some-index/_settings?pretty --data-binary '

---
analysis:
  analyzer:
    default: { type: keyword }
'
```

I believe that problem is here https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoaderFactory.java#L73-L79

PS: it made my day when I saw this lines, you guys are the best)
</description><key id="164873000">19366</key><summary>_settings API fails to determine source format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">popravich</reporter><labels /><created>2016-07-11T16:00:26Z</created><updated>2016-07-14T09:39:41Z</updated><resolved>2016-07-13T19:45:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-11T16:05:56Z" id="231781361">The Elasticsearch REST APIs are JSON-based, so you can and should just submit the request here as a JSON request, submitting as YAML is not going to work here because of that detection.
</comment><comment author="bleskes" created="2016-07-13T12:52:30Z" id="232345408">actually we do support YAML as a body. The reason why it doesn't work is that the initial space in your body throws off the [auto detection](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java#L247). This works:

```
boaz$ curl -XPUT localhost:9200/some-index/_settings?pretty --data-binary '---
analysis:
  analyzer:
    default: { type: keyword }
'
```

Sadly we currently do not listen to the content type http header. This is discussed in #19388 . The result of that issue will also determine whether we want to be friendlier to yaml skip leading whitespace (as we now do for JSON), which I think we can keep this one closed.
</comment><comment author="popravich" created="2016-07-13T12:57:21Z" id="232346483">Hi,
As just mentioned in related issue, YAML parser can parse JSON, so it can be used as one.

Your example doesn't work as well, it fails because of `SettingsLoaderFactory.loaderFromSource`
</comment><comment author="bleskes" created="2016-07-13T13:40:14Z" id="232357853">&gt; Your example doesn't work as well

bleh, you are right. I checked the cluster settings action, not the index one (where my analysis is correct). On the index level you are correct, it doesn't work now and the JSON detection borks the yaml one. I'm opening it again, but it will have to wait for #19388 to see what is the best course of action.
</comment><comment author="clintongormley" created="2016-07-13T19:45:11Z" id="232465328">Content detection is just flawed.  We should remove it as per #19388.  Current content detection would also break SMILE or CBOR if it happened to contain a `{` character
</comment><comment author="bleskes" created="2016-07-14T09:39:41Z" id="232617100">@clintongormley the content detection on the setting part is indeed broken, but in other places (like the indexing code) we look at byte markers at the beginning of the request, so that doesn't happen. I'll respond to the removal part on the other ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clean up significant terms aggregation results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19365</link><project id="" key="" /><description>- Clean up the generics around significant terms aggregation results
- Reduce code duplicated between `SignificantLongTerms` and `SignificantStringTerms` by creating `InternalMappedSignificantTerms` and moving common things there where possible.
- Migrate to `NamedWriteable`
- Line length fixes while I was there
</description><key id="164868636">19365</key><summary>Clean up significant terms aggregation results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T15:41:34Z</created><updated>2016-07-13T02:18:28Z</updated><resolved>2016-07-13T02:08:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-11T15:41:59Z" id="231774048">@colings86 this is basically the same set of changes that I made for `InternalTerms` made to `InternalSignificantTerms`.
</comment><comment author="colings86" created="2016-07-12T08:32:23Z" id="231973419">LGTM
</comment><comment author="nik9000" created="2016-07-13T02:18:28Z" id="232236176">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow bucket_script to operate on date_range buckets.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19364</link><project id="" key="" /><description>Executing a bucket_script in the result of a date_range aggregation returns a cast error.

```
DELETE test

PUT test/test/1
{
  "tag": "a",
  "time": "2016-05-10",
  "value1": 10,
  "value2": 12
}

PUT test/test/2
{
  "tag": "a",
  "time": "2016-05-10",
  "value1": 10,
  "value2": 12
}

PUT test/test/3
{
  "tag": "b",
  "time": "2016-05-10",
  "value1": 10,
  "value2": 12
}

GET test/_search
{
  "size": 0,
  "aggs": {
    "b1": {
      "terms": {
        "field": "tag",
        "size": 2
      },      
      "aggs": {
        "m1": {
          "avg": {
            "field": "value1"
          }
        },
        "m2": {
          "max": {
            "field": "value2"
          }
        },
        "s1": {
          "bucket_script": {
            "buckets_path": {
              "v1": "m1",
              "v2": "m2"
            },
            "script": "v1*v2"
          }
        }
      }
    }
  }
}


GET test/_search
{
  "size": 0,
  "aggs": {
    "b1": {
      "date_range": {
        "field": "time",
        "ranges": [
          {
            "from": "2016-05-09",
            "to": "2016-05-11"
          }
        ]
      },     
      "aggs": {
        "m1": {
          "avg": {
            "field": "value1"
          }
        },
        "m2": {
          "max": {
            "field": "value2"
          }
        },
        "s1": {
          "bucket_script": {
            "buckets_path": {
              "v1": "m1",
              "v2": "m2"
            },
            "script": "v1*v2"
          }
        }
      }
    }
  }
}
```

The last aggregation execution returns the following.

```
{
   "error": {
      "root_cause": [],
      "type": "reduce_search_phase_exception",
      "reason": "[reduce] ",
      "phase": "merge",
      "grouped": true,
      "failed_shards": [],
      "caused_by": {
         "type": "class_cast_exception",
         "reason": "org.joda.time.DateTime cannot be cast to java.lang.Number"
      }
   },
   "status": 503
}
```

Even if we solve this issue the above example will still fail without #14600.

Using the hack on #14600 the following works:

```
GET test/_search
{
  "size": 0,
  "aggs": {
    "b1": {
      "filters": {
        "filters": {
          "a": {
            "range": {
              "time": {
                "gte": "2016-05-09",
                "lte": "2016-05-11"
              }
            }
          }
        }
      },      
      "aggs": {
        "m1": {
          "avg": {
            "field": "value1"
          }
        },
        "m2": {
          "max": {
            "field": "value2"
          }
        },
        "s1": {
          "bucket_script": {
            "buckets_path": {
              "v1": "m1",
              "v2": "m2"
            },
            "script": "v1*v2"
          }
        }
      }
    }
  }
}
```

/cc @colings86 
</description><key id="164863242">19364</key><summary>Allow bucket_script to operate on date_range buckets.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmusa</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-07-11T15:18:57Z</created><updated>2016-08-22T17:38:13Z</updated><resolved>2016-08-22T17:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>More cleanups around tests that require HTTP to be enabled.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19363</link><project id="" key="" /><description>I moved some tests into out integ-test-zip that really can simply run against
an external cluster. These tests are still subclassing EsIntegTestCase but run
against the real ES cluster started by the integ tests.

This also converts most of the uses in test of `http.enabled` to use the constant
this makes it much simpler to find places that set that value.
</description><key id="164851707">19363</key><summary>More cleanups around tests that require HTTP to be enabled.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T14:31:45Z</created><updated>2016-07-18T08:38:18Z</updated><resolved>2016-07-11T18:44:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-11T15:18:43Z" id="231766642">update: I moved all the http specific tests to a `qa/smoke-test-http` project instead of `integ-test-zip`  @rjernst can you take a look please
</comment><comment author="rjernst" created="2016-07-11T15:33:46Z" id="231771504">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeServiceTests.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsNotSetIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsRegexIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsDisabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsEnabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java</file></files><comments><comment>More cleanups aroung tests that require HTTP to be enalbed. (#19363)</comment></comments></commit></commits></item><item><title>Batch process node left and node failure 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19362</link><project id="" key="" /><description>This commit backports commit df7ad9970b42d15e377130966054070574016316
from master to 2.4.

Relates #19289, relates #19282
</description><key id="164841151">19362</key><summary>Batch process node left and node failure 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v2.4.0</label></labels><created>2016-07-11T13:47:50Z</created><updated>2016-07-11T15:18:19Z</updated><resolved>2016-07-11T14:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-11T13:49:07Z" id="231739388">@ywelsch This is the backport of #19289, do you mind taking a quick peek since I had to shuffle a few things around?
</comment><comment author="ywelsch" created="2016-07-11T13:59:24Z" id="231742393">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Export saved searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19361</link><project id="" key="" /><description>Hello everyone

I use Elasticsearch and Kibana.

Is there a way to export my saved searches' json request with a get request?
I would like to do so automatically. Not using the kibana UI.

Thank you
</description><key id="164829088">19361</key><summary>Export saved searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dpappas</reporter><labels /><created>2016-07-11T12:48:59Z</created><updated>2016-07-11T13:01:10Z</updated><resolved>2016-07-11T13:01:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-11T13:01:08Z" id="231727269">This question looks like it would best be addressed through Kibana channels. I recommend posting a question in the Kibana category on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure that that InnerHitBuilder uses rewritten queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19360</link><project id="" key="" /><description>If a nested, has_child or has_parent query's inner query gets rewritten then the InnerHitBuilder should use that rewritten form too, otherwise this can cause exceptions in a later stage.

Also fixes a bug that HasChildQueryBuilder's rewrite method overwrites max_children with min_children value, which causes the bug reported in #19353
</description><key id="164808225">19360</key><summary>Ensure that that InnerHitBuilder uses rewritten queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T10:40:40Z</created><updated>2016-07-12T14:27:58Z</updated><resolved>2016-07-12T14:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-12T07:19:58Z" id="231958022">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove ability to gather index statistics in scripts (aka IndexLookup)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19359</link><project id="" key="" /><description>[IndexLookup](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-advanced-scripting.html) was added to have the ability to access term statistics from scripts. However, it has some limitations and it is not clear whether the effort to maintain the code is worth it (see also https://github.com/elastic/elasticsearch/pull/19334/files/7df2d06eb62dbbc5cc365153088993d70801c286#r70103016). I'll add the pros and cons as far as I know them here:
- when used for scoring IndexLookup cannot make use of sparse postings lists like a similarity and will therefore always be slower than a custom similarity
- IndexLookup always needs the list of tokens used in advance, cannot make use of rewrite query
- when used within a script field and scroll it will be terribly slow because all the caching mechanisms to get for example document frequency are currently broken, see https://github.com/elastic/elasticsearch/issues/17110
- it is rarely used

pros:
- IndexLookup makes it easy to test custom similarities as a script before one actually goes and writes a custom similarity
- there is currently no other way to access term statistics in a script (none that I know of anyway)

I'd be a little sad to see it go because it was useful for some of my experiments. But if I am the only one then so be it. I'd also be willing to work on a replacement. 
</description><key id="164797849">19359</key><summary>Remove ability to gather index statistics in scripts (aka IndexLookup)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Scripting</label><label>:Similarities</label><label>adoptme</label><label>breaking</label></labels><created>2016-07-11T09:42:32Z</created><updated>2017-05-16T16:10:10Z</updated><resolved>2017-05-16T16:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T09:44:22Z" id="232908194">Discussed in FixItFriday.  Given that:
- this is a really advanced use case
- it performs badly and would he hard (impossible?) to fix
- it could be replaced by a similarity (or possibly a native Java script)

... we're in favour of removing this functionality.
</comment><comment author="brwe" created="2016-07-18T11:01:06Z" id="233300053">&gt; it could be replaced by a similarity (or possibly a native Java script)

When I wrote `scripts` I meant native script as well. I agree we can use a custom similarity instead of a script for scoring but in all other places where we can use scripts I do not see any way to for example access term frequencies even within native scripts. I at least used IndexLookup there as well. 
For example I used it to get the term frequencies for text classification with naive bayes in an native script. I also used it to extract the term frequencies for a given list of terms to feed that information to an external tool.

&gt; when used within a script field and scroll it will be terribly slow because all the caching mechanisms to get for example document frequency are currently broken, see #17110

To clarify, #17110 applies to all lookups we have right now and is not just an inherent problem of just IndexLookup although it I think it hurts Index Lookup most. 
There is one problem for IndexLookup that makes it slower than it was in 1.x even if not used within scan and scroll: IndexLookup currently computes document frequencies over the whole shard once per segment and not once per shard as it was on 1.x. While this decreases performance on 2.x it is not horrible and it is not impossible to fix that.

Again, I'd be willing to work on fixing the current problems or on an alternative implementation to IndexLookup but I would first like to hear why people think it is so important to remove it. Also, if there is a way to replace this functionality right now with a native script please point me to it, I might just be missing it.
</comment><comment author="rmuir" created="2016-07-18T15:11:14Z" id="233358049">If we really want to use scripting to implement similarity, IMO the simplest approach would be:

```
public class ExpressionSimilarity extends SimilarityBase {
   public ExpressionSimilarity(String expression) { ... }
   ...
}
```

It could just expose the simplified statistics of SimilarityBase via the ValueSource api, and should really be as fast as e.g. DFRSimilarity.

IMO it doesn't necessarily make Similarity any easier, because the true challenges are elsewhere: its really hard to get this stuff right. Many published algorithms struggle with this: e.g. especially with ES not removing stopwords by default, actual distributions are very unlike the expected and can cause problems in some cases. Even the classic BM25 IDF had an issue with this, but its not the only instance.

The code comments in DFR basic models give examples of problems:

```
 * WARNING: for terms that do not meet the expected random distribution
 * (e.g. stopwords), this model may give poor performance, such as
 * abnormally high or NaN scores for low tf values.
```

When possible we applied simple adjustments (e.g. BM25, DFR geometric model), but in other cases it is not obvious how or if things can even be fixed mathematically :) It is very expert stuff.
</comment><comment author="s1monw" created="2016-07-18T15:28:55Z" id="233363606">++ to a scripted similarity (must be a fast script ie. expressions) 
</comment><comment author="clintongormley" created="2016-07-18T15:44:25Z" id="233368546">@rmuir that works for the scoring aspect.  @brwe is also using these scripts to export term freq statistics for ML purposes.  That was the original purpose of this feature.  Any ideas how this part could be better implemented?
</comment><comment author="rmuir" created="2016-07-18T16:17:54Z" id="233378662">The only thing i can suggest is to see if we can avoid mixing all these concerns in one api. It sounds like we have our current scripting api trying to tackle 3 different concerns:
- per-doc: (typical script field/script score)
- per-term-per-doc: (Similarity)
- per-term??? (ml-type stuff)
  
  Similarity is a per-term-per-doc api. Much other stuff like script-score functionality is per-doc api. These two are two different perspectives. Thats why i think e.g. ExpressionSimilarity would be simpler (i am playing around with it now), because it would be "its own thing", and removed from here. I think it will also be easier on the user, too.

"exporting frequencies for ML purposes" isn't concrete enough for my understanding to know what would be needed (e.g. would it just be a proper per-term api or something more complicated?, would it go out to all shards and really return those correct numbers?), but it would be great to separate this too.

I am suspicious of generic apis that claim to solve all ML problems.  Lets at least separate this concern from complex issues like term weighting and script-based ranking and fields, because its nothing like those two things, and those two things are nothing like each other :) 
</comment><comment author="brwe" created="2016-07-18T19:01:16Z" id="233425296">&gt; "exporting frequencies for ML purposes" isn't concrete enough for my understanding to know what would be needed (e.g. would it just be a proper per-term api or something more complicated?

Here is what I did:
- for a given list of terms, export per document the term frequencies as sparse vector (so for terms "a", "b", "c" you get something like [[0,2], [3,1]] if the doc is "a c a a" per document)
- train some classifier outside elasticsearch
- store the parameters back to elasticsearch somewhere
- execute a native script that uses these parameters together with the term frequencies per document to get some classification going

In the export I used IndexLookup to get the frequencies for training documents. In the script execution I used the term frequencies for new documents to execute the classification. I hope that makes a little more sense. Not sure in how much detail I have to get here. 

BTW, this was a little bit of a hassle because in a way this is an intersection of terms in document and the list of terms I was interested in so I tried with a mixture of  fielddata and term vectors and IndexLookup. 
Conceptually it seemed to me a similar problem to search just that the number of terms is huge and the "score" that is outputted can be several numbers as well. But I never got to convert that vague feeling into code.
</comment><comment author="rmuir" created="2016-07-18T19:02:40Z" id="233425694">i dont understand how this really scales. if the term is common it may need to transfer millions and billions of things across the wire.
</comment><comment author="brwe" created="2016-07-18T19:12:52Z" id="233428401">it would not be slower than scroll in general I think. or what do you mean?
</comment><comment author="rmuir" created="2016-07-18T21:41:00Z" id="233467720">It doesn't lend itself to a top-N approach in any way, hence i think it makes sense to try to divorce it from the search API.
</comment><comment author="s1monw" created="2016-07-21T10:18:17Z" id="234214135">so I think we are on the right path here, IMO what we are missing is:
- a way to script similarities (that would be awesome)
- a way to script a scorer

lemme explain the latter, from my perspective the last remaining purpose of the index lookup was to have full control over the linear combination of the individual term scores. Now if you think about it its really a way to change what _BooleanScorer_ does but with added flexibility? I think if we add a dedicated query that calls a script to calculate the linear combination for each subqueries score we are at the point that @brwe has everything she needs. the input of the script can simply be a double array holding all score of the subqueries where the index is their ordinal of how they are specified?  

@brwe @rmuir does this make sense to you?
</comment><comment author="rmuir" created="2016-07-21T10:25:49Z" id="234215685">Why does a user need to change how scores are combined? Mathematically what else make sense? Users often ask for multiply because they think they want that, but that is supported via sum of logs.
</comment><comment author="s1monw" created="2016-07-21T10:48:44Z" id="234220044">I think, and please correct me if I am wrong @brwe that the context here is for instance something like logistic regression where your score is the result of some sigmod function (I know negative scores might be a problem here but there is a way to model that). The result of this could be used in aggregations to see how many docs fall into a certain class. I am really just making things up but I can totally see the value for experiments like this.
</comment><comment author="rmuir" created="2016-07-21T10:54:22Z" id="234221085">As i stated above, I don't think we should piggyback such things on our script search api... sorry, its not search to me at all.
</comment><comment author="s1monw" created="2016-07-21T11:06:56Z" id="234223491">&gt; As i stated above, I don't think we should piggyback such things on our script search api... sorry, its not search to me at all.

I think you misunderstood my comments, I am suggestion to remove the IndexLookup and extend ScirptQuery to also score. Since we already have a scorer on `LeafSearchScript` the script can just access it and call `Scorer#getChildren` and do whatever it wants? I mean all we need to guarantee is that the iteration order of this method is consistent?
</comment><comment author="rmuir" created="2016-07-21T11:37:19Z" id="234229126">I sense my concerns may not have been communicated very well either.

Scoring APIs (in both lucene and elasticsearch) are a sensitive part of the system, a real hotspot. Can we please not try to wedge things like document classification into the scoring system? It has a difficult job already: serious challenges especially in ES (e.g. very slow scripting language as the default).

If we want to support document classification, lets do that with some other API. I don't think it makes sense to use the search datastructures, algorithms, and apis for this problem: but if we want to do that, let's please make that an implementation detail, but expose a very simple api to the user.

As far as things being "flexible", it scares me to hear that term. Please, none of this is personal to any
developers, but this is like a raging infection in the elasticsearch codebase. There are far too many overcomplicated abstractions in this codebase, and despite that fact, a significant portion of our developers time is on refactoring. In my opinion, those abstractions are not helping out, instead they just make things complicated.

I really like how the linux kernel explains the problem[1](you kinda gotta translate the gist of this from C to Java or REST api or whatever is relevant):

```
* Abstraction layers

Computer Science professors teach students to make extensive use of
abstraction layers in the name of flexibility and information hiding.
Certainly the kernel makes extensive use of abstraction; no project
involving several million lines of code could do otherwise and survive.
But experience has shown that excessive or premature abstraction can be
just as harmful as premature optimization.  Abstraction should be used to
the level required and no further.

At a simple level, consider a function which has an argument which is
always passed as zero by all callers.  One could retain that argument just
in case somebody eventually needs to use the extra flexibility that it
provides.  By that time, though, chances are good that the code which
implements this extra argument has been broken in some subtle way which was
never noticed - because it has never been used.  Or, when the need for
extra flexibility arises, it does not do so in a way which matches the
programmer's early expectation.  Kernel developers will routinely submit
patches to remove unused arguments; they should, in general, not be added
in the first place.
```

[1] https://www.kernel.org/doc/Documentation/development-process/4.Coding

So instead of making the search api "flexible" enough to also do document classification, and instead of trying to make it "flexible" enough to support vague, ill-defined concepts like "ML purposes", I would prefer instead we added smaller, simpler concrete apis that address those needs. For example an API around document classification could be added that is very simple and easy to use, and has minimal parameters. Because its been separated from scoring, its no longer stuck with a priority queue + DAAT approach, it can use whatever datastructures it wants.

The search api already has enough problems, its already doing enough "dark magic", its already overwhelmed. Can we please separate this stuff?
</comment><comment author="s1monw" created="2016-07-21T12:21:06Z" id="234237284">I am 100% behind that statement. I think we should try to simplify the entire scripting API etc. let detach this from vague statements and look at issues like https://github.com/elastic/elasticsearch/issues/17116 it's pretty much what I described above and if we say we go with #17116 to allow this flexibility on that issue then we can just move on. But apparently the ability to combine scores is something users would like to have. I am more than in favor to simplify the script API if we can to a minimum and use higher level APIs like in that issue to add flexibility that doesn't cost much of performance here
</comment><comment author="rmuir" created="2016-07-21T13:13:54Z" id="234249010">I don't think we should do #17116 either. Instead of multiplying users can use the sum of logs.
</comment><comment author="brwe" created="2016-07-21T16:38:55Z" id="234311173">&gt; For example an API around document classification could be added that is very simple and easy to use, and has minimal parameters. Because its been separated from scoring, its no longer stuck with a priority queue + DAAT approach, it can use whatever datastructures it wants.

Just to be sure: You mean an actual api like 

GET index/type/doc_id/_classify

?
</comment><comment author="rmuir" created="2016-07-21T17:10:25Z" id="234319737">Sorry, I don't really have a concrete design for how classification should work: which algorithms are best or how we should expose it in our APIs.

Just hypothetically, i would think of it as a document enrichment type of thing, so maybe we start from say an ingest processor or whatever, that adds a new field (the assigned "classification") to the document, and then figure out from there how it should work. That would imply classifying documents before they are indexed at all, based on some model, whatever that is. 

Maybe that model is something complicated, and based on a bunch of index statistics from a separate index ("the training set"). Maybe its just a decision tree based on a big pile of regular expressions. Or maybe its best to leave tasks outside of ES to tools like nltk. Honestly, I have no idea :)
</comment><comment author="brwe" created="2016-07-21T17:43:52Z" id="234328977">&gt; Maybe that model is something complicated, and based on a bunch of index statistics from a separate index ("the training set"). Maybe its just a decision tree based on a big pile of regular expressions. Or maybe its best to leave tasks outside of ES to tools like nltk. 

Ok, so here is another approach: Say we had a "scripting language" that can do nothing but read model parameters for trained models (trees, logistic regression, whatever) that come from somewhere external  and then execute the read model on a document.
So it would not really a scripting language because the "scripts" are really just parameters and the execution is hard coded but it would use the scripting mechanism so it can be used in aggregation, ingest etc. 
Would that be far away enough from search? Could we make it so that at least this "scripting language" would have access to term statistics? 
</comment><comment author="rmuir" created="2016-07-21T21:28:40Z" id="234389763">I don't understand the desire to use the scripting mechanism as the way to do this. Why wouldn't we just have the proper hooks in those apis (e.g. add an aggregation component, add an ingest component) to do this instead? e.g. bundled as a plugin.

Doing it with the scripting api (and plumbing it via scoring) adds unnecessary complexity and pressure to those components.
</comment><comment author="s1monw" created="2016-07-22T10:06:45Z" id="234506387">ok given that we should scoring for scoring ;) and we have some really strong arguments towards not mixing concerns while rather minor arguments towards opening things up more for satisfy the usecase of _ML_ or _classification_ we should remove this API without a full replacement. I would still like us to explore the way of a scripted similarity, I think this could help quite a bit for experimenting with scoring. @rmuir WDYT?
</comment><comment author="rmuir" created="2016-07-22T11:03:41Z" id="234516722">&gt; ok given that we should scoring for scoring ;) and we have some really strong arguments towards not mixing concerns while rather minor arguments towards opening things up more for satisfy the usecase of ML or classification we should remove this API without a full replacement. I would still like us to explore the way of a scripted similarity, I think this could help quite a bit for experimenting with scoring. @rmuir WDYT?

Are users of ES typically information retrieval researchers? I can't think of who else would need that.

Like i said: if we really truly need it, then ExpressionSimilarity is the way to do it. However, I feel like all the time i would invest in such a feature may be wasted: nobody would ever use it :( 

It really should not be so hard to remove things in elasticsearch. This issue is a classic example! That's why elasticsearch has half a million lines of code.
</comment><comment author="s1monw" created="2016-07-22T12:35:09Z" id="234532069">&gt; Like i said: if we really truly need it, then ExpressionSimilarity is the way to do it. However, I feel like all the time i would invest in such a feature may be wasted: nobody would ever use it :(

can it be a lucene feature,  I could see some folks use it...
</comment><comment author="s1monw" created="2016-07-22T12:40:34Z" id="234533130">well to begin with I think lets remove the IndexLookup and then we can have individual issues about improvements and then we can still decide if it makes sense to add such a similarity.
</comment><comment author="softwaredoug" created="2016-07-22T22:06:50Z" id="234668747">Just want to chime into this issue as a practioner. I have used term statistics in scripts and find them  very handy. The use case that comes to mind is a small collection, hosted somewhere that doesn't allow plugins, that cares about relevance. 

Sure In a larger scale system, scripting isn't a great answer. But many folks do search at a smaller scale. These orgs look at you sideways when you start talking about maintaining custom scorers and similarities for them. They are terrified of Java and Lucene. And they don't want to keep having to hire someone to maintain even a simple similarity. 

Second Elasticsearch seems to be being used as a general purpose matching/ranking system in analytics solutions, not strictly a search engine. I'm not just speaking from my own experience at OSC, but from Elastics own marketing. Often when doing weird things like searching DNA or the predicting weather or building recommenders what a "term" is becomes a data modeling exercise. 

In these cases erring towards a flexible "frameworky" approach IMO works best. Allowing several options to perform arbitrary scoring computations is really important for these users. I don't think the flexibility argument is merit less based on the very cool things I see folks doing in Lucene. 

So my bias as a practitioner, not as someone steeped in the ES code base, would be to caution you against any change that didn't have an alternative for the folks that don't want to or can't maintain Java plugins. And something that let the non text use cases continue to do well with Elasticsearch. 

I really like the idea of scripting scorers and similarities. That removes a lot of Java code and adds a ton for relevance use cases. 
</comment><comment author="softwaredoug" created="2016-07-22T23:54:43Z" id="234683946">Ive seen it mentioned a few times here uncertainty why why users should need to control how scores are combined. I think it's vital. Lucene based search has been successful compared to commercial offerings precisely because of how much you can customize its behavior. And the more of that I can do outside of Java, the easier it is for me to implement that power. (Otherwise these days I can just go plug in algolia or something and be done with it.)

For example a "term" in Lucene world as I'm sure everyone has seen may have nothing to do with text. It might have something to do with product affinities in a recommendation system. Or ngrams of DNA. Or identifiers in some taxonomy. Or something even more exotic. And who knows then what the term stats around it are supposed to mean and how they might be used? In this context, the more control one can have before having to deal with custom plugins the more successful ES can be at solving the meaty search probs out there. 

A high quality ES solution is still only as free as your time is (or as powerful as your time investment is). So while I do see more "IR researchers" in pure Lucene and Solr land, there's still quite a few using Elasticsearch precisely because it hits a nice sweet spot between control and ease of use compared to Solr or pure Lucene. 

So again :+1: to scripted scorers and similarities. I see a lot of power there. 

(PS I'm just trying to offer a "outside" practitoners devils advocate for your efforts. Totally get you guys have a large complex code base to maintain and can empathize with the need to tidy things up and avoid bugs. Keep up the great work! :) )
</comment><comment author="rmuir" created="2016-07-25T13:16:42Z" id="234949848">&gt; For example a "term" in Lucene world as I'm sure everyone has seen may have nothing to do with text. It might have something to do with product affinities in a recommendation system. Or ngrams of DNA. Or identifiers in some taxonomy. Or something even more exotic. And who knows then what the term stats around it are supposed to mean and how they might be used? In this context, the more control one can have before having to deal with custom plugins the more successful ES can be at solving the meaty search probs out there.

Sorry this argument just does not hold. For example BM25 tells us exactly how to handle such "non-textual relevance features". Please read it. 

http://www.staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf
</comment><comment author="softwaredoug" created="2016-07-25T17:44:08Z" id="235027422">Great article Rob, I believe I've read it in the past. BM25 is pretty awesome for what it's built for (document search)

The area you cite provides a very general framework for a linear combination of scalars. It says

&gt; Here are some Vi functions that we have used with success in the past for different features 
&gt; (lists sigmoid, log, etc)

The linear combination looks like the cosine similarity summation under the Bool query. But most of Elasticsearch's documentation (and my experience) points to multiplying text relevance by these scalar factors instead of trying to treat them as another "boolean clause" to optimize for as the first tool to reach for. Indeed, this is what Elasticsearch's function_score_query does by default. It's likely both approaches have their applications.

Additionally, like I said BM25 is proven at general document search. This is only one application of Elasticsearch. I still believe Lucene-based search's sweet spot is as a framework in that (1) it's focus is on deep configurability (2) it offers a variety of ways of thinking about and configure "relevance". People want to use Elasticsearch to build recommendations systems, perform image search, and search chemical structures. They have something unique to what they're doing, that's why they didn't grab an off the shelf product that did what they wanted.

I don't want to derail this thread. I'm just trying to offer my support &amp; perspective on where I see Elasticsearch in the marketplace when people are making the "buying decision." Little features like tf in scripts seem like a small thing, but in some sense its these little bits of configurability that sell me Lucene-based search as opposed to a commercial "plug and play" model where all the decisions are made for me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexField.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafIndexLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/PositionIterator.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/TermPosition.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupIT.java</file></files><comments><comment>Remove script access to term statistics (#19462)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/search/lookup/LeafIndexLookup.java</file></files><comments><comment>Scripting: Deprecate index lookup (#24691)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/script/LeafSearchScript.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptEngine.java</file><file>plugins/examples/script-expert-scoring/src/main/java/org/elasticsearch/example/expertscript/ExpertScriptPlugin.java</file><file>plugins/examples/script-expert-scoring/src/test/java/org/elasticsearch/example/expertscript/ExpertScriptClientYamlTestSuiteIT.java</file></files><comments><comment>Scripting: Replace advanced and native scripts with ScriptEngine docs (#24603)</comment></comments></commit></commits></item><item><title>Strengthen assertions if random failures are not injected by AbstractIndicesClusterStateServiceTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19358</link><project id="" key="" /><description>The unit tests for `IndicesClusterStateService` currently inject random failures upon shard creation/ routing upate / mapping update etc. This PR makes injecting failures optional so that stronger assertions can be made about the local indices / shard state in case of no failures.
</description><key id="164795470">19358</key><summary>Strengthen assertions if random failures are not injected by AbstractIndicesClusterStateServiceTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>test</label></labels><created>2016-07-11T09:29:46Z</created><updated>2016-07-15T16:32:04Z</updated><resolved>2016-07-15T16:32:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-11T14:32:43Z" id="231752285">Neat!
</comment><comment author="jasontedor" created="2016-07-15T16:24:29Z" id="232999447">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/indices/cluster/AbstractIndicesClusterStateServiceTestCase.java</file></files><comments><comment>Strengthen assertions when random failures are not injected by AbstractIndicesClusterStateServiceTestCase (#19358)</comment></comments></commit></commits></item><item><title>Catch assertion errors on commit and turn it into a real exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19357</link><project id="" key="" /><description>Lucene IndexWriter asserts on files existing on the filesystem but
some tests throw IOException explicitly on those operatiosn such that
some tests trip asserts. We had this before on InternalEngine#ctor
and added some logic there to catch only a specific assertions based
on some excepition stack analysis. This change applies the same logic
to the IndexWriter#commit part of the engine since it can hit the same
issue.
This also fixes a self-suppression issue in Store.java.

Closes #19356
</description><key id="164787371">19357</key><summary>Catch assertion errors on commit and turn it into a real exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-11T08:45:55Z</created><updated>2016-07-11T09:41:06Z</updated><resolved>2016-07-11T09:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-11T08:46:22Z" id="231675771">@mikemccand  can you take a look
</comment><comment author="mikemccand" created="2016-07-11T08:57:07Z" id="231678144">LGTM, nice find.

Maybe we should fix IW to throw `CorruptIndexException` instead of `AssertionError`?  This exact situation has also caused problems for Lucene's tests, and it's crazy that the caller needs to go and unwrap `AssertionError` and poke around to figure out which exception it was...
</comment><comment author="s1monw" created="2016-07-11T09:20:46Z" id="231683397">++ to upgrade this assertion to a hard check and throw CIE?
</comment><comment author="mikemccand" created="2016-07-11T09:38:19Z" id="231687307">OK I just removed these assertions from IW entirely: https://github.com/apache/lucene-solr/commit/044aabfb30b3da02378c33392a86adc7942921a8

Turns out they are not needed anymore because the low level APIs IW is using in Lucene were improved themselves to catch these cases even w/o assertions enabled.

So when we upgrade to Lucene 6.2 we can remove the unwrapping of `AssertionError`.
</comment><comment author="s1monw" created="2016-07-11T09:41:06Z" id="231687975">AWESOME! thanks mike
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>Catch assertion errors on commit and turn it into a real exception (#19357)</comment></comments></commit></commits></item><item><title>Build Failure: org.elasticsearch.search.basic.SearchWithRandomIOExceptionsIT.testRandomDirectoryIOExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19356</link><project id="" key="" /><description>I was unable to reproduce these so far, happened twice in recent test runs:

```
gradle :core:integTest -Dtests.seed=1FFB9F0D780D57E7 -Dtests.class=org.elasticsearch.search.basic.SearchWithRandomIOExceptionsIT -Dtests.method="testRandomDirectoryIOExceptions" -Dtests.security.manager=true -Dtests.jvm.argline="-XX:-UseConcMarkSweepGC -XX:+UseG1GC" -Dtests.locale=fr-CA -Dtests.timezone=Asia/Baku
```

Most of the exceptions in the log are expected, hard to spot the real cause, one of the two failures looks as if a thread is lingering around, the other one shows an uncaught exception for a set of lucene files.

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/1062/console
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1489/console
</description><key id="164775941">19356</key><summary>Build Failure: org.elasticsearch.search.basic.SearchWithRandomIOExceptionsIT.testRandomDirectoryIOExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2016-07-11T07:30:44Z</created><updated>2017-02-27T10:44:42Z</updated><resolved>2017-02-27T10:44:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-11T08:48:07Z" id="231676143">@spinscale I opened a PR for this - thx for opening the issue
</comment><comment author="abeyad" created="2017-02-24T20:37:52Z" id="282398223">This test still fails sporadically, though I can't get it to reproduce.  It has failed about 3 times in the last month.  Here is the console output from a CI run for one of the failures.

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=sles/651/consoleFull</comment><comment author="s1monw" created="2017-02-26T06:45:24Z" id="282537021">I think this fails due to a recent refactoring...

```
ERROR   0.83s J0 | SearchWithRandomIOExceptionsIT.testRandomDirectoryIOExceptions &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: Failed to execute phase [fetch], 
   &gt; 	at __randomizedtesting.SeedInfo.seed([DD628C72414F644:442A44EF3A521DE5]:0)
   &gt; 	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:270)
   &gt; 	at org.elasticsearch.action.search.FetchSearchPhase$1.onFailure(FetchSearchPhase.java:93)
   &gt; 	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:581)
   &gt; 	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
   &gt; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt; 	at java.lang.Thread.run(Thread.java:745)
   &gt; Caused by: java.lang.NullPointerException
   &gt; 	at org.apache.lucene.search.TopDocs$MergeSortQueue.&lt;init&gt;(TopDocs.java:128)
   &gt; 	at org.apache.lucene.search.TopDocs.mergeAux(TopDocs.java:240)
   &gt; 	at org.apache.lucene.search.TopDocs.merge(TopDocs.java:230)
   &gt; 	at org.elasticsearch.action.search.SearchPhaseController.sortDocs(SearchPhaseController.java:265)
   &gt; 	at org.elasticsearch.action.search.FetchSearchPhase.innerRun(FetchSearchPhase.java:101)
   &gt; 	at org.elasticsearch.action.search.FetchSearchPhase.access$000(FetchSearchPhase.java:45)
   &gt; 	at org.elasticsearch.action.search.FetchSearchPhase$1.doRun(FetchSearchPhase.java:88)
   &gt; 	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:596)
   &gt; 	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt; 	... 3 more

```

I will take a look at it today or tomorrow</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchPhaseControllerTests.java</file></files><comments><comment>Factor out filling of TopDocs in SearchPhaseController (#23380)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>Catch assertion errors on commit and turn it into a real exception (#19357)</comment></comments></commit></commits></item><item><title>[WIP] Use seq no for shard recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19355</link><project id="" key="" /><description>This is a POC PR to gather feedback regarding the approach taken. There are a bunch of nocommits, indicated potential discussion points and things that merit a better solution. If we decide we like the changes I will break this into smaller PRs and add tests etc.
### High level mechanics of seq no based recovery

Using global checkpoints, we have a guarantee of the shard history between a recovery replica and the current primary. The replica removes all operations above the last known global checkpoint and asks the primary to replay all operations it misses. Since we can't roll back to arbitrary points in the history, we rollback to the last lucene commit (later on, NRT if possible) that gives us what we need. This means we will replay operations from before the global check point. If the requested operations are not to be found in the primary translog we fall back to the normal file based sync.

Note that the last point means that _for now_ we can't use seq no based recovery after restart as the primary always flushes after recovering, leaving it's translog empty. We can use sequence numbers to recover a replica that was isolated away from the cluster and rejoined. We can also use them to recover shards that have not been synced-flushed but did go through their global checkpoint sync and have no doc mismatch.
### PR highlights / caveats

1) Add a custom deletion policy that makes sure we keep needed old commits to do rall back.
2) Modify recovery target code to figure out which operations are needed and add the to the recovery requests.
3) Modify recovery source code to track replayed operations and match them to the requested range. If operations are missed, we retry the recovery and explicitly disable seq no based recoveries. Note that long term the plan is to be able to detect this in advance and do the right thing without the need for a retry
4) Persist global checkpoint in the translog. Our original plan was to persist the last known global checkpoint with each lucene commit. This means that when recovering we are likely to roll back a commit point (as we use the last one's checkpoint and roll back to a previous commit). However, persisting the global checkpoint in each translog checkpoint allows us to have better knowledge which makes it much more likely that we will only keep one lucene commit point and be able to use this.
5) Improve recovery testing infra structure to be share more with production code and simulate all the above.
</description><key id="164775453">19355</key><summary>[WIP] Use seq no for shard recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label><label>review</label><label>WIP</label></labels><created>2016-07-11T07:27:04Z</created><updated>2016-11-16T15:17:43Z</updated><resolved>2016-11-16T15:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typo on analyze.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19354</link><project id="" key="" /><description>A closing semicolon should be needed, right?
</description><key id="164769227">19354</key><summary>Fix typo on analyze.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">minagoro</reporter><labels><label>docs</label></labels><created>2016-07-11T06:35:28Z</created><updated>2016-07-11T13:49:45Z</updated><resolved>2016-07-11T13:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="minagoro" created="2016-07-11T06:49:25Z" id="231654747">I have signed the CLA now, please confirm.
</comment><comment author="clintongormley" created="2016-07-11T13:49:45Z" id="231739577">thanks @minagawa-sho - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fix typo on analyze.asciidoc (#19354)</comment></comments></commit></commits></item><item><title>has_child query returns no results when used with date range query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19353</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha4

**JVM version**: 1.8.0_60

**Description of the problem including expected versus actual behavior**:
The has_child query does not return expected results when it contains a date based range query. I created a document type called **questions** which has a child document type called **answers**. The answers document has a **date** field. Once I insert sample data in elasticsearch, I can query the **answers** document successfully with a date range query. However, the has_child query does not return any results when the date range query utilizes any date value that precedes the answer dates. The following steps explain the issue further.

**Steps to reproduce**:
1) Create an index called "sandbox"

```
PUT /sandbox
```

2) Create a document mapping for "answers" with a parent type of "questions".

```
PUT /sandbox/_mapping/answers
{
  "_parent": {
      "type": "questions" 
  }
  , "properties": {
    "answer" : {"type": "text"},
    "date" : {"type": "date"}
  }
}
```

3) Add a "questions" document

```
PUT /sandbox/questions/1
{
  "question" : "Why is the sky blue?"
}
```

4) Add a couple of "answers" document

```
PUT /sandbox/answers/1?parent=1
{
  "answer" : "Due to scattering of sunlight",
  "date" : "2016-05-01"
}
```

```
PUT /sandbox/answers/2?parent=1
{
  "answer" : "Due to refraction of light",
  "date" : "2016-06-01"
}
```

5) Find all answers with a date value of gte "2016-04-01". 
THIS WORKS AS EXPECTED. It returns the 2 answers documents as expected.

```
GET /sandbox/answers/_search
{
  "query" : {
    "range": {
      "date": {
        "gte": "2016-04-01"
      }
    }
  }
}
```

6) Now use the has_child query to get questions with answers that have date gte "2016-04-01". 
**DOES NOT WORK AS EXPECTED**. This returns 0 hits. I expected the 1 "questions" document in the response.

```
GET /sandbox/questions/_search
{
  "query" : {
    "has_child": {
      "type": "answers",
      "query": {
        "range": {
          "date": {
            "gte": "2016-04-01"
          }
        }
      }
    }
  }
}
```
</description><key id="164730742">19353</key><summary>has_child query returns no results when used with date range query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ayadav77</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2016-07-10T19:46:23Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-07-12T14:27:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-11T07:48:24Z" id="231664183">The `has_child` query operates at the index level, you cannot restrict the type of your query like you're doing in your snippet: 
`GET /sandbox/questions/_search` 
Try with
`/sandbox/_search`
</comment><comment author="martijnvg" created="2016-07-11T08:02:41Z" id="231666826">&gt; The has_child query operates at the index level, you cannot restrict the type of your query like you're doing in your snippet: 

This is possible, since 2.0 if I recall correctly. 

This really seems to be caused by a bug in: [HasChildQueryBuilder#rewrite(...)](https://github.com/elastic/elasticsearch/blob/8c40b2b54eac3e3ab3c41ece5c758be75173191b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java#L477)

Fixing that makes the query work as expected. I'll open a PR.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java</file></files><comments><comment>inner_hits: Ensure that that InnerHitBuilder uses rewritten queries</comment></comments></commit></commits></item><item><title>TranslogCorruptedException while attempting to recover ES cluster (2.3.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19352</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue. Note that whether you're filing a bug report or a
feature request, ensure that your submission is for an
[OS that we support](https://www.elastic.co/support/matrix#show_os).
Bug reports on an OS that we do not support or feature requests
specific to an OS that we do not support will be closed.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**: 8u29

**OS version**: Ubuntu 16.10

**Description of the problem including expected versus actual behavior**:
After node failure, cluster fails to re-index with TranslogCorruptedException.

We have a 4-node cluster.  One of the nodes (cluster leader) failed due to a cpu failure.  The other 3 nodes were stopped while the failed node was fixed.  Once the node was fixed, the leader was brought back up and the other three nodes restarted.  Re-indexing begins, but two of the nodes start spitting out errors and the cluster fails to start/re-index.

Expected Behavior:
ES will re-index and recover the cluster to "green" health.

Actual Behavior:
Several nodes produce a stream of errors like:

```
[logstash-telemetry-2016.07.09][[logstash-telemetry-2016.07.09][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed
to create engine]; nested: TranslogCorruptedException[expected shard UUID [[68 46 45 4e 49 38 4e 74 51 65 71 61 33 36 6b 66 33 47 6e 58 56 67]] but got: [[52 6a 6b 5f 50 59 4f
46 54 6d 79 6a 4c 36 45 5a 61 76 33 51 4d 41]] this translog file belongs to a different translog];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

and the cluster fails to recover.  The leader eventually reports a Java OOM exception, even though each node has more than enough heap for normal operations.

**Steps to reproduce**:
unsure.  The sequence of events was:
1.  leader crashed due to hardware failure
2.  other three nodes were stopped by hand (process, not system)
3.  leader was recovered/restarted
4.  other three nodes were started and joined the cluster
5.  two nodes immediately began reporting TranslogCorruptedException
6.  cluster continues to index, but eventually fails

**Provide logs (if relevant)**:

Errors are a repeating stream like this:

```
[logstash-telemetry-2016.07.09][[logstash-telemetry-2016.07.09][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed
to create engine]; nested: TranslogCorruptedException[expected shard UUID [[68 46 45 4e 49 38 4e 74 51 65 71 61 33 36 6b 66 33 47 6e 58 56 67]] but got: [[52 6a 6b 5f 50 59 4f
46 54 6d 79 6a 4c 36 45 5a 61 76 33 51 4d 41]] this translog file belongs to a different translog];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-telemetry-2016.07.09][[logstash-telemetry-2016.07.09][3]] EngineCreationFailureException[failed to create engine]; nested: TranslogCorruptedException[expec
ted shard UUID [[68 46 45 4e 49 38 4e 74 51 65 71 61 33 36 6b 66 33 47 6e 58 56 67]] but got: [[52 6a 6b 5f 50 59 4f 46 54 6d 79 6a 4c 36 45 5a 61 76 33 51 4d 41]] this translo
g file belongs to a different translog];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
    ... 5 more
Caused by: TranslogCorruptedException[expected shard UUID [[68 46 45 4e 49 38 4e 74 51 65 71 61 33 36 6b 66 33 47 6e 58 56 67]] but got: [[52 6a 6b 5f 50 59 4f 46 54 6d 79 6a 4
c 36 45 5a 61 76 33 51 4d 41]] this translog file belongs to a different translog]
    at org.elasticsearch.index.translog.TranslogReader.open(TranslogReader.java:235)
```
</description><key id="164722019">19352</key><summary>TranslogCorruptedException while attempting to recover ES cluster (2.3.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaychris</reporter><labels><label>:Translog</label><label>feedback_needed</label></labels><created>2016-07-10T16:05:59Z</created><updated>2017-02-08T15:55:06Z</updated><resolved>2016-07-11T15:55:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-11T08:10:34Z" id="231668339">That error indicates that the translog files belong to a different lucene index than the one used when opening them (we bake a uuid into both those files and make sure it matches). Can anything in your problem / how you recovered explain this? are you running on a shared failed system by any chance? 
</comment><comment author="jaychris" created="2016-07-11T15:55:03Z" id="231778166">We're using local volumes for each node, so I don't know how they would have gotten mixed.

In the meantime, I resolved this by restoring an earlier snapshot (from before the failure).  There might have been a little bit of lost data from the time between the snapshot and the failure, but it's minimal.  

After restoring from backup, the cluster was able to re-index successfully.

I would have left it alone for troubleshooting if it were anything other than our production cluster, but we couldn't afford to be down for too long.
</comment><comment author="ygokirmak" created="2017-02-08T13:20:08Z" id="278326693">@bleskes is there any workaround for solving such cases. I have same case where elasticsearch is running on docker with shared volume for data.  I removed docker container by mistake and seems new container has a different lucene uuid

I see this error message
```
elk-elasticsearch_1  | [2017-02-08 13:11:54,781][WARN ][indices.cluster          ] [Superia] [[audit-2017.01.27][3]] marking and sending shard failed due to [failed recovery]
elk-elasticsearch_1  | [audit-2017.01.27][[audit-2017.01.27][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: TranslogCorruptedException[expected shard UUID [[6e 38 6d 35 79 38 74 37 51 78 6d 53 72 77 5a 4f 35 4d 51 5a 64 41]] but got: [[78 75 63 32 74 64 36 4d 53 43 47 58 62 62 57 55 78 66 4d 72 6d 77]] this translog file belongs to a different translog];
```

and it seems this UUID is written in 
```
&gt; hexdump /nodes/0/indices/audit-2017.01.27/3/translog/translog-3.tlog 
0000000 d73f 176c 7408 6172 736e 6f6c 0067 0000
0000010 0002 0000 7816 6375 7432 3664 534d 4743
0000020 6258 5762 7855 4d66 6d72 0077          
000002b
```

I think it can be solved with some magic scripting but prefer a better workaround :)</comment><comment author="bleskes" created="2017-02-08T15:55:06Z" id="278368328">@ygokirmak I don't know exactly what happened but I found it very strange that a translog from one lucene index ended up in another's folder. Are you sure the lucene index is the one you want to have? If so you can upgrade to 5.x and use this [command line tool](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html#corrupt-translog-truncation) . Watch out though - you'd lose some data.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup usage of http.enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19351</link><project id="" key="" /><description>Several tests required http.enabled where it was unnecessary.
We also had RestMainActionIT which tests what two of our REST tests
test already so I removed it.
The explicit use of http.enabled: false is also obsolet since our
test do that by default.
</description><key id="164684957">19351</key><summary>Cleanup usage of http.enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label></labels><created>2016-07-09T20:46:33Z</created><updated>2016-07-11T08:21:03Z</updated><resolved>2016-07-11T08:21:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-09T20:50:06Z" id="231555189">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/rest/action/main/RestMainActionIT.java</file></files><comments><comment>Cleanup usage of http.enabled (#19351)</comment></comments></commit></commits></item><item><title>String Query for java API dosen't find some documents by name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19350</link><project id="" key="" /><description>Hello! 

First, sorry for my poor english, but let me try to explain my problem. 

I'm working in an application using elasticsearch java api for managed my documents. 
Everything works fine, i'm able to search in DB and save on my index, i can count my documents aggregate by field and a lot of cool things, but i stucked on a weird problem. 

When i trying to search my document by field called name, some documents doesn't not return on search. 

Let me give an example:

My documents is look like this(just for example):

id: 1
name: book
type: pdf

---

id: 2
name: Test of my search service
type: zip

When i trying to search, if i search by name, send as parameter the value "book", it works fine, but when i trying to search, send my parameter value "service", the result is empty. 

Here my search code:

`SearchRequestBuilder src1 = client.prepareSearch()
                    .setQuery(QueryBuilders.queryStringQuery(parameter)
                            .field("name"));`

Anyone knows, why this search doesn't find my parameter value "service" on name field of document with id 2?

Thanks!
</description><key id="164677917">19350</key><summary>String Query for java API dosen't find some documents by name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EngErik</reporter><labels /><created>2016-07-09T17:55:12Z</created><updated>2016-07-09T18:42:25Z</updated><resolved>2016-07-09T18:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-09T18:42:25Z" id="231549237">This is a general question but Elastic reserves GitHub for bug reports and feature requests. You can ask general questions on the [Elastic Discourse forum](https://discuss.elastic.co). 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove cloud.enabled setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19349</link><project id="" key="" /><description>This setting `cloud.enabled` has been removed in master (5.0) which is nice.
I think we should also remove it in 2.4 as it is:
- useless
- error prone for our java transport client users. See https://discuss.elastic.co/t/transportclient-and-creating-an-s3-snapshot-repository/55088

If we don't remove this setting in 2.4, we need than to document this setting in every cloud related plugin documentation.

Thoughts?
- Remove?
- Document?
- Both (document in 2.3, 2.2, 2.1, 2.0 branches) and remove in 2.4?
</description><key id="164660900">19349</key><summary>Remove cloud.enabled setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Cloud Azure</label><label>:Plugin Cloud GCE</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-07-09T10:17:18Z</created><updated>2016-07-19T16:19:07Z</updated><resolved>2016-07-19T16:19:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-11T13:43:19Z" id="231737731">&gt; Both (document in 2.3, 2.2, 2.1, 2.0 branches) and remove in 2.4?

Document, and ignore the setting in 2.4 with deprecation logging.  We don't want a breaking change if we can avoid it.
</comment><comment author="dadoonet" created="2016-07-19T16:19:07Z" id="233686214">So that looks strange. I tried to look at the code today and reproduce the issue reported and was not able.

Here is what I did:
- Launch a node with default settings and `cloud-aws` plugin.
- Create a TransportClient like this:

```
        TransportClient client = new TransportClient.Builder()
                .addPlugin(CloudAwsPlugin.class)
                .build();
        client.addTransportAddress(new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 9300)));

        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
                .setType("s3").setSettings(Settings.EMPTY).get();
```

This does not raise the issue reported.
Also in code we default `cloud.enabled` to `true`. So I can't understand how this happened.

I think I should close it for now and may be reopen if the issue is confirmed by the user (on discuss).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove CustomNodeAttributes extension point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19348</link><project id="" key="" /><description>The DiscoveryNodeService exists to register CustomNodeAttributes which
plugins can add. This is not necessary, since plugins can already add
additional attributes, and use the node attributes prefix.

This change removes the DiscoveryNodeService, and converts the only
consumer, the ec2 discovery plugin, to add the ec2 availability zone
in additionalSettings().
</description><key id="164649607">19348</key><summary>Remove CustomNodeAttributes extension point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-07-09T04:46:14Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-09T19:46:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-09T07:03:07Z" id="231519871">Nice! 

Wondering why you labeled it as Breaking. It does not break a plugin feature right?
It only breaks for potential plugin developers AFAICT.
</comment><comment author="rjernst" created="2016-07-09T07:05:49Z" id="231519971">&gt; It only breaks for potential plugin developers AFAICT

Yes, but based on earlier PRs we've done for modifying the plugin api, we have been marking them breaking.

I'm not sure what moving the code will do (I don't see why it belongs as a static method there anymore than in the plugin), but I will figure out a way to unit test.
</comment><comment author="rjernst" created="2016-07-09T16:40:51Z" id="231543297">@dadoonet I pushed a commit with unit tests.
</comment><comment author="s1monw" created="2016-07-09T19:43:34Z" id="231552224">left one suggestions LGTM otherwise
</comment><comment author="dadoonet" created="2016-07-09T19:46:02Z" id="231552349">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPluginTests.java</file></files><comments><comment>Merge pull request #19348 from rjernst/deguice_attrs</comment></comments></commit></commits></item><item><title>template issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19347</link><project id="" key="" /><description>if one template name is template_one, args "template":"a_";another name is template_two,args "template":"aa_".If i create index name is "aa_20160709",which template will use.
</description><key id="164641709">19347</key><summary>template issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jacksu</reporter><labels /><created>2016-07-09T01:04:13Z</created><updated>2016-07-09T13:20:25Z</updated><resolved>2016-07-09T01:07:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-09T01:07:45Z" id="231505569">This is a general question but Elastic reserves GitHub for bug reports and feature requests. You can ask general questions on the [Elastic Discourse forums](https://discuss.elastic.co). Also, note that you could experiment with this very easily after checking the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html#multiple-templates) regarding ordering and know right away.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove child injectors from guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19346</link><project id="" key="" /><description>This change removes the ability for guice to have child injectors (and
the entire concept of parent injectors) from our fork of guice. The
methodology for removing was simple: I removed createChildInjector, and
continued to remove methods and members that were unused until my head
was spinning. The motivation for this change is to limit what our fork
of guice gives us access to, so we don't regress and start adding back
more complicated uses.
</description><key id="164635614">19346</key><summary>Remove child injectors from guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T23:35:41Z</created><updated>2016-07-09T19:33:06Z</updated><resolved>2016-07-09T19:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-09T19:32:28Z" id="231551683">I didn't even look - trash it
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/inject/Injector.java</file><file>core/src/main/java/org/elasticsearch/common/inject/InjectorBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java</file><file>core/src/main/java/org/elasticsearch/common/inject/InjectorShell.java</file><file>core/src/main/java/org/elasticsearch/common/inject/Injectors.java</file><file>core/src/main/java/org/elasticsearch/common/inject/ModulesBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/inject/PrivateModule.java</file><file>core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java</file><file>core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java</file><file>core/src/main/java/org/elasticsearch/common/inject/assistedinject/Parameter.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/ConvertedConstantBinding.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/Element.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/LinkedKeyBinding.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/ProviderBinding.java</file><file>core/src/main/java/org/elasticsearch/common/inject/spi/ProviderKeyBinding.java</file></files><comments><comment>Merge pull request #19346 from rjernst/deguice_guice</comment></comments></commit></commits></item><item><title>Modify foreach processor to accept a single processor instead of collection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19345</link><project id="" key="" /><description>@BigFunger, @Bargs, and @talevy had a discussion on Zoom earlier today, and this is one of the issues that we discussed.
### Summary

I would like to see the foreach processor reworked so that it only accepts a single processor instead of an array of processors as it does now.
### Background

I have been working on the UI for ingest pipelines, and specifically I have been trying to implement the foreach processor. The UI uses the verbose setting on the simulate API to report back to the user. 

This way, when the user add or edits a processor, I can use the output from the parent processor as the input of the next and provide them with the data necessary to build out their processors. This is a problem in the context of the foreach processor because I can't provide the user with the input and output of each of the processors defined within the foreach processor.

Per our discussion, it also make sense to structure the foreach processor in this way because it follows the patterns that have been established in the other processors. For example, if you want to apply an `uppercase` processor to more than one field in the document, you need to create one `uppercase` processor for each field you want to act on.
### Example

With a pipeline definition of the following:

``` json
{
  "pipeline": {
    "description": "",
    "processors": [
      {
        "split": {
          "tag": "processor_1",
          "field": "message",
          "separator": " "
        }
      },
      {
        "foreach": {
          "tag": "processor_2",
          "field": "message",
          "processors": [
            {
              "uppercase": {
                "tag": "processor_3",
                "field": "_value"
              },
              "lowercase": {
                "tag": "processor_4",
                "field": "_value"
              },
              "uppercase": {
                "tag": "processor_5",
                "field": "_value"
              }
            }
          ]
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "message": "these are the words of a sentence"
      }
    }
  ]
}
```

I would expect the following output:

``` json
{
  "docs": [
    {
      "processor_results": [
        {
          "tag": "processor_1",
          "doc": {
            "_type": "_type",
            "_id": "_id",
            "_index": "_index",
            "_source": {
              "message": [
                "these",
                "are",
                "the",
                "words",
                "of",
                "a",
                "sentence"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-06T21:27:14.585+0000"
            }
          }
        },
        {
          "tag": "processor_3",
          "doc": {
            "_type": "_type",
            "_id": "_id",
            "_index": "_index",
            "_source": {
              "message": [
                "THESE",
                "ARE",
                "THE",
                "WORDS",
                "OF",
                "A",
                "SENTENCE"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-06T21:27:14.585+0000"
            }
          }
        },
        {
          "tag": "processor_4",
          "doc": {
            "_type": "_type",
            "_id": "_id",
            "_index": "_index",
            "_source": {
              "message": [
                "these",
                "are",
                "the",
                "words",
                "of",
                "a",
                "sentence"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-06T21:27:14.585+0000"
            }
          }
        },
        {
          "tag": "processor_5",
          "doc": {
            "_type": "_type",
            "_id": "_id",
            "_index": "_index",
            "_source": {
              "message": [
                "THESE",
                "ARE",
                "THE",
                "WORDS",
                "OF",
                "A",
                "SENTENCE"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-06T21:27:14.585+0000"
            }
          }
        },
        {
          "tag": "processor_2",
          "doc": {
            "_type": "_type",
            "_id": "_id",
            "_index": "_index",
            "_source": {
              "message": [
                "THESE",
                "ARE",
                "THE",
                "WORDS",
                "OF",
                "A",
                "SENTENCE"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-06T21:27:14.585+0000"
            }
          }
        }
      ]
    }
  ]
}
```

Instead, I get this back:

``` json
{
  "docs": [
    {
      "processor_results": [
        {
          "tag": "processor_1",
          "doc": {
            "_id": "_id",
            "_type": "_type",
            "_index": "_index",
            "_source": {
              "message": [
                "these",
                "are",
                "the",
                "words",
                "of",
                "a",
                "sentence"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-08T21:36:14.400+0000"
            }
          }
        },
        {
          "tag": "processor_2",
          "doc": {
            "_id": "_id",
            "_type": "_type",
            "_index": "_index",
            "_source": {
              "message": [
                "these",
                "are",
                "the",
                "words",
                "of",
                "a",
                "sentence"
              ]
            },
            "_ingest": {
              "timestamp": "2016-07-08T21:36:14.400+0000"
            }
          }
        }
      ]
    }
  ]
}
```
</description><key id="164623624">19345</key><summary>Modify foreach processor to accept a single processor instead of collection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">BigFunger</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-07-08T21:53:44Z</created><updated>2017-01-20T00:18:39Z</updated><resolved>2016-07-13T20:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Bargs" created="2016-07-08T23:14:48Z" id="231494288">👍  In addition to @BigFunger's points about the requirements for the ingest UI, I think this change will make pipelines much easier to reason about, without really losing any power. When I started trying to think about the possible outcomes of deeply nested forEach processors, with an arbitrary number of sub-processors, and an arbitrary number of failure branches, my head started to hurt.
</comment><comment author="martijnvg" created="2016-07-11T07:28:23Z" id="231660688">I'm +1 with only allowing a single processor inside the `foreach` processor. The complexity can really increase quickly, which is unintended and doesn't serve the `foreach` processor's goal (performing on operation on all values in an array).
</comment><comment author="clintongormley" created="2016-07-11T13:53:18Z" id="231740586">👍 
</comment><comment author="talevy" created="2016-07-11T16:50:37Z" id="231794027">looks like there is consensus around this. I'll get on this
</comment><comment author="Bargs" created="2016-07-12T20:28:30Z" id="232170044">If foreach only accepts one processor, would it be more user friendly to expose this functionality as a flag that can be applied to any processor, as opposed to being a special processor that wraps other processors? 

For the nested loop use case (foreach inside a foreach) the user could simply specify the level of nesting they require. So to modify the nested numbers in a field value like `[[1, 2], [3, 4]]` they could declare a processor like this:

```
{
  "set": {
    "field": "_value",
    "value": "foo",
    "foreach": 2
  }
}
```

To me this feels a bit more declarative and natural if we only support looping one processor, but maybe it's just me.

**Edit** I was thinking through this as I typed it, just realized the example doesn't specify the parent field. I guess that would have to be specified as another field, which makes things more complex, so I'm not so sure I like the idea anymore.
</comment><comment author="talevy" created="2016-07-12T22:43:42Z" id="232204462">here is the PR for this issue: #19402 

@Bargs: Although I do agree `foreach` feels like it should be more of a 1st class citizen of the pipeline DSL, I am not sure how generally applicable this is to all types of processors and fields. I think that leaving it as its own Processor should still be fine for now.
</comment><comment author="hogbinj" created="2017-01-19T21:19:08Z" id="273901750">However it makes removing fields in an attachment array difficult there seems to be a limit of only one foreach in a Processors pipeline

[https://discuss.elastic.co/t/tuning-attachment-ingest-with-arrays-get-rid-of-the-raw-data/72220](url)</comment><comment author="talevy" created="2017-01-19T21:45:15Z" id="273908385">@hogbinj a little confused what to you mean. Here is a sample Simulate API request that runs a pipeline with multiple foreach processors

```
POST _ingest/pipeline/_simulate
{
  "pipeline" : {
    "processors" : [
      {
        "foreach" : {
          "field": "field",
          "processor" : {
            "uppercase" : { "field" : "_ingest._value.data" }
          }
        }
      },
      {
        "foreach" : {
          "field": "field",
          "processor" : {
            "remove" : { "field" : "_ingest._value.data" }
          }
        }
      }
    ]
  },
  "docs" : [
    {
      "_source" : {
        "field": [{"data": "a"}, {"data": "b"}, {"data": "c"}]
      }
    }
  ]
}
```</comment><comment author="hogbinj" created="2017-01-19T22:20:45Z" id="273917224">You cant do that in elastic cloud. 

Kibana only allows one foreach
![image](https://cloud.githubusercontent.com/assets/6162120/22127938/1de2030e-de96-11e6-9c80-1855ae2d50da.png)

</comment><comment author="talevy" created="2017-01-19T22:26:48Z" id="273918645">ah, that is because there is a missing closing brace before that second `foreach`

```
"processors" : [ { "foreach" : {...} }, { "foreach" : {...} }]
```</comment><comment author="hogbinj" created="2017-01-19T22:35:28Z" id="273920627">OK now the pipeline is correct syntax as Kibana takes it, although sense or DevTools is still throwing an error HOWEVER.....
![image](https://cloud.githubusercontent.com/assets/6162120/22128192/4ea2943a-de97-11e6-84cf-ab08a9767b68.png)

When I ingest data through the pipeline I've lost the whole attachment field. not just the data field

![image](https://cloud.githubusercontent.com/assets/6162120/22128236/9147cf9e-de97-11e6-9450-c83ff31c7b1b.png)



</comment><comment author="hogbinj" created="2017-01-19T22:38:52Z" id="273921393">Arrrgghh! Curly Bracket hell!!  Working now.  Thank you so much.  Needed one more closing bracket

![image](https://cloud.githubusercontent.com/assets/6162120/22128342/f947a63c-de97-11e6-9ea6-49eae1f284d1.png)
</comment><comment author="talevy" created="2017-01-20T00:18:38Z" id="273940421">sweet! happy to hear it all worked out!</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorTests.java</file></files><comments><comment>update foreach processor to only support one applied processor. (#19402)</comment></comments></commit></commits></item><item><title>Throwing error while importing geopoint field in elasticsearch using cmd</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19344</link><project id="" key="" /><description>**Version:** elasticsearch 2.3.3
**Operating System:** Windows 8.1

**Mapping File:**
{
"mappings": {
"ticket": {
"properties": {
"Action": {
"type": "string", "index" : "not_analyzed"
},
"Case Number": {
"type": "string"
},
"Department": {
"type": "string"
},
"Case Record Type": {
"type": "string"
},
"Centerline": {
"type": "geo_point",
"lat_lon": true
},
"Service Request Type": {
"type": "string"
},
"Status": {
"type": "string", "index" : "not_analyzed"
},
"Origin": {
"type": "string", "index" : "not_analyzed"
},
"Subject": {
"type": "string", "index" : "not_analyzed"
},
"Street": {
"type": "string"
},
"Contact Name": {
"type": "string", "index" : "not_analyzed"
},
"Created Date": {
"format": "date_time",
"type": "date"
},
"Closed Date": {
"format": "date_time",
"type": "date"
},
"SLA": {
"type": "string"
}
}
}
}
## }

**Sample Data:** ticket_1.json
{
"index": {
"_index": "tickets",
"_type": "Police Department",
"_id": 10670953
}
}{
"Case Number": "10670953",
"Department": "Police Department",
"Case Record Type": "Abandoned Vehicle",
"Service Request Type": "Abandoned Vehicle",
"Status": "Closed",
"Origin": "Phone",
"Subject": "Abandoned Vehicle",
"Street": "310 E WILLIAM ST",
"Contact Name": "",
"Centerline": [-75.12612642,
39.99305799],
"Created Date": "2016-04-08T18:32:12.000Z",
"Closed Date": "2016-04-25T16:23:26.000Z",
"SLA": "30 Business Days"
}{
"index": {
"_index": "tickets",
"_type": "License &amp; Inspections",
"_id": 10670956
}
}{
"Case Number": "10670956",
"Department": "License &amp; Inspections",
"Case Record Type": "Building Dangerous",
"Service Request Type": "Building Dangerous Occupied",
"Status": "In-Progress",
"Origin": "Phone",
"Subject": "Building Dangerous",
"Street": "642 S 55TH ST",
"Contact Name": "Steven Jones",
"Centerline": [-75.23307959,
39.94990503],
"Created Date": "2016-04-08T18:33:55.000Z",
"Closed Date": null,
"SLA": "5 Business Days"
}

---

**Command used for uploading data into elasticsearch**

curl -XPUT localhost:9200/tickets/_bulk?pretty --data-binary @ticket_1.json

---

**Error Thrown:**
{
"took" : 31,
"errors" : true,
"items" : [ {
"index" : {
"_index" : "tickets",
"_type" : "Police Department",
"_id" : "10670953",
"status" : 400,
"error" : {
"type" : "mapper_parsing_exception",
"reason" : "failed to parse",
"caused_by" : {
"type" : "illegal_state_exception",
"reason" : "Mixing up field types: class org.elasticsearch.index.mapper.core.DoubleFieldMapper$DoubleFieldType != class org.e
lasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field Centerline"
}
}
}
}, {
"index" : {
"_index" : "tickets",
"_type" : "License &amp; Inspections",
"_id" : "10670956",
"status" : 400,
"error" : {
"type" : "mapper_parsing_exception",
"reason" : "failed to parse",
"caused_by" : {
"type" : "illegal_state_exception",
"reason" : "Mixing up field types: class org.elasticsearch.index.mapper.core.DoubleFieldMapper$DoubleFieldType != class org.e
lasticsearch.index.mapper.geo.BaseGeoPointFieldMapper$GeoPointFieldType on field Centerline"
}
}
}
} ]
}
</description><key id="164586373">19344</key><summary>Throwing error while importing geopoint field in elasticsearch using cmd</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sauravsharma001</reporter><labels /><created>2016-07-08T18:14:31Z</created><updated>2016-07-13T13:46:57Z</updated><resolved>2016-07-11T13:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-11T13:38:23Z" id="231736378">You create a mapping called `ticket`, but then you create docs with types `Police Department` and `License &amp; Inspections`, without the geo point mapping
</comment><comment author="sauravsharma001" created="2016-07-11T15:54:55Z" id="231778133">Thanks @clintongormley. That resolved the error.
</comment><comment author="vishnubraj" created="2016-07-13T13:46:57Z" id="232359700">How did you solve it.. i am also having problem in creating maual ip type. Please look at this ticket https://discuss.elastic.co/t/mixing-up-field-types-error-ip-field-is-not-getting-added/55332/2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add RestController method for deprecating in one step</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19343</link><project id="" key="" /><description>This adds an extra method, registerWithDeprecatedHandler, to register both a normal handler and a deprecated handler at the same time. This helps with renaming methods as opposed to _just_ deprecated methods.

/cc @spinscale 
</description><key id="164571933">19343</key><summary>Add RestController method for deprecating in one step</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T16:52:13Z</created><updated>2016-07-13T20:14:16Z</updated><resolved>2016-07-13T17:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-13T16:58:51Z" id="232418871">LGTM! one minor nit comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `elasticsearch-translog` CLI tool with `truncate` command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19342</link><project id="" key="" /><description>This adds the `bin/elasticsearch-translog` bin file that will be used
for CLI tasks pertaining to Elasticsearch. Currently it implements only
a single sub-command, `truncate-translog`, that creates a truncated
translog for a given folder.

Here's what running the tool looks like:

```
λ bin/elasticsearch-translog truncate -d data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/
Checking existing translog files
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!   WARNING: Elasticsearch MUST be stopped before running this tool   !
!                                                                     !
!   WARNING:    Documents inside of translog files will be lost       !
!                                                                     !
!   WARNING:          The following files will be DELETED!            !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-10.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-18.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-21.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-12.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-25.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-29.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-2.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-5.tlog
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-41.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-6.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-37.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-24.ckp
--&gt; data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-11.ckp

Continue and DELETE files? [y/N] y
Reading translog UUID information from Lucene commit from shard at [data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/index]
Translog Generation: 3
Translog UUID      : AxqC4rocTC6e0fwsljAh-Q
Removing existing translog files
Creating new empty checkpoint at [data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog.ckp]
Creating new empty translog at [data/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0/translog/translog-3.tlog]
Done.
```

It also includes a `-b` batch operation that can be used to skip the
confirmation diaglog.

Resolves #19123
</description><key id="164570927">19342</key><summary>Add `elasticsearch-translog` CLI tool with `truncate` command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>feature</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T16:46:50Z</created><updated>2016-07-29T11:48:46Z</updated><resolved>2016-07-26T15:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-07-14T16:55:58Z" id="232725626">As someone not familiar with much about the trans-log, I'm curious as to when you would want to use this command, maybe you give me an example?  Thanks in advance.
</comment><comment author="dakrone" created="2016-07-14T16:59:17Z" id="232726507">&gt; As someone not familiar with much about the trans-log, I'm curious as to when you would want to use this command, maybe you give me an example? Thanks in advance.

Sure! So starting in 5.x, if your translog gets corrupted in any way (human error, bitflip on the hard drive, etc), without this you should have to trash the entire shard. This is because you can no longer just delete the translog if you don't care about losing the data, you also can't replace the corrupt translog with a clean one from another shard (in contains the shard UUID so it would know and flip out).

This tool allows you to accept the loss of all the documents in the translog by creating a new, empty translog for the given shard, so you can at least recover the data in the shard, even if you lose the translog documents.
</comment><comment author="dakrone" created="2016-07-14T22:20:50Z" id="232809561">@mikemccand thanks for taking a look! I pushed three more commits that changes this to:
- Use `DirectoryReader.listCommits()` and `IndexCommit.getUserData()` to read the UUID and generation without opening an IndexWriter
- Hold the lock open for the duration of the tool running to prevent the shard from being changed while this is running
- Enhance the warning message to mention that documents in the translogs will be lost
- Use the `Checkpoint` class to write the empty checkpoint instead of a raw byte array
</comment><comment author="mikemccand" created="2016-07-15T13:39:33Z" id="232953103">Thanks @dakrone, I left a few more comments, I think it's close!
</comment><comment author="dakrone" created="2016-07-15T16:25:18Z" id="232999644">Thanks @mikemccand! I pushed more commits:
- Actually acquire the write lock this time (and add a test for it that fails it if didn't acquire the lock)
- Fsync all the things that need fsyncing
- Don't use any raw byte arrays, goes through TranslogWriter and returns the length instead of hardcoding "43" in the checkpoint
</comment><comment author="mikemccand" created="2016-07-15T17:05:44Z" id="233009568">LGTM, thanks @dakrone!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FieldValueFactorFunctionParser retrieves the SearchContext with SearchContext.current()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19341</link><project id="" key="" /><description>This only works when executing search requests, since in that case there is a live search context. However this breaks the percolator which needs to parse queries when indexing documents.

For the record this seems to be fixed on master (5.0) thanks to the query refactoring. However 2.4 is still affected.
</description><key id="164568817">19341</key><summary>FieldValueFactorFunctionParser retrieves the SearchContext with SearchContext.current()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v5.1.1</label></labels><created>2016-07-08T16:35:04Z</created><updated>2016-11-21T13:23:15Z</updated><resolved>2016-10-18T07:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-11T07:58:25Z" id="231665967">`RandomScoreFunctionBuilder` has the same problem, still in master: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/functionscore/RandomScoreFunctionBuilder.java#L139
</comment><comment author="clintongormley" created="2016-10-18T07:42:23Z" id="254431238">@javanna @jpountz is this still a problem in 5.0?
</comment><comment author="jpountz" created="2016-10-18T07:45:46Z" id="254431953">It's not indeed! Fixed by #20778.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest Client: add short performRequest method variants without params and/or body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19340</link><project id="" key="" /><description>Users wanting to send a request by providing only its method and endpoint, effectively the only two required arguments, shouldn't need to pass in an empty map and a null entity for the body. While at it we can also add a variant to send requests by specifying only method, endpoint and params, but not body. Headers remain a vararg as last argument, so they can always optionally be provided.

Closes #19312
</description><key id="164567307">19340</key><summary>Rest Client: add short performRequest method variants without params and/or body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T16:26:58Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-07-11T08:37:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T16:30:31Z" id="231407042">Left a minor thing but LGTM.
</comment><comment author="javanna" created="2016-07-08T20:11:19Z" id="231460418">thanks @nik9000  I pushed another commit and updated more calls to use the new short methods
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpCompressionIT.java</file><file>core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsDisabledIT.java</file><file>core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java</file><file>core/src/test/java/org/elasticsearch/plugins/ResponseHeaderPluginIT.java</file><file>core/src/test/java/org/elasticsearch/rest/CorsNotSetIT.java</file><file>core/src/test/java/org/elasticsearch/rest/CorsRegexIT.java</file><file>core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file></files><comments><comment>Merge pull request #19340 from javanna/enhancement/perform_request_variants</comment></comments></commit></commits></item><item><title>Add bootstrap check for default cluster name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19339</link><project id="" key="" /><description>This commit adds a bootstrap check that the cluster name is changed from
the default cluster name.

Relates #19330
</description><key id="164553577">19339</key><summary>Add bootstrap check for default cluster name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label></labels><created>2016-07-08T15:18:27Z</created><updated>2016-07-11T10:15:50Z</updated><resolved>2016-07-08T22:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T15:23:49Z" id="231389317">LGTM
</comment><comment author="clintongormley" created="2016-07-08T15:45:55Z" id="231395361">I'm not convinced that this should be a bootstrap check.  It seems overly onerous to me - changing the cluster name is nothing like making sure you have enough open file handles.
</comment><comment author="jasontedor" created="2016-07-10T15:34:51Z" id="231595038">@clintongormley This idea has been floated before to help address misconfigured nodes. However, after consideration of the complexity of changing the cluster name and how it relates to the on-disk data folder, and considering also #18554, I have closed this PR. Thanks for the helpful discussion we had about it. :smile: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure metadata folder is not resurrected when loading latest state file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19338</link><project id="" key="" /><description>If `MetaDataStateFormat.loadLatestState()` is called while the folder on which it operates is being deleted, it is possible that this method resurrects the folder. Possibly affects folders for shard state metadata, index metadata or global metadata.

Test failure showing the issue:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=sles/729/console

Log snippets showing issue:

```
java.lang.AssertionError
  2&gt;    at __randomizedtesting.SeedInfo.seed([63923DF9F1B5687D]:0)
  2&gt;    at org.elasticsearch.env.NodeEnvironment.deleteShardDirectoryUnderLock(NodeEnvironment.java:491)
  2&gt;    at org.elasticsearch.indices.IndicesService.deleteShardStore(IndicesService.java:664)
  2&gt;    at org.elasticsearch.index.IndexService.onShardClose(IndexService.java:418)
  2&gt;    at org.elasticsearch.index.IndexService.access$100(IndexService.java:97)
  2&gt;    at org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:496)
  2&gt;    at org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:481)
  2&gt;    at org.elasticsearch.index.store.Store.closeInternal(Store.java:391)
  2&gt;    at org.elasticsearch.index.store.Store.access$000(Store.java:119)
  2&gt;    at org.elasticsearch.index.store.Store$1.closeInternal(Store.java:140)
  2&gt;    at org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64)
  2&gt;    at org.elasticsearch.index.store.Store.decRef(Store.java:373)
  2&gt;    at org.elasticsearch.index.store.Store.close(Store.java:381)
  2&gt;    at org.elasticsearch.index.IndexService.closeShard(IndexService.java:403)
  2&gt;    at org.elasticsearch.index.IndexService.removeShard(IndexService.java:375)
  2&gt;    at org.elasticsearch.index.IndexService.close(IndexService.java:236)
  2&gt;    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:504)
  2&gt;    at org.elasticsearch.indices.IndicesService.deleteIndex(IndicesService.java:566)
  2&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndices(IndicesClusterStateService.java:244)
  2&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:178)
  2&gt;    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:691)
  2&gt;    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:855)
  2&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:450)
  2&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  2&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  2&gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=63923DF9F1B5687D -Dtests.class=org.elasticsearch.indices.state.RareClusterStateIT -Dtests.method="testUnassignedShardAndEmptyNodesInRoutingTable" -Dtests.security.manager=true -Dtests.locale=mt-MT -Dtests.timezone=Asia/Brunei
```

and 

```
 1&gt; [2016-07-08 07:33:46,159][DEBUG][org.elasticsearch.gateway] [node_t0] /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/sles/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_63923DF9F1B5687D-001/tempDir-001/d0/nodes/0/indices/CZHA3myYTsCy1yArn911ow/0/_state/state-0.st: failed to read [state-], ignoring...
  1&gt; java.nio.file.NoSuchFileException: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/sles/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_63923DF9F1B5687D-001/tempDir-001/d0/nodes/0/indices/CZHA3myYTsCy1yArn911ow/0/_state/state-0.st
  1&gt;    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
  1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
  1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
  1&gt;    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
  1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
  1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
  1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
  1&gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newByteChannel(HandleTrackingFS.java:240)
  1&gt;    at org.apache.lucene.mockfile.FilterFileSystemProvider.newByteChannel(FilterFileSystemProvider.java:212)
  1&gt;    at org.apache.lucene.mockfile.HandleTrackingFS.newByteChannel(HandleTrackingFS.java:240)
  1&gt;    at java.nio.file.Files.newByteChannel(Files.java:361)
  1&gt;    at java.nio.file.Files.newByteChannel(Files.java:407)
  1&gt;    at org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:77)
  1&gt;    at org.elasticsearch.gateway.MetaDataStateFormat.read(MetaDataStateFormat.java:184)
  1&gt;    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:319)
  1&gt;    at org.elasticsearch.index.shard.ShardPath.loadShardPath(ShardPath.java:115)
  1&gt;    at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:133)
  1&gt;    at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:57)
  1&gt;    at org.elasticsearch.action.support.nodes.TransportNodesAction.nodeOperation(TransportNodesAction.java:143)
  1&gt;    at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:268)
  1&gt;    at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:264)
  1&gt;    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69)
  1&gt;    at org.elasticsearch.transport.TransportService$5.doRun(TransportService.java:517)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

The failing assertion checks after successful shard deletion that the shard folder has gone. It fails as the folder reappears. The reason for this is that there is still a concurrent operation on the node loading shard state metadata (triggered by async fetching).
</description><key id="164551975">19338</key><summary>Ensure metadata folder is not resurrected when loading latest state file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-07-08T15:10:38Z</created><updated>2017-05-23T09:02:22Z</updated><resolved>2017-05-23T09:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-07-11T09:52:20Z" id="231690527">It really is quite scary to have to fork Lucene's entire `SimpleFSDirectory` here!  @s1monw suggested allowing a subclass to override the `mkdir` behavior on init in Lucene ... I'll explore this.
</comment><comment author="mikemccand" created="2016-07-11T13:25:25Z" id="231733073">&gt;  @s1monw suggested allowing a subclass to override the mkdir behavior on init in Lucene 

I opened https://issues.apache.org/jira/browse/LUCENE-7375 for this
</comment><comment author="dakrone" created="2016-09-12T22:09:36Z" id="246511057">@ywelsch is this stalled pending the linked Lucene change, or is it waiting on other review?
</comment><comment author="ywelsch" created="2016-12-08T09:07:15Z" id="265689590">no progress on the Lucene end. Should we proceed with the current solution here?</comment><comment author="bleskes" created="2016-12-08T09:16:07Z" id="265691412">Maybe another route here is to use `SimpleFSIndexInput` directly, rather than going through a directory? we lose some testing assertions, but maybe it's worth it for this simple case? (read only, closing files in the same methods etc.</comment><comment author="elasticmachine" created="2017-02-23T18:14:41Z" id="282074249">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="imotov" created="2017-03-24T18:36:52Z" id="289109524">Just want to mention that this test failed 5 times so far in March. The latest failure is here https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.x+multijob-unix-compatibility/os=ubuntu/619/console</comment><comment author="ywelsch" created="2017-04-04T14:14:46Z" id="291513172">&gt; Maybe another route here is to use SimpleFSIndexInput directly, rather than going through a directory? 

@bleskes The issue is that `SimpleFSIndexInput ` is only package-visible (although its constructor is public), see also https://github.com/elastic/elasticsearch/pull/19338#r70124957</comment><comment author="ywelsch" created="2017-05-23T09:02:22Z" id="303336580">I think the easiest solution here would be to have Lucene expose SimpleFSIndexInput as public. We have been having test failures for more than a year now and it seems impossible to get this PR done, so I will just close it.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Let fast vector highlighter also extract terms from the nested query's inner query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19337</link><project id="" key="" /><description>PR for #19265
</description><key id="164546956">19337</key><summary>Let fast vector highlighter also extract terms from the nested query's inner query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Highlighting</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T14:47:23Z</created><updated>2016-07-13T07:14:45Z</updated><resolved>2016-07-13T06:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T15:28:00Z" id="231390489">I don't know the `ToParentBlockJoinQuery` at all but this is how I'd hack support for it into the FVH. I might try to open a Lucene issue to get it in instead/in addition, but otherwise LGTM.
</comment><comment author="jpountz" created="2016-07-08T16:07:46Z" id="231400954">+1 to the Lucene issue and LGTM to this one
</comment><comment author="martijnvg" created="2016-07-09T08:52:36Z" id="231523994">@nik9000 @jpountz I'll open an issue in Lucene. I initially thought that the highlighter module didn't depend on the join module, but I was wrong, it does!
</comment><comment author="martijnvg" created="2016-07-13T07:14:45Z" id="232275153">The Lucene issue for referencing: https://issues.apache.org/jira/browse/LUCENE-7376
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Clean up more messy tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19336</link><project id="" key="" /><description>Same as #19280 and #19302 but for more tests. We're getting close to the end but there is still a bunch of tests to clean up.

Related to #13837.
</description><key id="164538298">19336</key><summary>[TEST] Clean up more messy tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T14:05:58Z</created><updated>2016-07-25T15:04:34Z</updated><resolved>2016-07-25T15:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T16:37:48Z" id="231408858">Left some minor stuff. LGTM.
</comment><comment author="tlrx" created="2016-07-25T15:04:34Z" id="234980513">Thanks @nik9000 ! I updated and merged the code according to your comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptSettings.java</file><file>core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/script/FileScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupIT.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/StoredScriptsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyIndexedScriptTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/TemplateQueryBuilderTests.java</file><file>test/framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment> [TEST] Kill remaining lang-groovy messy tests</comment></comments></commit></commits></item><item><title>Track network metrics between nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19335</link><project id="" key="" /><description>Typically Elasticsearch doesn't work well in cross-datacentre architectures, but how can you define that?   So long as there is reliable and ample network connection between 2 sites, why not?   If Elasticsearch had insight into the reliability of it's relationship to other nodes in the cluster, this could serve as a vital cluster health metrics.

To know that, it would be great if each ES node could track shard transfer rate, ping time, packet loss, relative uptime, etc, metrics against any/all other known nodes.   Also tracking  minimum_masters stable time from each node's perspective would be useful too.

The results of the metrics could be used in diagnosing or indicating stability problems due to network issues.   The availability metrics would be skewed by node restarts, etc, but it would still be highly useful.  The transfer rate data would always be consistent.
</description><key id="164535097">19335</key><summary>Track network metrics between nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2016-07-08T13:49:45Z</created><updated>2016-10-20T14:50:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T09:50:25Z" id="232909340">Discussed in FixItFriday.  Agreed that at least some of these metrics would be good to have, but it would be a time-consuming and tedious job to add these stats.  Nice to have, but maybe not worth the effort?

I'll mark it as adoptme and high hanging fruit
</comment><comment author="ywelsch" created="2016-07-15T10:07:55Z" id="232912740">A simpler way to get started here might be to log warnings on the node (similar to slow logs). If pinging takes longer than a (user-definable) threshold, we could for example log a warning. Same for slow shard transfer rates etc.
</comment><comment author="inqueue" created="2016-10-20T14:40:59Z" id="255125596">+1 @ywelsch
</comment><comment author="jpcarey" created="2016-10-20T14:50:47Z" id="255128658">+1 @ywelsch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate elasticsearch native script examples to the main repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19334</link><project id="" key="" /><description>I have been maintaining a separate project that demonstrates how to build different native scripts for elasticsearch-native-script-example. I think it might make sense to add this project to the main repo similarly to jvm-example.

Closes #14662
</description><key id="164534773">19334</key><summary>Migrate elasticsearch native script examples to the main repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v5.0.0-beta1</label></labels><created>2016-07-08T13:48:07Z</created><updated>2016-08-11T22:07:37Z</updated><resolved>2016-08-11T22:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-07-18T15:14:49Z" id="233359154">@rjernst I removed all examples of using indexLookup() from this PR. Could you take another look?
</comment><comment author="imotov" created="2016-08-10T16:10:11Z" id="238917351">@rjernst any chance you can review it?
</comment><comment author="rjernst" created="2016-08-11T20:27:26Z" id="239281722">@imotov I left some comments. A general concern that I have is we should not really be showing these simple examples, because then users will find the examples and copy/paste them, when in fact these should be done in normal scripts (with painless or expressions). The only case I see for native scripts is when someone needs to call some custom code, but that should be extremely rare and advanced (which would be fine to have examples for, but then show an example of calling custom code, not trivial examples doing things that can be done in painless). Everything else should be done through painless examples.  Another concern I have is someone thinking that copying these native scripts will be faster because they are "native" (a general problem with native scripts altogether). I think there is a misbelief that native scripts are always faster. In fact, the current scripting api in how they access fields can cause them to be _slower_ than eg expressions.

My comments are only a general caution for us to think about and discuss; there is nothing generally wrong with the gradle side or anything like that, it is only apprehension about them in general, now that I have gone through all of the examples (I stopped in my first review once I saw the index stats examples).
</comment><comment author="imotov" created="2016-08-11T21:02:25Z" id="239291507">These examples are simple because their purpose was to show how to interact with elasticsearch and not how to call non-trivial custom code. I think they are useful because they are simple. If we make them complex and heavy by pulling some large 3rd party dependencies it will confuse users who will follow these examples. I think pulling this code out, compiling it, installing a plugin, rebuilding the plugin every time elasticsearch version changes are good enough deterrents that would convince any reasonable user to use painless instead of maintaining native plugins unless they really have to. 

However, I understand your concern about sending a wrong message by placing these example into the main repo and I am fine with continuing to maintain these examples as a separate project in my spare time. It would be great if we could decide if we want this in or not rather sooner than later, though. 
</comment><comment author="rjernst" created="2016-08-11T21:16:53Z" id="239295380">I don't think we need to pull in a separate library to show an example, but there could be some dummy function being called which is documented as "do something special that cannot be done in normal scripts", and also comments in the readme/docs that recommend first trying to build scripts with painless/expressions, and only resorting to native scripts if their is some complex code which must be called that cannot be done inside painless. I had thought BigInteger was available inside painless, but it looks like not, so perhaps that is a sufficient example for now. But the scripted metric agg?

At minimum, if we are going to merge this (I'm on the fence personally, I would like to hear opinions from others), we should have the readme/docs warn that these should only be used in rare circumstances.
</comment><comment author="jdconrad" created="2016-08-11T21:25:52Z" id="239297767">I understand the desire to make simple examples for native scripts to just show the structure; however, I agree with @rjernst here that this may give the wrong impression.

@rmuir and I went through a bunch of scripting samples from actual users pulled from tickets, and I would wager 75% or more of them were almost direct copies from the examples.  I really feel like these scripts should show off something that can't be done via some other scripting language, Expressions, Painless, Groovy, Javascript, Python, etc, otherwise a user may be lead to believe he/she needs to do this for something that could be easily taken care of with another language.  This is an advanced feature and advanced examples should be okay here.

Our documentation doesn't lend itself to easily figure out that another language could be used should a user Google to native scripts documentation directly.  This should likely also be fixed, but it also does emphasize the existing problem that a user will copy an example for a native script when it's not necessary.
</comment><comment author="rjernst" created="2016-08-11T21:31:18Z" id="239299216">@imotov I wonder if we should consider removing native scripts altogether? But we could still have custom java scripts, it would just be examples of how to build a script engine. Eg, making NativeScriptEngineService the example we have? It's not really any different amount of setup code, it would just be different, and it could be very clear in the docs, as well as just the naming (building a script engine, vs having a "native script" which sounds so much more lightweight) that this is a heavyweight solution for advanced use cases.  Just a thought...
</comment><comment author="imotov" created="2016-08-11T21:55:07Z" id="239305330">First of all, I agree with a need of a large disclaimer, saying that in most cases, users should use painless.

&gt; @rmuir and I went through a bunch of scripting samples from actual users pulled from tickets, and I would wager 75% or more of them were almost direct copies from the examples.

@jdconrad Are these native scripts or just script samples? In either case, do you think there is a bit of selection bias with this sampling? It seems to me that users are more likely to ask questions about scripts when they just start working with them and they are more likely to start with provided examples. Moreover, they might be reluctant of posting large internal scripts on public forums. 

&gt; @imotov I wonder if we should consider removing native scripts altogether? 

All script that were added to the project were added in response to needs of concrete users who needed to have a script that plays a particular role or uses a certain technique. If it was just about writing a native script this plugin would have contained only one script. We can remove native scripts and replace them with a single NativeScriptEngineService (native scrip are now really toothless in 5.0 anyway) but it wouldn't change the need for different scripts used in different contexts. So, I am not sure how it would help. 
</comment><comment author="jdconrad" created="2016-08-11T21:59:22Z" id="239306397">@imotov I completely agree that there is a likely selection bias here, but I would speculate these are the users that are the most vulnerable to falling into the trap of following a native script example without understanding there may be a better option available to them.

As a side note, out of a personal curiosity, are there any examples of real users native scripts that you have available?  I'd just like to see what people are using them for in case there are features that we could possibly add to Painless.
</comment><comment author="imotov" created="2016-08-11T22:05:06Z" id="239307795">&gt; I would speculate these are the users that are the most vulnerable to falling into the trap of following a native script example without understanding there may be a better option available to them.

I disagree with this statement because I think we made it hard enough for users to fall into this trap, but I cannot offer anything better than my contra-speculation :)
</comment><comment author="rjernst" created="2016-08-11T22:05:06Z" id="239307797">The current examples I see here are:
1. isPrime
2. adding popularity to scoring
3. adding randomness to scoring
4. using scripted_metric

Of these, the 2 and 3 shouldn't be done in native scripts: even if someone asked for how to do it, they should be pointed to existing docs we have on using an expression script for adding popularity, and a function score to add randomness.  For scripted_metric, I think anyone that can understand how scripted metrics work (with multiple scripts) would be trivially able to use it from any scripting language (whether that be with native scripts or not, it is just how the script is referenced in the scripted metric request).  But the example here is trival and I would not want someone to find this example and be doing scripted metrics with native scripts.

&gt; We can remove native scripts and replace them with a single NativeScriptEngineService (native scrip are now really toothless in 5.0 anyway) but it wouldn't change the need for different scripts used in different contexts. So, I am not sure how it would help.

I think it would help in the naming at least, and understanding that these are heavyweight things (an engine) instead of something lightweight (a script).
</comment><comment author="imotov" created="2016-08-11T22:07:37Z" id="239308398">Yep, you are right. It's kind of pointless with all the interesting stuff that used index lookup being removed. Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Warn about slow node performance eg disk I/O</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19333</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**:1.8.0_60

**OS version**: centos-7.0

**Description of the problem including expected versus actual behavior**:
my cluster consists  of 10 data node, when one node stuck on disk io(because of hardware problem),  the whole cluster write stuck for several minutes(&gt; 10 minutes),  the bad node was not removed from cluster automatically.
**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
there may be a thread to monitor disk io timeout, if it happened and exceed a configured threshold, remove the bad node from cluster.
</description><key id="164530379">19333</key><summary>Warn about slow node performance eg disk I/O</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">curu</reporter><labels><label>:Cluster</label><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2016-07-08T13:25:57Z</created><updated>2016-07-21T05:33:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T10:01:11Z" id="232911446">Hi @curu 

We discussed this in FixItFriday.  Removing a node because of slow disk I/O is quite an aggressive decision to make.  I would be hesitant to have Elasticsearch make this decision.  eg you remove one node, so the other nodes have to do shard recovery.  now they're slow too, so we remove another node, etc.

Instead, we could log warnings about things like slow disk I/O and a monitoring system could pick up on these warnings and alert the sysadmin.
</comment><comment author="andrestc" created="2016-07-16T14:17:59Z" id="233132421">Hi @clintongormley, I think we could expose the i/o wait % on the node stats api. Might be a good metric to watch for i/o problems and can the api can be easily read by external monitoring systems.
</comment><comment author="jasontedor" created="2016-07-16T15:17:13Z" id="233135109">It is worth mentioning that I/O wait was considered in #15915 but we ultimately pulled it out. 
</comment><comment author="curu" created="2016-07-21T05:33:05Z" id="234160941">@clintongormley , agreed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add blocking socket based MockTcpTransport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19332</link><project id="" key="" /><description>Today we have a bunch of tests that use netty transport for several reasons
these tests use it because they need to run some tcp based transport. Yet, this
couples our tests tightly to the netty implementation which should be tested on it's own.
This change adds a plain socket based blocking TcpTransport implementation that is used by
default in tests if local transport is suppressed or if network is selected.
It also adds another tcp network implementation as a showcase how the interface works.
</description><key id="164528783">19332</key><summary>Add blocking socket based MockTcpTransport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T13:17:38Z</created><updated>2016-07-18T08:30:25Z</updated><resolved>2016-07-11T10:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-08T18:40:03Z" id="231439832">This looks great! I left a few small comments, LGTM as is though except for the question about commented out code.
</comment><comment author="s1monw" created="2016-07-08T19:10:54Z" id="231447447">@rjernst I pushed a new commit
</comment><comment author="rjernst" created="2016-07-08T19:28:12Z" id="231451231">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/transport/NetworkExceptionHelper.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTcpTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/MockTcpTransportPlugin.java</file><file>test/framework/src/test/java/org/elasticsearch/transport/MockTcpTransportTests.java</file></files><comments><comment>Add blocking socket based MockTcpTransport (#19332)</comment></comments></commit></commits></item><item><title>Query evaluation: Add Discounted Cumulative Gain ranking metric</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19331</link><project id="" key="" /><description>Similar to #19300, this adds another query ranking  evaluation metric to the `rank_eval` feature branch. Computation follows the alternative (2nd) formulation of DCG in https://en.wikipedia.org/wiki/Discounted_cumulative_gain. Documents not rated by the user are assumed to have a rating of 0. I didn't add normalization (nDCG) so far but this should be easily extendable, maybe with an added `normalize` option.

Relates to #19195
</description><key id="164528659">19331</key><summary>Query evaluation: Add Discounted Cumulative Gain ranking metric</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>feature</label><label>review</label><label>WIP</label></labels><created>2016-07-08T13:16:58Z</created><updated>2016-08-08T14:49:08Z</updated><resolved>2016-08-08T14:49:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-07-08T13:17:31Z" id="231356637">@MaineC and another metric, can you take a look when you are back?
</comment><comment author="cbuescher" created="2016-07-27T15:18:11Z" id="235619020">@MaineC I added a "normalize" option which should now calculate the Normalized DCG. I also added an option for providing a rating for "unknown" documents, but I'm not sure how useful this is. Also there are some edge cases that need some more attention (e.g. what if there's more rated docs than returned docs in search (because of e.g. size limit), how should this affect the metric), but I think this is good for a first review.
</comment><comment author="MaineC" created="2016-08-03T12:35:38Z" id="237224872">Checked the changes, LGTM
</comment><comment author="MaineC" created="2016-08-08T12:47:41Z" id="238224999">As discussed on Slack: LGTM, trusting your rebasing skills. The test you adjusted looks pretty much like the test that occasionally failed for me when running the tests for #19648 - so looking forward to getting the fix.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename default cluster name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19330</link><project id="" key="" /><description>Today, when not explicitly configured, the cluster name defaults to `elasticsearch`. With the broader role ES serves in the stack (specifically when looking at it in Kibana 5.0+) this can be quite confusing.

We should instead name it `my_elastic`. We believe this name is appropriate as:
- it communicates the cluster role in the context of the elastic stack
- the `my_` prefix is "annoying" enough to encourage the users to renamed it :)
</description><key id="164524584">19330</key><summary>Rename default cluster name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Core</label><label>enhancement</label><label>v6.0.0</label></labels><created>2016-07-08T12:53:37Z</created><updated>2017-05-03T06:55:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-08T13:13:48Z" id="231355815">I'd personally name it `unnamed_cluster` or `unnamed_elastic_cluster` I think it should be clear that it must be changed. `my_` isn't clear IMO
</comment><comment author="jasontedor" created="2016-07-08T14:48:34Z" id="231379735">We don't need to pick a Windows 95-esque name to encourage users to change the cluster name. We can just add a bootstrap check that requires a non-default cluster name if the instance is running in production mode.
</comment><comment author="jasontedor" created="2016-07-08T15:23:08Z" id="231389112">I opened #19339.
</comment><comment author="jasontedor" created="2016-07-10T15:32:41Z" id="231594918">After consideration of the complexity of changing the cluster name and how it relates to the on-disk data folder, and considering also #18554, I have closed #19339. I do not think we should choose an intentionally "annoying" name. 
</comment><comment author="uboness" created="2016-07-10T22:36:49Z" id="231615102">@jasontedor I'm fine with a bootstrap check... but that's still missing the point raised in this issue. The current name doesn't play well with our stack and we need to change it. 

`unnamed`, `cluster` or `elastic` all don't play well on their own. `my_elastic` is still my vote (even if for some it brings back memories from the past). With the direction kibana is going, It's the one name that when opening Kibana and reading `/cluster/my_elastic` in the breadcrumbs, actually makes sense and to some extend friendly.

Big -1 on leaving it as is
</comment><comment author="jasontedor" created="2016-07-12T11:48:08Z" id="232022067">@uboness My previous comment was not a comment for leaving it as is, just a comment against choosing an annoying name.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a unit test that sends random requests among 3 nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19329</link><project id="" key="" /><description>This adds a test that uses transport implementation and sends random requests
to 3 different nodes, the request handlers maybe forwarding the requests to yet another node
etc. until returning the response. This test basically tests that nodes are not deadlocking
in a distributed fashion.
</description><key id="164510993">19329</key><summary>Add a unit test that sends random requests among 3 nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>review</label><label>test</label></labels><created>2016-07-08T11:23:02Z</created><updated>2016-07-08T12:13:36Z</updated><resolved>2016-07-08T12:13:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-08T11:54:46Z" id="231341418">I like it, LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file></files><comments><comment>Add a unit test that sends random requests among 3 nodes (#19329)</comment></comments></commit></commits></item><item><title>Disable service in pre-uninstall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19328</link><project id="" key="" /><description>Today in the packaging removal scripts, we disable the service in
post-uninstall. Yet, this happens after service files have been
erased. On some systems, this can cause the service disable to fail
leaving behind state causing the service to be enabled on subsequent
installs. This commit moves the service disabling to the pre-uninstall
script to prevent this issue.
</description><key id="164507968">19328</key><summary>Disable service in pre-uninstall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T11:02:20Z</created><updated>2016-07-08T11:58:52Z</updated><resolved>2016-07-08T11:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dliappis" created="2016-07-08T11:43:29Z" id="231339634">Awesome! Clearly this is the right order of doing this. Thanks for being thorough with the tests!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Disable service in pre-uninstall</comment></comments></commit></commits></item><item><title>Forbid the usage or `range` queries with a range based on current time in percolator queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19327</link><project id="" key="" /><description>If there are percolator queries containing `range` queries with ranges based on the current time then this can lead to incorrect results if the `percolate` query gets cached.  These ranges are changing each time the `percolate` query gets executed and if this query gets cached then the results will be based on how the range was at the time when the `percolate` query got cached.

The ExtractQueryTermsService has been renamed `QueryAnalyzer` and now only deals with analyzing the query (extracting terms and deciding if the entire query is a verified match) . The `PercolatorFieldMapper` is responsible for adding the right fields based on the analysis the `QueryAnalyzer` has performed, because this is highly dependent on the field mappings. Also the `PercolatorFieldMapper` is responsible for creating the percolate query.
</description><key id="164507670">19327</key><summary>Forbid the usage or `range` queries with a range based on current time in percolator queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking</label><label>review</label></labels><created>2016-07-08T11:00:20Z</created><updated>2016-07-08T16:32:33Z</updated><resolved>2016-07-08T12:23:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-08T11:04:38Z" id="231333760">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19326</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="164503585">19326</key><summary>2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kam10101</reporter><labels /><created>2016-07-08T10:33:16Z</created><updated>2016-07-08T10:34:52Z</updated><resolved>2016-07-08T10:34:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-08T10:34:52Z" id="231328784">This pull request appears to have been opened in error, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest Client: add slash to log line when missing between host and uri</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19325</link><project id="" key="" /><description>Rest Client: add slash to log line when missing between host and uri

Closes #19314
</description><key id="164502431">19325</key><summary>Rest Client: add slash to log line when missing between host and uri</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T10:26:11Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-07-08T10:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-08T10:36:11Z" id="231329024">The change looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file></files><comments><comment>Merge pull request #19325 from javanna/fix/request_logger_missing_slash</comment></comments></commit></commits></item><item><title>Add RepositoryPlugin interface for registering snapshot repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19324</link><project id="" key="" /><description>Repository plugins currently use a lot of custom classes like
RepositoryName and RepositorySettings in order to use guice to construct
repository implementations. But repositories now only really need their
settings to be constructed. Anything else they need (eg a cloud client)
can be constructed within the plugin, instead of via guice.

This change makes repository plugins use the new pull model. It removes
guice from the construction of Repository objects (no more child
injectors) and also from all repository plugins.
</description><key id="164472456">19324</key><summary>Add RepositoryPlugin interface for registering snapshot repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T07:14:29Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-08T21:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-08T08:22:30Z" id="231302945">oh man this is so good! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/RepositoryPlugin.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesModule.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryName.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryTypesRegistry.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureRepositoryModule.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettings.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceIntegTestCase.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceTests.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilterTests.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobStoreTests.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStorageModule.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStoragePlugin.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepository.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageService.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageBlobStoreRepositoryTests.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsPlugin.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/S3Module.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/RepositoryS3SettingsTests.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3RepositoryTests.java</file></files><comments><comment>Merge pull request #19324 from rjernst/repository_deguice2</comment></comments></commit></commits></item><item><title>unassigned shards with cluster in docker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19323</link><project id="" key="" /><description>here is my compose file,escp is my docker images files

```
elasticsearch_master:
    #image: elasticsearch:latest
    image: escp
    command: "elasticsearch \
      -Des.cluster.name=dcluster \
      -Des.node.name=esmaster \
      -Des.node.master=true \
      -Des.node.data=true \    
      -Des.node.client=false \
      -Des.discovery.zen.minimum_master_nodes=1"

    volumes:
      - "${PWD}/es/config:/usr/share/elasticsearch/config"
      - "${PWD}/esdata/node:/usr/share/elasticsearch/data"
      - "${PWD}/es/plugins:/usr/share/elasticsearch/plugins"      
    environment:
       - ES_HEAP_SIZE=512m
    ports:
      - "9200:9200"
      - "9300:9300"

elasticsearch1:
    #image: elasticsearch:latest
    image: escp
    command: "elasticsearch \ 
      -Des.cluster.name=dcluster \
      -Des.node.name=esnode1 \
      -Des.node.data=true \
      -Des.node.client=false \
      -Des.node.master=false \
      -Des.discovery.zen.minimum_master_nodes=1 \
      -Des.discovery.zen.ping.unicast.hosts=elasticsearch_master"
    links:
      - elasticsearch_master
    volumes:
      - "${PWD}/es/config:/usr/share/elasticsearch/config"
      - "${PWD}/esdata/node1:/usr/share/elasticsearch/data"
      - "${PWD}/es/plugins:/usr/share/elasticsearch/plugins"
    environment:
       - ES_HEAP_SIZE=512m

elasticsearch2:
    #image: elasticsearch:latest
    image: escp
    command: "elasticsearch \
      -Des.cluster.name=dcluster \
      -Des.node.name=esnode2 \
      -Des.node.data=true \
      -Des.node.client=false \
      -Des.node.master=false \
      -Des.discovery.zen.minimum_master_nodes=1 \
      -Des.discovery.zen.ping.unicast.hosts=elasticsearch_master"
    links:
      - elasticsearch_master
    volumes:
      - "${PWD}/es/config:/usr/share/elasticsearch/config"
      - "${PWD}/esdata/node2:/usr/share/elasticsearch/data"
      - "${PWD}/es/plugins:/usr/share/elasticsearch/plugins"
    environment:
       - ES_HEAP_SIZE=512m
```

this is config file

```

index.number_of_shards: 1
index.number_of_replicas: 0
network.host: 0.0.0.0
```

after running

```

           Name                         Command               State                       Ports
--------------------------------------------------------------------------------------------------------------------
est_elasticsearch1_1         /docker-entrypoint.sh elas ...   Up      9200/tcp, 9300/tcp
est_elasticsearch2_1         /docker-entrypoint.sh elas ...   Up      9200/tcp, 9300/tcp
est_elasticsearch_master_1   /docker-entrypoint.sh elas ...   Up      0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp
```

but when i create new index there will show UNASSIGNED...

```

curl -s '192.168.99.100:9200/_cluster/health?pretty'
{
  "cluster_name" : "dcluster",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 1,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 0.0
}
```

check nodes

```

curl -s '192.168.99.100:9200/_cat/nodes?v'
host       ip         heap.percent ram.percent load node.role master name
172.17.0.2 172.17.0.2           13          33 0.00 d         *      esmaster
172.17.0.3 172.17.0.3           16          33 0.00 d         -      esnode1
172.17.0.4 172.17.0.4           13          33 0.00 d         -      esnode2
```

check shards

```

curl -s '192.168.99.100:9200/_cat/shards'
abcq 0 p UNASSIGNED
```

check allocation

```

curl -s '192.168.99.100:9200/_cat/allocation?v'
shards disk.indices disk.used disk.avail disk.total disk.percent host       ip         node
     0           0b   223.4gb      9.5gb    232.9gb           95 172.17.0.4 172.17.0.4 esnode2
     0           0b   223.4gb      9.5gb    232.9gb           95 172.17.0.2 172.17.0.2 esmaster
     0           0b   223.4gb      9.5gb    232.9gb           95 172.17.0.3 172.17.0.3 esnode1
     1                                                                                 UNASSIGNED
```

check setting

```

curl 'http://192.168.99.100:9200/_cluster/settings?pretty'
{
  "persistent" : { },
  "transient" : { }
}
```

enabled reroute

```

curl 'http://192.168.99.100:9200/_cluster/settings?pretty'
{
  "persistent" : { },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "true"
        }
      }
    }
  }
}
```

reroute index abcq

```

    curl -XPOST http://192.168.99.100:9200/_cluster/reroute?pretty -d '{ 
    "commands" : [ 
        { 
            "allocate" : {
                "index" : "abcq",
                "shard" : 0, 
                "node" : "esnode2", 
                "allow_primary" : true 
            } 
        } 
    ] 
}'
```

get error bellow

```

{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "[allocate] allocation of [abcq][0] on node {esnode2}{Pisl95VUSPmZa3Ga_e3sDA}{172.17.0.4}{172.17.0.4:9300}{master=false} is not allowed, reason: [YES(shard is primary)][YES(no allocation awareness enabled)][NO(more than allowed [90.0%] used disk on node, free: [4.078553722498398%])][YES(allocation disabling is ignored)][YES(primary shard can be allocated anywhere)][YES(node passes include/exclude/require filters)][YES(shard is not allocated to same node or host)][YES(total shard limit disabled: [index: -1, cluster: -1] &lt;= 0)][YES(allocation disabling is ignored)][YES(no snapshots are currently running)][YES(below primary recovery limit of [4])]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "[allocate] allocation of [abcq][0] on node {esnode2}{Pisl95VUSPmZa3Ga_e3sDA}{172.17.0.4}{172.17.0.4:9300}{master=false} is not allowed, reason: [YES(shard is primary)][YES(no allocation awareness enabled)][NO(more than allowed [90.0%] used disk on node, free: [4.078553722498398%])][YES(allocation disabling is ignored)][YES(primary shard can be allocated anywhere)][YES(node passes include/exclude/require filters)][YES(shard is not allocated to same node or host)][YES(total shard limit disabled: [index: -1, cluster: -1] &lt;= 0)][YES(allocation disabling is ignored)][YES(no snapshots are currently running)][YES(below primary recovery limit of [4])]"
  },
  "status" : 400
}
```

why i create new index get unassigned, can any one help? thanks.
</description><key id="164453247">19323</key><summary>unassigned shards with cluster in docker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nathan-zhu</reporter><labels /><created>2016-07-08T03:37:19Z</created><updated>2016-07-08T07:13:37Z</updated><resolved>2016-07-08T06:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-08T06:43:45Z" id="231287227">Please use discuss.elastic.co for these kind of questions. With that said, the reroute output which you showed gives you the answer: `more than allowed [90.0%] used disk on node, free: [4.078553722498398%]`. The shard is not allocated as disk based shard allocation prevents it as the disk usage is above the low water mark (`_cat/allocation` shows you have only `9.5gb` out of `232.9gb` available ). Free more disk space or adjust the low/high watermarks (see here: https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-allocator.html )
</comment><comment author="nathan-zhu" created="2016-07-08T07:13:37Z" id="231291553">@ywelsch got it, i will change settings and try again. thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add note to issue template regarding supported OS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19322</link><project id="" key="" /><description>This commit adds a note to the GitHub issue template noting that bug
reports on OS that we do not support or feature requests for OS that we
do not support will be closed.
</description><key id="164447266">19322</key><summary>Add note to issue template regarding supported OS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label></labels><created>2016-07-08T02:27:09Z</created><updated>2016-07-08T14:12:32Z</updated><resolved>2016-07-08T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T03:00:15Z" id="231264180">Lgtm. I think the question mark should be a period.
On Jul 7, 2016 10:27 PM, "Jason Tedor" notifications@github.com wrote:

&gt; This commit adds a note to the GitHub issue template noting that bug
&gt; reports on OS that we do not support or feature requests for OS that we
&gt; 
&gt; ## do not support will be closed.
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/19322
&gt; Commit Summary
&gt; - Add note to issue template regarding supported OS
&gt; 
&gt; File Changes
&gt; - _M_ .github/ISSUE_TEMPLATE.md
&gt;   https://github.com/elastic/elasticsearch/pull/19322/files#diff-0 (6)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/19322.patch
&gt; - https://github.com/elastic/elasticsearch/pull/19322.diff
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19322, or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLos2Vl-VfVg4t-_RP3chvvxsDm0T2ks5qTbWBgaJpZM4JHq8W
&gt; .
</comment><comment author="jasontedor" created="2016-07-08T14:12:32Z" id="231369984">Thanks for reviewing and especially catching the typo @nik9000.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add note to issue template regarding supported OS</comment></comments></commit></commits></item><item><title>Migrate range, date_range, and geo_distance aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19321</link><project id="" key="" /><description>Just another step towards removing aggregation's streams in favor of using NamedWriteable.
</description><key id="164442522">19321</key><summary>Migrate range, date_range, and geo_distance aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T01:34:36Z</created><updated>2016-07-11T17:13:57Z</updated><resolved>2016-07-11T17:13:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-08T01:34:42Z" id="231254356">@colings86 another one!
</comment><comment author="colings86" created="2016-07-08T13:36:18Z" id="231360936">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>slight clarification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19320</link><project id="" key="" /><description /><key id="164438136">19320</key><summary>slight clarification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlenRSmith</reporter><labels><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T00:47:06Z</created><updated>2016-07-08T06:29:50Z</updated><resolved>2016-07-08T06:29:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19320 from elastic/GlenRSmith-patch-1</comment></comments></commit></commits></item><item><title>Show ignored error in verbose simulate processor results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19319</link><project id="" key="" /><description>It would be nice to include the error (that was ignored in processors with `ignore_failure` flag on) with the document returned in a /_simulate?verbose response.

So that the processor's result would look like this:

```
[
...
{
"tag" : "processor_tag",
"ignored_error": { "error" : ... },
"doc" : {...}
},
...
]
```

Even though the processor technically succeeded, it is useful to have the ignored failure (if one is present) for debugging purposes
</description><key id="164436156">19319</key><summary>Show ignored error in verbose simulate processor results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-08T00:25:47Z</created><updated>2016-07-13T20:32:10Z</updated><resolved>2016-07-13T20:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-11T07:13:56Z" id="231658241">So when we decorate (`TrackingResultProcessor#TrackingResultProcessor`) the pipeline's processors we would set `ignoreFailure` always to false on the newly returned `CompoundProcessor` and in `TrackingResultProcessor#execute(...)` we would remember the caught exception but not throw it? 
</comment><comment author="talevy" created="2016-07-11T17:06:19Z" id="231798492">@martijnvg, exactly. This means that `SimulateProcessorResult` would not be either an error or a document, but instead could contain both an error and a document to be rendered.

So here: https://github.com/talevy/elasticsearch/blob/28fd684eef75592346e72c92177e7dd86af82608/core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java#L52-L52

would change to be something like this:

```
processorResultList.add(new SimulateProcessorResult(actualProcessor.getTag(), new IngestDocument(ingestDocument), e));
```

and then the `SimulateProcessorResult#toXContent` would property render the json response as suggested in the issue description.
</comment><comment author="martijnvg" created="2016-07-11T18:44:15Z" id="231826820">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/TrackingResultProcessorTests.java</file></files><comments><comment>show ignored errors in verbose simulate result (#19404)</comment></comments></commit></commits></item><item><title>rethrow script compilation exceptions into ingest configuration exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19318</link><project id="" key="" /><description>Currently, mustache compile exceptions in pipelines are reported as regular ES exceptions, e.g.

``` json
{
  "error": {
    "root_cause": [
      {
        "type": "general_script_exception",
        "reason": "Failed to compile inline script [{{#join}}{{/join}}] using lang [mustache]"
      }
    ],
    "type": "general_script_exception",
    "reason": "Failed to compile inline script [{{#join}}{{/join}}] using lang [mustache]",
    "caused_by": {
      "type": "mustache_exception",
      "reason": "Mustache function [join] must contain one and only one identifier"
    }
  },
  "status": 500
}
```

This makes it difficult to gather which property in the pipeline configuration is causing this exception. This PR wraps these exceptions in ElasticsearchParseException so that we can relay the information
</description><key id="164428662">19318</key><summary>rethrow script compilation exceptions into ingest configuration exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T23:20:17Z</created><updated>2016-07-20T17:37:57Z</updated><resolved>2016-07-20T17:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-07-07T23:49:14Z" id="231240389">@rmuir @jdconrad updated the ElasticsearchParseException to carry the original cause with it. thanks!
</comment><comment author="martijnvg" created="2016-07-08T08:19:28Z" id="231302435">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/AppendProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/FailProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/RemoveProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SetProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/AppendProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/FailProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/RemoveProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SetProcessorFactoryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java</file></files><comments><comment>rethrow script compilation exceptions into ingest configuration exceptions (#19318)</comment></comments></commit></commits></item><item><title>Use output of rescoring when top hits aggregation is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19317</link><project id="" key="" /><description>**Describe the feature**:

When using the top_hits aggregation it is sorting by the scores prior to rescore running. I put together an trimmed down example script that demonstrates this. The score from the main query is returned as 2.6434526, but the score as seen in top_hits aggregation is 1.

The larger goal is to perform a search over a geographic area, take the top N (we use 8192) hits per shard, and then show the top M hits per grid point. While the example below uses a match_all for simplicity with a 10km area, the query could just as well be run with a query_string query, additional filters, and over a much larger area. I would like to keep the original limits put in place in the rescore phase and only aggregate over those 8k results per shard.  Utilizing the result of rescoring is incredibly important for choosing the best items per grid point in my use case.

```
#!/bin/sh

curl -s -XDELETE localhost:9200/top_hits_rescore | jq -c .
curl -s -XPUT localhost:9200/top_hits_rescore/ -d '{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}' | jq -c .;

curl -s -XPUT localhost:9200/top_hits_rescore/_mapping/page -d '{
  "properties": {
    "title": { "type": "string" },
    "weight": { "type": "long" },
    "coord": { "type": "geo_point", "lat_lon": true }
  }
}' | jq -c .

curl -s -XPOST localhost:9200/top_hits_rescore/page/ -d '{
  "title": "foo",
  "weight": 42,
  "coord": { "lat": 1.2345, "lon": 5.4321 }
}' | jq -c .

curl -s -XPOST localhost:9200/top_hits_rescore/_flush | jq -c .

curl -s -XGET localhost:9200/top_hits_rescore/page/_search -d '{
  "rescore": [
    {
      "query": {
        "rescore_query": {
          "function_score": {
            "functions": [ {
              "field_value_factor": {
                "modifier": "log2p",
                "field": "weight"
              }
            } ]
          }
        }
      }
    }
  ],
  "query": {
    "bool": {
      "must": [ { "match_all": {} } ],
      "filter": [ {
        "geo_distance": {
          "distance": "10km",
          "coord": { "lat": 1.2345, "lon": 5.4321 }
        }
      } ]
    }
  },
  "aggs": {
    "grid": {
      "geohash_grid": { "field": "coord" },
      "aggs": {
        "top_grid_hits": { "top_hits": {} }
      }
    }
  }
}' | jq .
```
</description><key id="164423264">19317</key><summary>Use output of rescoring when top hits aggregation is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebernhardson</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2016-07-07T22:40:11Z</created><updated>2016-10-20T10:50:49Z</updated><resolved>2016-10-20T10:50:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-15T10:03:50Z" id="232911983">We should be able to add this, as long as we can reuse the existing rescore infrastructure
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java</file></files><comments><comment>Rescorer should be applied in the TopHits aggregation (#20978)</comment></comments></commit></commits></item><item><title>IpFieldBwCompatIT fails with seed A72FE66BDD9F236C</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19316</link><project id="" key="" /><description>`IpFieldBwCompatIT` fails with `@Seed("A72FE66BDD9F236C")`. It times out. This is the CI failure:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/726/consoleFull

I'm not sure what is up with it but I don't think the failure is all that recent.
</description><key id="164417060">19316</key><summary>IpFieldBwCompatIT fails with seed A72FE66BDD9F236C</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>jenkins</label></labels><created>2016-07-07T21:59:22Z</created><updated>2016-07-18T15:45:47Z</updated><resolved>2016-07-18T15:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-08T08:12:53Z" id="231301283">@jpountz could you take a look?
</comment><comment author="jpountz" created="2016-07-18T13:07:32Z" id="233322931">@nik9000 Do you remember if that seed reproduced for you?
</comment><comment author="nik9000" created="2016-07-18T13:20:22Z" id="233325997">IIRC it did, yes. It timed out when I ran the whole suite iirc.
</comment><comment author="jpountz" created="2016-07-18T15:36:59Z" id="233366179">Thanks, I tried a bit more and managed to reproduce it a couple times (it does not reproduce all the time). It looks like classloading gets stuck, every time I hit this failure, a thread was stuck with the following stack trace:

```
"elasticsearch[node_sd3][search][T#2]" #60 daemon prio=5 os_prio=0 tid=0x00007feda8d48800 nid=0x30fc in Object.wait() [0x00007fed425e9000]
   java.lang.Thread.State: RUNNABLE
        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes.&lt;clinit&gt;(ValuesSource.java:72)
        at org.elasticsearch.search.aggregations.support.AggregationContext.bytesField(AggregationContext.java:167)
        at org.elasticsearch.search.aggregations.support.AggregationContext.originalValuesSource(AggregationContext.java:138)
        at org.elasticsearch.search.aggregations.support.AggregationContext.valuesSource(AggregationContext.java:79)
        at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:51)
        at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:225)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:102)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:61)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:103)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:322)
        at org.elasticsearch.search.action.SearchTransportService$SearchQueryByIdTransportHandler.messageReceived(SearchTransportService.java:323)
        at org.elasticsearch.search.action.SearchTransportService$SearchQueryByIdTransportHandler.messageReceived(SearchTransportService.java:320)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69)
        at org.elasticsearch.transport.TransportService$5.doRun(TransportService.java:517)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

Maybe it has something to do with instantiating a nested sub class from a static block, I could find this issue as well that might be related even though here I can only find a single thread that is waiting on initializing this class: https://bugs.openjdk.java.net/browse/JDK-8139224.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java</file></files><comments><comment>Try to prevent classloading deadlock.</comment></comments></commit></commits></item><item><title>What is the deal with GeoDistanceIT#testDuelOptimizations?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19315</link><project id="" key="" /><description>It calls `assertDuelOptimization` which is fairly confusing but doesn't seem to assert a whole lot. In fact I'm not really sure what it does assert.

When you run the whole suite with `@Seed("A72FE66BDD9F236C")` it GCs fairly hard.

This cam up in:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/726/console
</description><key id="164412013">19315</key><summary>What is the deal with GeoDistanceIT#testDuelOptimizations?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Geo</label><label>jenkins</label><label>test</label></labels><created>2016-07-07T21:31:19Z</created><updated>2016-07-07T21:42:53Z</updated><resolved>2016-07-07T21:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-07T21:42:02Z" id="231216060">Is this a duplicate of #19263?
</comment><comment author="nik9000" created="2016-07-07T21:42:47Z" id="231216257">Dupe.
</comment><comment author="nik9000" created="2016-07-07T21:42:53Z" id="231216285">I should have searched.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Logline of request logger in java rest client missing slash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19314</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0-alpha4
**JVM version**:
1.8
**OS version**:
Mac OS Sierra
**Description of the problem including expected versus actual behavior**:
This is a very minor thing, the logline for logging requests is missing a slash. It looks like this:

```
request [GET http://127.0.0.1:9201_cluster/health] returned [HTTP/1.1 200 OK]
```

It should be like this:

```
request [GET http://127.0.0.1:9201/_cluster/health] returned [HTTP/1.1 200 OK]
```

The executed request is like this:

```
            Response response = client.performRequest(
                    "GET",
                    "_cluster/health",
                    new Hashtable&lt;&gt;(),
                    null);
```

I do not enter the slash before cluster, but it works, so the logs should add the slash as well.
</description><key id="164402374">19314</key><summary>Logline of request logger in java rest client missing slash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jettro</reporter><labels><label>:Java REST Client</label><label>bug</label></labels><created>2016-07-07T20:41:05Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-07-08T10:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file></files><comments><comment>Rest Client: add slash to log line when missing between host and uri</comment></comments></commit></commits></item><item><title>High CPU usage when idle </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19313</link><project id="" key="" /><description>All of a sudden my ES data nodes started experiencing high CPU usage the cluster is completely idled nothing is being queried and Logstash is disabled at the moment. Any help would be appreciate it. I have attached the hot thread dump 

::: {elk}{-WMa7rZzTK-zmfOHen82yg}{}{:9300}{data=false, master=false}
   Hot threads at 2016-07-07T15:36:28.156Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

::: {es-1}{85Ud2FkPSS-aBJmVlv8OkA}{}{:9300}
   Hot threads at 2016-07-07T15:36:32.992Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   96.2% (481.1ms out of 500ms) cpu usage by thread 'elasticsearch[es-1][management][T#4]'
     9/10 snapshots sharing following 32 elements
       java.io.FilePermission.init(FilePermission.java:203)
       java.io.FilePermission.&lt;init&gt;(FilePermission.java:277)
       java.lang.SecurityManager.checkRead(SecurityManager.java:888)
       sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
       sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
       sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
       sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
       java.nio.file.Files.readAttributes(Files.java:1737)
       java.nio.file.Files.size(Files.java:2332)
       org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:210)
       org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:150)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1541)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       sun.nio.fs.UnixNativeDispatcher.readdir(Native Method)
       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.readNextEntry(UnixDirectoryStream.java:168)
       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.hasNext(UnixDirectoryStream.java:201)
       org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:192)
       org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:203)
       org.elasticsearch.index.store.FsDirectoryService$1.listAll(FsDirectoryService.java:127)
       org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
       org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:57)
       org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1538)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

```
6.0% (29.7ms out of 500ms) cpu usage by thread 'elasticsearch[es-1][bulk][T#4]'
 2/10 snapshots sharing following 11 elements
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:68)
   org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:392)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:291)
   org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:283)
   org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
   org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 7/10 snapshots sharing following 10 elements
   sun.misc.Unsafe.park(Native Method)
   java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
   java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
   java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
   org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 unique snapshot
   java.security.Permissions.implies(Permissions.java:178)
   org.elasticsearch.bootstrap.ESPolicy.implies(ESPolicy.java:85)
   java.security.ProtectionDomain.implies(ProtectionDomain.java:281)
   java.security.AccessControlContext.checkPermission(AccessControlContext.java:450)
   java.security.AccessController.checkPermission(AccessController.java:884)
   java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
   java.lang.SecurityManager.checkWrite(SecurityManager.java:979)
   sun.nio.fs.UnixChannelFactory.open(UnixChannelFactory.java:247)
   sun.nio.fs.UnixChannelFactory.newFileChannel(UnixChannelFactory.java:136)
   sun.nio.fs.UnixChannelFactory.newFileChannel(UnixChannelFactory.java:148)
   sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:175)
   java.nio.channels.FileChannel.open(FileChannel.java:287)
   java.nio.channels.FileChannel.open(FileChannel.java:335)
   org.elasticsearch.index.translog.Checkpoint.write(Checkpoint.java:88)
   org.elasticsearch.index.translog.TranslogWriter.writeCheckpoint(TranslogWriter.java:314)
   org.elasticsearch.index.translog.TranslogWriter.checkpoint(TranslogWriter.java:304)
   org.elasticsearch.index.translog.BufferingTranslogWriter.sync(BufferingTranslogWriter.java:132)
   org.elasticsearch.index.translog.TranslogWriter.syncUpTo(TranslogWriter.java:288)
   org.elasticsearch.index.translog.Translog.ensureSynced(Translog.java:672)
   org.elasticsearch.index.shard.IndexShard.sync(IndexShard.java:1633)
   org.elasticsearch.action.support.replication.TransportReplicationAction.processAfterWrite(TransportReplicationAction.java:1035)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:295)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
   org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
   org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
   org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
   org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)

4.9% (24.5ms out of 500ms) cpu usage by thread 'elasticsearch[es-1][bulk][T#3]'
 2/10 snapshots sharing following 11 elements
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:68)
   org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:392)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:291)
   org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:283)
   org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
   org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
   org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 8/10 snapshots sharing following 10 elements
   sun.misc.Unsafe.park(Native Method)
   java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
   java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
   java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
   org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
```

::: {es-3}{MjnDX2u9TUinY7Km-8cjMA}{}{:9300}
   Hot threads at 2016-07-07T15:36:28.304Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   99.0% (494.9ms out of 500ms) cpu usage by thread 'elasticsearch[es-3][management][T#3]'
     8/10 snapshots sharing following 30 elements
       java.lang.SecurityManager.checkRead(SecurityManager.java:888)
       sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
       sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
       sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
       sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
       java.nio.file.Files.readAttributes(Files.java:1737)
       java.nio.file.Files.size(Files.java:2332)
       org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:210)
       org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:150)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1541)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 31 elements
       sun.nio.fs.UnixNativeDispatcher.stat0(Native Method)
       sun.nio.fs.UnixNativeDispatcher.stat(UnixNativeDispatcher.java:286)
       sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:70)
       sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:52)
       sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
       sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
       java.nio.file.Files.readAttributes(Files.java:1737)
       java.nio.file.Files.size(Files.java:2332)
       org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:210)
       org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:150)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1541)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {es-2}{qlxYBjbsSRC2okNPFV8baQ}{}{:9300}
   Hot threads at 2016-07-07T15:36:33.434Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   97.5% (487.7ms out of 500ms) cpu usage by thread 'elasticsearch[es-2][management][T#2]'
     6/10 snapshots sharing following 38 elements
       java.io.UnixFileSystem.canonicalize0(Native Method)
       java.io.UnixFileSystem.canonicalize(UnixFileSystem.java:172)
       java.io.File.getCanonicalPath(File.java:618)
       java.io.FilePermission$1.run(FilePermission.java:215)
       java.io.FilePermission$1.run(FilePermission.java:203)
       java.security.AccessController.doPrivileged(Native Method)
       java.io.FilePermission.init(FilePermission.java:203)
       java.io.FilePermission.&lt;init&gt;(FilePermission.java:277)
       java.lang.SecurityManager.checkRead(SecurityManager.java:888)
       sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
       sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
       sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
       sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
       java.nio.file.Files.readAttributes(Files.java:1737)
       java.nio.file.Files.size(Files.java:2332)
       org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:210)
       org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:150)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
       org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1541)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 18 elements
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
       org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
       org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
       org.elasticsearch.index.store.Store.stats(Store.java:293)
       org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
       org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
       org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

```
8.3% (41.2ms out of 500ms) cpu usage by thread 'elasticsearch[es-2][refresh][T#1]'
 2/10 snapshots sharing following 18 elements
   org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:503)
   org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:615)
   org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:424)
   org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:286)
   org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:261)
   org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:251)
   org.apache.lucene.index.FilterDirectoryReader.doOpenIfChanged(FilterDirectoryReader.java:104)
   org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
   org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:154)
   org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
   org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176)
   org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
   org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:672)
   org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:661)
   org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1349)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 5/10 snapshots sharing following 6 elements
   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
   java.util.concurrent.LinkedTransferQueue.poll(LinkedTransferQueue.java:1277)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)

7.3% (36.5ms out of 500ms) cpu usage by thread 'elasticsearch[es-2][refresh][T#2]'
 2/10 snapshots sharing following 15 elements
   org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:286)
   org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:261)
   org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:251)
   org.apache.lucene.index.FilterDirectoryReader.doOpenIfChanged(FilterDirectoryReader.java:104)
   org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
   org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:154)
   org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
   org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176)
   org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
   org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:672)
   org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:661)
   org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1349)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 5/10 snapshots sharing following 9 elements
   sun.misc.Unsafe.park(Native Method)
   java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
   java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:734)
   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
   java.util.concurrent.LinkedTransferQueue.poll(LinkedTransferQueue.java:1277)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
```
</description><key id="164394228">19313</key><summary>High CPU usage when idle </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pip00</reporter><labels /><created>2016-07-07T20:00:29Z</created><updated>2016-07-07T22:37:13Z</updated><resolved>2016-07-07T20:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pip00" created="2016-07-07T20:01:59Z" id="231190507">I can post the stats of my nodes if required. Thank you! 
</comment><comment author="abeyad" created="2016-07-07T20:04:19Z" id="231191101">Could you kindly post any questions on the discussion forum: https://discuss.elastic.co
Github issues are reserved for bugs/features/issues.
</comment><comment author="pip00" created="2016-07-07T20:04:52Z" id="231191234">I did I was just wondering if it was a bug in ES
</comment><comment author="abeyad" created="2016-07-07T20:09:07Z" id="231192336">Likely not, there are many reasons why you could have CPU usage even if not actively executing an operation (e.g. Lucene merging of shard segments in the background)
</comment><comment author="jasontedor" created="2016-07-07T22:37:13Z" id="231228511">Your hot threads show that bulk indexing activity is in fact occurring, as well as some stats requests, and some refreshes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Options to perform a request with the new Rest client </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19312</link><project id="" key="" /><description>**Describe the feature**:
At the moment the method to perform a request using the new Rest client (5.0 alpha 4) looks like this:

```
Response response = client.performRequest(
            "GET",
            "_cluster/health",
            new Hashtable&lt;&gt;(),
            null);
```

If you do not need to provide parameters, you have to provide an empty Map. And if you do not need a request body you have to pass _null_. For me an additional method like this seems nice to have.

```
Response response = client.performRequest(
            "GET",
            "_cluster/health");
```
</description><key id="164389285">19312</key><summary>Options to perform a request with the new Rest client </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jettro</reporter><labels><label>:Java REST Client</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T19:33:17Z</created><updated>2016-07-13T12:24:59Z</updated><resolved>2016-07-11T08:37:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-08T10:26:26Z" id="231327275">We discussed this in FixItFriday, we will add this method and also another one that accepts parameters but not the body. PR will come soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file></files><comments><comment>Rest Client: add short performRequest method variants without params and/or body</comment></comments></commit></commits></item><item><title>Recommend multiple path.data instead of RAID 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19311</link><project id="" key="" /><description>As per https://www.elastic.co/blog/performance-indexing-2-0 - I presume this advice is still correct for 5?
</description><key id="164383776">19311</key><summary>Recommend multiple path.data instead of RAID 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>docs</label></labels><created>2016-07-07T19:03:58Z</created><updated>2017-06-30T16:52:52Z</updated><resolved>2017-06-30T16:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-07T19:15:32Z" id="231178936">It is actually a lot more complicated than "always use multiple data paths". Multiple data paths is the most replica-friendly option but it won't perform as well as RAID-0 in many cases because each shard is on one disk.
</comment><comment author="jimmyjones2" created="2016-07-10T21:00:33Z" id="231610607">How about:

```
Add more SSDs to improve throughput. While an OS-level RAID 0 device may provide
more throughput for a single shard, with multiple shards consider using multiple
`path.data` paths so that a failed device will only result in the loss of some shards.
```
</comment><comment author="clintongormley" created="2016-07-11T13:46:48Z" id="231738751">In this case it is probably worth being more verbose.  There is a lot of info packed into that para and it'd be easier to understand by explaining in more detail.
</comment><comment author="bra-fsn" created="2017-01-31T09:16:19Z" id="276312518">BTW, how well does Elasticsearch handles the case, where the IO freezes on a failing device?</comment><comment author="elasticmachine" created="2017-01-31T09:16:21Z" id="276312524">Can one of the admins verify this patch?</comment><comment author="elasticmachine" created="2017-02-23T18:14:42Z" id="282074254">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="rjernst" created="2017-06-09T05:25:54Z" id="307298562">@jimmyjones2 Are you still interesting in finishing this PR?</comment><comment author="elasticmachine" created="2017-06-09T05:25:55Z" id="307298564">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="jasontedor" created="2017-06-30T16:52:52Z" id="312318759">No additional feedback, closing.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support authentication with reindex-from-remote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19310</link><project id="" key="" /><description>Without this reindex-from-remote isn't compatible with any sort of security or reverse proxy servers. With this it should be compatible with both. This is how you authenticate:

```
curl -XPOST localhost:9200/_reindex -d'{
  "source": {
    "remote": {
      "host": "https://remoteHost:9200", # http or https are fine
      "username": "Aladdin",
      "password": "open sesame"
    }
    "index": "source",
  }
  "dest": {
    "index": "dest"
  }
}'
```

Optionally, you can also add headers like this:

```
curl -XPOST localhost:9200/_reindex -d'{
  "source": {
    "remote": {
      "host": "https://remoteHost:9200", # http or https are fine
      "headers": {
        "header-1": "value-1",
        "header-2": "value-2"
      }
      "username": "Aladdin",
      "password": "open sesame"
    }
    "index": "source",
  }
  "dest": {
    "index": "dest"
  }
}'
```
</description><key id="164380470">19310</key><summary>Support authentication with reindex-from-remote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T18:48:34Z</created><updated>2016-07-27T18:22:59Z</updated><resolved>2016-07-27T18:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-07T18:50:12Z" id="231172079">@javanna I think this is more about the java http client and how we want to test the feature than it is about reindex's internals. So I think you are an appropriate reviewer if you have time.
</comment><comment author="nik9000" created="2016-07-21T21:02:17Z" id="234383056">@javanna I've pushed a pile of new commits to this that should address all the open issues. I hope.
</comment><comment author="javanna" created="2016-07-25T18:33:27Z" id="235042615">I left a few comments but looks good!
</comment><comment author="nik9000" created="2016-07-27T14:15:36Z" id="235598451">@javanna I rebased so I could pick up the async changes and renamed the exception.
</comment><comment author="javanna" created="2016-07-27T14:33:16Z" id="235603993">left a few more comments
</comment><comment author="nik9000" created="2016-07-27T16:04:25Z" id="235634101">@javanna I think addressed your comments.
</comment><comment author="nik9000" created="2016-07-27T16:10:42Z" id="235635993">I found another thing on my own around headers. I'll push another patch for that in a bit.
</comment><comment author="javanna" created="2016-07-27T16:12:27Z" id="235636543">left a couple of minor comments. LGTM otherwise. Feel free to push once those are addressed.
</comment><comment author="nik9000" created="2016-07-27T16:32:52Z" id="235642786">Thanks for staying with this feature @javanna! Its been a long haul!

I added 890933a which is a little fiddly. Can you have a look?
</comment><comment author="javanna" created="2016-07-27T18:22:59Z" id="235674663">🎉 🎈 🎉 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tighten ensure atomic move cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19309</link><project id="" key="" /><description>This commit tightens the cleanup after possible errors while ensuring
the filesystem supports atomic move.

Closes #19036
</description><key id="164366024">19309</key><summary>Tighten ensure atomic move cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T17:38:03Z</created><updated>2016-07-07T18:51:30Z</updated><resolved>2016-07-07T18:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-07T18:23:24Z" id="231164701">LGTM. Though, are we sure it closes that issue?
</comment><comment author="jasontedor" created="2016-07-07T18:33:00Z" id="231167318">&gt; Though, are we sure it closes that issue?

@bleskes I don't think that issue is our issue, but this is as tight as we can get the cleanup I think?
</comment><comment author="ppf2" created="2016-07-07T18:51:30Z" id="231172422">Thx @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file></files><comments><comment>Tighten ensure atomic move cleanup</comment></comments></commit></commits></item><item><title>Migrate Vagrant tests for Fedora to Fedora 24</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19308</link><project id="" key="" /><description>This commit migrates the Vagrant box for Fedora for the packaging tests
from Fedora 22 to Fedora 24 as Fedora 22 reached end-of-line upon the
release of Fedora 24.
</description><key id="164362817">19308</key><summary>Migrate Vagrant tests for Fedora to Fedora 24</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T17:21:51Z</created><updated>2016-07-07T17:31:30Z</updated><resolved>2016-07-07T17:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-07T17:31:11Z" id="231150066">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Migrate Vagrant tests for Fedora to Fedora 24</comment></comments></commit></commits></item><item><title>[CI] tests.class=org.elasticsearch.ingest.useragent.UserAgentRestIT fails in 30_custom_regex.yaml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19307</link><project id="" key="" /><description>```
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [ingest.put_pipeline] returned [400 Bad Request] [{"error":{"root_cause":[{"type":"parse_exception","reason":"[regex_file] regex file [test-regexes.yaml] doesn't exist (has to exist at node startup)","header":{"processor_type":"user_agent","property_name":"regex_file"}}],"type":"parse_exception","reason":"[regex_file] regex file [test-regexes.yaml] doesn't exist (has to exist at node startup)","header":{"processor_type":"user_agent","property_name":"regex_file"}},"status":400}]
   &gt;    at __randomizedtesting.SeedInfo.seed([BF3FA2D48D03AEDB:376B9D0E23FFC323]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:108)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:399)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

This is might be related to b4defafcb26ab86843bbe3464a4cf54cdc978696

Reproduces with:

```
gradle :plugins:ingest-user-agent:integTest -Dtests.seed=BF3FA2D48D03AEDB -Dtests.class=org.elasticsearch.ingest.useragent.UserAgentRestIT -Dtests.method="test {yaml=ingest-useragent/30_custom_regex/Test user agent processor with custom regex file}" -Dtests.es.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops" -Dtests.locale=ko-KR -Dtests.timezone=Europe/Stockholm
```
</description><key id="164362233">19307</key><summary>[CI] tests.class=org.elasticsearch.ingest.useragent.UserAgentRestIT fails in 30_custom_regex.yaml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-07-07T17:18:34Z</created><updated>2016-07-07T19:30:28Z</updated><resolved>2016-07-07T19:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-07T17:18:44Z" id="231146627">@martijnvg can you take a look?
</comment><comment author="martijnvg" created="2016-07-07T19:17:08Z" id="231179374">@dakrone yes, I'll have a look.
</comment><comment author="jasontedor" created="2016-07-07T19:18:28Z" id="231179683">@martijnvg I pushed a skip in 7d13906fcd44a4b3a9ea4deb51a35149777db5bf.
</comment><comment author="martijnvg" created="2016-07-07T19:30:28Z" id="231182691">thanks @jasontedor @dakrone! this was an actual bug.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/ingest-user-agent/src/main/java/org/elasticsearch/ingest/useragent/IngestUserAgentPlugin.java</file></files><comments><comment>ingest: Fixed left over rename to 'ingest-user-plugin'</comment></comments></commit></commits></item><item><title>Rest Client: HostsSniffer to set http as default scheme</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19306</link><project id="" key="" /><description> The assumption in HostsSniffer is that all of the arguments have been properly provided and validated through HostsSniffer.Builder, except they weren't, as the scheme didn't have a default value and when not set would cause NPEs down the road. Improved tests to catch this also.
</description><key id="164361010">19306</key><summary>Rest Client: HostsSniffer to set http as default scheme</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T17:12:09Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-07-08T08:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-07T17:13:17Z" id="231145165">thanks @jettro for raising this.
</comment><comment author="nik9000" created="2016-07-07T17:32:34Z" id="231150463">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file></files><comments><comment>Merge pull request #19306 from javanna/fix/default_sniffer_scheme</comment></comments></commit></commits></item><item><title>Loading modules from class path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19305</link><project id="" key="" /><description>We use embedded elastic heavily (WAR) and with modularization we are facing the issue of not being able to use native elastic modules particularly lang-expression because even if we drop its JARS into our class path the module is not getting registered. I was wondering if elastic team would consider an alternative and explicit way to register modules on class path such as a properties files in META-INF or programmatic registration or even giving elastic url(s)/streams for modules/plugins jars/resources so that elastic could load it with a separate class loader as normal. 
Currently it is very locked down and inflexible that modules (and plugins) can only be loaded from home directory. When we deliver our application to third party for deployment and maintenance it is very odd, error prone and unsafe to deliver bunch of jars and property files to be placed in external directories making sure that during upgrades old stuff get removed and new stuff gets placed in proper directories where the deployers only expect WAR and context files
</description><key id="164359508">19305</key><summary>Loading modules from class path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2016-07-07T17:04:36Z</created><updated>2016-08-30T22:03:25Z</updated><resolved>2016-08-30T22:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-07T18:25:50Z" id="231165384">If the modules directory exists as it should, the modules will be loaded, even when constructing a Node directly. You just need the proper environment.
</comment><comment author="roytmana" created="2016-07-07T20:57:26Z" id="231205056">@rjernst Yes it will load if I have modules directory under elastic home. it will even load if modules jars are in my class path (rather than under modules) as long as i put plugin-descriptor.properties and plugin-security.policy under modules (not sure it is supported configuration) but my point is that with embedded deployments of elastic it is expected that all application components are inside WAR file and there is no need to manage a whole bunch of additional JARs and properties making sure they are properly upgraded with each system upgrade. That's why I am asking for more open module deployment and registration mechanism. For us it will be a nightmare because we have control over people who perform deployments we will end up with messed up modules directory and we have quite a few apps with elastic deployed 
</comment><comment author="roytmana" created="2016-07-07T22:01:40Z" id="231220777">@rjernst  Another viable option would be supporting not just modules and plugins directories but modules.zip and plugins.zip for packaged modules. With that integrity of modules inside the archive could be ensured and upgrade becomes just a matter of replacing modules.zp. All it takes is an abstraction over file based modules/plugins access so it can work over file system or zip file. I would be happy too develop it if it is acceptable for the elastic team. Actually there will b every little to do 
Java 7 already have File system over ZIP file

http://docs.oracle.com/javase/7/docs/technotes/guides/io/fsp/zipfilesystemprovider.html
</comment><comment author="roytmana" created="2016-07-07T23:09:34Z" id="231234103">The environment an dPluginService are actually implemented on top of FileSystem so using Zip file system is possible instead of the default one

just to try I plugged this into Environment class

```
      URI uri = URI.create("jar:"+ homeFile.toUri() + "/modules.zip");
      FileSystem fs = zipFileSystems.get(uri);
      if (fs == null) {
        Map&lt;String, String&gt; env = new HashMap&lt;&gt;();
        env.put("create", "true");
        try {
          fs = FileSystems.newFileSystem(uri, env);
          zipFileSystems.put(uri, fs);
        } catch (IOException e) {
          throw new IllegalArgumentException(e);
        }
      }
      modulesFile = fs.getPath("/");
```

and when my modules.zip only have plugin-descriptor.properties and plugin-security.policy while actual JARS are on class path it worked just fine. Unfortunately if I bundle JARS in modules it fails in checkJarHell because it jumps from Path representation linked to Zip file file system to relative URIs then files could not be found
</comment><comment author="jprante" created="2016-07-08T08:15:48Z" id="231301757">@roytmana excellent suggestion. 

Adding plugins in zip filesystems is also how I implemented pluggable web apps in my Groovy web app platform so I would welcome this as an opportunity to better package and deploy plugins for ES.
</comment><comment author="jasontedor" created="2016-08-30T22:03:25Z" id="243596049">We [do not support](https://www.elastic.co/blog/elasticsearch-the-server) embedded Elasticsearch nodes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>network.publish_host does not pull the appropriate values from network.bind_host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19304</link><project id="" key="" /><description>**Elasticsearch version**:

2.3

**JVM version**:

1.8

**OS version**:

CentOS 7 and CentOS 6

**Description of the problem including expected versus actual behavior**:

The documentation indicates that network.publish_host should be pulled from network.bind_host if it is not explicitly set. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#advanced-network-settings

In my tests, network.publish_host was being set to 127.0.0.1 even when network.bind_host was being explicitly set.

My setup uses the  expanded YAML format and does not collapse the keys. I do not know if this bug is present with collapsed keys.

Example config snippet:

``` yaml
'network' =&gt; {
  'bind_host' =&gt; '10.5.2.3'
}
```

**Steps to reproduce**:
1. Start a cluster with network.bind_host set to an explicit IP and without network.publish_host set in the expanded YAML format. 
</description><key id="164352707">19304</key><summary>network.publish_host does not pull the appropriate values from network.bind_host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trevor-vaughan</reporter><labels><label>:Settings</label><label>docs</label></labels><created>2016-07-07T16:30:05Z</created><updated>2016-07-08T15:14:12Z</updated><resolved>2016-07-08T15:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-08T15:14:12Z" id="231386701">Thanks for reporting @trevor-vaughan - I've updated the docs
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update network.asciidoc</comment></comments></commit></commits></item><item><title>Update resiliency docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19303</link><project id="" key="" /><description>Adds clarifications about Jepsen tests and new section on issues with versioning.
</description><key id="164324244">19303</key><summary>Update resiliency docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>docs</label></labels><created>2016-07-07T14:34:47Z</created><updated>2016-07-08T15:31:10Z</updated><resolved>2016-07-08T15:30:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-07T15:18:19Z" id="231110093">LGTM.
</comment><comment author="clintongormley" created="2016-07-08T14:38:50Z" id="231377041">Minor grammar changes, but LGTM
</comment><comment author="ywelsch" created="2016-07-08T15:31:10Z" id="231391381">Thanks @jasontedor @clintongormley.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update resiliency docs (#19303)</comment></comments></commit></commits></item><item><title>[TEST] Clean up more messy tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19302</link><project id="" key="" /><description>Same as #19280 but for more tests. 

Related to #13837.
</description><key id="164313848">19302</key><summary>[TEST] Clean up more messy tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T13:51:30Z</created><updated>2016-07-07T15:51:15Z</updated><resolved>2016-07-07T15:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-07T14:05:14Z" id="231087456">Left some minor stuff but LGTM.
</comment><comment author="tlrx" created="2016-07-07T15:51:14Z" id="231121762">Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptSettings.java</file><file>core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/script/FileScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupIT.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/StoredScriptsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyIndexedScriptTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/TemplateQueryBuilderTests.java</file><file>test/framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment> [TEST] Kill remaining lang-groovy messy tests</comment></comments></commit></commits></item><item><title>Async REST client implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19301</link><project id="" key="" /><description>I have been working on adding an async variant of the `performRequest` method to our `RestClient`. We currently have only the sync version, and we use apache http client internally. The http client is created by the `RestClient`, or it can be created externally and provided (see [here](https://github.com/elastic/elasticsearch/blob/master/client/rest/src/main/java/org/elasticsearch/client/RestClient.java#L384)), so that anything can be customized as long as supported by the underlying http client implementation.

For the async method we are thinking about using apache async http client. The apache async client class implements a different interface (`HttpAsyncClient` rather than `HttpClient`), although the way to configure it is similar to its sync sibling that it depends on. We would like to use only one client internally, which would have to be the async one then, in which case the sync `performRequest` method would just forward to the async one and wait for a response from it.

There are a couple of concerns around relying solely on the apache async http client:
- the async client doesn't seem as widely used as the sync one. People are mostly familiar with the sync one. Most of our users (hard to measure, just guts feeling) will use our sync method, but will end up using the async client internally anyways if we go for using async only internally, which is transparent only up to the point where users provide their own instance of the client with their own configuration (ssl, basic auth, custom timeouts...).
- apache async client doesn't support (yet) transparent content decompression, which means people are not going to be able to make advantage of http compressed responses out of the box. The client won't provide the needed "accept" header, so no compressed responses will be returned.

There may be other limitations that I haven't bumped into yet, suggestions are welcome. In general, do we think that the compressed response limitation is a big deal? I would say no but I opened this issue so we can discuss and come to a conclusion or evaluate alternatives.

A couple of alternatives to using only the apache async client internally:
1) Support both clients and use one or the other depending on the method called, or have two different `RestClient` implementations. The latter may sound cleaner, but it ends up complicating things quite a bit, especially moving forward with the high level client (e.g. which client impl is needed/required/supported for each specific api?).
2) Use an async client only internally, but not the apache one. It needs to be able to send get and delete requests with body (or allow to make it possible with few lines of code).
</description><key id="164280077">19301</key><summary>Async REST client implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>discuss</label></labels><created>2016-07-07T10:40:13Z</created><updated>2016-07-13T12:24:59Z</updated><resolved>2016-07-08T10:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-07T11:48:29Z" id="231055897">My thoughts on this:

&gt; In general, do we think that the compressed response limitation is a big deal? 

Given that (some) other language clients also don't support it out of the box, I'd say no. It may make a difference for requests with larger bodies (like bulk requests) but I have experienced that it's sometimes worse in the higher percentile latencies (99th percentile and up) for smaller requests like search requests.

&gt; async client doesn't seem as widely used as the sync one

That may be true but the question is whether there are more mature alternatives than the Apache async client.

Regarding the suggested alternatives:

&gt; Support both clients and use one or the other depending on the method called, or have two different RestClient implementations

I'd tend not to do that because this could likely get a maintenance nightmare or a huge time sink when hunting for bugs. If we use just one implementation internally something either breaks or it doesn't (or always behaves slow or whatever) but at least it does not depend whether users choose the sync or the async version of our API.
</comment><comment author="nik9000" created="2016-07-07T12:55:53Z" id="231069168">I also like the idea of having one implementation internally. Implementing the synchronous version of the API by doing the async call and blocking on the future seems fairly sane. I think it is worth looking at the performance impacts of it though.

Regarding maturity - it might be worth looking at something like okhttp. I get the sense httpclient async doesn't see all that much use. It feels kind of like an afterthought. okhttp doesn't break sync and async into two libraries which gives me the sense that async is more a core design choice. I don't know if it is a good choice, just an option that might be worth looking into.
</comment><comment author="nik9000" created="2016-07-07T12:59:54Z" id="231070145">I just checked - okhttp only depends on okio which depends on nothing. Collectively they pull in animal sniffer and android as optional dependencies. We wouldn't ship either one but we'd still have to poke the third party audit a bit.
</comment><comment author="javanna" created="2016-07-07T13:13:50Z" id="231073523">&gt; I think it is worth looking at the performance impacts of it though.

Yes! #19281 will help.

&gt; Regarding maturity - it might be worth looking at something like okhttp

Last time I checked, I didn't find a way to send get requests with a body with it, and that didn't seem like something that could be patched (https://github.com/square/okhttp/blob/master/okhttp/src/main/java/okhttp3/internal/http/HttpMethod.java#L36). Maybe that one per se shouldn't be a reason not to consider it though, not sure.
</comment><comment author="nik9000" created="2016-07-07T13:53:12Z" id="231083929">&gt; Maybe that one per se shouldn't be a reason not to consider it though, not sure.

For better or worse we support GET with a body and we need to test it and the http client is the thing that does that.

I opened https://github.com/square/okhttp/issues/2706. Even if they wanted to implement it simple timing may make us using it impossible. But I figured it was worth a shot.
</comment><comment author="javanna" created="2016-07-08T10:29:27Z" id="231327834">We discussed this in FixItFriday. Everybody agrees that we should go with only one http client implementation otherwise things would become hard to maintain. We will go with apache async http client and see how that works out. It is not a huge deal to change the underlying implementation later anyways, besides user configuration that would need to be adapted. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query evaluation: Add reciprocal rank</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19300</link><project id="" key="" /><description>This adds a second query evaluation metric alongside precision_at. Reciprocal Rank is defined as 1/rank, where rank is the position of the first relevant
document in the search result. The results are averaged across all queries across the sample of queries, according to
https://en.wikipedia.org/wiki/Mean_reciprocal_rank
This PR is against the WIP rank-eval feature branch and based on #19283.

Relates to #19195
</description><key id="164279749">19300</key><summary>Query evaluation: Add reciprocal rank</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>feature</label></labels><created>2016-07-07T10:38:20Z</created><updated>2016-07-27T14:26:45Z</updated><resolved>2016-07-27T14:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-07-07T10:40:08Z" id="231043773">@MaineC opening this PR mostly so you can have a look, its WIP since it is based on #19283 which should be merged first. This only gives an impression about what other evalution metrics might look like.
</comment><comment author="MaineC" created="2016-07-20T13:51:22Z" id="233954965">Before merging I'd like to take a closer look at the original publication of the metric to make sure everything is fine. Other than that, looks good.
</comment><comment author="cbuescher" created="2016-07-26T16:42:54Z" id="235328744">@MaineC I added an optional parameter to the metric to specify the maximal allowed rank up to which the relevant first document is searched and added this to the unit and rest tests. Let me know what you think. btw. you mentioned an original publication, which would that be?
</comment><comment author="MaineC" created="2016-07-27T09:18:11Z" id="235532377">@cbuescher So, I found https://www.wikipendium.no/TDT4117_Information_retrieval#mean-reciprocal-rank-mrr which looks slightly different from what the wikipedia page. Now with this being just yet another wikipage I naively thought there must be an original paper somewhere - after digging for half an hour through references I don't find any. So scratch that comment of mine.
</comment><comment author="MaineC" created="2016-07-27T09:19:53Z" id="235532812">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to insert a parent-child Document?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19299</link><project id="" key="" /><description>when I use java client to insert a document, there's some errors:
unexpected error during the primary phase for action [indices:data/write/index], request [index {[userindex][useritem][1172], source[{"id":1172,"parent":"1172","text":"useritem1172"}]}]
java.lang.IllegalArgumentException: Can't specify parent if no parent field has been configured

&gt; User user = new User(i, "user" + i, (int) (Math.random() \* 100));
&gt;  byte[] userJson = JSONObject.toJSONBytes(user);
&gt;             IndexResponse response = client.prepareIndex("userindex", "user", user.getId().toString()).setSource(userJson).get();
&gt;             System.out.println(i + ", created=" + response.isCreated());
&gt; 
&gt;  Item item = new Item(i, "useritem" + i, user.getId().toString());
&gt;             byte[] itemJson = JSONObject.toJSONBytes(item);
&gt;             IndexResponse response1 = client.prepareIndex("userindex", "useritem", item.getId().toString()).setParent(user.getId().toString()).setSource(itemJson).get();
&gt;             System.out.println(i + ", itemcreated=" + response1.isCreated());

should I make mapping first and how to make?
</description><key id="164275195">19299</key><summary>How to insert a parent-child Document?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">debug529</reporter><labels /><created>2016-07-07T10:12:05Z</created><updated>2016-07-07T10:14:52Z</updated><resolved>2016-07-07T10:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-07T10:14:51Z" id="231038964">Please ask questions on the Elastic [Discourse forums](https://discuss.elastic.co/), we use GitHub for bug reports and feature requests only. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES not listening to port 9200 or any port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19298</link><project id="" key="" /><description>Ver 4.xx
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="164273867">19298</key><summary>ES not listening to port 9200 or any port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qubusp</reporter><labels /><created>2016-07-07T10:04:30Z</created><updated>2016-07-07T10:12:19Z</updated><resolved>2016-07-07T10:12:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-07T10:12:18Z" id="231038443">There is no useful content here, closing. The title suggests to me that you have a question and not a bug report. Please use the Elastic [Discourse forum](https://discuss.elastic.co/).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Init scripts reports "OK" when Elasticsearch fails to start from bad YAML</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19297</link><project id="" key="" /><description>Hello! The Chef cookbook for Elasticsearch has received a bug report regarding the packaging of ES, specifically the init scripts and status reporting. Here's the actual report: https://github.com/elastic/cookbook-elasticsearch/issues/483

In short, the user is seeing the init script for Ubuntu 14.04 report "OK" when starting Elasticsearch, even though parsing of the YAML configuration file has failed. Perhaps Elasticsearch needs a 'configtest' akin to the way Apache and Nginx have implemented it, or perhaps the init script just needs improvement to catch when Elasticsearch has exited or failed to start.

**Elasticsearch version**: 2.3.3
**JVM version**: 1.8.0_91
**OS version**: ubuntu 14.04

**Steps to reproduce**:
1. Add `http.cors.allow-origin: *` to elasticsearch.yml
2. Attempt to start the service

**Provide logs (if relevant)**:
The user reporting the issue found this in his logs:

```
Exception in thread "main" SettingsException[Failed to load settings from [elasticsearch.yml]]; nested: ScannerException[while scanning an alias
 in 'reader', line 41, column 25:
    http.cors.allow-origin: *
                            ^
expected alphabetic or numeric character, but found but found

 in 'reader', line 41, column 26:
    http.cors.allow-origin: *
                             ^
];
Likely root cause: while scanning an alias
 in 'reader', line 41, column 25:
    http.cors.allow-origin: *
                            ^
```
</description><key id="164267382">19297</key><summary>Init scripts reports "OK" when Elasticsearch fails to start from bad YAML</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">martinb3</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-07-07T09:30:43Z</created><updated>2017-03-31T14:04:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-07T13:09:42Z" id="231072500">I think this is a duplicate of https://github.com/elastic/elasticsearch/issues/12716 and Elasticsearch is failing to return the proper exit code because of the double daemonization
</comment><comment author="clintongormley" created="2016-07-07T13:10:23Z" id="231072660">@martinb3 any chance you could try what is suggested https://github.com/elastic/elasticsearch/issues/12716#issuecomment-224799775 and let us know if it fixes your issue?
</comment><comment author="martinb3" created="2016-07-07T13:17:50Z" id="231074478">Howdy @clintongormley -- I searched the tracker but didn't find that one -- sorry about that! I'll ask the original bug reporter to try the workaround.
</comment><comment author="clintongormley" created="2016-07-07T16:22:12Z" id="231131124">@martinb3 no worries - i want to get this one fixed, so would be good to have corroboration 
</comment><comment author="jasontedor" created="2016-07-07T18:32:10Z" id="231167087">@martinb3 Can you share some more details here? I think on System V init compatible systems we should be okay here but maybe not on systemd systems? I think that Ubuntu 14.04 is a System V init compatible system by default?

The Elasticsearch JVM does successfully start, it just dies during node start because of the malformed configuration file. We do have sleeps in our scripts attempting to detect when this happens and I just tested it on a clean Ubuntu 14.04 install and it did the right thing:

``` bash
# cat /etc/elasticsearch/elasticsearch.yml
node.name: test
# purposefully malformed
 cluster.name: test
# service elasticsearch start
 * Starting Elasticsearch Server [fail]
#
```
</comment><comment author="clintongormley" created="2016-07-08T07:30:07Z" id="231294020">@jasontedor but does the double daemonization mentioned in #12716 actually make sense? If we remove that, surely we wouldn't need the sleep?
</comment><comment author="jasontedor" created="2016-07-08T09:27:37Z" id="231315879">@clintongormley To be clear, the double daemonization needs to be removed and we should just rely on daemonization from the underlying service implementation but we still need a sleep somewhere otherwise in cases like this Elasticsearch will successfully daemonize and later die. The successful daemonization will appear like successful startup, so a sleep is needed to give it time to die if it's going to.
</comment><comment author="lassizci" created="2016-09-06T13:32:33Z" id="244950762">Struggling with this as well on systemd system. the problem with `Type=simple` is that there's no mechanism to know whether the service started or not. It has to be found out afterwards with `status`. `ExecStartPost` could have some sleep in which case if ES (hopefully) dies during the sleep, the startup failure would be reported, but then again, arbitary sleeps aren't always reliable. There seems to be at least https://github.com/faljse/SDNotify which would allow to send actual notifications for service `Type=notify`
</comment><comment author="colings86" created="2017-03-31T13:50:46Z" id="290717412">@jasontedor Is this still an issue? Could it be closed?</comment><comment author="jasontedor" created="2017-03-31T14:04:41Z" id="290721000">@colings86 Yes, this is still an issue and this should remain open.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent TransportReplicationAction to route request based on stale local routing table</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19296</link><project id="" key="" /><description>Backport of #16274 to 2.4.0.
Closes #19187
</description><key id="164257842">19296</key><summary>Prevent TransportReplicationAction to route request based on stale local routing table</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v2.4.0</label></labels><created>2016-07-07T08:40:42Z</created><updated>2016-08-26T13:32:06Z</updated><resolved>2016-07-07T15:04:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-07T10:55:56Z" id="231046673">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlights not found during search with snowball analyzer in Elastic Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19295</link><project id="" key="" /><description>I have created index for documents by setting the "snowball" analyzer for "content" filed in mapping.

Please check the following mapping for "content" filed of document.

"content": {
              "type": "attachment",
              "path": "full",
              "fields": {
                 "content": {
                    "type": "string",
                    "store": true,
                    "term_vector": "with_positions_offsets",
                    "analyzer": "snowball"
                 },
                 "author": {
                    "type": "string"
                 },
                 "title": {
                    "type": "string",
                    "store": true
                 },
                 "name": {
                    "type": "string"
                 },
                 "date": {
                    "type": "date",
                    "format": "dateOptionalTime"
                 },
                 "keywords": {
                    "type": "string"
                 },
                 "content_type": {
                    "type": "string"
                 },
                 "content_length": {
                    "type": "integer"
                 },
                 "language": {
                    "type": "string"
                 }
              }
           }
While searching for document, we are also fetching the matching snippets using the following search query.

GET  _search
{
  "query" : {
    "bool" : {
      "must" : {
        "bool" : {
          "should" : [ {
            "match" : {
              "content" : {
                "query" : "\"book to bill ratio\"",
                "type" : "phrase"
              }
            }
          } ],
          "minimum_should_match" : "1"
        }
      }
    }
  },
  "highlight" : {
    "fields" : {
      "content" : { }  
    }
  }
}
We have noticed an issue in searching for phrases, whenever stop words (such as "to", "and", "is" etc) present in search phrase, above query is able to find the matching document. But, it failed to find the matching snippets inside the document.

We are using the Elastic search version: 1.7.3

Appreciate your help.
</description><key id="164248920">19295</key><summary>Highlights not found during search with snowball analyzer in Elastic Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vdvc9</reporter><labels /><created>2016-07-07T07:46:22Z</created><updated>2016-07-07T07:59:32Z</updated><resolved>2016-07-07T07:59:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-07T07:59:32Z" id="231009481">You also opened this on discuss.elastic.co: https://discuss.elastic.co/t/highlights-not-found-during-search-with-snowball-analyzer-in-elastic-search/54913

Definitely the best place to start a discussion or ask questions.

So closing here for now. Will reopen if it appears to be an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add notes about sparsity.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19294</link><project id="" key="" /><description>cc @GlenRSmith 
</description><key id="164246129">19294</key><summary>Add notes about sparsity.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>review</label></labels><created>2016-07-07T07:27:37Z</created><updated>2016-07-07T16:47:13Z</updated><resolved>2016-07-07T15:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-07T07:28:58Z" id="231003801">@clintongormley any idea why I am getting `asciidoc: WARNING: general.asciidoc: line 58: section title out of sequence: expected level 2, got level 3 at lib/ES/Util.pm line 68.` when building docs with this PR?
</comment><comment author="nik9000" created="2016-07-07T12:45:35Z" id="231066988">Docs look good to me. I'm not sure about the asciidoc warning though.
</comment><comment author="clintongormley" created="2016-07-07T13:17:11Z" id="231074334">@jpountz all of your level 2 `===` headers are floated, so you can't use an unfloated level 3 `====` header
</comment><comment author="jpountz" created="2016-07-07T15:49:36Z" id="231121184">Thank you @nik9000 @clintongormley and @tlrx !
</comment><comment author="GlenRSmith" created="2016-07-07T16:47:13Z" id="231137979">@jpountz Huge thanks for doing this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JAR HELL - ES plugin-specific libraries conflicts with ES_HOME/lib libraries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19293</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**: 1.7

**OS version**: Windows 8

**Description of the problem including expected versus actual behavior**: 
My crawl-plugin is dependent on Apache Tika 1.8 jar, when I try to start ES, it throws a Jar-Hell.
I've tried the isolation plugin property as well, seems like it's not supported. Looking for a solution to this problem or a right way to get working.
Please find the trace below :
D:\ES_BINARY\elasticsearch-2.3.2\bin&gt;elasticsearch
[2016-07-07 00:44:16,298][INFO ][node                     ] [Monstro the Mighty] ve
[2016-07-07 00:44:16,299][INFO ][node                     ] [Monstro the Mighty] in
Exception in thread "main" java.lang.IllegalStateException: failed to load bundle [
rch-2.3.2/plugins/crawl-plugin/tika-app-1.8.jar] due to jar hell
Likely root cause: java.lang.IllegalStateException: jar hell!
class: com.sun.jna.AltCallingConvention
**jar1: C:\ES_BINARY\elasticsearch-2.3.2\lib\jna-4.1.0.jar
jar2: C:\ES_BINARY\elasticsearch-2.3.2\plugins\crawl-plugin\tika-app-1.8.jar**
        at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:282)
        at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:186)
        at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:129)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:158)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="164241947">19293</key><summary>JAR HELL - ES plugin-specific libraries conflicts with ES_HOME/lib libraries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chindhuhari</reporter><labels /><created>2016-07-07T06:56:03Z</created><updated>2016-07-07T09:57:59Z</updated><resolved>2016-07-07T07:54:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-07T07:54:40Z" id="231008551">Please use discuss.elastic.co for questions.

In mapper-attachments plugin we reduced the dependencies Tika is using for that. Have a look at the pom.xml in 2.3 branch. Might help you.
</comment><comment author="chindhuhari" created="2016-07-07T09:38:03Z" id="231031030">Hi @dadoonet !
Sorry, I'll post it on the discuss page from now on. I thought it might be an issue that has to be fixed.
The problem is, the Tika jar here is being used by my custom crawl plugin. There is a jna jar in the elasticsearch's lib/ which is causing conflicts, which in turn is throwing a jar hell. **What do we do when the libraries we want to use has conflicts with elasticsearch's libraries.** 
I've pasted the trace, please do take a look.
Thanks!
</comment><comment author="dadoonet" created="2016-07-07T09:46:56Z" id="231033007">You basically have to fix JAR conflicts on your side. So remove any jar which might conflict with elasticsearch.
Think about Jar Hell detector as a safe guard. It protects you from running unexpected code.

BTW, having a crawler as an elasticsearch plugin does not sound like a good idea to me.

Disclaimer: I wrote [FSCrawler](https://github.com/dadoonet/fscrawler) which was a river plugin but is now a standalone application.
As you can imagine I have no conflict anymore with elasticsearch librairies as I don't run my code embedded in elasticsearch :p .
</comment><comment author="chindhuhari" created="2016-07-07T09:54:41Z" id="231034749">Hi again ! Thanks for your immediate reply.
The problem is, the Tika jar(used by my plugin) by itself has a conflict with the jna jar used by elasticsearch. As in, tika-app-1.8.jar structure :
                     **tika-app-1.8.jar\com\sun\jna\     &lt;- This package and obviously the classes inside that is matching with the jna jar in ES_HOME/lib/
So, there is no scope of removing the jna out of the tika jar (repackaging the tika jar), as we are not allowed to that here :(**

I totally agree with you and have already seen the FSCrawler plugin :) months ago, just that it's something that might not be allowed at the place I work :|

Thanks much !
</comment><comment author="dadoonet" created="2016-07-07T09:57:59Z" id="231035426">As I said, if you do the [exact same thing we are doing](https://github.com/elastic/elasticsearch/blob/2.3/plugins/mapper-attachments/pom.xml#L38-L402), I believe you won't have a conflict.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplified repository api for snapshot/restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19292</link><project id="" key="" /><description>The api for snapshot/restore was split up between two interfaces,
Repository and IndexShardRepository. There was also complex
initialization and injection between the two. However, there is always a
one to one relationship between the two.

This change moves the IndexShardRepository api into Repository, as well
as updates the API so as not to require any services to be injected for
sublcasses.
</description><key id="164203129">19292</key><summary>Simplified repository api for snapshot/restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-07-07T00:14:53Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-07T20:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-07T19:12:22Z" id="231178100">I like the _idea_ of have two classes, on for the metadata stuff handling and one for the shard handling. I think I'd prefer there be some container and you get the class you need from it. I agree that it is kind of a mess now, with guice being involved at weird times and I'm not sure how to get there from here, but I kind of think this isn't the right way to start.
</comment><comment author="rjernst" created="2016-07-07T19:26:29Z" id="231181698">&gt; I like the idea of have two classes, on for the metadata stuff handling and one for the shard handling

But why? They are currently both always constructed on all nodes, and the Repository always contains the IndexShardRepository. There is just no reason to have them separate. Yes some methods will be invoked for metadata (on master) and others on shards (to eg actually snapshot or restore), but that is more clear now with those methods taking the IndexShard they will snapshot or restore into.
</comment><comment author="nik9000" created="2016-07-07T19:30:44Z" id="231182760">Yeah, I only really like the idea of having them separate for clarity. I agree that the methods should take an IndexShard.
</comment><comment author="rjernst" created="2016-07-07T19:33:48Z" id="231183505">Clarity is achieved through method names, inputs and documentation. I don't think we need to have separate classes since, as I said, both classes are constructed on all nodes anyways. Also note that there were crazy links between the two classes that made the construction more difficult, like the rate limit listener interface (the Repository would have to set this listener on the IndexShardRepository so it could have metrics on rate limiting updated). Now that is not necessary. Having these separate really only complicates things.
</comment><comment author="rjernst" created="2016-07-07T19:55:14Z" id="231188759">@nik9000 I pushed a new commit with methods renamed, and tweaked the javadocs a tad.
</comment><comment author="nik9000" created="2016-07-07T20:03:21Z" id="231190857">LGTM. It makes it better and is still clear enough. I wish it didn't have to be this way but I accept it.
</comment><comment author="rjernst" created="2016-07-07T20:04:12Z" id="231191063">Thanks Nik!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesModule.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryModule.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryNameModule.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoryTypesRegistry.java</file><file>core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/uri/URLIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStoragePlugin.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepository.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsPlugin.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/RepositoryS3SettingsTests.java</file></files><comments><comment>Merge pull request #19292 from rjernst/repository_deguice</comment></comments></commit></commits></item><item><title>Build Failure: org.elasticsearch.snapshots.FsBlobStoreRepositoryIT.testMultipleSnapshotAndRollback</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19291</link><project id="" key="" /><description>I was unable to reproduce this, but I'm not running Windows.

Reproduce With:
gradle :core:integTest -Dtests.seed=94DC6FDB9849F3FD -Dtests.class=org.elasticsearch.snapshots.FsBlobStoreRepositoryIT -Dtests.method="testMultipleSnapshotAndRollback" -Dtests.es.logger.level=DEBUG -Dtests.assertion.disabled=org.elasticsearch -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=762m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=cs -Dtests.timezone=America/Fort_Wayne

Build Failure:
http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3485/testReport/junit/org.elasticsearch.snapshots/FsBlobStoreRepositoryIT/testMultipleSnapshotAndRollback/
</description><key id="164200493">19291</key><summary>Build Failure: org.elasticsearch.snapshots.FsBlobStoreRepositoryIT.testMultipleSnapshotAndRollback</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>jenkins</label><label>test</label></labels><created>2016-07-06T23:49:52Z</created><updated>2016-09-14T17:46:01Z</updated><resolved>2016-09-14T17:26:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-07-07T00:15:08Z" id="230944965">I'll look into it

On Wed, Jul 6, 2016 at 7:50 PM Jack Conradson notifications@github.com
wrote:

&gt; I was unable to reproduce this, but I'm not running Windows.
&gt; 
&gt; Reproduce With:
&gt; gradle :core:integTest -Dtests.seed=94DC6FDB9849F3FD
&gt; -Dtests.class=org.elasticsearch.snapshots.FsBlobStoreRepositoryIT
&gt; -Dtests.method="testMultipleSnapshotAndRollback"
&gt; -Dtests.es.logger.level=DEBUG -Dtests.assertion.disabled=org.elasticsearch
&gt; -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=762m
&gt; -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops
&gt; -XX:+AggressiveOpts" -Dtests.locale=cs -Dtests.timezone=America/Fort_Wayne
&gt; 
&gt; Build Failure:
&gt; 
&gt; http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/3485/testReport/junit/org.elasticsearch.snapshots/FsBlobStoreRepositoryIT/testMultipleSnapshotAndRollback/
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19291, or mute the
&gt; thread
&gt; https://github.com/notifications/unsubscribe/ABjkQVPeKGIlF6GLK1g0DIPLtL8WTogxks5qTD81gaJpZM4JGnQ4
&gt; .
</comment><comment author="javanna" created="2016-09-14T15:51:43Z" id="247059792">@abeyad news on this? Looks like a security manager problem happened only once and never again. 
</comment><comment author="abeyad" created="2016-09-14T17:26:38Z" id="247090608">@javanna I think this can be closed, you're right and I was never able to reproduce it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Low performance of source filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19290</link><project id="" key="" /><description>**Elasticsearch version**:
2.3.3 on Elastic cloud

**JVM version**:
Not sure

**OS version**:
Not sure

**Description of the problem including expected versus actual behavior**:
When doing a query and providing a list of fields in `_source`, the filtering of those fields takes a considerable amount of time.

**Steps to reproduce**:
1. Given an index with at least a thousand documents
2. Make a query that matches at least a thousand documents and set `size: 1000`
3. Provide a list of fields  in the `_source` request param

This request takes a couple of seconds (specifically, the `took` field in the response body).

In contrast, when you set `_source: true` so that it returns all fields of the source, the request takes 100ms (but the transfer takes longer if each whole document is big)

It seems to be the case that most of the time goes into filtering the fields of every source object, and not reading the objects. If that is the case, I think there should be room for improvement in reformatting 1000 documents in less time.

I wouldn't mind making this my first contribution to ES. I'd propose to first, make a pull request to highlight source filtering in the profiler as I don't think that is measured. Make a second pull request that to improve the performance of source filtering. 

If this seems reasonable, I'd love to hear your thoughts as well as any pointers,

Thanks
</description><key id="164179300">19290</key><summary>Low performance of source filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luisobo</reporter><labels><label>:REST</label><label>feedback_needed</label></labels><created>2016-07-06T21:31:46Z</created><updated>2017-03-31T13:41:42Z</updated><resolved>2017-03-31T13:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-07T13:00:02Z" id="231070181">Yes, using source filtering adds an extra overhead compares with no filtering at all. As you said, in order to correctly filters the fields the source of every search hit must be loaded, parsed and then filtered. On 1000 hits that represent a fair amount of work. My own experiment shows that most of the time is spent at loading and parsing time, not at filtering time. But if you have any idea of improvement I'd be glad to discuss and help here.

&gt; In contrast, when you set _source: true so that it returns all fields of the source, the request takes 100ms (but the transfer takes longer if each whole document is big)

Right, when there's no source filtering, the source of every search hit is loaded and passed "as it is" to the coordinating node.

Out of curiosity, did you try to use the [Response Filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering) feature? I think it can filters search hits source now. It should be more efficient in your case because it only filters the "output", resulting in less data to transfert over the network.
</comment><comment author="luisobo" created="2016-07-11T15:30:20Z" id="231770406">@tlrx if I understand correctly, loading and parsing time goes to both filtered and non-filtered requests, is this correct? If that is the case, I don't understand why it would only affect source-filtered requests.

In either case, I'm surprised that filtering 1000 documents takes several seconds. I get that these are nested documents and that there is wildcards involved, still, seconds seems too much, just at an intuition level.

I did use response filtering and it didn't make a difference in my testing.

What I had in mind is start by profiling the code and get a better understanding. I wanted to see if anyone had any thoughts or pointers (and comply with the contributing guidelines by opening a ticket first 😜 )
</comment><comment author="nik9000" created="2016-07-11T15:33:27Z" id="231771407">&gt; In either case, I'm surprised that filtering 1000 documents takes several seconds.

Yeah, it is. It'd be a great thing to profile. We expect not filtering to be much more efficient but I didn't expect it to be _that_ much more efficient.
</comment><comment author="clintongormley" created="2016-07-11T15:49:22Z" id="231776412">&gt;  if I understand correctly, loading and parsing time goes to both filtered and non-filtered requests

Loading yes, parsing no.  Non-filtered requests just pass back the binary blob in the JSON, there's no parsing.
</comment><comment author="tlrx" created="2016-07-25T15:18:26Z" id="234984824">@luisobo There's certainly a room for improvement, that would be great if you can profile the code. Out of curiosity, how many shards are involved in your bench?
</comment><comment author="colings86" created="2017-03-31T13:41:42Z" id="290715070">No further feedback</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Batch process node left and node failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19289</link><project id="" key="" /><description>Today when a node is removed the cluster (it leaves or it fails), we
submit a cluster state update task. These cluster state update tasks are
processed serially on the master. When nodes are removed en masse (e.g.,
a rack is taken down or otherwise becomes unavailable), the master will
be slow to process these failures because of the resulting reroutes and
publishing of each subsequent cluster state. We improve this in this
commit by processing the node removals using the cluster state update
task batch processing framework.

Closes #19282
</description><key id="164142388">19289</key><summary>Batch process node left and node failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>:Discovery</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T18:26:04Z</created><updated>2016-07-11T13:49:29Z</updated><resolved>2016-07-11T12:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-07T09:48:02Z" id="231033289">@jasontedor I've left some comments / questions. I would prefer that we remove the `DuplicateClusterStateUpdateTaskException` change from this PR. At the moment it's not used by the node batching here and I don't think we should add it as long as we don't have an actual use for it (I also think that there might be another solution for this, but that's another discussion - let's not have it on this PR).
</comment><comment author="jasontedor" created="2016-07-07T12:15:59Z" id="231060844">@ywelsch Thanks for the feedback. I've responded to all of your comments.
</comment><comment author="ywelsch" created="2016-07-07T20:17:14Z" id="231194453">LGTM. Thanks @jasontedor!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeRemovalClusterStateTaskExecutorTests.java</file></files><comments><comment>Batch process node left and node failure</comment></comments></commit></commits></item><item><title>Adds support for cockroach actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19288</link><project id="" key="" /><description>A cockroach action is a transport-like action that is using the cluster state instead of transport to start tasks and send tasks responses. This allows cockroach tasks to survive restart of coordinating and executing nodes. A cockroach action can be implemented by extending TransportCockroachAction. TransportCockroachAction will start the task by using CockroachService, which controls cockroach tasks lifecycle.  See TestCockroachActionPlugin for an example implementing a cockroach action.

This PR just adds an infrastructure for creating cockroach tasks and tests. The first use of the tasks will be to migrate snapshot/restore functionality to it. Since it's a big change, it probably makes more sense to target 6.0.0 with it.
</description><key id="164130553">19288</key><summary>Adds support for cockroach actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label></labels><created>2016-07-06T17:28:24Z</created><updated>2016-08-11T11:41:36Z</updated><resolved>2016-08-01T00:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-20T16:30:40Z" id="234004164">If we need this, then can we please name it something useful and not "cute" (not that cockroaches are cute, just that it is a toy name).  It seems this is more of a "persistent action" or "persistent task"? There should also be package level javadocs explaining all the classes involved and what is necessary to create a new one of these "things" (eg extend this class like this, and extend that class like that, register here, etc).
</comment><comment author="imotov" created="2016-07-20T16:56:40Z" id="234011497">@rjernst the "If we need this..." part hurts, but I will try to ignore it since the rest of your comment is very helpful. I agree, the "persistent action/task" would be a great name but, unfortunately, it is already used by tasks that persist the results on disk, and I was concerned that it might cause confusion. A few of us brainstormed on a concise name for a while, but as you know naming in computer science is a difficult problem, so, this is all we could come up with. I ran it by a few of my colleagues and they liked it so the name stuck. If you have a better name, please let me know. I will be glad to rename it. The package level javadocs suggestion is really useful. Thank you. I will definitely add that. 
</comment><comment author="rjernst" created="2016-07-20T17:42:44Z" id="234024681">&gt;  the "If we need this..." part hurts

I did not mean it to be offensive, sorry. I meant it as "let's be absolutely sure we need to add thousands of lines of infrastructure for this". I see lots of things like this in the codebase that seem like a good idea at the time, but then only a handful of things are converted to the "new model" and we get stuck with tons of half used concepts and abstractions. I want to make sure that will not be the case here. Will all task actions be converted to this?

Regarding the name, will those disk persisted actions be converted to this new model? If so, maybe change those to another name temporarily? If not, why do we need both disk and cluster state persisted actions? If we really need both, then ClusterPersistedAction or ClusterPersistedTask I guess...
</comment><comment author="imotov" created="2016-07-20T18:40:00Z" id="234041736">&gt; I see lots of things like this in the codebase that seem like a good idea at the time, but then only a handful of things are converted to the "new model" and we get stuck with tons of half used concepts and abstractions.

Yes, I share your frustration here. 

&gt; Will all task actions be converted to this?

No. I really hope not. I mean, theoretically, we could do that but it would be quite disastrous since these actions are pretty heavy-weight comparing to normal transport actions. We have a small subset of actions that need to run for a prolong period of time and be resilient to node restarts. Two current examples of such actions are snapshot and restore. In a sense this PR is a generalized, reusable version of what snapshot and restore are currently doing in terms of their lifecycle. I am just extracting and de-duplication it and making it available to other actions such as benchmark api, for example. Even without future modules, when we have this framework in place, we will be able to remove a lot of very complex code from snapshot/restore module and allow it focus on moving files around instead of dealing with all this convoluted lifecycle management logic. 

&gt; If we really need both, then ClusterPersistedAction or ClusterPersistedTask I guess...

Yes, we need both since these are perpendicular concerns. ClusterPersistedTask sounds to me like a task that persists results into the cluster state. We also toyed with ResilientAction, but it makes all other actions sound non-resilient, which I though might offend Boaz. How does DurableAction sound to you? Other options that I considered are Enduring, Lasting, Undying and Everlasting. They sound kind of meh, but I can go with any of them if they sound "useful". Any other suggestions? 
</comment><comment author="abeyad" created="2016-07-21T02:10:49Z" id="234137745">&gt; How does DurableAction sound to you?

Sounds as reasonable a name as any proposed.  
</comment><comment author="abeyad" created="2016-07-21T02:13:22Z" id="234138019">@imotov I left an initial round of feedback.  Overall, the code is very intuitive and easy to follow.  Great work!
</comment><comment author="bleskes" created="2016-07-21T12:35:26Z" id="234240178">&gt; makes all other actions sound non-resilient, which I though might offend Boaz. 

Thanks for thinking of me. I wouldn't mind it for that reason but I think it does make all other task sound non-resilient and I think that's a discredit to them.

I'm also +1 on Ryan's suggestion to try and not use Cockroaches. Doesn't feel like the right kind of fun to me. 

I wasn't part of the said long discussion, so please tell me I'm all wrong but here is what I gather from all the little comments I read. We basically have two types of tasks:

1) A long running task, typically an administrative task. Those task need to survive the loss of any node and therefore we use the cluster state to coordinate them so if the node that initially received the request dies (and it might, because the task is long), it wouldn't be a problem.  Snapshot and Restore fit into this category but I though also (conceptually) reindexing.  We can't do it yet because there is no way at the moment recover from a crash of the coordinating node. That will come with Seq Numbers.

2) A short running task, which is typically a result of a client request. Those requests a fast and if the coordinating node dies, there is no way to respond to the client anyway. Basically all our transport requests result in these tasks (unless they opt out).

Orthogonal to this, we have the question of whether we want to persist the result of a task once it has completed. It seems to me that we actually only want this for long running tasks of the first category, but we implemented for 2 as well because of reindexing which is technically a type 2 because of the reason stated above.

With that in mind, I would suggest that we use `TransportTasks` (which @imotov already kind of used when he said "Transport Actions") for the second and `ClusterTasks` for the first. 

Looking the code for the persistency aspect it seems it's implemented by `PersistedTaskInfo` which wraps `TaskInfo` and maybe `PersistentActionListener`. I think the first one is OK and doesn't really present an issue. For the second one we can maybe rename to `PersistResultActionListener` . It seems this is what it does. 
</comment><comment author="rmuir" created="2016-07-22T12:02:00Z" id="234526096">&gt; I did not mean it to be offensive, sorry. I meant it as "let's be absolutely sure we need to add thousands of lines of infrastructure for this". I see lots of things like this in the codebase that seem like a good idea at the time, but then only a handful of things are converted to the "new model" and we get stuck with tons of half used concepts and abstractions. I want to make sure that will not be the case here. Will all task actions be converted to this?

I'm still confused here. Why are we adding this exactly? What is the concrete need?

The problem is, its so easy to add thousands of lines of code for no reason, yet its insanely hard to remove code in elasticsearch, no matter how esoteric (see #19359)

This means, elasticsearch code will continue to grow, unsustainably out of control, unless @rmuir steps up.

Which I am doing right now. Please, lets not add code unless there is a concrete need. It is literally impossible for the abstractions to be correct. Please don't add more abstractions than necessary either: you are doing them wrong if you are not building them around a concrete need, and they will only need to be re-done again.
</comment><comment author="imotov" created="2016-07-22T17:24:49Z" id="234604252">&gt; I'm still confused here. Why are we adding this exactly? What is the concrete need?

We are not adding anything new and it's not an esoteric construct. This code is essentially already in snapshot and restore, but it's tied deep inside and sprinkled with file moving logic, which makes it very difficult to test and reason about. I am just trying to extract it into a more testable and cleaner construct, that can be used by other services as well (for example we can consider moving reindex api and watchers to it at some point). The main driving factor is to refactor snapshot and restore first. I was thinking about doing this in one go (add long running task and remove corresponding code from snapshot and restore) but the resulting PR is simply not manageable. So, I am trying to get it in chunks. 
</comment><comment author="imotov" created="2016-08-01T00:48:23Z" id="236467794">After the extensive discussion with @mikemccand that took place on Friday and over the weekend I realized that it was a terrible mistake to open this PR without discussing it with @rjernst and @rmuir first. I totally agree, we simply cannot have so many esoteric features in Elasticsearch and the recent discussion with @mikemccand made me realize the important role that @rmuir plays in keeping this number in check. I am extremely grateful to @rmuir for his tireless effort and I am closing this PR as a small token of appreciation. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix replica-primary inconsistencies when indexing during primary relocation with ongoing replica recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19287</link><project id="" key="" /><description>Primary relocation violates two invariants that ensure proper interaction between document replication and peer recoveries, ultimately leading to documents not being properly replicated (see #19248 for more details).

Closes #19248
</description><key id="164118415">19287</key><summary>Fix replica-primary inconsistencies when indexing during primary relocation with ongoing replica recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T16:27:21Z</created><updated>2016-07-19T12:07:59Z</updated><resolved>2016-07-19T12:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-18T09:08:37Z" id="233279696">thx @ywelsch . I left some comments. Can we also maybe add some tests to RecoverySourceHandlerTests to test the new relocation behavior (waiting for a cluster state version and fail on move to relocated )?
</comment><comment author="ywelsch" created="2016-07-18T12:04:17Z" id="233310318">@bleskes I've pushed a new change addressing comments. I've also added the unit tests that you asked for but I am not convinced that they are too useful.
</comment><comment author="bleskes" created="2016-07-18T12:05:54Z" id="233310593">@ywelsch I'm wondering if changing the title to `Fix replica-primary inconsistencies when indexing during primary relocation with ongoing replica recoveries` will better describe the situation?  or even take over the issue's title? (we use PR to drive change lists)
</comment><comment author="bleskes" created="2016-07-18T21:29:04Z" id="233464803">LGTM. Thanks @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryWaitForClusterStateRequest.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file></files><comments><comment>Fix replica-primary inconsistencies when indexing during primary relocation with ongoing replica recoveries (#19287)</comment></comments></commit></commits></item><item><title>Duplicated documents after bulk create and server restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19286</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: OpenJDK 1.8.0_91

**OS version**: Ubuntu 16.04

**Description of the problem including expected versus actual behavior**:
Documents are not properly indexed when Elasticsearch is restarted just after bulk request is send.
Bulk request is retried because of _connection refused_ error. Finally random documents are duplicated (same content, different id) in the index.

**Steps to reproduce**:
1. Start sending 1000 bulk requests with 100 documents each, one after another. First request example:
   
   ```
   curl -XPOST 'localhost:9200/_bulk' -d '
              {"index":{"_index" : "index1", "_type": "test"}}
              {"counter": "1", "package": "1/1000"}
              {"index":{"_index" : "index1", "_type": "test"}}
              {"counter": "2", "package": "1/1000"}
              ...
              {"index":{"_index" : "index1", "_type": "test"}}
              {"counter": "100", "package": "1/1000"}
   '
   ```
2. Restart Elasticsearch just after server starts receiving requests. 
   `sudo service elasticsearch restart`
3. Repeat previous step couple of times.
4. If bulk request fails (Connection refused) because of Elasticsearch restart, repeat sending request until it succeed.
5. After all bulk requests are sent, index document count is grater then 10000.
   `curl -XGET http://localhost:9200/index1/_count`
   {"count":**10326**,"_shards":{"total":5,"successful":5,"failed":0}}

**Provide logs (if relevant)**:

```
[2016-07-06 15:49:28,441][INFO ][node                     ] [ubuntu1-node] stopping ...
[2016-07-06 15:49:28,485][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:146)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:89)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:85)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:356)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:351)
    at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishAsFailed(TransportReplicationAction.java:567)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleException(TransportReplicationAction.java:527)
    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-07-06 15:49:28,512][WARN ][transport                ] [ubuntu1-node] Transport response handler not found of id [43]
[2016-07-06 15:49:28,863][INFO ][node                     ] [ubuntu1-node] stopped
[2016-07-06 15:49:28,863][INFO ][node                     ] [ubuntu1-node] closing ...
[2016-07-06 15:49:28,868][INFO ][node                     ] [ubuntu1-node] closed
```
</description><key id="164094024">19286</key><summary>Duplicated documents after bulk create and server restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jt1</reporter><labels /><created>2016-07-06T14:47:39Z</created><updated>2016-07-06T15:32:07Z</updated><resolved>2016-07-06T15:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-06T15:11:29Z" id="230802906">Elasticsearch does not provide exactly-once delivery guarantees. If you want to avoid duplicates you can provide an explicit document id, for example:

```
curl -XPOST 'localhost:9200/_bulk' -d '
           {"index":{"_index" : "index1", "_type": "test", "_id" : "1"}}
           {"counter": "1", "package": "1/1000"}
           {"index":{"_index" : "index1", "_type": "test", "_id" : "2"}}
           {"counter": "2", "package": "1/1000"}
           ...
           {"index":{"_index" : "index1", "_type": "test", "_id" : "3"}}
           {"counter": "100", "package": "1/1000"}
'
```
</comment><comment author="bleskes" created="2016-07-06T15:32:07Z" id="230809118">&gt; Elasticsearch does not provide exactly-once delivery guarantees.

I get what Yannick means, but I'm not sure this statement is 100% clear.  The problem is more that the API you are using is built for a one off usage. The main reason is that those documents do not have an id, which means ES will auto generate a unique one. , When you send twice, ES has no way of know that they are really the same documents and it will generate a new id for those.

If you need these kind of things you should generate your own ids (see http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html ) and use those in your requests. Another alternative, is to use the Ids ES returns in the response for bulks for the retries. Of course, this means you need to actually get the response and store that information.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't flood the logs when we detect enormous exception in ExceptionHe…</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19285</link><project id="" key="" /><description>…lper#unwrapCause

This code seems to be a circuit breaker to prevent digging too deeply in the
exception causes.
If we enter this circuit breaker then it's likely that something very bad is
happening. Unfortunately we ask the logger to log the full stack of all the causes.
It can generate gigs of logs in few minutes and filling up all disk space.
It happened on a cluster affected by what seem to be recursion bug (c.f. #19187)
where it generated 27gig of logs in less than 5 minutes.
While this code is useful to debug problematic exceptions it may generate too
many lines causing "no space left on devices" errors thus making debugging the
root cause even harder.
</description><key id="164084160">19285</key><summary>Don't flood the logs when we detect enormous exception in ExceptionHe…</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nomoa</reporter><labels><label>:Logging</label><label>bug</label><label>discuss</label><label>review</label></labels><created>2016-07-06T14:07:58Z</created><updated>2016-07-07T13:06:11Z</updated><resolved>2016-07-07T08:06:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-06T14:13:15Z" id="230783946">I throw `v2.4.0` because I expect it'd be nice to get backported. 27GB of logs is bad.

Speaking of 27GB of logs, maybe we should change the default logging configuration to roll on size too? I've always done that in previous Java projects just to prevent craziness like that from taking down the system.
</comment><comment author="nomoa" created="2016-07-06T14:39:32Z" id="230792498">Having a stack is important I agree, maybe the proper fix would be to add a new circuit breaker in the logger itself?
Looks like we can pass our own log4j.throwableRenderer in log4j, Having a custom ThrowableRenderer would allow us to add some checks and limit log verbosity with giant exceptions.
</comment><comment author="nik9000" created="2016-07-06T14:42:47Z" id="230793588">&gt; Looks like we can pass our own log4j.throwableRenderer in log4j, Having a custom ThrowableRenderer would allow us to add some checks and limit log verbosity with giant exceptions.

@jasontedor this seems interesting.
</comment><comment author="nik9000" created="2016-07-06T14:44:38Z" id="230794205">It looks like in log4j2 that feature is gone and instead you'd need to use something in the pattern layout. https://logging.apache.org/log4j/2.x/manual/layouts.html#PatternLayout
</comment><comment author="nik9000" created="2016-07-06T14:45:11Z" id="230794376">There is a `depth` option but no reference to what it does....
</comment><comment author="rmuir" created="2016-07-06T15:11:40Z" id="230802977">Yes, I would greatly prefer it if we preserved stacktraces and did not make these helper methods more lenient/exception discarders.

Instead we should just change what we write, that is better.

Perhaps you can use the logic from StartupError somehow: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/bootstrap/StartupError.java

There is a lot of logic there for compressing guice exceptions and other issues. 
</comment><comment author="jasontedor" created="2016-07-06T16:14:45Z" id="230823269">&gt; Yes, I would greatly prefer it if we preserved stacktraces and did not make these helper methods more lenient/exception discarders.

+1, I think there's a real issue in #19187 but it's not clear what it is. Without stack traces it'd be hopeless to uncover.
</comment><comment author="nomoa" created="2016-07-06T18:45:33Z" id="230867059">Agreed.
Would it make sense to wrap giant exceptions detected by ExceptionHelper with something like StartupError and skip all the "Caused by" lines (`&lt;&lt; skipped XXX "Caused by" lines &gt;&gt;`) and print only the root cause?
</comment><comment author="jasontedor" created="2016-07-07T03:13:37Z" id="230968280">&gt; Would it make sense to wrap giant exceptions detected by ExceptionHelper with something like StartupError and skip all the "Caused by" lines (`&lt;&lt; skipped XXX "Caused by" lines &gt;&gt;`) and print only the root cause?

Alas, I don't think it's that simple. Here it looks like we can get away with it because the inner causes do not contain stack traces but I think that's only because they are lost in the serialization. Yet, if there were no serialization involved here, the interior causes would have stack traces and those stack traces are extremely important. This means that "skipping" is not generally applicable.

I'm sorry that your disks filled up. Are your logs mounted on a separate partition? Do you have log compression and log rotation in place? No matter what Elasticsearch does, I think that needs to be in place.

I don't think we should do anything that might cause a loss of stack traces. It's too important, there have been bugs that we only got one shot at. Rather than chopping all stack traces off at their knees, we just need to take each issue on a case-by-case basis. In this case, it looks like you're losing to primary routing chasing its own tail which is addressed in future versions of Elasticsearch.
</comment><comment author="nomoa" created="2016-07-07T08:06:17Z" id="231010804">OK, I'm certainly biased by this particular issue.
Intermediate stacks are explicitly disabled by `RemoteTransportException#fillInStackTrace`.
Rotating logs would have helped to prevent disk full but I would have lost relevant logs just before the issue started.
Disk full is annoying for sure but extracting relevant information out of 27g of logs is hard and when I saw that `unwrapCause` does not want to loop into more than 10 causes but ask the logger to do so I found that strange.
I suppose that there are no good solutions here and this warning is useful in many other cases.
I'm closing this PR because I don't have great ideas.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add catchAnalyzer precommit task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19284</link><project id="" key="" /><description>The idea here is to add a simple check to find broken catch blocks. It detects a couple of common problems:
- throwing a different exception, but forgetting to pass the original one to its ctor, or calling initCause/addSuppressed/etc. So information about what originally happened is lost!
- swallowing an exception completely (e.g. just logging it and moving on).

It does some flow analysis (+ hacks) to track the original exception via all possible codepaths. If the method can return without throwing anything, or throws a different exception that doesn't have the original chained to it in some way, it fails.

It also knows about rethrowers that we have, and some helper methods "boxers" that wrap exceptions in other ones.

Scans are true to the bytecode: so don't be surprised by the errors for broken catch blocks with try-with-resources/finally/etc. These aren't duplicates, that's what javac is actually doing with your code.

I think if we really want to swallow an exception, its ok to annotate the method with `@SwallowsException` saying that its intentional. Otherwise in many cases its a bug.

Unfortunately, there are thousands of violations. Even just annotating them all with `@SwallowsException(reason = "?")` takes forever (I did some of them, but quickly gave up). We have to figure out a way to iteratively clean this up...
</description><key id="164071984">19284</key><summary>Add catchAnalyzer precommit task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>PITA</label></labels><created>2016-07-06T13:11:14Z</created><updated>2016-11-16T15:21:40Z</updated><resolved>2016-11-16T15:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-06T13:49:31Z" id="230776688">I reviewed the easy parts of the code (all but the asm bits) and I very much want it. I'm not sure what the right path is to get it merged though without failing the build for days while we fix all the issues it throws back. Maybe have a file where we store all of the known bad files and intentionally ignore them. That way we at least stop adding more of these. Then we work on removing them and finally remove support for (or just configuration for) the file? I know that is the strategy we used with line length and we still 1200 files that violate, but at least the list isn't growing any more and we're slowly removing them.

Working on this in a long running branch seems like a bad choice because you'll end up with a zillion conflicts as people continue to add things.
</comment><comment author="rmuir" created="2016-07-06T13:51:30Z" id="230777293">Yes, that is my problem. I don't feel the scanner should be made more complex because the codebase is such a mess, but that is just me. Maybe we can add the thing as a standalone task (_not_ called by precommit) as a step. Then it can be run manually and we can mark all the bad guys and keep up.

I've already had a branch for this thing for over a week and its very painful.
</comment><comment author="nik9000" created="2016-07-06T13:58:24Z" id="230779255">&gt; I've already had a branch for this thing for over a week and its very painful.

Yeah. I can't imagine that working out well over a month or something.

A stand alone task doesn't stop us from adding more failures though, and that seems really important when migrating to a thing like that. It is better than a branch though. If you can get someone to review the analysis code I'm +1 on doing it as a stand alone task just to get it merged and then we can think about other solutions. I like the file idea because it is _fairly_ fast to generate the files from failures but I understand about making it overcomplicated as a migration step.
</comment><comment author="nik9000" created="2016-07-06T13:59:48Z" id="230779692">If you can write a script that adds the annotations from failures then we could add all the annotations with the script and merge it that way?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query evaluation: Adding rest layer parsing and response rendering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19283</link><project id="" key="" /><description>Adding parsing for the rest request for the first prototypical implementation of the `rank_eval` endpoint. Also extending the existing rest test and adding rendering of the response. This PR is against the WIP rank-eval feature branch.

Relates to #19195
</description><key id="164071185">19283</key><summary>Query evaluation: Adding rest layer parsing and response rendering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>feature</label><label>review</label></labels><created>2016-07-06T13:06:57Z</created><updated>2016-07-21T11:11:31Z</updated><resolved>2016-07-21T11:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-07-06T13:11:07Z" id="230766444">@MaineC for you to take a look then you are back. I made the rest request and tests work, mostly trying to stick to the syntax proposed in the comments and the original issue but changed it in some places where I thought it might be useful. Also changed a few of the classes representing the whole qa query request (e.g. deleted the RatedQuery, introduced the RatedDocument instead). This is all just a proposal, I will leave a few inline comments to motivate some of the changes. 
</comment><comment author="MaineC" created="2016-07-20T13:22:26Z" id="233947029">I think this is awesome progress. There are a couple things we need to iron out, most of which you already pointed to in your comments, but I think we can do so after merging in separate PRs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node failures are not batch processed by the master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19282</link><project id="" key="" /><description>Currently when nodes fail we go through the failures one by one, each resulting in a cluster state publish. This can cause an issue on large clusters that lose a significant number of nodes. 

@bleskes 
</description><key id="164070452">19282</key><summary>Node failures are not batch processed by the master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jjfalling</reporter><labels><label>:Discovery</label></labels><created>2016-07-06T13:03:04Z</created><updated>2016-07-11T12:30:09Z</updated><resolved>2016-07-11T12:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-06T13:08:46Z" id="230765921">yes, that's certainly an optimization that can be done (we have talked about this before but not gotten around to it yet. It's simpler now to implement with the batching infrastructure in the clusterservice). Would you like to contribute a PR?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add benchmark for REST / transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19281</link><project id="" key="" /><description>This PR adds a benchmark for checking performance of transport client vs. the new REST client.

This is a WIP PR and I've just created it so you can give early feedback.

It currently benchmarks only bulk indexing (based on the geonames benchmarks) and also contains tons of TODOs.

I plan to account for proper warmup, restructure the code a bit and also add some search benchmarks.

Relates #18735
</description><key id="164064217">19281</key><summary>Add benchmark for REST / transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Benchmark</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T12:28:12Z</created><updated>2016-07-27T16:57:16Z</updated><resolved>2016-07-26T09:02:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-06T12:31:25Z" id="230757688">@javanna the preliminary benchmark PR is now up. I'll continue to work on it but feel free to run your own benchmarks already.
</comment><comment author="danielmitterdorfer" created="2016-07-07T12:59:04Z" id="231069959">@javanna I've now pushed also a search benchmark and corrected a few things. Please see the README for usage instructions.
</comment><comment author="danielmitterdorfer" created="2016-07-19T12:03:26Z" id="233611569">Now that OpenHFT/Chronicle-Core#14 is merged we can eventually remove our infrastructure code for the REST client benchmark and eventually use JLBH (after the next release of Chronicle core). But I'd probably do this in a separate PR.
</comment><comment author="javanna" created="2016-07-25T09:41:31Z" id="234907944">I left a few minor comments, LGTM otherwise, let's get it in!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/AbstractBenchmark.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/BenchmarkRunner.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/BenchmarkTask.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/metrics/Metrics.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/metrics/MetricsCalculator.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/metrics/Sample.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/metrics/SampleRecorder.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/ops/bulk/BulkBenchmarkTask.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/ops/bulk/BulkRequestExecutor.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/ops/search/SearchBenchmarkTask.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/ops/search/SearchRequestExecutor.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/rest/RestClientBenchmark.java</file><file>client/benchmark/src/main/java/org/elasticsearch/client/benchmark/transport/TransportClientBenchmark.java</file></files><comments><comment>Add client benchmark</comment></comments></commit><commit><files /><comments><comment>Remove duplicate dependency declaration for http client (#19580)</comment></comments></commit></commits></item><item><title>[TEST] Move back some messy tests from Groovy plugin to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19280</link><project id="" key="" /><description>After #13834 many tests that used Groovy scripts (for good or bad reason) in their tests have been moved in the `lang-groovy` module and the issue #13837 has been created to track these messy tests in order to clean them up.

This PR moves `BucketSelectorIT`, `BucketScriptIT`, `HDRPercentileRanksIT` and `HDRPercentilesIT` back at their original place in core, removes the dependency on Groovy, changes the scripts in order to use the mocked script engine `MockScriptEngine` instead, and change them back to integration tests.

This PR also changes `MockScriptEngine` and `MockScriptPlugin` so that it is easier, I hope, to mock scripts.
</description><key id="164051921">19280</key><summary>[TEST] Move back some messy tests from Groovy plugin to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T11:08:08Z</created><updated>2016-07-07T13:28:33Z</updated><resolved>2016-07-07T13:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-06T12:59:41Z" id="230763736">I left a few comments. I love the idea of using a script-looking implementation as the id of the script.

I think I'd prefer if the custom scripts used for aggregations were shared. I see that not all of them are shared, but the ones that are it'd be nice if they were in a top level class in an appropriate package. Mostly because I think those implementations might drift apart over time and while they are the same now they'll change and we'll have subtle bugs you can only find by comparing two nearly identical chunks of code.
</comment><comment author="tlrx" created="2016-07-06T13:13:38Z" id="230767032">@nik9000 Thanks for your review! I updated the code, can you please have another look?

&gt; I think I'd prefer if the custom scripts used for aggregations were shared. I see that not all of them are shared, but the ones that are it'd be nice if they were in a top level class in an appropriate package. Mostly because I think those implementations might drift apart over time and while they are the same now they'll change and we'll have subtle bugs you can only find by comparing two nearly identical chunks of code.

I agree too, let's do that.
</comment><comment author="nik9000" created="2016-07-06T14:03:12Z" id="230780721">LGTM
</comment><comment author="tlrx" created="2016-07-07T13:28:33Z" id="231077208">Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptSettings.java</file><file>core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/script/FileScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupIT.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/StoredScriptsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java</file><file>core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexLookupTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyIndexedScriptTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/TemplateQueryBuilderTests.java</file><file>test/framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment> [TEST] Kill remaining lang-groovy messy tests</comment></comments></commit></commits></item><item><title>NLP will throw when no source for search result with compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19279</link><project id="" key="" /><description>**Elasticsearch version**: 2.1.1(2.x)

**JVM version**:  1.8.0_66-b17 

**OS version**: Mac OS X 10.10.5 

**Description of the problem including expected versus actual behavior**:
java.lang.NullPointerException will be thrown when converting search response to json if search hits without no source returned(just specify search field) and compress is enable, because  the **isCompressed** method of **LZFCompressor** and **DeflateCompressor**  will check the parameter 'bytes' is null or not before do uncompress operation.
 Stacktrace can be referenced at log section.

**Steps to reproduce**:
1. Use SearchRequestBuilder.addField()  to add search result field before execute search operation and it will not return source of document for search response hits.
2. Execute search operation.
3. Get the reference SearchResponse and use  com.alibaba.fastjson.JSON.toJSONString() to convert it into json string, you will got NPE

**Provide logs (if relevant)**:

java.lang.NullPointerException: null
    at org.elasticsearch.common.compress.lzf.LZFCompressor.isCompressed(LZFCompressor.java:54) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:74) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.common.compress.CompressorFactory.uncompressIfNeeded(CompressorFactory.java:118) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.search.internal.InternalSearchHit.sourceRef(InternalSearchHit.java:200) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.search.internal.InternalSearchHit.getSourceRef(InternalSearchHit.java:219) ~[elasticsearch-2.1.1.jar:2.1.1]
    at Serializer_17.write1(Unknown Source) ~[na:na]
    at Serializer_17.write(Unknown Source) ~[na:na]
    at com.alibaba.fastjson.serializer.ArraySerializer.write(ArraySerializer.java:64) ~[fastjson-1.2.8.jar:na]
    at com.alibaba.fastjson.serializer.JSONSerializer.writeWithFieldName(JSONSerializer.java:403) ~[fastjson-1.2.8.jar:na]
    at Serializer_16.write1(Unknown Source) ~[na:na]
    at Serializer_16.write(Unknown Source) ~[na:na]
    at com.alibaba.fastjson.serializer.JSONSerializer.writeWithFieldName(JSONSerializer.java:403) ~[fastjson-1.2.8.jar:na]
    at Serializer_14.write1(Unknown Source) ~[na:na]
    at Serializer_14.write(Unknown Source) ~[na:na]
    at com.alibaba.fastjson.serializer.JSONSerializer.writeWithFieldName(JSONSerializer.java:403) ~[fastjson-1.2.8.jar:na]
    at Serializer_12.write1(Unknown Source) ~[na:na]
    at Serializer_12.write(Unknown Source) ~[na:na]
    at com.alibaba.fastjson.serializer.JSONSerializer.write(JSONSerializer.java:374) ~[fastjson-1.2.8.jar:na]
    at com.alibaba.fastjson.JSON.toJSONString(JSON.java:394) ~[fastjson-1.2.8.jar:na]
    at com.alibaba.fastjson.JSON.toJSONString(JSON.java:382) ~[fastjson-1.2.8.jar:na]
</description><key id="164044235">19279</key><summary>NLP will throw when no source for search result with compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">XZhi</reporter><labels><label>:Internal</label><label>adoptme</label><label>bug</label></labels><created>2016-07-06T10:19:58Z</created><updated>2016-11-10T09:54:43Z</updated><resolved>2016-11-10T09:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-06T12:55:52Z" id="230762850">I think this has been fixed by https://github.com/elastic/elasticsearch/pull/18957

Could you try your test on master and let us know if it is indeed fixed?
</comment><comment author="XZhi" created="2016-07-17T00:16:48Z" id="233157968">Sorry for replying late.

I reviewed the changed code for  #18957, but I don't think it is fixed in it.
The NPE is thrown by LZFCompressor or DeflateCompressor when _source is not returned and _source is not be disabled.  I guess it will be fixed at LZFCompressor and DeflateCompressor from the stacktrace.
</comment><comment author="thanhtien522" created="2016-11-09T10:48:07Z" id="259385679">I also met this issue. Function `sourceRef()` in class InternalSearchHit always throw NullPointerException when `source` is `null`.

Elasticsearch version: 2.4.0
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>core/src/test/java/org/elasticsearch/search/internal/InternalSearchHitTests.java</file></files><comments><comment>Add null check in InternalSearchHit#sourceRef to prevent NPE (#21431)</comment></comments></commit></commits></item><item><title>Dependencies: Update to jopt-5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19278</link><project id="" key="" /><description>The new version of jopt allows us to remove a couple of TODOs in the code.

Closes #12368
</description><key id="164021393">19278</key><summary>Dependencies: Update to jopt-5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>review</label><label>upgrade</label></labels><created>2016-07-06T08:10:42Z</created><updated>2016-07-07T06:50:11Z</updated><resolved>2016-07-07T06:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-06T08:19:45Z" id="230708305">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file></files><comments><comment>Remove dead code for checking exclusive options</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/ElasticsearchCliTests.java</file><file>core/src/test/java/org/elasticsearch/cli/CommandTests.java</file><file>test/framework/src/main/java/org/elasticsearch/bootstrap/ESElasticsearchCliTestCase.java</file></files><comments><comment>Dependencies: Update to jopt-5.0 (#19278)</comment></comments></commit></commits></item><item><title>Migrate terms aggregation to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19277</link><project id="" key="" /><description>Migrate terms aggregation to NamedWriteable and clean up serialization, sharing lots more code.
</description><key id="163990342">19277</key><summary>Migrate terms aggregation to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T03:16:02Z</created><updated>2016-07-07T21:14:30Z</updated><resolved>2016-07-07T21:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-06T03:16:14Z" id="230661300">@colings86, sorry, this one is larger than the others.
</comment><comment author="colings86" created="2016-07-06T09:32:58Z" id="230724245">@nik9000 I left a few comments
</comment><comment author="nik9000" created="2016-07-06T17:46:33Z" id="230850202">@colings86 I pushed some updates. I wasn't able to get unmapped terms to fail with a format. I think this is the magic bit of `UnmappedTerms`:

``` java
    @Override
    public InternalAggregation doReduce(List&lt;InternalAggregation&gt; aggregations, ReduceContext reduceContext) {
        for (InternalAggregation agg : aggregations) {
            if (!(agg instanceof UnmappedTerms)) {
                return agg.reduce(aggregations, reduceContext);
            }
        }
        return this;
    }
```
</comment><comment author="colings86" created="2016-07-07T08:09:32Z" id="231011418">@nik9000 thanks for making the changes. This LGTM now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove guice from transport client helper classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19276</link><project id="" key="" /><description>This change removes injection for constructors of
TransportClientNodesService and TransportProxyClient.
</description><key id="163988065">19276</key><summary>Remove guice from transport client helper classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-06T02:49:29Z</created><updated>2016-07-06T12:28:43Z</updated><resolved>2016-07-06T03:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-06T02:54:00Z" id="230658992">Cool
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file></files><comments><comment>Merge pull request #19276 from rjernst/transport_client_deguice</comment></comments></commit></commits></item><item><title>Monitoring plugin indices stuck in failed recovery status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19275</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

5.0.0-alpha3

**JVM version**:

The docker image inherits from the official java:8-jre image.

**OS version**:

Official docker image (https://hub.docker.com/_/elasticsearch/, Debian Jessie) running on a Fedora 23 host.

**Description of the problem including expected versus actual behavior**:

Shards created by the Monitoring X-Pack plugin are stuck trying to recover. Kibana reports "Elasticsearch is still initializing the Monitoring indices". The logs are given below, notably "marking and sending shard failed due to [failed recovery]" and "IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine];".

This has happened twice (out of 2 times tried) on a testing environment following the steps given below. A few things to note:
- This happens only on the shards created by the monitoring plugin, custom indices and kibana's indices are properly initialized and all data seem to be in place and can be queried.
- When the environment is first set up, all looks ok. The problem occurs when it is shut down and restarted.
- Similar issues indicate data corruption. I'm not sure why this would happen only on the Monitoring indices though. Related issue: https://github.com/elastic/elasticsearch/issues/2598
- Elastic Search continuously tries to recover the index, logging the same entry 1-7 times per second. This results in gigabytes of logs in small amount of time, while the machine becomes sluggish for quite a while due to high memory usage, possibly related to continuous efforts for index recovery. Pull request https://github.com/elastic/elasticsearch/pull/11269 released in 1.6 is supposed to fix this, but it still occurs in 5.0.0-alpha3, related issue: https://github.com/elastic/elasticsearch/issues/11681.
- Even if there is a reason for Elastic Search to retry several times per second, not infinitely though, there is no reason to clutter the logs up to 7 times per second. Related issue: https://github.com/elastic/elasticsearch/issues/19164.

I do not recall having the same problem with Marvel and ElasticSearch 2 when following the same steps.

Any suggestions on how to figure out if this is due to data corruption, and why only on the monitoring plugin indices? Not sure if it is related, but other indices would have really low write rate, since this is a test environment, not sure how many writes per second the monitoring plugin would do.

**Steps to reproduce**:
1. Initialise a single node Elastic Search cluster in a docker image, connected to another docker image running Kibana. X-Pack plugin should be installed in both docker images.
2. Give some time for initializing the indices, and load Kibana - all should be looking good, including the Monitoring plugin. Use the cluster for a while, send some records to Elastic Search.
3. Stop the cluster, for example with "docker-compose stop".
4. Restart the cluster. The monitoring indices can't be recovered.

**Provide logs (if relevant)**:

```
{"log":"[2016-07-05 20:56:55,776][INFO ][gateway                  ] [Flying Tiger] recovered [18] indices into cluster_state\n","stream":"stdout","time":"2016-07-05T20:56:55.776542565Z"}
{"log":"[2016-07-05 20:57:05,422][INFO ][monitor.jvm              ] [Flying Tiger] [gc][35] overhead, spent [369ms] collecting in the last [1.2s]\n","stream":"stdout","time":"2016-07-05T20:57:05.468691606Z"}
{"log":"[2016-07-05 20:57:26,236][WARN ][indices.cluster          ] [Flying Tiger] [[.monitoring-data-2][0]] marking and sending shard failed due to [failed recovery]\n","stream":"stdout","time":"2016-07-05T20:57:26.238632342Z"}
{"log":"RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Flying Tiger}{CgS1ucvZSay5Rjw07LqhMQ}{172.17.0.6}{172.17.0.6:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.238689525Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$0(IndexShard.java:1369)\n","stream":"stdout","time":"2016-07-05T20:57:26.238706279Z"}
{"log":"\u0009at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)\n","stream":"stdout","time":"2016-07-05T20:57:26.238720854Z"}
{"log":"\u0009at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n","stream":"stdout","time":"2016-07-05T20:57:26.23873453Z"}
{"log":"\u0009at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n","stream":"stdout","time":"2016-07-05T20:57:26.238750303Z"}
{"log":"\u0009at java.lang.Thread.run(Thread.java:745)\n","stream":"stdout","time":"2016-07-05T20:57:26.238766104Z"}
{"log":"Caused by: [.monitoring-data-2/-4ZQISQpRQu5B58rxnkYgg][[.monitoring-data-2][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.238789176Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:229)\n","stream":"stdout","time":"2016-07-05T20:57:26.238805723Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$0(StoreRecovery.java:78)\n","stream":"stdout","time":"2016-07-05T20:57:26.238818972Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)\n","stream":"stdout","time":"2016-07-05T20:57:26.238834596Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:76)\n","stream":"stdout","time":"2016-07-05T20:57:26.238849856Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1119)\n","stream":"stdout","time":"2016-07-05T20:57:26.238862794Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$0(IndexShard.java:1365)\n","stream":"stdout","time":"2016-07-05T20:57:26.23887556Z"}
{"log":"\u0009... 4 more\n","stream":"stdout","time":"2016-07-05T20:57:26.238889044Z"}
{"log":"Caused by: [.monitoring-data-2/-4ZQISQpRQu5B58rxnkYgg][[.monitoring-data-2][0]] EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.23890146Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.\u003cinit\u003e(InternalEngine.java:139)\n","stream":"stdout","time":"2016-07-05T20:57:26.238917752Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n","stream":"stdout","time":"2016-07-05T20:57:26.238934861Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1430)\n","stream":"stdout","time":"2016-07-05T20:57:26.238947889Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1414)\n","stream":"stdout","time":"2016-07-05T20:57:26.238960702Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:928)\n","stream":"stdout","time":"2016-07-05T20:57:26.238972961Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:890)\n","stream":"stdout","time":"2016-07-05T20:57:26.238985065Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:225)\n","stream":"stdout","time":"2016-07-05T20:57:26.239019441Z"}
{"log":"\u0009... 9 more\n","stream":"stdout","time":"2016-07-05T20:57:26.239032857Z"}
{"log":"Caused by: java.io.EOFException\n","stream":"stdout","time":"2016-07-05T20:57:26.239044567Z"}
{"log":"\u0009at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)\n","stream":"stdout","time":"2016-07-05T20:57:26.239056161Z"}
{"log":"\u0009at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)\n","stream":"stdout","time":"2016-07-05T20:57:26.239068626Z"}
{"log":"\u0009at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)\n","stream":"stdout","time":"2016-07-05T20:57:26.239084091Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Checkpoint.\u003cinit\u003e(Checkpoint.java:52)\n","stream":"stdout","time":"2016-07-05T20:57:26.23910018Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:81)\n","stream":"stdout","time":"2016-07-05T20:57:26.239113599Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:238)\n","stream":"stdout","time":"2016-07-05T20:57:26.239125431Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Translog.\u003cinit\u003e(Translog.java:180)\n","stream":"stdout","time":"2016-07-05T20:57:26.239137437Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:229)\n","stream":"stdout","time":"2016-07-05T20:57:26.239149796Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.\u003cinit\u003e(InternalEngine.java:136)\n","stream":"stdout","time":"2016-07-05T20:57:26.239164805Z"}
{"log":"\u0009... 15 more\n","stream":"stdout","time":"2016-07-05T20:57:26.239179966Z"}
{"log":"[2016-07-05 20:57:26,238][WARN ][cluster.action.shard     ] [Flying Tiger] [.monitoring-data-2][0] received shard failed for target shard [[.monitoring-data-2][0], node[CgS1ucvZSay5Rjw07LqhMQ], [P], s[INITIALIZING], a[id=kB0PjIVzSkSdXq_MYghS-A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-07-05T20:56:47.076Z]]], source shard [[.monitoring-data-2][0], node[CgS1ucvZSay5Rjw07LqhMQ], [P], s[INITIALIZING], a[id=kB0PjIVzSkSdXq_MYghS-A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-07-05T20:56:47.076Z]]], message [failed recovery], failure [RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Flying Tiger}{CgS1ucvZSay5Rjw07LqhMQ}{172.17.0.6}{172.17.0.6:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]\n","stream":"stdout","time":"2016-07-05T20:57:26.251208298Z"}
{"log":"RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Flying Tiger}{CgS1ucvZSay5Rjw07LqhMQ}{172.17.0.6}{172.17.0.6:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.25132659Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$0(IndexShard.java:1369)\n","stream":"stdout","time":"2016-07-05T20:57:26.251345996Z"}
{"log":"\u0009at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:392)\n","stream":"stdout","time":"2016-07-05T20:57:26.25136007Z"}
{"log":"\u0009at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n","stream":"stdout","time":"2016-07-05T20:57:26.251373122Z"}
{"log":"\u0009at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n","stream":"stdout","time":"2016-07-05T20:57:26.251386727Z"}
{"log":"\u0009at java.lang.Thread.run(Thread.java:745)\n","stream":"stdout","time":"2016-07-05T20:57:26.251399962Z"}
{"log":"Caused by: [.monitoring-data-2/-4ZQISQpRQu5B58rxnkYgg][[.monitoring-data-2][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.251413566Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:229)\n","stream":"stdout","time":"2016-07-05T20:57:26.251446542Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$0(StoreRecovery.java:78)\n","stream":"stdout","time":"2016-07-05T20:57:26.251462264Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)\n","stream":"stdout","time":"2016-07-05T20:57:26.251474711Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:76)\n","stream":"stdout","time":"2016-07-05T20:57:26.251487195Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1119)\n","stream":"stdout","time":"2016-07-05T20:57:26.251499122Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$0(IndexShard.java:1365)\n","stream":"stdout","time":"2016-07-05T20:57:26.25151102Z"}
{"log":"\u0009... 4 more\n","stream":"stdout","time":"2016-07-05T20:57:26.251523225Z"}
{"log":"Caused by: [.monitoring-data-2/-4ZQISQpRQu5B58rxnkYgg][[.monitoring-data-2][0]] EngineCreationFailureException[failed to create engine]; nested: EOFException;\n","stream":"stdout","time":"2016-07-05T20:57:26.251535417Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.\u003cinit\u003e(InternalEngine.java:139)\n","stream":"stdout","time":"2016-07-05T20:57:26.251548104Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n","stream":"stdout","time":"2016-07-05T20:57:26.251560111Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1430)\n","stream":"stdout","time":"2016-07-05T20:57:26.251572432Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1414)\n","stream":"stdout","time":"2016-07-05T20:57:26.251584408Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:928)\n","stream":"stdout","time":"2016-07-05T20:57:26.251596442Z"}
{"log":"\u0009at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:890)\n","stream":"stdout","time":"2016-07-05T20:57:26.251608984Z"}
{"log":"\u0009at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:225)\n","stream":"stdout","time":"2016-07-05T20:57:26.251621031Z"}
{"log":"\u0009... 9 more\n","stream":"stdout","time":"2016-07-05T20:57:26.251633349Z"}
{"log":"Caused by: java.io.EOFException\n","stream":"stdout","time":"2016-07-05T20:57:26.251644844Z"}
{"log":"\u0009at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)\n","stream":"stdout","time":"2016-07-05T20:57:26.251656861Z"}
{"log":"\u0009at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)\n","stream":"stdout","time":"2016-07-05T20:57:26.251668845Z"}
{"log":"\u0009at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)\n","stream":"stdout","time":"2016-07-05T20:57:26.251680411Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Checkpoint.\u003cinit\u003e(Checkpoint.java:52)\n","stream":"stdout","time":"2016-07-05T20:57:26.251692468Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:81)\n","stream":"stdout","time":"2016-07-05T20:57:26.251704403Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:238)\n","stream":"stdout","time":"2016-07-05T20:57:26.251719643Z"}
{"log":"\u0009at org.elasticsearch.index.translog.Translog.\u003cinit\u003e(Translog.java:180)\n","stream":"stdout","time":"2016-07-05T20:57:26.251731953Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:229)\n","stream":"stdout","time":"2016-07-05T20:57:26.251744128Z"}
{"log":"\u0009at org.elasticsearch.index.engine.InternalEngine.\u003cinit\u003e(InternalEngine.java:136)\n","stream":"stdout","time":"2016-07-05T20:57:26.25175681Z"}
{"log":"\u0009... 15 more\n","stream":"stdout","time":"2016-07-05T20:57:26.251779075Z"}
```
</description><key id="163956959">19275</key><summary>Monitoring plugin indices stuck in failed recovery status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">krystalcode</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2016-07-05T22:03:28Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-07-11T13:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-05T22:13:24Z" id="230618570">&gt; Official docker image (https://hub.docker.com/_/elasticsearch/, Debian Jessie) running on a Fedora 23 host.

There is not an official Docker image that is affiliated with Elastic. To be clear, the "official" Docker image on Docker Hub is not affiliated with Elastic.
</comment><comment author="clintongormley" created="2016-07-06T11:39:53Z" id="230748424">It looks like your monitoring index is corrupt for some reason.  What I don't understand is why this keeps trying to recover. It should try 5 times then stop, given that https://github.com/elastic/elasticsearch/pull/18467 is also in alpha3.

/cc @s1monw 
</comment><comment author="clintongormley" created="2016-07-06T11:45:46Z" id="230749411">Was this a fresh install?  The monitoring index was created the first time you started Elasticsearch? Was there a disk full event?
</comment><comment author="clintongormley" created="2016-07-06T11:46:29Z" id="230749563">Also, i don't understand why the log message says `Recovery failed from null into {Flying Tiger}`.  Why `null`?
</comment><comment author="clintongormley" created="2016-07-06T11:52:23Z" id="230750580">@krystalcode Any chance we can have to logs from the first startup through shutdown and the second startup to the point where you start seeing this message?
</comment><comment author="clintongormley" created="2016-07-06T12:12:11Z" id="230754069">@krystalcode I've just failed to replicate this without Docker.  Any chance you could try without Docker to see if it still happens?
</comment><comment author="krystalcode" created="2016-07-07T00:18:38Z" id="230945462">@clintongormley yes this was a fresh install (or better a fresh launch of docker image), with elastic search data volume mounted on a clean directory. The monitoring index must have been created when I first started elastic search since it certainly did not exist before, and Kibana did not report any problems with the monitoring plugin the first time - so it must have been successfully created. I will look for more logs tomorrow, even though I don't think there is more information apart from 1+ gigabyte of the log provided, repeated. I will also try to recreate the situation with and without docker and report back.
</comment><comment author="krystalcode" created="2016-07-07T00:25:23Z" id="230946421">Also, no there was no disk full event, and I do not know what "from null" should normally be, so I can't interpret it.
</comment><comment author="s1monw" created="2016-07-07T09:39:10Z" id="231031294">@clintongormley the null is not important, it just means the `source node` is null since it's a store recovery. we can improve that
</comment><comment author="clintongormley" created="2016-07-07T13:11:50Z" id="231073009">@krystalcode so far we have been unable to reproduce this.  Any chance you can check again starting from a clean docker instance and see if you can reliably reproduce?
</comment><comment author="krystalcode" created="2016-07-11T04:14:31Z" id="231638729">I tried to reproduce this 2 more times, but I haven't been able to either. Unfortunately I haven't kept the logs from the 2 times I had this failure, because they were larger than 2 gigabytes and I deleted them.

I think we can assume that there was a disk failure that caused data corruption. The fact that it happened consistently on the monitoring plugin indexes, which is why I raised this issue, might be because the monitoring indexes had more data compared to other indexes on them so it was more likely that they would get corrupted.

Is there any need to discuss the fact that elastic search failed to stop reallocating the shard (as pointed out https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230753966)? If not, or if this discussion belongs to https://github.com/elastic/elasticsearch/issues/19164, we can close this issue.
</comment><comment author="clintongormley" created="2016-07-11T13:48:23Z" id="231739187">thanks for the feedback @krystalcode 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_id is not accessible by script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19274</link><project id="" key="" /><description>Docs show the `_id` field being accessed by a script:

```
GET my_index/_search
{
  "script_fields": {
    "UID": {
      "script": "doc['_id']" 
    }
  }
}
```

https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-id-field.html

But this does not seem to be possible.   Try replacing _id with _uid, and see the difference.

**Elasticsearch version**:
2.3
</description><key id="163935314">19274</key><summary>_id is not accessible by script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>docs</label></labels><created>2016-07-05T20:10:59Z</created><updated>2016-07-06T11:26:47Z</updated><resolved>2016-07-06T11:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow allocation and replication of closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19273</link><project id="" key="" /><description>Being able to close indices is a great way to reduce memory usage in an ES cluster with lots of indices.  Unfortunately, it makes cluster expansion less automatic -- closed indices don't want to relocate when new nodes are added to the cluster.

It would be great it one could instruct ES to allow relocation of closed indices, too.

Similarly, imagine you have an index with 1 replica.  Imagine this index is closed.  Imagine that the node where there the replica was located dies.  Ooops, at this point this index doesn't have a replica.  I don't know if that will turn the index/cluster yellow or ES won't react to that, but it would be great it one could tell ES to create a replica on another node to make up for the lost replica.
</description><key id="163906599">19273</key><summary>Allow allocation and replication of closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels /><created>2016-07-05T17:44:02Z</created><updated>2016-07-05T18:07:15Z</updated><resolved>2016-07-05T18:07:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-05T18:07:15Z" id="230556152">Duplicates #12963
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Die with dignity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19272</link><project id="" key="" /><description>Today when a thread encounters a fatal unrecoverable error that
threatens the stability of the JVM, Elasticsearch marches on. This
includes out of memory errors, stack overflow errors and other errors
that leave the JVM in a questionable state. Instead, the Elasticsearch
JVM should die when these errors are encountered. This commit causes
this to be the case.

Relates #19231
</description><key id="163898513">19272</key><summary>Die with dignity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>breaking</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T17:00:26Z</created><updated>2016-07-08T19:01:50Z</updated><resolved>2016-07-07T18:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-05T18:25:59Z" id="230561324">+1
</comment><comment author="tlrx" created="2016-07-05T20:17:31Z" id="230590245">LGTM, I like it
</comment><comment author="tlrx" created="2016-07-05T20:18:20Z" id="230590460">We might want to document the exit codes somewhere too
</comment><comment author="jasontedor" created="2016-07-05T21:22:27Z" id="230607121">&gt; We might want to document the exit codes somewhere too

@tlrx I think that's a good idea, but I'm unsure of a good place to document it. Do you or @clintongormley have any ideas?
</comment><comment author="dadoonet" created="2016-07-05T22:08:11Z" id="230617529">FYI we documented something similar for plugins: https://www.elastic.co/guide/en/elasticsearch/plugins/current/_other_command_line_parameters.html
</comment><comment author="clintongormley" created="2016-07-06T12:16:59Z" id="230754899">@jasontedor not perfect, but perhaps a section under [Setup](https://www.elastic.co/guide/en/elasticsearch/reference/master/setup.html)?
</comment><comment author="jasontedor" created="2016-07-07T16:03:38Z" id="231125860">@tlrx @clintongormley I pushed docs in 74c1708c9e3aac2badf0f8ddb886235d19fc8321; could you take a look?
</comment><comment author="tlrx" created="2016-07-07T16:05:33Z" id="231126424">LGTM
</comment><comment author="clintongormley" created="2016-07-08T15:41:39Z" id="231394206">Sorry I'm late to this.  You can also get the PID from the node info API:

```
curl -XGET "http://192.168.1.10:9200/_nodes/process"

{
  "_nodes": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "cluster_name": "elasticsearch",
  "nodes": {
    "uOPHTfzUQUyGRuxdSUBniQ": {
      "name": "Karl Lykos",
      "transport_address": "127.0.0.1:9300",
      "host": "127.0.0.1",
      "ip": "127.0.0.1",
      "version": "5.0.0-alpha4",
      "build_hash": "3f5b994",
      "http_address": "127.0.0.1:9200",
      "roles": [
        "master",
        "data",
        "ingest"
      ],
      "process": {
        "refresh_interval_in_millis": 1000,
        "id": 26283,  ##### HERE #####
        "mlockall": false
      }
    }
  }
}
```
</comment><comment author="jasontedor" created="2016-07-08T19:01:50Z" id="231445280">@clintongormley Yes, and the cat nodes API too. The concern I have there is that that will list all the nodes in the cluster when only the local node is needed and that leads to more effort on the end-user? I'm happy to add these though if you think otherwise. 😄
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/ElasticsearchUncaughtExceptionHandler.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/ElasticsearchUncaughtExceptionHandlerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Die with dignity</comment></comments></commit></commits></item><item><title>Fix stored_fields message 'is not longer supported'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19271</link><project id="" key="" /><description>Fix grammar in error message.  I see it in the Kibana console as this;

`Error: [illegal_argument_exception] The parameter [fields] is not longer supported, please use [stored_fields] to retrieve stored fields or _source filtering if the field is not stored`

`is not longer supported` should be either `is not supported` or `is no longer supported`.  This PR changes it to `is no longer supported`.
</description><key id="163865221">19271</key><summary>Fix stored_fields message 'is not longer supported'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeeDr</reporter><labels><label>:Exceptions</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T14:31:47Z</created><updated>2016-07-29T11:49:54Z</updated><resolved>2016-07-05T14:56:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-05T14:33:12Z" id="230495673">LGTM, thanks for fixing it
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file></files><comments><comment>Merge pull request #19271 from LeeDr/fixStoredFieldsMessage</comment></comments></commit></commits></item><item><title>Fix Refresh API reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19270</link><project id="" key="" /><description>The original sentence did not make sense, opening a PR instead of committing directly to check if "an immediate refresh" is along the lines of what was meant here.
</description><key id="163847570">19270</key><summary>Fix Refresh API reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T13:13:39Z</created><updated>2016-07-05T16:37:28Z</updated><resolved>2016-07-05T16:37:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cwurm" created="2016-07-05T16:27:34Z" id="230529545">@nik9000  I agree, mine wasn't capturing your intent either. How about the current version?
</comment><comment author="nik9000" created="2016-07-05T16:28:17Z" id="230529734">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchSecurityException.java</file><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRunnable.java</file><file>core/src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/TaskOperationFailure.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/NodeExplanation.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/TransportClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BackoffPolicy.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestHandler.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/Retry.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/AutoCreateIndex.java</file><file>core/src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>core/src/main/java/org/elasticsearch/action/support/DelegatingActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>core/src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/PlainListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportActions.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/BootstrapCheck.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/cli/MultiCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/SettingCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/Terminal.java</file><file>core/src/main/java/org/elasticsearch/cli/UserException.java</file><file>core/src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskConfig.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java</file><file>core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateFilter.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/DelayedAllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/BasePrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/MacAddressProvider.java</file><file>core/src/main/java/org/elasticsearch/common/NamedRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/Randomness.java</file><file>core/src/main/java/org/elasticsearch/common/UUIDGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReferenceStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/CompositeBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ReleasablePagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/component/LifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java</file><file>core/src/main/java/org/elasticsearch/common/compress/DeflateCompressor.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/ShapesAvailability.java</file><file>core/src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/hash/MurmurHash3.java</file><file>core/src/main/java/org/elasticsearch/common/inject/Scope.java</file><file>core/src/main/java/org/elasticsearch/common/inject/State.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java</file><file>core/src/main/java/org/elasticsearch/common/io/Channels.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>core/src/main/java/org/elasticsearch/common/lease/Releasables.java</file><file>core/src/main/java/org/elasticsearch/common/logging/ESLogger.java</file><file>core/src/main/java/org/elasticsearch/common/logging/LoggerMessageFormat.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTokenStream.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/ElasticsearchDirectoryReader.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/netty/KeepFrameDecoder.java</file><file>core/src/main/java/org/elasticsearch/common/network/Cidrs.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/text/Text.java</file><file>core/src/main/java/org/elasticsearch/common/text/UTF8SortedAsUnicodeComparator.java</file><file>core/src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/LocalTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/PortsRange.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/util/ByteArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java</file><file>core/src/main/java/org/elasticsearch/common/util/DoubleArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java</file><file>core/src/main/java/org/elasticsearch/common/util/FloatArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/IntArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/LongArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/AbstractRunnable.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/SuspendableRefContainer.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadContext.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>core/src/main/java/org/elasticsearch/discovery/AckClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files /><comments><comment>Reword Refresh API reference (#19270)</comment></comments></commit></commits></item><item><title>_version does not uniquely identify a particular version of a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19269</link><project id="" key="" /><description>@aphyr recently discovered this resilience issue [https://github.com/crate/crate/issues/3711] while running the jespen test suite against Crate.
After I created an integration test (based on current ES master) [https://github.com/crate/elasticsearch/commit/41ed5ebe7304710fda4de4e69479e17081042c38] out of the relevant jepsen code using your nice network partition simulation helper, I was able to reproduce this error not only using Crate but also using plain Elasticsearch.

I've reproduced this issue on ES 2.3, 5.0-alpha3 &amp; master.
The longer the test is running the more often it will fail, with current default runtime of 180sec it fails almost always on my machine. (the relevant jepsen test is running 360sec)

Currently I've no real idea why this is happening, my guess is that some reads are reading a stale version value but I did not yet figured out how/why.
I've also run this scenario on a single node with one shard because my first guess was that this is maybe not network partition related but this test never failed..

I've read the current ES resilience issues and I couldn't see anything which could be related to this issue, but I'm also not completely sure.
</description><key id="163847172">19269</key><summary>_version does not uniquely identify a particular version of a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels><label>:CRUD</label><label>resiliency</label></labels><created>2016-07-05T13:11:35Z</created><updated>2016-07-07T17:09:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-05T19:52:21Z" id="230583845">@seut @aphyr the issue you're observing is due to dirty reads (which can happen in all current ES versions). ES does not provide any stronger read guarantees at the moment. I've quickly hacked an integration test to illustrate why dirty reads are at play here ( https://gist.github.com/ywelsch/8a5334cd59d922f5c48074fec578e71c).

Note that we have made some major improvements in ES v5.0.0 to ensure that all replicas have the same data once the cluster is healed and that we don't lose acknowledged writes. We have also ported the published Jepsen scenarios to our testing infrastructure (successfully passing). To verify that we are properly modeling the original Jepsen tests, we are spending some effort as well to update the original tests so that they compile against current ES versions. While we’re constantly improving the resiliency of the system we are also spending some effort on documenting the above read/write guarantees and illustrating them with test cases under simulated conditions (see section “Documentation of guarantees” in our resiliency docs: https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html).
</comment><comment author="aphyr" created="2016-07-06T17:02:16Z" id="230837715">&gt; We have also ported the published Jepsen scenarios to our testing infrastructure (successfully passing).

If your Jepsen tests are passing, you... may want to revisit them. One of the original Jepsen tests was for linearizable operations, and since ES is clearly _not_ linearizable, your version of the tests probably shouldn't pass.
</comment><comment author="ywelsch" created="2016-07-07T17:09:52Z" id="231144229">@aphyr right, I should have been more specific. The challenge we're solving first is not to lose acknowledged writes. We only ported the parts of the Jepsen tests that account for this aspect. I've opened a PR to update the docs to that effect (#19303).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested query fails when document has more than one nested field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19268</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.2

**JVM version**: 1.7.0_71

**OS version**: OSX 10.9.5

**Description of the problem including expected versus actual behavior**:

I have an index mapping where documents have two nested fields. I'm able to filter by the first nested field, but not the second.

**Steps to reproduce**:

First create an index whose mapping defines two nested fields

``` json
PUT nesting_test/
{
    "mappings": {
        "test": {
            "properties": {
                "a": {
                    "type": "nested",
                    "properties": {
                        "value": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                },
                "b": {
                    "type": "nested",
                    "properties": {
                        "value": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            }
        }
    }
}
```

Index a document

``` json
POST nesting_test/test/1
{
    "a": [
        {"value": "hello"},
        {"value": "world"}
    ],
    "b": [
        {"value": "what"},
        {"value": "is up"}
    ]
}
```

Filter on the first nested field, `a` - this should succeed (return one hit)

``` json
POST nesting_test/test/_search
{
    "filter" : {
        "nested": {
            "path": "a",
            "filter": {"term": {
                "value": "hello"
            }}
        }
    }
}
```

Filter on the second nested field, `b` - this should fail (return no hits, when it should return one)

``` json
POST nesting_test/test/_search
{
    "filter" : {
        "nested": {
            "path": "b",
            "filter": {"term": {
                "value": "what"
            }}
        }
    }
}
```
</description><key id="163837382">19268</key><summary>Nested query fails when document has more than one nested field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pineapplemachine</reporter><labels /><created>2016-07-05T12:19:45Z</created><updated>2016-07-05T12:29:12Z</updated><resolved>2016-07-05T12:26:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-05T12:26:03Z" id="230463869">This is a problem with mappings in 1.x, which is fixed in 2.x.  The `value` field in the second query is being expanded to the first field ending with `value`, ie `a.value`.  If you specify `b.value` then the query works.
</comment><comment author="pineapplemachine" created="2016-07-05T12:29:11Z" id="230464471">That works. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updates in bulk api always update shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19267</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_45

**OS version**: OS X 10.11.5

**Description of the problem including expected versus actual behavior**:
Bulk api with updates respond _shards.successful &gt; 0 even if new _source not different from the old _source.

**Steps to reproduce**:

```
nano request
{ "update" : { "_index" : "test", "_type" : "test", "_id" : "1" } }
{ "doc" : {"data" : "some_data"}, "doc_as_upsert" : true }

# returning _shards.successful = 1
curl -XPOST 'localhost:9200/_bulk' --data-binary "@requests";

# sending request again. This should return _shards.successful = 0 but instead returns _shards.successful = 1
curl -XPOST 'localhost:9200/_bulk' --data-binary "@requests";
```

Here is example with _update api

```
# returns _shards.successful = 1
curl -XPOST localhost:9200/test/test/1/_update?pretty -d '{ "doc" : {"data" : "some_data"}, "doc_as_upsert" : true }'

# sending request again. Returns _shards.successful = 0
curl -XPOST localhost:9200/test/test/1/_update?pretty -d '{ "doc" : {"data" : "some_data"}, "doc_as_upsert" : true }'
```
</description><key id="163831094">19267</key><summary>Updates in bulk api always update shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gkozyryatskyy</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-07-05T11:40:10Z</created><updated>2016-07-26T15:16:19Z</updated><resolved>2016-07-26T15:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-08T09:15:45Z" id="231313483">The `update` API should change to return number of successful shards.
</comment><comment author="gkozyryatskyy" created="2016-07-08T12:16:50Z" id="231344940">@clintongormley 
There was a noop (data was the same), that is why shards was not updated and _shards.successful = 0.
But the point is, that `bulk` api with `update` request in it, should detect noop too and return _shards.successful = 0 in the second call too. That is why `bulk` API with `update` in it, should be changed and return _shards.successful = 0 after update with same `_source`. Am I correct?
</comment><comment author="clintongormley" created="2016-07-08T16:32:15Z" id="231407466">@gkozyryatskyy sure - you could argue it either way. I think it is more meaningful to say that the request was processed correctly for this shard, even though it is a noop
</comment><comment author="gkozyryatskyy" created="2016-07-08T17:40:20Z" id="231424226">@clintongormley 
If it will return _shards.successful = 1for the same `_source`(request was processed correctly for this shard, even though it is a noop), it will be impossible to understand whether the data was reindexed (i.e does `_source` in `update` request changes the existed `_source`?). Exactly this functionality I need in `bulk` + `update` request. =)
</comment><comment author="clintongormley" created="2016-07-11T13:30:12Z" id="231734366">@gkozyryatskyy a better way of doing this would be in this PR https://github.com/elastic/elasticsearch/pull/9736 which was closed only because of a lack of response. We'd still accept this feature if somebody wanted to pick up where this PR left off.
</comment><comment author="gkozyryatskyy" created="2016-07-11T13:49:44Z" id="231739571">@clintongormley 
PR #9736 seems really a better way. It will be great if you can reopen/merge this feature.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/DocWriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/AsyncBulkByScrollActionTests.java</file></files><comments><comment>Add _operation field to index, update, delete responses</comment></comments></commit></commits></item><item><title>Add otto-de/flummi client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19266</link><project id="" key="" /><description>Added flummi to list of community-contributed clients. Flummi is a new Java HTTP/REST client for elastic search.
</description><key id="163825212">19266</key><summary>Add otto-de/flummi client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BastianVoigt</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-07-05T11:03:55Z</created><updated>2016-07-06T12:32:20Z</updated><resolved>2016-07-06T12:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-05T11:21:36Z" id="230452460">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="BastianVoigt" created="2016-07-06T07:13:35Z" id="230696010">Do I really need to sign it just for adding a link to your Wiki page? 
</comment><comment author="BastianVoigt" created="2016-07-06T07:14:01Z" id="230696092">Or could you just add it for me? 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add otto-de/flummi client to plugins</comment></comments></commit></commits></item><item><title>FVH doesn't highlight nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19265</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.2

**JVM version**: 1.8.0_25

**OS version**: OS X 10.11.5

**Description of the problem including expected versus actual behavior**:  
Fast Vector Highlighter doesn't highlight nested fields, whereas Plain Highlighter does (but has a [fragment_size bug](https://github.com/elastic/elasticsearch/issues/9442) so I can't use it). I would expect FVH to be able to highlight everything.

**Steps to reproduce**:

``` sh
curl -XPUT 'localhost:9200/nested_fvh?pretty' -d '{
  "mappings": {
    "type1": {
      "properties": {
        "nested1": {
          "type": "nested",
          "properties": {
            "field1": { 
              "type": "string", 
              "term_vector" : "with_positions_offsets" 
            }
          }
        }
      }
    }
  }
}
'

curl -XPUT 'localhost:9200/nested_fvh/type1/1?pretty' -d '{
  "nested1": {
    "field1": "Hello World!"
  }
}
'


curl -XGET 'http://localhost:9200/nested_fvh/type1/_search?pretty' -d '{
  "query": {
    "nested": {
      "path": "nested1",
      "query": {
        "match": {
          "nested1.field1": "hello"
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "nested1.field1": { 
        "type": "fvh"
      }
    }
  }
}
'
```

**Output (fvh)**:
&lt;details&gt;
&lt;summary&gt;

FVH doesn't highlight nested&lt;/summary&gt;



``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.4231198,
    "hits" : [ {
      "_index" : "nested_fvh",
      "_type" : "type1",
      "_id" : "1",
      "_score" : 0.4231198,
      "_source" : {
        "nested1" : {
          "field1" : "Hello World!"
        }
      }
    } ]
  }
}
```

&lt;/details&gt;

**Output (plain)**:
&lt;details&gt;
&lt;summary&gt;

Plain does highlight nested&lt;/summary&gt;



``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.4231198,
    "hits" : [ {
      "_index" : "nested_fvh",
      "_type" : "type1",
      "_id" : "1",
      "_score" : 0.4231198,
      "_source" : {
        "nested1" : {
          "field1" : "Hello World!"
        }
      },
      "highlight" : {
        "nested1.field1" : [ "&lt;em&gt;Hello&lt;/em&gt; World!" ]
      }
    } ]
  }
}
```

&lt;/details&gt;
</description><key id="163824047">19265</key><summary>FVH doesn't highlight nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">sfcgeorge</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-07-05T10:56:48Z</created><updated>2016-07-13T06:36:04Z</updated><resolved>2016-07-13T06:36:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-05T11:20:24Z" id="230452261">(Oooh `&lt;details&gt;` tags! 👍 )

This should have been fixed with https://issues.apache.org/jira/browse/LUCENE-5929 but apparently it is still not working, neither in 2.3 nor in master.

@martijnvg could you take a look please?
</comment><comment author="gmoskovicz" created="2016-07-06T15:26:37Z" id="230807409">@clintongormley @martijnvg i have a similar case. Looks like when position offset is set, the highlighting doesn't work as expected. I have a full repro case that can be used in SENSE:

```
DELETE test
PUT test
{
  "mappings": {
    "test_type": {
      "properties": {
        "nested_field": {
          "type": "nested",
          "properties": {
            "text": {
              "type": "string",
              "term_vector": "with_positions_offsets",
              "fields": {
                "raw": {
                  "type": "string",
                  "term_vector": "with_positions_offsets"
                }
              }
            }
          }
        }
      }
    }
  }
} 

POST test/test_type
{
  "nested_field": [
    {
      "text": "text field"
    }
  ]
}

POST test/_search
{
  "query": {
    "nested": {
      "query": {
        "query_string": {
          "query": "text",
          "fields": [
            "nested_field.text.raw"
          ]
        }
      },
      "path": "nested_field"
    }
  },
  "highlight": {
    "fields": {
      "nested_field.text.raw": {}
    }
  },
  "fielddata_fields": ["nested_field.text.raw"]
}
```

Can you please confirm that it's related to this bug?

Just to clarify, it's not related to multi_field. If you just use the text property you will get the same behaviour.

Indeed, if you remove `"term_vector": "with_positions_offsets"` then it starts working. Must be related to this.
</comment><comment author="clintongormley" created="2016-07-06T15:42:17Z" id="230812362">&gt; Indeed, if you remove "term_vector": "with_positions_offsets" then it starts working. Must be related to this.

@gmoskovicz that's because it uses the plain highlighter if term offsets are disabled.
</comment><comment author="martijnvg" created="2016-07-07T07:02:40Z" id="230999255">@clintongormley @sfcgeorge @gmoskovicz I wonder if we should support highlighting on `nested`, `has_child` and `has_parent` query at all? This isn't the first problem that the highlighters have with these queries. I think instead we should promote the use of inner hits more. With inner hits highlighting the nested object work and on top of this it is more accurate too (since each nested object will be highlighted in isolation):

``` json
{
  "query": {
    "nested": {
      "path": "nested1",
      "query": {
        "match": {
          "nested1.field1": "hello"
        }
      },
      "inner_hits": {
        "highlight": {
          "fields": {
            "nested1.field1": {
              "type": "fvh"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="sfcgeorge" created="2016-07-07T09:06:18Z" id="231023799">@martijnvg That feels like making the programmer do something the system should be doing. I get that the implementation is hard because Lucene etc etc but ideally I'd like nested fields to be more seamless and easy to use, not require even more effort.

Using the global highlights as I currently am makes mapping the match back to the original object in my ORM very difficult. Using `inner_hits` would make that much easier BUT it doesn't work with searching `_all`. I need to be able to search `_all` so can't use `inner_hits` unfortunately. It's not ideal, but I'm stuck with global highlight, no `fragment_size` and `plain` highlighter for now.
</comment><comment author="martijnvg" created="2016-07-07T09:20:42Z" id="231027172">@sfcgeorge I see, if you just query the `_all` field than using highlighting via `inner_hits` for that isn't very straightforward. However I do think that if fields inside nested objects are queried specifically then using highlighting via inner_hits should be used.
</comment><comment author="sfcgeorge" created="2016-07-07T10:05:12Z" id="231036993">@martijnvg We need a Google-like search box that searches and highlights `_all`, but also advanced search on specific fields and nested fields, at the same time. If `_all` could percolate highlighting to `inner_hits` that would be perfect and much easier, but I don't think it can work. 

&lt;details&gt;
&lt;summary&gt;

My failed attempts:&lt;/summary&gt;


Both return correct result but no highlighting. 

Here I guess the `match_all` is stopping the highlighting.

``` sh
curl -XGET 'http://localhost:9200/nested_fvh/type1/_search?pretty' -d '{
  "query": {
    "bool": {
      "must": [
        {
          "simple_query_string": {
            "query": "hello"
          }
        },
        {
          "nested": {
            "path": "nested1",
            "filter": {
              "match_all": {}
            },
            "inner_hits": {
              "highlight": {
                "require_field_match": false,
                "fields": {
                  "nested1.field1": {
                    "type": "plain"
                  }
                }
              }
            }
          }
        }
      ]
    }
  }
}
'
```

Global inner hits sounded promising, this seems like it would be a great use-case for it if it worked. 

``` sh
curl -XGET 'http://localhost:9200/nested_fvh/type1/_search?pretty' -d '{
  "query": {
    "simple_query_string": {
      "query": "hello"
    }
  },
  "inner_hits": {
    "inner_hits_name1": {
      "path": { 
        "nested1": {
          "highlight": {
            "require_field_match": false,
            "fields": {
              "nested1.field1": {
                "type": "plain"
              }
            }
          }
        }
      }
    }
  }
}
'
```

&lt;/details&gt;
</comment><comment author="clintongormley" created="2016-07-07T10:32:20Z" id="231042296">The FVH requires positions and offsets to be indexed, but highlighting is performed at the top level document and the positions and offsets for nested documents are inside the nested documents, so highlighting can't access them.

Secondly, highlighting nested documents at the top level will produce incorrect results, eg:

&lt;details&gt;

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "text": {
              "type": "text"
            },
            "num": {
              "type": "integer"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": [
    {
      "text": "brown",
      "num": 1
    },
    {
      "text": "cow",
      "num": 2
    }
  ]
}

GET t/_search
{
  "query": {
    "nested": {
      "path": "foo",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "foo.text": "brown cow"
              }
            },
            {
              "match": {
                "foo.num": 1
              }
            }
          ]
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "foo.text": {}
    }
  }
}
```

&lt;/details&gt;

returns highlight snippets `brown` and `cow`,  while `cow` shouldn't have been highlighted.  Highlighting with inner hits works correctly:

&lt;details&gt;

```
GET t/_search
{
  "query": {
    "nested": {
      "path": "foo",
      "inner_hits": {
        "_source": false,
        "highlight": {
          "fields": {
            "foo.text": {}
          }
        }
      },
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "foo.text": "brown cow"
              }
            },
            {
              "match": {
                "foo.num": 1
              }
            }
          ]
        }
      }
    }
  }
}
```

&lt;/details&gt;

That said, if you want to be able to use the FVH on nested fields at the top level (with the incorrect results), then you should be able to use `copy_to` to copy the nested values into a top-level field and highlight on that:

&lt;details&gt;

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "text": {
              "type": "text",
              "copy_to": "foo_text"
            }
          }
        },
        "foo_text": {
          "type": "text",
          "term_vector": "with_positions_offsets",
          "store": true
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": [
    {
      "text": "brown"
    },
    {
      "text": "cow"
    }
  ]
}

GET t/_search
{
  "query": {
    "nested": {
      "path": "foo",
      "query": {
        "match": {
          "foo.text": "brown cow"
        }
      }
    }
  },
  "highlight": {
    "require_field_match": false,
    "fields": {
      "foo_text": {
        "type": "fvh"
      }
    }
  }
}
```

&lt;/details&gt;

Unfortunately, this doesn't work for some reason.  It works with the `plain` highlighter but not with `fvh`.  This IS a bug and @martijnvg is going to investigate.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java</file></files><comments><comment>fvh: Also extract terms from the nested query' inner query.</comment></comments></commit></commits></item><item><title>Add `scaled_float`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19264</link><project id="" key="" /><description>This is a tentative to revive #15939 motivated by elastic/beats#1941.
Half-floats are a pretty bad option for storing percentages. They would likely
require 2 bytes all the time while percentages really don't need more than one
byte.

So this PR exposes a new `scaled_float` type that requires a `scaling_factor`
and internally indexes `value*scaling_factor` in a long field. Compared to the
original PR it exposes a lower-level API so that the trade-offs are clearer and
avoids any reference to fixed precision that might imply that this type is more
accurate (actually it is _less_ accurate).

In addition to being more space-efficient for some use-cases that beats is
interested in, this is also faster that `half_float` unless we can improve the
efficiency of decoding half-float bits (which is currently done using software)
or until Java gets first-class support for half-floats.
</description><key id="163810222">19264</key><summary>Add `scaled_float`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T09:40:18Z</created><updated>2016-07-20T13:03:28Z</updated><resolved>2016-07-18T11:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-05T13:18:09Z" id="230475330">To give more detailed information about why half floats are not enough, here is a table that gives disk usage for storing 10M random floats between 0 and 1 depending on the mapping:

| Mapping | Points disk usage (kB) | Doc values disk usage (kB) | Total |
| --- | --- | --- | --- |
| float | 49728 | 34180 | 83908 |
| half float | 26560 | 19532 | 46092 |
| scaled float (factor=4000) | 25744 | 14652 | 40396 |
| scaled float (factor=100) | 13044 | 9768 | 22812 |

I chose `4000` and `100` as scaling factors because `4000` means 0.025% accuracy, which is better than what a half float can do for this particular use case (floats between 0 and 1) yet requires less disk, and `100` because I suspect it would be enough for many metrics like cpu utilization with its 1% accuracy.

Of course this is not a good benchmark since this is fake data, but given how points and doc values work this simulates the worst case and real data could expect even better disk utilization.
</comment><comment author="jpountz" created="2016-07-12T16:37:11Z" id="232103139">Updated numbers with https://issues.apache.org/jira/browse/LUCENE-7371: 

| Mapping | Points disk usage (kB) | Doc values disk usage (kB) | Total |
| --- | --- | --- | --- |
| float | 40312 | 34180 | 74492 |
| half float | 23092 | 19532 | 42624 |
| scaled float (factor=4000) | 22792 | 14652 | 37444 |
| scaled float (factor=100) | 12984 | 9768 | 22752 |
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ScaledFloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/ScaledFloatFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/ScaledFloatFieldTypeTests.java</file></files><comments><comment>Add `scaled_float`. #19264</comment></comments></commit></commits></item><item><title>GeoDistanceIT goes OOM </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19263</link><project id="" key="" /><description>with this see:
 `gradle :core:integTest -Dtests.seed=9D639ADBB7706439 -Dtests.class=org.elasticsearch.search.sort.GeoDistanceIT -Dtests.method="testDuelOptimizations" -Dtests.security.manager=true -Dtests.locale=el -Dtests.timezone=CET`

I run into:

```
==&gt; Test Info: seed=9D639ADBB7706439; jvm=1; suite=1
Suite: org.elasticsearch.search.sort.GeoDistanceIT
  1&gt; [2016-07-05 10:38:56,227][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78, reason=Function not implemented
  1&gt; [2016-07-05 10:38:56,229][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
  1&gt; [2016-07-05 10:38:59,333][INFO ][org.elasticsearch.search.sort] [GeoDistanceIT#testDuelOptimizations]: setup test
  1&gt; [2016-07-05 10:38:59,404][INFO ][org.elasticsearch.test   ] Setup InternalTestCluster [SUITE-CHILD_VM=[0]-CLUSTER_SEED=[5060446888734553576]-HASH=[1046378D01417]-cluster] with seed [463A51A247C0F5E8] using [0] dedicated masters, [3] (data) nodes and [0] coord only nodes
  1&gt; [2016-07-05 10:39:00,853][INFO ][org.elasticsearch.node   ] [node_s0] version[5.0.0-alpha4-SNAPSHOT], pid[27139], build[Unknown/Unknown], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_66/25.66-b17]
  1&gt; [2016-07-05 10:39:00,853][INFO ][org.elasticsearch.node   ] [node_s0] initializing ...
  1&gt; [2016-07-05 10:39:00,894][INFO ][org.elasticsearch.plugins] [node_s0] modules [], plugins [org.elasticsearch.test.InternalSettingsPlugin, org.elasticsearch.test.ESIntegTestCase$TestSeedPlugin]
  1&gt; [2016-07-05 10:39:05,899][INFO ][org.elasticsearch.env    ] [node_s0] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [100.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
  1&gt; [2016-07-05 10:39:05,899][INFO ][org.elasticsearch.env    ] [node_s0] heap size [491mb], compressed ordinary object pointers [true]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:09, stalled for 10.2s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:39:11,629][INFO ][org.elasticsearch.node   ] [node_s0] initialized
  1&gt; [2016-07-05 10:39:11,631][INFO ][org.elasticsearch.node   ] [node_s0] starting ...
  1&gt; [2016-07-05 10:39:11,637][INFO ][org.elasticsearch.transport] [node_s0] publish_address {local[1]}, bound_addresses {local[1]}
  1&gt; [2016-07-05 10:39:11,683][INFO ][org.elasticsearch.cluster.service] [node_s0] new_master {node_s0}{HeZ3nsHmTeGJV-xBA8Jnqw}{0z-S-nOMTZCUCGx8mo96tw}{local}{local[1]}, reason: local-disco-initial_connect(master)
  1&gt; [2016-07-05 10:39:11,693][INFO ][org.elasticsearch.node   ] [node_s0] started
  1&gt; [2016-07-05 10:39:11,702][INFO ][org.elasticsearch.node   ] [node_s1] version[5.0.0-alpha4-SNAPSHOT], pid[27139], build[Unknown/Unknown], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_66/25.66-b17]
  1&gt; [2016-07-05 10:39:11,702][INFO ][org.elasticsearch.node   ] [node_s1] initializing ...
  1&gt; [2016-07-05 10:39:11,702][INFO ][org.elasticsearch.plugins] [node_s1] modules [], plugins [org.elasticsearch.test.InternalSettingsPlugin, org.elasticsearch.test.ESIntegTestCase$TestSeedPlugin]
  1&gt; [2016-07-05 10:39:11,721][INFO ][org.elasticsearch.env    ] [node_s1] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [100.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
  1&gt; [2016-07-05 10:39:11,721][INFO ][org.elasticsearch.env    ] [node_s1] heap size [491mb], compressed ordinary object pointers [true]
  1&gt; [2016-07-05 10:39:11,818][INFO ][org.elasticsearch.gateway] [node_s0] recovered [0] indices into cluster_state
  1&gt; [2016-07-05 10:39:11,984][INFO ][org.elasticsearch.node   ] [node_s1] initialized
  1&gt; [2016-07-05 10:39:11,985][INFO ][org.elasticsearch.node   ] [node_s1] starting ...
  1&gt; [2016-07-05 10:39:11,991][INFO ][org.elasticsearch.transport] [node_s1] publish_address {local[2]}, bound_addresses {local[2]}
  1&gt; [2016-07-05 10:39:11,994][INFO ][org.elasticsearch.cluster.service] [node_s0] added {{node_s1}{BAEudZyYTtKvHdV3mCitGA}{A5mMsb-jTqS5Ks27ZBM2Ng}{local}{local[2]},}, reason: local-disco-receive(from node[{node_s1}{BAEudZyYTtKvHdV3mCitGA}{A5mMsb-jTqS5Ks27ZBM2Ng}{local}{local[2]}])
  1&gt; [2016-07-05 10:39:12,006][INFO ][org.elasticsearch.cluster.service] [node_s1] detected_master {node_s0}{HeZ3nsHmTeGJV-xBA8Jnqw}{0z-S-nOMTZCUCGx8mo96tw}{local}{local[1]}, added {{node_s0}{HeZ3nsHmTeGJV-xBA8Jnqw}{0z-S-nOMTZCUCGx8mo96tw}{local}{local[1]},}, reason: local-disco-receive(from master)
  1&gt; [2016-07-05 10:39:12,016][INFO ][org.elasticsearch.node   ] [node_s1] started
  1&gt; [2016-07-05 10:39:12,031][INFO ][org.elasticsearch.node   ] [node_s2] version[5.0.0-alpha4-SNAPSHOT], pid[27139], build[Unknown/Unknown], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_66/25.66-b17]
  1&gt; [2016-07-05 10:39:12,031][INFO ][org.elasticsearch.node   ] [node_s2] initializing ...
  1&gt; [2016-07-05 10:39:12,032][INFO ][org.elasticsearch.plugins] [node_s2] modules [], plugins [org.elasticsearch.test.InternalSettingsPlugin, org.elasticsearch.test.ESIntegTestCase$TestSeedPlugin]
  1&gt; [2016-07-05 10:39:12,045][INFO ][org.elasticsearch.env    ] [node_s2] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [100.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
  1&gt; [2016-07-05 10:39:12,045][INFO ][org.elasticsearch.env    ] [node_s2] heap size [491mb], compressed ordinary object pointers [true]
  1&gt; [2016-07-05 10:39:12,110][INFO ][org.elasticsearch.node   ] [node_s2] initialized
  1&gt; [2016-07-05 10:39:12,110][INFO ][org.elasticsearch.node   ] [node_s2] starting ...
  1&gt; [2016-07-05 10:39:12,111][INFO ][org.elasticsearch.transport] [node_s2] publish_address {local[3]}, bound_addresses {local[3]}
  1&gt; [2016-07-05 10:39:12,113][INFO ][org.elasticsearch.cluster.service] [node_s0] added {{node_s2}{kEqhOhmdRSSF_ZMEtNcjiA}{IkTxKRhtQ8KTk-9Cyr3GPw}{local}{local[3]},}, reason: local-disco-receive(from node[{node_s2}{kEqhOhmdRSSF_ZMEtNcjiA}{IkTxKRhtQ8KTk-9Cyr3GPw}{local}{local[3]}])
  1&gt; [2016-07-05 10:39:12,136][INFO ][org.elasticsearch.cluster.service] [node_s1] added {{node_s2}{kEqhOhmdRSSF_ZMEtNcjiA}{IkTxKRhtQ8KTk-9Cyr3GPw}{local}{local[3]},}, reason: local-disco-receive(from master)
  1&gt; [2016-07-05 10:39:12,138][INFO ][org.elasticsearch.cluster.service] [node_s2] detected_master {node_s0}{HeZ3nsHmTeGJV-xBA8Jnqw}{0z-S-nOMTZCUCGx8mo96tw}{local}{local[1]}, added {{node_s0}{HeZ3nsHmTeGJV-xBA8Jnqw}{0z-S-nOMTZCUCGx8mo96tw}{local}{local[1]},{node_s1}{BAEudZyYTtKvHdV3mCitGA}{A5mMsb-jTqS5Ks27ZBM2Ng}{local}{local[2]},}, reason: local-disco-receive(from master)
  1&gt; [2016-07-05 10:39:12,145][INFO ][org.elasticsearch.node   ] [node_s2] started
  1&gt; [2016-07-05 10:39:12,198][INFO ][org.elasticsearch.search.sort] test using _default_ mappings: [{"_default_":{}}]
  1&gt; [2016-07-05 10:39:12,449][INFO ][org.elasticsearch.search.sort] [GeoDistanceIT#testDuelOptimizations]: starting test
  1&gt; [2016-07-05 10:39:12,555][INFO ][org.elasticsearch.cluster.metadata] [node_s0] [index] creating index, cause [api], templates [random_index_template], shards [7]/[1], mappings [_default_, type]
  1&gt; [2016-07-05 10:39:12,758][INFO ][org.elasticsearch.plugins] [transport_client_node_s1] modules [], plugins []
  1&gt; [2016-07-05 10:39:12,791][INFO ][org.elasticsearch.transport] [transport_client_node_s1] publish_address {local[4]}, bound_addresses {local[4]}
  1&gt; [2016-07-05 10:39:12,804][INFO ][org.elasticsearch.plugins] [transport_client_node_s0] modules [], plugins []
  1&gt; [2016-07-05 10:39:12,823][INFO ][org.elasticsearch.transport] [transport_client_node_s0] publish_address {local[5]}, bound_addresses {local[5]}
  1&gt; [2016-07-05 10:39:12,885][INFO ][org.elasticsearch.plugins] [transport_client_node_s2] modules [], plugins []
  1&gt; [2016-07-05 10:39:12,907][INFO ][org.elasticsearch.transport] [transport_client_node_s2] publish_address {local[6]}, bound_addresses {local[6]}
  1&gt; [2016-07-05 10:39:13,030][INFO ][org.elasticsearch.cluster.routing.allocation] [node_s0] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[index][1], [index][2], [index][5], [index][4], [index][2], [index][5]] ...]).
  1&gt; [2016-07-05 10:39:13,264][INFO ][org.elasticsearch.cluster.routing.allocation] [node_s0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[index][5], [index][2], [index][2], [index][5]] ...]).
  1&gt; [2016-07-05 10:39:13,645][INFO ][org.elasticsearch.search.sort] Index [10195] docs async: [false] bulk: [true] partitions [11]
  1&gt; [2016-07-05 10:39:13,753][INFO ][org.elasticsearch.cluster.metadata] [node_s0] [index/mJJm22qWT_S83ojJ_oIxUQ] create_mapping [RANDOM_BOGUS_TYPE______]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:19, stalled for 20.2s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:39:25,297][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=ARC, distance=1797.0km, origin=(-59.52571451701207, -97.76032857491252)
  1&gt; [2016-07-05 10:39:25,645][INFO ][org.elasticsearch.search.sort] 121 hits
  1&gt; [2016-07-05 10:39:25,645][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=SLOPPY_ARC, distance=1797.0km, origin=(-59.52571451701207, -97.76032857491252)
  1&gt; [2016-07-05 10:39:25,679][INFO ][org.elasticsearch.search.sort] 121 hits
  1&gt; [2016-07-05 10:39:25,679][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=ARC, distance=8402.0km, origin=(-30.631741764098656, -172.2723431422603)
  1&gt; [2016-07-05 10:39:26,265][INFO ][org.elasticsearch.search.sort] 1921 hits
  1&gt; [2016-07-05 10:39:26,265][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=SLOPPY_ARC, distance=8402.0km, origin=(-30.631741764098656, -172.2723431422603)
  1&gt; [2016-07-05 10:39:26,527][INFO ][org.elasticsearch.search.sort] 1921 hits
  1&gt; [2016-07-05 10:39:26,528][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=ARC, distance=1363.0km, origin=(-69.94611459615871, -151.37323663888245)
  1&gt; [2016-07-05 10:39:26,584][INFO ][org.elasticsearch.search.sort] 103 hits
  1&gt; [2016-07-05 10:39:26,584][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=SLOPPY_ARC, distance=1363.0km, origin=(-69.94611459615871, -151.37323663888245)
  1&gt; [2016-07-05 10:39:26,616][INFO ][org.elasticsearch.search.sort] 103 hits
  1&gt; [2016-07-05 10:39:26,616][INFO ][org.elasticsearch.search.sort] Now testing GeoDistance=ARC, distance=1317.0km, origin=(-86.05583303929738, 112.60762123704916)
  1&gt; [2016-07-05 10:39:28,774][INFO ][org.elasticsearch.monitor.jvm] [node_s0] [gc][17] overhead, spent [335ms] collecting in the last [1s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:29, stalled for 30.2s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:39:30,074][INFO ][org.elasticsearch.monitor.jvm] [node_s1] [gc][18] overhead, spent [316ms] collecting in the last [1s]
  1&gt; [2016-07-05 10:39:30,173][INFO ][org.elasticsearch.monitor.jvm] [node_s2] [gc][18] overhead, spent [316ms] collecting in the last [1s]
  1&gt; [2016-07-05 10:39:34,018][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][19] overhead, spent [3.7s] collecting in the last [3.9s]
  1&gt; [2016-07-05 10:39:34,018][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][19] overhead, spent [3.7s] collecting in the last [3.8s]
  1&gt; [2016-07-05 10:39:34,037][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][19] overhead, spent [3.8s] collecting in the last [4.2s]
  1&gt; [2016-07-05 10:39:36,673][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][20] overhead, spent [2.2s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:39:36,673][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][20] overhead, spent [2.2s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:39:36,673][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][20] overhead, spent [2.2s] collecting in the last [2.6s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:39, stalled for 40.2s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:49, stalled for 50.2s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:39:59, stalled for 60.2s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:09, stalled for 70.3s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:19, stalled for 80.3s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:29, stalled for 90.3s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:39, stalled for  100s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:49, stalled for  110s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:40:59, stalled for  120s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:41:09, stalled for  130s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:41:19, stalled for  140s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:41:29, stalled for  150s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:41:39, stalled for  160s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:41:47,363][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][21] overhead, spent [1.9m] collecting in the last [9.2s]
  1&gt; [2016-07-05 10:41:47,365][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][21] overhead, spent [48.2s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:41:47,376][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][21] overhead, spent [48.2s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:41:47,428][WARN ][org.elasticsearch.transport] [transport_client_node_s0] Received response for a request that has timed out, sent [96692ms] ago, timed out [41ms] ago, action [cluster:monitor/nodes/liveness], node [{#transport#-1}{65cGS8JbRICTfQiJ2J0NXg}{local}{local[1]}], id [63]
  1&gt; [2016-07-05 10:41:47,429][INFO ][org.elasticsearch.client.transport] [transport_client_node_s0] failed to get node info for {#transport#-1}{65cGS8JbRICTfQiJ2J0NXg}{local}{local[1]}, disconnecting...
  1&gt; ReceiveTimeoutTransportException[[][local[1]][cluster:monitor/nodes/liveness] request_id [63] timed out after [96651ms]]
  1&gt;    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:820)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:391)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-07-05 10:41:47,429][INFO ][org.elasticsearch.client.transport] [transport_client_node_s1] failed to get node info for {#transport#-1}{yzWaILXVRrK2bShnxGu_gQ}{local}{local[2]}, disconnecting...
  1&gt; ReceiveTimeoutTransportException[[][local[2]][cluster:monitor/nodes/liveness] request_id [65] timed out after [83184ms]]
  1&gt;    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:820)
  1&gt;    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:391)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-07-05 10:41:47,428][WARN ][org.elasticsearch.transport] [transport_client_node_s1] Received response for a request that has timed out, sent [91531ms] ago, timed out [8347ms] ago, action [cluster:monitor/nodes/liveness], node [{#transport#-1}{yzWaILXVRrK2bShnxGu_gQ}{local}{local[2]}], id [65]
  2&gt; Ιουλ 05, 2016 10:41:47 ΠΜ com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException
  2&gt; WARNING: Uncaught exception in thread: Thread[elasticsearch[node_s1][search][T#2],5,TGRP-GeoDistanceIT]
  2&gt; java.lang.OutOfMemoryError: GC overhead limit exceeded
  2&gt;    at __randomizedtesting.SeedInfo.seed([9D639ADBB7706439]:0)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:98)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:71)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:71)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:71)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:71)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:71)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.relateAndRecurse(GeoPointNumericTermsEnum.java:101)
  2&gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointNumericTermsEnum.computeRange(GeoPointNumericTermsEnum.java:70)
  1&gt; [2016-07-05 10:41:48,371][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][22] overhead, spent [1.1m] collecting in the last [2.1m]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:41:49, stalled for  170s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:41:48,382][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][22] overhead, spent [1.1m] collecting in the last [2.1m]
  1&gt; [2016-07-05 10:41:49,711][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][23] overhead, spent [690ms] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:41:49,711][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][23] overhead, spent [690ms] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:41:49,712][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][23] overhead, spent [690ms] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:41:50,806][INFO ][org.elasticsearch.monitor.jvm] [node_s1] [gc][24] overhead, spent [341ms] collecting in the last [1s]
  1&gt; [2016-07-05 10:41:50,806][INFO ][org.elasticsearch.monitor.jvm] [node_s0] [gc][24] overhead, spent [341ms] collecting in the last [1s]
  1&gt; [2016-07-05 10:41:50,806][INFO ][org.elasticsearch.monitor.jvm] [node_s2] [gc][24] overhead, spent [341ms] collecting in the last [1s]
  1&gt; [2016-07-05 10:41:58,032][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][27] overhead, spent [5s] collecting in the last [5.2s]
  1&gt; [2016-07-05 10:41:58,033][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][27] overhead, spent [5s] collecting in the last [5.2s]
  1&gt; [2016-07-05 10:41:58,032][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][27] overhead, spent [5s] collecting in the last [5.2s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:00, stalled for  182s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:42:00,687][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][28] overhead, spent [2.3s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:42:00,687][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][28] overhead, spent [2.3s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:42:00,687][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][28] overhead, spent [2.3s] collecting in the last [2.6s]
  1&gt; [2016-07-05 10:42:02,523][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][29] overhead, spent [1.6s] collecting in the last [1.8s]
  1&gt; [2016-07-05 10:42:02,523][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][29] overhead, spent [1.6s] collecting in the last [1.8s]
  1&gt; [2016-07-05 10:42:02,523][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][29] overhead, spent [1.6s] collecting in the last [1.8s]
  1&gt; [2016-07-05 10:42:03,911][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][30] overhead, spent [1.1s] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:42:03,911][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][30] overhead, spent [1.1s] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:42:03,912][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][30] overhead, spent [1.1s] collecting in the last [1.3s]
  1&gt; [2016-07-05 10:42:05,680][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][31] overhead, spent [1.3s] collecting in the last [1.7s]
  1&gt; [2016-07-05 10:42:05,680][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][31] overhead, spent [1.3s] collecting in the last [1.7s]
  1&gt; [2016-07-05 10:42:05,681][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][31] overhead, spent [1.3s] collecting in the last [1.7s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:10, stalled for  192s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:42:12,722][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][33] overhead, spent [4.9s] collecting in the last [5.9s]
  1&gt; [2016-07-05 10:42:12,722][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][33] overhead, spent [4.9s] collecting in the last [5.9s]
  1&gt; [2016-07-05 10:42:12,722][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][33] overhead, spent [4.9s] collecting in the last [5.9s]
  1&gt; [2016-07-05 10:42:15,004][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][34] overhead, spent [1.7s] collecting in the last [2.2s]
  1&gt; [2016-07-05 10:42:15,004][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][34] overhead, spent [1.7s] collecting in the last [2.2s]
  1&gt; [2016-07-05 10:42:15,004][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][34] overhead, spent [1.7s] collecting in the last [2.2s]
  1&gt; [2016-07-05 10:42:16,731][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][35] overhead, spent [1.3s] collecting in the last [1.7s]
  1&gt; [2016-07-05 10:42:16,731][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][35] overhead, spent [1.3s] collecting in the last [1.7s]
  1&gt; [2016-07-05 10:42:16,731][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][35] overhead, spent [1.3s] collecting in the last [1.7s]
  1&gt; [2016-07-05 10:42:19,183][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][36] overhead, spent [2.2s] collecting in the last [2.4s]
  1&gt; [2016-07-05 10:42:19,183][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][36] overhead, spent [2.2s] collecting in the last [2.4s]
  1&gt; [2016-07-05 10:42:19,184][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][36] overhead, spent [2.2s] collecting in the last [2.4s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:20, stalled for  202s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:42:22,605][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][38] overhead, spent [2.1s] collecting in the last [2.4s]
  1&gt; [2016-07-05 10:42:22,605][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][38] overhead, spent [2.1s] collecting in the last [2.4s]
  1&gt; [2016-07-05 10:42:22,605][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][38] overhead, spent [2.1s] collecting in the last [2.4s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:30, stalled for  212s at: GeoDistanceIT.testDuelOptimizations
  1&gt; [2016-07-05 10:42:32,959][INFO ][org.elasticsearch.monitor.jvm] [node_s2] [gc][old][41][64] duration [7.4s], collections [1]/[8.3s], total [7.4s]/[2.5m], memory [249.4mb]-&gt;[301.3mb]/[491mb], all_pools {[young] [5mb]-&gt;[189.6kb]/[57.5mb]}{[survivor] [56.5mb]-&gt;[0b]/[56.5mb]}{[old] [187.8mb]-&gt;[301.1mb]/[341.5mb]}
  1&gt; [2016-07-05 10:42:32,959][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][41] overhead, spent [7.7s] collecting in the last [8.3s]
  1&gt; [2016-07-05 10:42:32,960][INFO ][org.elasticsearch.monitor.jvm] [node_s1] [gc][old][41][64] duration [7.4s], collections [1]/[8.3s], total [7.4s]/[2.5m], memory [249.4mb]-&gt;[301.3mb]/[491mb], all_pools {[young] [5mb]-&gt;[225kb]/[57.5mb]}{[survivor] [56.5mb]-&gt;[0b]/[56.5mb]}{[old] [187.8mb]-&gt;[301.1mb]/[341.5mb]}
  1&gt; [2016-07-05 10:42:32,960][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][41] overhead, spent [7.7s] collecting in the last [8.3s]
  1&gt; [2016-07-05 10:42:32,960][INFO ][org.elasticsearch.monitor.jvm] [node_s0] [gc][old][41][64] duration [7.4s], collections [1]/[8.3s], total [7.4s]/[2.5m], memory [249.4mb]-&gt;[301.6mb]/[491mb], all_pools {[young] [5mb]-&gt;[464.7kb]/[57.5mb]}{[survivor] [56.5mb]-&gt;[0b]/[56.5mb]}{[old] [187.8mb]-&gt;[301.1mb]/[341.5mb]}
  1&gt; [2016-07-05 10:42:32,960][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][41] overhead, spent [7.7s] collecting in the last [8.3s]
  1&gt; [2016-07-05 10:42:36,837][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][42] overhead, spent [3.1s] collecting in the last [3.8s]
  1&gt; [2016-07-05 10:42:36,837][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][42] overhead, spent [3.1s] collecting in the last [3.8s]
  1&gt; [2016-07-05 10:42:36,837][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][42] overhead, spent [3.1s] collecting in the last [3.8s]
  1&gt; [2016-07-05 10:42:38,746][WARN ][org.elasticsearch.monitor.jvm] [node_s2] [gc][43] overhead, spent [1.5s] collecting in the last [1.9s]
  1&gt; [2016-07-05 10:42:38,746][WARN ][org.elasticsearch.monitor.jvm] [node_s0] [gc][43] overhead, spent [1.5s] collecting in the last [1.9s]
  1&gt; [2016-07-05 10:42:38,746][WARN ][org.elasticsearch.monitor.jvm] [node_s1] [gc][43] overhead, spent [1.5s] collecting in the last [1.9s]
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:40, stalled for  222s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:42:50, stalled for  232s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:00, stalled for  242s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:10, stalled for  252s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:20, stalled for  262s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:30, stalled for  272s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:40, stalled for  282s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:43:50, stalled for  292s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:00, stalled for  302s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:10, stalled for  312s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:21, stalled for  322s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:31, stalled for  332s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:41, stalled for  342s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:44:51, stalled for  352s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:01, stalled for  362s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:11, stalled for  372s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:21, stalled for  382s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:31, stalled for  392s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:41, stalled for  402s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:45:51, stalled for  412s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:01, stalled for  422s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:11, stalled for  432s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:21, stalled for  442s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:31, stalled for  452s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:41, stalled for  462s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:46:51, stalled for  472s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:01, stalled for  482s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:11, stalled for  492s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:21, stalled for  502s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:31, stalled for  512s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:41, stalled for  522s at: GeoDistanceIT.testDuelOptimizations
HEARTBEAT J0 PID(27139@panthor.local): 2016-07-05T10:47:51, stalled for  532s at: GeoDistanceIT.testDuelOptimizations
```

sometimes it passes but mainly it blows up 
</description><key id="163801950">19263</key><summary>GeoDistanceIT goes OOM </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Geo</label><label>test</label></labels><created>2016-07-05T08:56:32Z</created><updated>2016-07-10T00:12:08Z</updated><resolved>2016-07-10T00:12:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-05T08:58:40Z" id="230423823">@nknize can you take a look
</comment><comment author="s1monw" created="2016-07-05T15:33:01Z" id="230513956">here is a ci failure:

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1426/console
</comment><comment author="nknize" created="2016-07-08T18:59:35Z" id="231444733">Its testing with a deprecated encoding. I muted it for now and will clean up the test.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/sort/GeoDistanceIT.java</file></files><comments><comment>Use GeoDistanceIT.testDuelOptimizations for bwc testing only</comment></comments></commit></commits></item><item><title>Remove fielddata circuit breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19262</link><project id="" key="" /><description>This was added at a time when fielddata was the norm. As of 5.0, fielddata is only supported on text fields and is disabled by default. So there is no way to go out of memory because of fielddata with default mappings.
</description><key id="163793689">19262</key><summary>Remove fielddata circuit breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>discuss</label></labels><created>2016-07-05T08:08:35Z</created><updated>2016-07-08T09:24:02Z</updated><resolved>2016-07-08T09:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-05T08:42:20Z" id="230420169">Fielddata may still be enabled on text fields for eg significant terms.  Is there a significant cost to keeping the circuit breaker?
</comment><comment author="dakrone" created="2016-07-05T16:36:53Z" id="230532033">&gt; So there is no way to go out of memory because of fielddata with default mappings.

This is only accurate for 5.0-only data, if someone upgrades a 2.3 cluster they may still be using field data. That said, I have to agree with Clint, someone may still enable fielddata, so I don't think it should be removed?
</comment><comment author="jpountz" created="2016-07-05T20:38:43Z" id="230595739">The cost is not clear, I was just annoyed that the circuit-breaking service leaks into the MappedFieldType API and was thinking that we could remove it now that fielddata is not a first-class feature anymore.
</comment><comment author="jpountz" created="2016-07-08T09:24:02Z" id="231315119">Discussed in FixitFriday: fielddata on analyzed strings are the reason why we added the circuit breaker so we cannot remove it unless we have a replacement.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename from `ingest-useragent` plugin to `ingest-user-agent` and its processor from `useragent` to `user_agent`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19261</link><project id="" key="" /><description>and on some other places did similar renaming. This is consistent with ES naming. Also made sure that the plugin's docs are navigable from the plugin docs.
</description><key id="163790150">19261</key><summary>Rename from `ingest-useragent` plugin to `ingest-user-agent` and its processor from `useragent` to `user_agent`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T07:46:06Z</created><updated>2016-07-07T12:51:33Z</updated><resolved>2016-07-07T12:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-05T09:36:19Z" id="230432224">LGTM! Thanks @martijnvg. 
</comment><comment author="jasontedor" created="2016-07-05T12:11:35Z" id="230461142">@martijnvg I just remembered that there are [two](https://github.com/elastic/elasticsearch/blob/768beea6c7d904e12ba91cb2abfb5cf2cc5cea05/qa/vagrant/src/test/resources/packaging/scripts/module_and_plugin_test_cases.bash#L231-L233) [places](https://github.com/elastic/elasticsearch/blob/768beea6c7d904e12ba91cb2abfb5cf2cc5cea05/qa/vagrant/src/test/resources/packaging/scripts/module_and_plugin_test_cases.bash#L364-L366) that are going to require changes in the Vagrant tests when we test [installing](https://github.com/elastic/elasticsearch/blob/768beea6c7d904e12ba91cb2abfb5cf2cc5cea05/qa/vagrant/src/test/resources/packaging/scripts/module_and_plugin_test_cases.bash#L231-L233) the plugin and when we test [removing](https://github.com/elastic/elasticsearch/blob/768beea6c7d904e12ba91cb2abfb5cf2cc5cea05/qa/vagrant/src/test/resources/packaging/scripts/module_and_plugin_test_cases.bash#L364-L366) the plugin.
</comment><comment author="martijnvg" created="2016-07-07T07:45:54Z" id="231006902">Thanks @jasontedor I'd would have totally missed this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Is there any more detailed elasticsearch dsl query document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19260</link><project id="" key="" /><description> I read elasticsearch official document, but the dsl query chapter content is little.Is there any more detailed document content about how to construct dsl query and really understand dsl structure?
</description><key id="163783777">19260</key><summary>Is there any more detailed elasticsearch dsl query document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justdoit0823</reporter><labels /><created>2016-07-05T07:01:10Z</created><updated>2016-07-06T12:28:30Z</updated><resolved>2016-07-05T07:20:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-05T07:20:57Z" id="230404583">Please ask questions on discuss.elastic.co.

You can read:
- https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html
</comment><comment author="justdoit0823" created="2016-07-05T07:25:43Z" id="230405388">But the above document link describe little information and is not systematic.
</comment><comment author="nik9000" created="2016-07-05T12:56:23Z" id="230470229">&gt; is not systematic.

Look in the table of contents to the right - it has entries like "Full text queries" and "Geo queries". It is all there.
</comment><comment author="justdoit0823" created="2016-07-06T02:31:13Z" id="230656541">@nik9000 , I have already read that content which you mentioned. There are some simple query example and no more structural content and rules about the whole dsl query.
</comment><comment author="clintongormley" created="2016-07-06T12:28:14Z" id="230757060">@justdoit0823 the definitive guide takes you through a long and detailed explanation of the query dsl, while the reference gives you details about each query 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate sampler and missing aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19259</link><project id="" key="" /><description>This is another step down the path to removing aggregation's special
"streams" which reimplement NamedWriteable.
</description><key id="163762932">19259</key><summary>Migrate sampler and missing aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T02:53:05Z</created><updated>2016-07-07T20:46:53Z</updated><resolved>2016-07-07T20:46:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-05T12:47:41Z" id="230468337">@colings86 another one!
</comment><comment author="colings86" created="2016-07-06T08:55:30Z" id="230715927">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove BucketStreams</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19258</link><project id="" key="" /><description>It isn't used.
</description><key id="163761056">19258</key><summary>Remove BucketStreams</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-05T02:27:54Z</created><updated>2016-07-06T11:09:13Z</updated><resolved>2016-07-05T13:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-05T02:28:21Z" id="230375603">@colings86 another one!
</comment><comment author="colings86" created="2016-07-05T07:36:31Z" id="230407243">LGTM. Didn't realise this isn't used, not sure when that changed
</comment><comment author="nik9000" created="2016-07-05T12:48:42Z" id="230468540">Yeah! It took me a bit to figure out that it wasn't used because we register everything with it so carefully.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GregorianCalendar no longer accessible in ES 2.3?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19257</link><project id="" key="" /><description>In ES 2.0 I had a script (in a separate file called via script_file) which would utilize GregorianCalendar as follows:
`
endDate = new GregorianCalendar();
`

After upgrading to 2.3, I get the following error at that line:

&gt; unable to resolve class GregorianCalendar 

If I try to import with the following, I get an error at the import line.
`
import java.util.GregorianCalendar;
`

&gt; unable to resolve class java.util.GregorianCalendar
</description><key id="163756364">19257</key><summary>GregorianCalendar no longer accessible in ES 2.3?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">twigbranch</reporter><labels /><created>2016-07-05T01:15:22Z</created><updated>2016-07-05T11:04:41Z</updated><resolved>2016-07-05T11:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-05T01:24:04Z" id="230369988">Could you be more precise about what you're doing? I'm supposing this is in a script but it sure would be nice if you gave a simple reproduction (commands that set up and reproduce the situation from a clean install yet presumably worked in an earlier version that you didn't specify). 
</comment><comment author="twigbranch" created="2016-07-05T02:03:43Z" id="230373340">Updated with a bit more detail. 
</comment><comment author="jasontedor" created="2016-07-05T02:08:26Z" id="230373764">It's best to not edit in-place after replies have occurred, it makes the thread confusing. It would still be nice to have a minimal reproduction. 
</comment><comment author="twigbranch" created="2016-07-05T02:19:38Z" id="230374811">After looking around I found this:
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-security.html

I added the following in a security policy file and the script works again:
`
grant {
    permission org.elasticsearch.script.ClassPermission "java.util.GregorianCalendar";
    permission org.elasticsearch.script.ClassPermission "groovy.time.TimeCategory";
};
`

It would be nice if these were already whitelisted.

Would be nice also if the upgrade guide listing the breaking api changes (https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-2.3.html#_groovy_dependencies) included the above link.
</comment><comment author="clintongormley" created="2016-07-05T11:04:41Z" id="230449637">This is already listed in the breaking changes here: https://www.elastic.co/guide/en/elasticsearch/reference/current/float.html#_scripting_and_security
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw translog corrupted exception on malformed op</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19256</link><project id="" key="" /><description>Today when reading a malformed operation from the translog, we throw an
assertion error that is immediately caught and wrapped into a translog
corrupted exception. This commit replaces this by electing to directly
throw a translog corrupted exception instead.

Additionally, this cleanup also addressed a double-wrapped translog
corrupted exception. Namely, verifying the checksum can throw a translog
corrupted exception which the existing code would catch and wrap again
in a translog corrupted exception.
</description><key id="163744063">19256</key><summary>Throw translog corrupted exception on malformed op</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>:Translog</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T21:23:03Z</created><updated>2016-07-05T11:01:40Z</updated><resolved>2016-07-04T23:22:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-04T21:27:18Z" id="230355932">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file></files><comments><comment>Throw translog corrupted exception on malformed op</comment></comments></commit></commits></item><item><title>Improve logging for batched cluster state updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19255</link><project id="" key="" /><description>We've been slowly improving batch support in `ClusterService` so service won't need to implement this tricky logic themselves. These good changes are blessed but our logging infra didn't catch up and we now log things like:

```
[2016-07-04 21:51:22,318][DEBUG][cluster.service          ] [node_sm0] processing [put-mapping [type1],put-mapping [type1]]:
```

Depending on the `source` string this can get quite ugly (mostly in the ZenDiscovery area). 

This PR adds some infra to improve logging, keeping the non-batched task the same. As result the above line looks like:

```
[2016-07-04 21:44:45,047][DEBUG][cluster.service          ] [node_s0] processing [put-mapping[type0, type0, type0]]: execute
```

ZenDiscovery waiting on join moved from:

```
[2016-07-04 17:09:45,111][DEBUG][cluster.service          ] [node_t0] processing [elected_as_master, [1] nodes joined),elected_as_master, [1] nodes joined)]: execute
```

To

```
[2016-07-04 22:03:30,142][DEBUG][cluster.service          ] [node_t3] processing [elected_as_master ([3] nodes joined)[{node_t2}{R3hu3uoSQee0B6bkuw8pjw}{p9n28HDJQdiDMdh3tjxA5g}{127.0.0.1}{127.0.0.1:30107}, {node_t1}{ynYQfk7uR8qR5wKIysFlQg}{wa_OKuJHSl-Oyl9Gis-GXg}{127.0.0.1}{127.0.0.1:30106}, {node_t0}{pweq-2T4TlKPrEVAVW6bJw}{NPBSLXSTTguT1So0JsZY8g}{127.0.0.1}{127.0.0.1:30105}]]: execute
```

As a bonus, I removed all `zen-disco` prefixes to sources from that area.
</description><key id="163737696">19255</key><summary>Improve logging for batched cluster state updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T20:06:15Z</created><updated>2016-07-05T10:47:11Z</updated><resolved>2016-07-04T20:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-04T20:06:40Z" id="230348620">@jasontedor care to take a look?
</comment><comment author="jasontedor" created="2016-07-04T20:30:46Z" id="230350803">@bleskes It looks good, I left a comment about the change shard state action messages and a small nit. I think that we should keep the `zen-disco` prefix though, it helps when filtering logs.
</comment><comment author="bleskes" created="2016-07-04T20:40:55Z" id="230351910">@jasontedor thx. zen-disco prefix is back
</comment><comment author="jasontedor" created="2016-07-04T20:50:06Z" id="230352761">&gt; thx. zen-disco prefix is back

Thanks. I'm fine with whatever you decide on the shard state action reasons. Fire at will.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file></files><comments><comment>Improve logging for batched cluster state updates (#19255)</comment></comments></commit></commits></item><item><title>Rename UserError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19254</link><project id="" key="" /><description>The top-level class Throwable represents all errors and exceptions in
Java. This hierarchy is divided into Error and Exception, the former
being serious problems that applications should not try to catch and the
latter representing exceptional conditions that an application might
want to catch and handle. This commit renames
org.elasticsearch.cli.UserError to org.elasticsearch.UserException to
make its name consistent with where it falls in this hierarchy.
</description><key id="163733698">19254</key><summary>Rename UserError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T19:18:03Z</created><updated>2016-07-05T11:01:58Z</updated><resolved>2016-07-04T23:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-04T21:37:00Z" id="230356611">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/cli/MultiCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/SettingCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/UserException.java</file><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java</file><file>core/src/test/java/org/elasticsearch/cli/CommandTests.java</file><file>core/src/test/java/org/elasticsearch/cli/MultiCommandTests.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/plugins/RemovePluginCommandTests.java</file></files><comments><comment>Rename UserError</comment></comments></commit></commits></item><item><title>Create-bwc index tool fails with mapping exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19253</link><project id="" key="" /><description>Trying to create a bwc index for 5.0.0-alpha4  with

```
python3 dev-tools/create_bwc_index.py 5.0.0-alpha4
```

fails with a mapping exception:

```
Traceback (most recent call last):
  File "dev-tools/create_bwc_index.py", line 461, in &lt;module&gt;
    main()
  File "dev-tools/create_bwc_index.py", line 457, in main
    create_bwc_index(cfg, version)
  File "dev-tools/create_bwc_index.py", line 421, in create_bwc_index
    generate_index(client, version, index_name)
  File "dev-tools/create_bwc_index.py", line 300, in generate_index
    'warmers': warmers
  File "/usr/local/lib/python3.5/site-packages/elasticsearch/client/utils.py", line 69, in _wrapped
    return func(*args, params=params, **kwargs)
  File "/usr/local/lib/python3.5/site-packages/elasticsearch/client/indices.py", line 103, in create
    params=params, body=body)
  File "/usr/local/lib/python3.5/site-packages/elasticsearch/transport.py", line 307, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/usr/local/lib/python3.5/site-packages/elasticsearch/connection/http_urllib3.py", line 93, in perform_request
    self._raise_error(response.status, raw_data)
  File "/usr/local/lib/python3.5/site-packages/elasticsearch/connection/base.py", line 105, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.RequestError: TransportError(400, 'mapper_parsing_exception')
```
</description><key id="163698228">19253</key><summary>Create-bwc index tool fails with mapping exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>blocker</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T15:27:09Z</created><updated>2016-07-18T10:23:25Z</updated><resolved>2016-07-18T10:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-06T13:43:29Z" id="230774888">@rjernst could you take a look at this please?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fix create_bwc_index for 5.x (#19469)</comment></comments></commit></commits></item><item><title>Fix potential AssertionError with include/exclude on terms aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19252</link><project id="" key="" /><description>We call `LongBitSet.set(start, end)`, which fails when `start &gt;= length`
(0 in that case).

Closes #18575
</description><key id="163693775">19252</key><summary>Fix potential AssertionError with include/exclude on terms aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T14:59:45Z</created><updated>2016-07-18T09:21:11Z</updated><resolved>2016-07-18T09:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-13T17:18:10Z" id="232424489">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/support/IncludeExcludeTests.java</file></files><comments><comment>Fix potential AssertionError with include/exclude on terms aggregations. #19252</comment></comments></commit></commits></item><item><title>Blank field names should be illegal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19251</link><project id="" key="" /><description>Today you can create a blank field name, eg:

```
PUT t/t/1
{
  "": 5
}
```

Or

```
PUT t/t/1
{
  "": {
    "foo": 5
  }
}
```

Blank field names should be illegal as they are not accessible via paths

Related https://github.com/elastic/kibana/issues/7617
</description><key id="163671863">19251</key><summary>Blank field names should be illegal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-beta1</label></labels><created>2016-07-04T12:58:17Z</created><updated>2016-09-14T14:43:24Z</updated><resolved>2016-08-29T02:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ObjectMapper.java</file></files><comments><comment>Validate blank field name</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ObjectMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/RootObjectMapper.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/BinaryFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/BooleanFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CompletionFieldMapper2xTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/CompletionFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DateFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/GeoShapeFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/IpFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/KeywordFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/NumberFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ObjectMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ScaledFloatFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/TextFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/TokenCountFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorFieldMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java</file><file>plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperTests.java</file></files><comments><comment>Validate blank field name</comment></comments></commit></commits></item><item><title>Bump version to 5.0.0-alpha5.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19250</link><project id="" key="" /><description>This way we can start adding backward-compatibility logic again using `Version.onOrAfter`.
</description><key id="163671714">19250</key><summary>Bump version to 5.0.0-alpha5.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T12:57:23Z</created><updated>2016-07-05T12:37:35Z</updated><resolved>2016-07-05T12:37:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Scroll should limit size= value, as the search API already does.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19249</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.1.2 / 2.3.3

**JVM version**: Java 8u91

**OS version**: Ubuntu 16.04

**Description of the problem including expected versus actual behavior**:
Specifying a high value for `size=` when using scroll causes OOM.

E.g: running `curl -XGET 'localhost:9200/*/_search?size=99999999'` results in:

```
{"error":{"root_cause":[{"type":"query_phase_execution_exception","reason":"Result window is too large, from + size must be less than or equal to: [10000] but was [99999999]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter."}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":".triggered_watches","node":"qmVwvx_9RDqdn6QG8n8SKQ","reason":{"type":"query_phase_execution_exception","reason":"Result window is too large, from + size must be less than or equal to: [10000] but was [99999999]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter."}}]},"status":500}
```

But when running `curl -XGET 'localhost:9200/*/_search?scroll=8m&amp;size=99999999`, the high value for `size=` is accepted, causing the node running OOM.

**Steps to reproduce**:
1. Start ES
2. Run `curl -XGET 'localhost:9200/*/_search?scroll=8m&amp;size=99999999`
3. Node runs OOM

**Provide logs (if relevant)**:

Logs when calling `_search?scroll=8m&amp;size=99999999`

```
[2016-07-04 13:34:08,734][INFO ][monitor.jvm              ] [Doctor Glitternight] [gc][young][1107][34] duration [781ms], collections [1]/[1.4s], total [781ms]/[7.7s], memory [440.3mb]-&gt;[394.6mb]/[990.7mb], all_pools {[young] [172.7mb]-&gt;[1mb]/[266.2mb]}{[survivor] [33.2mb]-&gt;[33.2mb]/[33.2mb]}{[old] [234.3mb]-&gt;[360.3mb]/[691.2mb]}
[2016-07-04 13:36:00,540][WARN ][rest.suppressed          ] /*/_search Params: {pretty=, size=99999999, scroll=8m, index=*}
java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.cache.recycler.PageCacheRecycler$1.newInstance(PageCacheRecycler.java:102)
    at org.elasticsearch.cache.recycler.PageCacheRecycler$1.newInstance(PageCacheRecycler.java:99)
    at org.elasticsearch.common.recycler.DequeRecycler.obtain(DequeRecycler.java:53)
    at org.elasticsearch.common.recycler.AbstractRecycler.obtain(AbstractRecycler.java:33)
    at org.elasticsearch.common.recycler.DequeRecycler.obtain(DequeRecycler.java:28)
    at org.elasticsearch.common.recycler.FilterRecycler.obtain(FilterRecycler.java:39)
    at org.elasticsearch.common.recycler.Recyclers$3.obtain(Recyclers.java:119)
    at org.elasticsearch.common.recycler.FilterRecycler.obtain(FilterRecycler.java:39)
    at org.elasticsearch.cache.recycler.PageCacheRecycler.bytePage(PageCacheRecycler.java:150)
    at org.elasticsearch.common.util.AbstractBigArray.newBytePage(AbstractBigArray.java:108)
    at org.elasticsearch.common.util.BigByteArray.resize(BigByteArray.java:140)
    at org.elasticsearch.common.util.BigArrays.resizeInPlace(BigArrays.java:425)
    at org.elasticsearch.common.util.BigArrays.resize(BigArrays.java:472)
    at org.elasticsearch.common.util.BigArrays.grow(BigArrays.java:489)
    at org.elasticsearch.common.io.stream.BytesStreamOutput.ensureCapacity(BytesStreamOutput.java:160)
    at org.elasticsearch.common.io.stream.BytesStreamOutput.writeBytes(BytesStreamOutput.java:90)
    at org.elasticsearch.common.io.stream.StreamOutput.write(StreamOutput.java:299)
    at com.fasterxml.jackson.core.json.UTF8JsonGenerator._flushBuffer(UTF8JsonGenerator.java:2003)
    at com.fasterxml.jackson.core.json.UTF8JsonGenerator.writeRaw(UTF8JsonGenerator.java:597)
    at com.fasterxml.jackson.core.util.DefaultIndenter.writeIndentation(DefaultIndenter.java:94)
    at com.fasterxml.jackson.core.util.DefaultPrettyPrinter.writeObjectEntrySeparator(DefaultPrettyPrinter.java:307)
    at com.fasterxml.jackson.core.json.UTF8JsonGenerator._writePPFieldName(UTF8JsonGenerator.java:354)
    at com.fasterxml.jackson.core.json.UTF8JsonGenerator.writeFieldName(UTF8JsonGenerator.java:181)
    at com.fasterxml.jackson.core.JsonGenerator.copyCurrentStructure(JsonGenerator.java:1557)
    at com.fasterxml.jackson.core.JsonGenerator.copyCurrentStructure(JsonGenerator.java:1566)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.copyCurrentStructure(JsonXContentGenerator.java:425)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.copyRawValue(JsonXContentGenerator.java:410)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeRawField(JsonXContentGenerator.java:363)
    at org.elasticsearch.common.xcontent.XContentBuilder.rawField(XContentBuilder.java:914)
    at org.elasticsearch.common.xcontent.XContentHelper.writeRawField(XContentHelper.java:378)
    at org.elasticsearch.search.internal.InternalSearchHit.toXContent(InternalSearchHit.java:476)
    at org.elasticsearch.search.internal.InternalSearchHits.toXContent(InternalSearchHits.java:184)
```

**Describe the feature**:
</description><key id="163658960">19249</key><summary>Scroll should limit size= value, as the search API already does.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jakommo</reporter><labels><label>:Scroll</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T11:37:49Z</created><updated>2016-07-11T20:10:10Z</updated><resolved>2016-07-11T20:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-06T13:44:01Z" id="230775062">@nik9000 could you take a look at this please?
</comment><comment author="nik9000" created="2016-07-06T13:50:57Z" id="230777138">Sure. Same window size do you think?
</comment><comment author="clintongormley" created="2016-07-06T13:51:27Z" id="230777276">I think so.  Obviously the message will need to be different :D 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file></files><comments><comment>Limit batch size when scrolling</comment></comments></commit></commits></item><item><title>Indexing during primary relocation with ongoing replica recoveries can lead to documents not being properly replicated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19248</link><project id="" key="" /><description>Primary relocation violates two invariants that ensure proper interaction between document replication and peer recoveries, ultimately leading to documents not being properly replicated. As this is quite a tricky issue to understand, I'll first give a short summary on how document replication and peer recoveries integrate:

1) Peer recoveries are triggered by the recovery target node (the one that wants to recover) but only successfully started on the recovery source node (holding the primary shard) when the source node knows that the recovery target shard exists. This check is realized by looking at the current cluster state on the recovery source node and checking the routing table if a corresponding initializing shard exists on the target node.
2) Once this check successfully passes the source node remembers the current position in the translog and syncs Lucene files to the target node (this is called phase 1). At the end of phase 1, the engine is started on the target recovery shard. From this moment on the shard accepts document writes.
3) In a second phase the source shard takes a snapshot of the translog, containing all writes that have been added since the saved position in the translog while the lucene files were copied to the target shard. The source shard then sends all the operations in the snapshot to the target shard. New operations that happen after the snapshot was taking a replicated to the target shard using the normal replication logic.

The following two invariants are (among others) required for data replication to properly integrate with peer recoveries:

**Invariant 1**: Document writes must be replicated based on the routing table of a cluster state that includes all shards which have ongoing or finished recoveries. This is ensured by the fact that do not start a recovery that is not reflected by the cluster state available on the primary node _and_ we always sample a fresh cluster state before starting to replicate write operations. 

**Invariant 2**: Every operation that is not part of the snapshot taken for phase 2, must be succesfully indexed on the target replica (pending shard level errors which will cause the target shard to be failed). To ensure this, we start replicating to the target shard as soon as the recovery start and open it's engine before we take the snapshot. All operations that are indexed after the snapshot was taken are guaranteed to arrive to the shard when it's ready to index them. Note that this also means that the replication doesn't fail a shard if it's not yet ready to recieve operations - it's a normal part of a recovering shard.

With primary relocations, the two invariants can be possibly violated. To illustrate the issues, let's consider a primary relocating while there is another replica shard recovering from the primary shard.

Invariant 1 can be violated if the target of the primary relocation is so lagging on cluster state processing that it doesn't even know about the new initializing replica. This is very rare in practice as replica recoveries take time to copy all the index files but it is a theoretical gap that surfaces in testing scenarios.

Invariant 2 can be violated even if the target primary knows about the initializing replica. This can happen if the target primary replicates an operation to the intializing shard and that operation arrives to the initializing shard _before_ it opens it's engine but arrives to the primary source _after_ it has taken the snapshot of the translog. Those operations will be currently missed on the new initializing replica.

The obvious easy fix for this will be to forbid any replica recovery while the primary is relocating. However, since primary relocation can take a long time (we do it in the background and throttle it) this will result in a large time window where the cluster will not be able to recover from a potential replica loss (either by a network hickup or a true node loss).

We currently working on a fix using the following two directions:
1) As part of the primary hand off before source and target, the source will make sure the target knows about  all ongoing recoveries. This will make sure invariant 1 can not violated.
2) We will not start the phase2 of recoveries (where the snapshot is taken) after the hand off has taken place (the source shard state is `RELOCATED`). Since we now guarantee that no operations are inflight while the hand off happens ( #15900 ), we know that from the moment operations are routed via the target primary, no new snapshots will be taken which is the premise of violating invariant 2.
</description><key id="163658834">19248</key><summary>Indexing during primary relocation with ongoing replica recoveries can lead to documents not being properly replicated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>resiliency</label></labels><created>2016-07-04T11:36:55Z</created><updated>2016-07-19T12:07:58Z</updated><resolved>2016-07-19T12:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryWaitForClusterStateRequest.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file></files><comments><comment>Fix replica-primary inconsistencies when indexing during primary relocation with ongoing replica recoveries (#19287)</comment></comments></commit></commits></item><item><title>Alias delete and addition silently fails occasionally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19247</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.2
**JVM version**:
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
**OS version**:
Centos 7
**Description of the problem including expected versus actual behavior**:
I was migrating data on our 12 node elastic cluster and created a replica index with a new mapping for every index (79 indices in total). Then I ran a post command to `http://localhost:9200/_aliases` on one of the nodes and posted a list of 158 actions consisting of a remove of the alias pointing at the old index and then an add of the same alias name to the new index.

So something like:

```
Remove: A - &gt; A1
Add: A -&gt; A2
```

For every index.

This appeared to work and the result of GET  on `http://localhost:9200/_aliases` showed the correct mapping that I was hoping to achieve.

There was a serious error however with two of the indices the results of a search gave the results of **both** the old and the new index. Also, attempting to index new documents failed with an error saying that you cannot index documents on an alias with two indices.

As mentioned the GET on `http://localhost:9200/_aliases` yielded the correct result, leaving me to feel that the operations on two of the indices were not carried out to completion.

Further because the system appeared to think that the aliases were like those given in the result of GET on `http://localhost:9200/_aliases` I could not delete the alias to the old index.

In the end, I managed to get the alias to work for the two broken indices by reversing the change that I made, then remaking it. I.e. applying:

```
Remove: A -&gt; A2
Add: A - &gt; A1
```

Then again:

```
Remove: A - &gt; A1
Add: A -&gt; A2
```

And this finally fixed it so that the state shown by GET on `http://localhost:9200/_aliases` matched the observed behaviour.

This is very serious error when doing mass migrations as you can have no trust in the results of the alias changes.

We did not find any errors in the elastic logs when this operation occurred. 

**Steps to reproduce**:
1. Change the aliases in a single command for many (O(100)) indices on a large cluster
2. Attempt to insert a document into every alias and look for errors

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="163648322">19247</key><summary>Alias delete and addition silently fails occasionally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smithsimonj</reporter><labels><label>:Aliases</label><label>:Cluster</label><label>feedback_needed</label></labels><created>2016-07-04T10:29:27Z</created><updated>2017-03-31T13:34:21Z</updated><resolved>2017-03-31T13:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-04T11:28:39Z" id="230269562">@smithsimonj I'll see whether I can reproduce this issue. Can you provide the exact command you ran for updating the aliases on the indices and also show the output of `http://localhost:9200/_aliases` when it was broken?
</comment><comment author="smithsimonj" created="2016-07-04T12:35:17Z" id="230280588">@ywelsch I'm afraid I can't as it comprises a complete list of our clients. I used a script to generate the command and they were all of the form like that given below. The script looks good. The `_aliases` result also looked correct but the behaviour of the system didn't match the `_aliases` result.

One of my colleagues verified that I wasn't going mad and had indeed run the correct commands at the time. He also verified that the elastic cluster was in a funky state.

I'm sorry this is of no help to you, but I am confident from what we saw that there was a bug at play. Let me know how you get on trying to replicate it.

`{
  "actions": [
    {
      "remove": {
        "index": "A-1464640477011",
        "alias": "A"
      }
    },
    {
      "add": {
        "index": "A-1466784086139",
        "alias": "A"
      }
    },
    {
      "remove": {
        "index": "B-1464640477011",
        "alias": "B"
      }
    },
    {
      "add": {
        "index": "B-1466784086139",
        "alias": "B"
      }
    },
  ...
}`
</comment><comment author="smithsimonj" created="2016-07-04T13:08:08Z" id="230286651">So I've anonymised the alias PUT body in this file: [elastic_bug_alias_command.txt](https://github.com/elastic/elasticsearch/files/346317/elastic_bug_alias_command.txt)

The elastic was initially set up so that all the old indices were aliased with the first part of the name (i.e. `aa -&gt; aa-1464640477011`. and the new indices i.e. `aa-1466784086139` did not have any aliases.

After this command the _aliases result looked as expected, but the behaviour of aliases `ah` and `bb` was as though:
- The indices `ah-1464640477011` and `ah-1466784086139` both had the alias `ah`
- The indices `bb-1464640477011` and `bb-1466784086139` both had the alias `bb`
</comment><comment author="ywelsch" created="2016-07-04T14:51:38Z" id="230307428">@smithsimonj I've simulated your scenario multiple times and cannot find an issue. Implementation-wise, nothing points to a bug here. Updating the aliases results in one a single cluster state update (all or nothing). This means that indices are resolved during indexing/search operations either on the old or the new cluster state. I suspect that something was wrong in the `_aliases` update command (maybe using a wrong alias name?). Removing a non-existing alias is a no-op and will be silently ignored. Can you replicate the issue on a test cluster?
</comment><comment author="smithsimonj" created="2016-07-07T08:23:50Z" id="231014216">Ok @ywelsch I'll try and write a script that replicates it
</comment><comment author="colings86" created="2017-03-31T13:34:21Z" id="290713188">No further feedback. If you write a script that replicates the issue on a recent release please reopen this issue</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>check for // norelease removed?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19246</link><project id="" key="" /><description>It seems like we removed the check for // norelease comments in https://github.com/elastic/elasticsearch/commit/5458d07ea28b80abfb28d1c6c096c9ec88ca but I am not sure this was intentional and if everybody is aware of it. Should we re introduce it? Or or is it still hidden somewhere and I just cannot find it?
</description><key id="163643140">19246</key><summary>check for // norelease removed?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>build</label><label>v2.4.0</label><label>v5.0.0-beta1</label></labels><created>2016-07-04T09:58:48Z</created><updated>2016-09-14T14:43:23Z</updated><resolved>2016-08-08T18:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-04T12:48:25Z" id="230282896">Reintroduce it I think.
</comment><comment author="jasontedor" created="2016-07-04T12:51:52Z" id="230283545">&gt; Should we re introduce it?

I would think yes as there are `norelease`s in the codebase right now that were added with the understanding that they would fail a release because they are marking items that must be changed before the next release. We can not change the meaning of that (to do nothing) with those having been added with that understanding.
</comment><comment author="rjernst" created="2016-08-08T18:22:11Z" id="238329643">I opened a PR to add it to the gradle build (the python release script won't be used anymore with the unified release): #19873

FYI, this is the current list of nocommits:

```
&gt; Found invalid patterns:
  - norelease on line 516 of core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
  - norelease on line 136 of core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
  - norelease on line 588 of core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
  - norelease on line 649 of core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add back norelease check when building a release</comment></comments></commit></commits></item><item><title>why the size of search thread pool in ES2.3.3 is smaller than before(f.e es version 1.3.0 )</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19245</link><project id="" key="" /><description>My server cpu cores are 24,today,i find the number of the search thread pool is 37 in my es version2.3.3;i want to know why this number is smaller than old version. what is the new default setting perpous?
thx for any answers~ 
</description><key id="163631493">19245</key><summary>why the size of search thread pool in ES2.3.3 is smaller than before(f.e es version 1.3.0 )</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kankedong</reporter><labels /><created>2016-07-04T08:55:03Z</created><updated>2016-07-04T09:04:16Z</updated><resolved>2016-07-04T09:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-04T09:04:16Z" id="230241881">Please ask your questions on discuss.elastic.co.

We can better help/answer there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cancel a search request in ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19244</link><project id="" key="" /><description>Add possibility to cancel a search request in ES.

When  a request is sent  from a web site and the end-user decide to go to another page, we will like to have the possibility  to stop the current search .
</description><key id="163628035">19244</key><summary>Cancel a search request in ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ESamir</reporter><labels><label>:Search</label><label>discuss</label><label>enhancement</label></labels><created>2016-07-04T08:34:25Z</created><updated>2016-07-04T14:55:40Z</updated><resolved>2016-07-04T14:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-04T09:01:23Z" id="230241279">I wonder if you have very loooong running requests.

Otherwise, I don't understand the need for such a feature as most of the time I'd expect search requests running in less than a second.
Can you elaborate a bit more?
</comment><comment author="clintongormley" created="2016-07-04T14:55:40Z" id="230308217">Duplicate of #12187
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing filter params spec in termvectors API and mtermvectors API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19243</link><project id="" key="" /><description>Term vectors API and Multi term vectors API supported filter params, but it's missing in spec files？

https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/termvectors.json

https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/mtermvectors.json
</description><key id="163627784">19243</key><summary>Missing filter params spec in termvectors API and mtermvectors API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">saxsir</reporter><labels /><created>2016-07-04T08:32:48Z</created><updated>2016-07-04T14:52:39Z</updated><resolved>2016-07-04T14:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-04T14:52:39Z" id="230307636">The `filter` param is part of the body, which is not spec'ed out in the REST specs at all.  We only specify the URL and query-string params.

See https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#docs-termvectors-terms-filtering
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable Checkstyle RedundantModifier</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19242</link><project id="" key="" /><description>This PR enables Checkstyle's [RedundantModifier](http://checkstyle.sourceforge.net/config_modifier.html#RedundantModifier) and fix the codebase.
</description><key id="163621639">19242</key><summary>Enable Checkstyle RedundantModifier</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T07:53:42Z</created><updated>2016-07-04T13:27:42Z</updated><resolved>2016-07-04T13:27:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-07-04T08:27:02Z" id="230234040">LGTM
</comment><comment author="tlrx" created="2016-07-04T13:27:42Z" id="230290682">Thanks @javanna !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add missing field type in the FieldStats response.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19241</link><project id="" key="" /><description>This change adds the type of the field in the fieldstats response.
It can be one of the following:
- "integer" for byte, short, integer and long
- "float" for float and double
- "date" for date
- "ip" for ip
- "string" for string, keyword and text.

Closes #17750
</description><key id="163618107">19241</key><summary>Add missing field type in the FieldStats response.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T07:28:11Z</created><updated>2016-09-16T10:08:17Z</updated><resolved>2016-07-06T14:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-04T12:50:22Z" id="230283247">&gt; whole-number

Sadly, the word integer is already used....
</comment><comment author="clintongormley" created="2016-07-04T15:19:38Z" id="230312691">&gt; Sadly, the word integer is already used....

Is that necessarily a problem?  I think it'd be ok to use `integer` and `float` instead of `whole-number` and `floating-point`, no?

@Bargs do we need to distinguish `text` from `keyword` for kibana's purposes? I think the `aggregatable` and  `searchable` flags are already sufficient for that. In which case, `string` instead of `text`?
</comment><comment author="Bargs" created="2016-07-05T14:10:14Z" id="230489049">&gt; do we need to distinguish text from keyword for kibana's purposes? I think the aggregatable and searchable flags are already sufficient for that.

What happens if a field with the same name is mapped as `text` in one index and `keyword` in another? Does it still show up as a conflict? Same question for `whole-number` and `floating-point`.

In theory `aggregatable` and `searchable` should be enough for us at the moment, but we might need additional flags if the capabilities of text/keyword diverge further in the future.
</comment><comment author="jimczi" created="2016-07-05T14:21:35Z" id="230492197">&gt; What happens if a field with the same name is mapped as text in one index and keyword in another? 
&gt; Does it still show up as a conflict?

It will show up as "text" in the fieldstats response without raising a conflict.

&gt; Same question for whole-number and floating-point.

Any field is remapped to one of the following: `whole_number`, `floating_point`, `text`, `ip` and `date`
Conflicts arise when the same field uses two or more remapped types on multiple indices. 
I choose `whole_number` and `floating_point` because `integer`and `float` might be misleading. As @clintongormley said it should not be a problem and the new proposal is to use:
`integer`, `float`, `string`, `ip` and `date`.
</comment><comment author="Bargs" created="2016-07-05T14:33:30Z" id="230495762">Ok, I think that should be fine. Kibana already normalizes everything into `number` and `string` today, so it's no different than what we're currently doing.
</comment><comment author="jimczi" created="2016-07-05T15:21:55Z" id="230510725">Ok, I replaced `whole_number`, `floating_point` and `text` with `integer`, `float` and `string`.
@nik9000 can you take a look ?
</comment><comment author="clintongormley" created="2016-07-06T11:16:39Z" id="230744638">@jimferenczi @jpountz from the point of view of aggs, do `integer` and `float` fields work together?  Wondering if we should combine these into `number`?
</comment><comment author="jimczi" created="2016-07-06T12:00:33Z" id="230752024">@clintongormley float and integer do not work together in term aggs. The following exception is thrown:

```
{
   "error": {
      "root_cause": [],
      "type": "reduce_search_phase_exception",
      "reason": "[reduce] ",
      "phase": "fetch",
      "grouped": true,
      "failed_shards": [],
      "caused_by": {
         "type": "aggregation_execution_exception",
         "reason": "Merging/Reducing the aggregations failed when computing the aggregation [ Name: test, Type: terms ] because: the field you gave in the aggregation query existed as two different types in two different indices"
      }
   },
   "status": 503
}
```
</comment><comment author="nik9000" created="2016-07-06T12:01:57Z" id="230752277">I've just been looking at that code - we probably could _make_ them work together if we wanted it hard enough. We'd convert everything to a double which'd lose precision compared to a long, but that is likely possible.
</comment><comment author="nik9000" created="2016-07-06T12:02:07Z" id="230752306">&gt; @nik9000 can you take a look ?

I'll review.
</comment><comment author="jpountz" created="2016-07-06T12:04:15Z" id="230752648">@nik9000 I've been considering doing that but we got bug reports against other aggs because of this precision loss, eg. #9545.
</comment><comment author="nik9000" created="2016-07-06T12:09:47Z" id="230753618">&gt; @nik9000 I've been considering doing that but we got bug reports against other aggs because of this precision loss, eg. #9545.

Yeah. I don't know the right thing. The conversion would be simple, but yeah.
</comment><comment author="nik9000" created="2016-07-06T12:10:19Z" id="230753723">Left some minor comments. Otherwise LGTM.
</comment><comment author="jimczi" created="2016-07-06T14:19:34Z" id="230786003">Thanks @nik9000 !
I've updated the description of the PR to reflect what has been merged.
</comment><comment author="Bargs" created="2016-07-06T19:11:02Z" id="230874213">Thanks for the quick fix here!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsAction.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java</file></files><comments><comment>Merge pull request #19241 from jimferenczi/field_stats_type</comment></comments></commit></commits></item><item><title>Support for "Span Near Multiple" queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19240</link><project id="" key="" /><description>The existing "Span Near" query type allows me to specify three terms which are all near each other.

```
{
    "span_near" : {
        "clauses" : [
            { "span_term" : { "field" : "value1" } },
            { "span_term" : { "field" : "value2" } },
            { "span_term" : { "field" : "value3" } }
        ],
        "slop" : 12,
        "in_order" : false,
        "collect_payloads" : false
    }
}
```

I would like a variant of this, but where the distance is measured between some base query and each of a collection of other queries.

For example,

```
{
    "span_near_multiple" : {
        "clause": { "span_term" : { "field" : "a" } },
        "near_clauses" : [
            { "span_term" : { "field" : "b" } },
            { "span_term" : { "field" : "c" } }
        ],
        "slop" : 2,
        "in_order" : false,
        "collect_payloads" : false
    }
}
```

This should match:
- a x b c
- b x x a x x c

The second of these examples is not matched by the existing Span Near query.

This should not match:
- a x x b

This is not the same as converting the query to a boolean query, because the following examples should not match:
- a x x b x x x x x x a x x c
- a x x b c x x a

If there is any way to get this behaviour from the existing queries, that would be good to know as well. But the best I can come up with is an approximation right now, and apparently the approximation is no longer good enough for one particular user.
</description><key id="163611991">19240</key><summary>Support for "Span Near Multiple" queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trejkaz</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-07-04T06:38:54Z</created><updated>2016-07-08T09:28:47Z</updated><resolved>2016-07-08T09:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-04T06:55:06Z" id="230218209">Would the below query work?

```
{
  "span_near": {
    "clauses": [
      {
        "span_term": {
          "field": "value1"
        }
      },
      {
        "span_or": {
          "clauses": [
            {
              "span_term": {
                "field": "value2"
              }
            },
            {
              "span_term": {
                "field": "value3"
              }
            }
          ]
        }
      }
    ],
    "slop": 2,
    "in_order": false,
    "collect_payloads": false
  }
}
```
</comment><comment author="trejkaz" created="2016-07-04T08:32:05Z" id="230235059">Using a span_or would match:
- a x x b

but should not.
</comment><comment author="jpountz" created="2016-07-04T08:37:19Z" id="230236114">This is the parth that confuses me a bit: how should the `near_clauses` be combined?
</comment><comment author="trejkaz" created="2016-07-04T09:27:57Z" id="230246820">Each clause in `near_clauses` should be within the specified range of the `clause`. Essentially the same as `span_near`, but with multiple terms being tested.
</comment><comment author="clintongormley" created="2016-07-04T15:04:50Z" id="230310008">This should work I think:

```
PUT t/t/1
{
  "text": "one two three four five six seven eight"
}

GET t/_search
{
  "query": {
    "span_containing": {
      "little": {
        "span_term": {
          "text": "three"
        }
      },
      "big": {
        "span_near": {
          "clauses": [
            {
              "span_term": {
                "text": "one"
              }
            },
            {
              "span_term": {
                "text": "three"
              }
            },
            {
              "span_term": {
                "text": "four"
              }
            }
          ],
          "slop": 2,
          "in_order": false
        }
      }
    }
  }
}
```
</comment><comment author="trejkaz" created="2016-07-04T23:25:16Z" id="230363240">That one doesn't match the already-provided example "a x b c".

Presumably the span_near returns the span from b to c, so a is not inside it.

It also doesn't match "b x x a x x c", because the span from b to c is greater than 2.
</comment><comment author="clintongormley" created="2016-07-05T09:30:02Z" id="230430864">I've played around with various options and find myself to be defeated. I've also found [this comment](https://lucidworks.com/blog/2009/07/18/the-spanquery/#comment-2222) from the author of the SpanWithinQuery which suggests that what you want might be possible by modifying the SpanWithinQuery.

We just inherit the span queries from Lucene, so I think you have two options: (a) open an issue on Lucene to convince somebody to build this query, or (b) write a plugin with a custom query based on SpanWithinQuery which does what you need. (If you do the latter, then please consider donating it to Lucene).
</comment><comment author="trejkaz" created="2016-07-05T23:17:03Z" id="230630642">I have made attempts to hack queries in Lucene to get a similar effect (funnily enough I appear on that thread you link as well, from 5 years ago when I was last looking at this stuff!) but found span queries too hard to understand in order to modify them. I think the most I was able to do was make a SpanNotNearQuery out of SpanNotQuery because it only required modifying one line...

I already filed an issue with Lucene themselves for SpanAndQuery/SpanNearAllQuery at the time as well, but years passed with nothing being done, so it seems like nobody else has been keen enough to have it. :/
</comment><comment author="trejkaz" created="2016-07-05T23:23:26Z" id="230631698">Another rewrite that I thought should work at the time went like this:

```
spanNearAll(main, [near1, near2], slop)
 -&gt; spanNot(
        main,
        spanOr([
            spanNot(main, near1, slop),
            spanNot(main, near2, slop)
        ])
```

Which is to say that you find all the spans where each sub-span is _not_ near the main span, and then exclude those spans from the results, so what I thought would be left is the spans where all of the clauses are near the main span.

In practice though, it failed the tests, and I never understood why.

Another rewrite that someone else said they were using went like this:

```
spanNearAll(main, [near1, near2], slop)
 -&gt; spanNear([
        main,
        spanNear(main, near1, slop),
        spanNear(main, near2, slop)
    ], 0)
```

This passes nearly all the tests, but there are some edge cases where it's still wrong, things like:

```
a x x x b c x x x a
```

It finds the span from a to b and then the span from c to a, and then decides that yes, the spans are near enough to match, and returns a false hit.
</comment><comment author="clintongormley" created="2016-07-08T09:28:47Z" id="231316143">Discussed in Fix it Friday.  This should be added in Lucene rather than in Elasticsearch, even if it is added to the Lucene sandbox.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Is it normal that the search is very slow for * prefix search?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19239</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:  1.7.2

**JVM version**: 1.7

**OS version**: suse 11

**Description of the problem including expected versus actual behavior**:

Our data scale is very large, if I use the condition: *1.1.1.1 to search, it will spend very long time.
who can describe the mechanism of search for \* which it is as the prefix of the condition, please?
is it really need so much time for search?

**Steps to reproduce**:
1.  build a large scale data enviroment
2.  use the condition: *1.1.1.1 to search

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="163599624">19239</key><summary>Is it normal that the search is very slow for * prefix search?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zyanbingchn</reporter><labels /><created>2016-07-04T04:05:41Z</created><updated>2016-07-04T04:23:34Z</updated><resolved>2016-07-04T04:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-07-04T04:23:32Z" id="230202186">Using a leading wildcard means that ES needs to check the queried field of _every_ matching doc to check if the value exists. It's like a table scan.

This isn't a bug, so if you have further questions around this, please ask on https://discuss.elastic.co :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Switch most search extensions from push to pull</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19238</link><project id="" key="" /><description>Switches most search behavior extensions from push (`onModule(SearchModule)`) to pull (`implements SearchPlugin`). This effort in general gives plugin authors a much cleaner view of _how_ to extend Elasticsearch and starts to set up portions of Elasticsearch as "the plugin API". This PR in particular does that for search-time behavior like customized suggesters, highlighters, score functions, and significance heuristics.

It also switches most such customization to being done at search module construction time which is much, much easier to reason about from a testing perspective. It also helps significantly in the process of de-guice-ing Elasticsearch's startup.

There are at least two major search time extensions that aren't covered in this PR that will simply have to wait for the next PR on the topic because this one has already grown large: custom aggregations and custom queries. These will likely live in the same `SearchPlugin` interface as well.
</description><key id="163594966">19238</key><summary>Switch most search extensions from push to pull</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-04T02:52:35Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-11T23:31:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-05T01:14:29Z" id="230369252">I think this is ready for you to review, @rjernst.
</comment><comment author="rjernst" created="2016-07-11T20:37:07Z" id="231856856">LGTM, I'm sorry this took so long to look at!
</comment><comment author="nik9000" created="2016-07-11T23:14:35Z" id="231893315">@rjernst I tried renaming the methods but they became a bit silly looking. So I added some more javadocs instead. I'll merge after my last test finishes unless you have any objections. If you have any problem with it after I've merged it just let me know and I'll change it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inform user of default credentials while installing xpack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19237</link><project id="" key="" /><description>After installing X-pack, the cluster is immediately locked with a default username/password combo that's in the docs, but not in the terminal output.
</description><key id="163575312">19237</key><summary>Inform user of default credentials while installing xpack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">loekvangool</reporter><labels><label>enhancement</label></labels><created>2016-07-03T19:15:32Z</created><updated>2016-10-18T08:47:35Z</updated><resolved>2016-07-04T14:44:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-04T14:44:11Z" id="230305976">Wrong repo
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to install ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19236</link><project id="" key="" /><description>I am trying to install ElasticSearch and when I am running bin/elasticsearch from cmd I am getting this error. Do you have any ideas what might be the problem and how can I fix it ? Thanks.

```
C:\Users\Marcus\Documents\ElasticSearch\elasticsearch-5.0.0-alpha4&gt;bin\elasticsearch.bat
[2016-07-03 19:13:24,266][INFO ][node                     ] [Blue Diamond] version[5.0.0-alpha4],pid[5968], build[3f5b994/2016-06-27T16:23:46.861Z], OS[Windows 7/6.1/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]
[2016-07-03 19:13:24,267][INFO ][node                     ] [Blue Diamond] initializing ...
Exception in thread "main" ElasticsearchException[Could not find plugin class [org.elasticsearch.script.mustache.MustachePlugin]]; nested: ClassNotFoundException[org.elasticsearch.script.mustache.MustachePlugin];
Likely root cause: java.lang.ClassNotFoundException: org.elasticsearch.script.mustache.MustachePlugin
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:814)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.elasticsearch.plugins.PluginsService.loadPluginClass(PluginsService.java:441)
        at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:412)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:134)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:212)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:173)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:175)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:175)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:250)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)

Refer to the log for complete error details.
```
</description><key id="163569738">19236</key><summary>Failed to install ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MarcusGitAccount</reporter><labels><label>feedback_needed</label></labels><created>2016-07-03T16:40:38Z</created><updated>2016-07-03T18:58:46Z</updated><resolved>2016-07-03T18:58:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-03T16:46:02Z" id="230162705">Did you download elasticsearch from the website? https://www.elastic.co/downloads/elasticsearch

Which exact binary did you download?
Did you just unzip and launch?

Thanks
</comment><comment author="MarcusGitAccount" created="2016-07-03T16:54:23Z" id="230163077">I have downloaded elasticsearch from the website, the 5.0.0-alpha4 version.
I have unzipped and launch the bin\elasticsearch.bat command from cmd. That's all I did. 

Marcus
</comment><comment author="dadoonet" created="2016-07-03T18:42:10Z" id="230168466">I just started a Windows 8 instance, downloaded a fresh version of elasticsearch and launched it.
Ran very well.

Not sure why you are seeing this.

Could you try again from scratch and report all the steps here?

Also check your system/user properties. May be you have some strange JVM options?
</comment><comment author="jasontedor" created="2016-07-03T18:58:46Z" id="230169282">I think that somehow you mangled the modules directory, and removed at a minimum the lang-mustache-5.0.0-alpha4.jar file. That is, I can reproduce your exact stack trace just by removing this file.

&lt;details&gt;
  &lt;summary&gt;

Removing lang-mustache-5.0.0-alpha4.jar&lt;/summary&gt;



``` bash
14:48:38 ⌁38% [jason:~/elasticsearch] $ rm elasticsearch-5.0.0-alpha4/modules/lang-mustache/lang-mustache-5.0.0-alpha4.jar
14:48:40 ⌁38% [jason:~/elasticsearch] $ ls -al ./elasticsearch-5.0.0-alpha4/modules/lang-mustache/
total 216
drwxr-xr-x   5 jason  staff    170 Jul  3 14:46 .
drwxr-xr-x  10 jason  staff    340 Jun 30 14:52 ..
-rw-r--r--   1 jason  staff  98451 Jun 30 14:51 compiler-0.9.1.jar
-rw-r--r--   1 jason  staff   1309 Jun 30 14:51 plugin-descriptor.properties
-rw-r--r--   1 jason  staff    902 Jun 30 14:51 plugin-security.policy
14:48:52 ⌁38% [jason:~/elasticsearch] $ ./elasticsearch-5.0.0-alpha4/bin/elasticsearch
[2016-07-03 14:49:00,904][INFO ][node                     ] [Hub] version[5.0.0-alpha4], pid[40059], build[3f5b994/2016-06-27T16:23:46.861Z], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]
[2016-07-03 14:49:00,905][INFO ][node                     ] [Hub] initializing ...
Exception in thread "main" ElasticsearchException[Could not find plugin class [org.elasticsearch.script.mustache.MustachePlugin]]; nested: ClassNotFoundException[org.elasticsearch.script.mustache.MustachePlugin];
Likely root cause: java.lang.ClassNotFoundException: org.elasticsearch.script.mustache.MustachePlugin
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:814)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.elasticsearch.plugins.PluginsService.loadPluginClass(PluginsService.java:441)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:412)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:134)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:212)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:173)
    at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:175)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:175)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:250)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
    at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
Refer to the log for complete error details.
14:49:01 ⌁38% [jason:~/elasticsearch] 1 $ 
```

&lt;/details&gt;

However, a clean install of Elasticsearch 5.0.0-alpha4 will include this file, and Elasticsearch will start fine:

&lt;details&gt;
&lt;summary&gt;

Clean install of Elasticsearch 5.0.0-alpha4&lt;/summary&gt;



``` bash
14:55:28 ⌁36% [jason:~/elasticsearch] $ rm -rf elasticsearch-5.0.0-alpha4/
14:55:34 ⌁36% [jason:~/elasticsearch] $ curl -L -O -sS https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/5.0.0-alpha4/elasticsearch-5.0.0-alpha4.zip
14:55:46 ⌁35% [jason:~/elasticsearch] $ unzip elasticsearch-5.0.0-alpha4.zip 
Archive:  elasticsearch-5.0.0-alpha4.zip
   creating: elasticsearch-5.0.0-alpha4/
   creating: elasticsearch-5.0.0-alpha4/lib/
  inflating: elasticsearch-5.0.0-alpha4/lib/elasticsearch-5.0.0-alpha4.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-core-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-analyzers-common-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-backward-codecs-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-grouping-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-highlighter-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-join-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-memory-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-misc-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-queries-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-queryparser-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-sandbox-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-spatial-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-spatial-extras-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-spatial3d-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/lucene-suggest-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/securesm-1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jopt-simple-4.9.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/hppc-0.7.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/joda-time-2.9.4.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/joda-convert-1.2.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jackson-core-2.7.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jackson-dataformat-smile-2.7.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jackson-dataformat-yaml-2.7.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jackson-dataformat-cbor-2.7.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/netty-3.10.5.Final.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/t-digest-3.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/HdrHistogram-2.1.6.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/spatial4j-0.6.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jts-1.13.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/log4j-1.2.17.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/apache-log4j-extras-1.2.17.jar  
  inflating: elasticsearch-5.0.0-alpha4/lib/jna-4.2.2.jar  
   creating: elasticsearch-5.0.0-alpha4/config/
  inflating: elasticsearch-5.0.0-alpha4/config/jvm.options  
  inflating: elasticsearch-5.0.0-alpha4/config/elasticsearch.yml  
  inflating: elasticsearch-5.0.0-alpha4/config/logging.yml  
   creating: elasticsearch-5.0.0-alpha4/bin/
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-plugin  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch.in.sh  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-systemd-pre-exec  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-plugin.bat  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch.in.bat  
  inflating: elasticsearch-5.0.0-alpha4/bin/service.bat  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch.bat  
  inflating: elasticsearch-5.0.0-alpha4/NOTICE.txt  
  inflating: elasticsearch-5.0.0-alpha4/README.textile  
  inflating: elasticsearch-5.0.0-alpha4/LICENSE.txt  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-service-mgr.exe  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-service-x64.exe  
  inflating: elasticsearch-5.0.0-alpha4/bin/elasticsearch-service-x86.exe  
   creating: elasticsearch-5.0.0-alpha4/modules/
   creating: elasticsearch-5.0.0-alpha4/modules/lang-painless/
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-painless/plugin-security.policy  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-painless/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-painless/asm-debug-all-5.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-painless/lang-painless-5.0.0-alpha4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-painless/antlr4-runtime-4.5.1-1.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/percolator/
  inflating: elasticsearch-5.0.0-alpha4/modules/percolator/percolator-5.0.0-alpha4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/percolator/plugin-descriptor.properties  
   creating: elasticsearch-5.0.0-alpha4/modules/lang-groovy/
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-groovy/plugin-security.policy  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-groovy/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-groovy/groovy-2.4.6-indy.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-groovy/lang-groovy-5.0.0-alpha4.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/ingest-common/
  inflating: elasticsearch-5.0.0-alpha4/modules/ingest-common/joni-2.1.6.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/ingest-common/ingest-common-5.0.0-alpha4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/ingest-common/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/ingest-common/jcodings-1.0.12.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/reindex/
  inflating: elasticsearch-5.0.0-alpha4/modules/reindex/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/reindex/reindex-5.0.0-alpha4.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/lang-mustache/
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-mustache/compiler-0.9.1.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-mustache/plugin-security.policy  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-mustache/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-mustache/lang-mustache-5.0.0-alpha4.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/lang-expression/
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/asm-tree-5.0.4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/plugin-security.policy  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/asm-commons-5.0.4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/lucene-expressions-6.1.0.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/antlr4-runtime-4.5.1-1.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/asm-5.0.4.jar  
  inflating: elasticsearch-5.0.0-alpha4/modules/lang-expression/lang-expression-5.0.0-alpha4.jar  
   creating: elasticsearch-5.0.0-alpha4/modules/aggs-matrix-stats/
  inflating: elasticsearch-5.0.0-alpha4/modules/aggs-matrix-stats/plugin-descriptor.properties  
  inflating: elasticsearch-5.0.0-alpha4/modules/aggs-matrix-stats/aggs-matrix-stats-5.0.0-alpha4.jar  
14:55:52 ⌁35% [jason:~/elasticsearch] $ ls -al elasticsearch-5.0.0-alpha4/modules/lang-mustache/
total 320
drwxr-xr-x   6 jason  staff    204 Jun 27 16:26 .
drwxr-xr-x  10 jason  staff    340 Jun 27 16:27 ..
-rw-rw-r--   1 jason  staff  98451 Jun 27 16:26 compiler-0.9.1.jar
-rw-rw-r--   1 jason  staff  49699 Jun 27 16:26 lang-mustache-5.0.0-alpha4.jar
-rw-rw-r--   1 jason  staff   1309 Jun 27 16:26 plugin-descriptor.properties
-rw-rw-r--   1 jason  staff    902 Jun 27 16:26 plugin-security.policy
14:56:01 ⌁35% [jason:~/elasticsearch] $ ./elasticsearch-5.0.0-alpha4/bin/elasticsearch
[2016-07-03 14:56:08,806][INFO ][node                     ] [Boost] version[5.0.0-alpha4], pid[40876], build[3f5b994/2016-06-27T16:23:46.861Z], OS[Mac OS X/10.11.5/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_92/25.92-b14]
[2016-07-03 14:56:08,807][INFO ][node                     ] [Boost] initializing ...
[2016-07-03 14:56:09,358][INFO ][plugins                  ] [Boost] modules [percolator, lang-mustache, lang-painless, reindex, aggs-matrix-stats, lang-expression, ingest-common, lang-groovy], plugins []
[2016-07-03 14:56:09,807][INFO ][env                      ] [Boost] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [346.6gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2016-07-03 14:56:09,807][INFO ][env                      ] [Boost] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-07-03 14:56:10,662][INFO ][node                     ] [Boost] initialized
[2016-07-03 14:56:10,662][INFO ][node                     ] [Boost] starting ...
[2016-07-03 14:56:10,734][INFO ][transport                ] [Boost] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-07-03 14:56:10,737][WARN ][bootstrap                ] [Boost] initial heap size [268435456] not equal to maximum heap size [2147483648]; this can cause resize pauses and prevents mlockall from locking the entire heap
[2016-07-03 14:56:10,737][WARN ][bootstrap                ] [Boost] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster
[2016-07-03 14:56:13,774][INFO ][cluster.service          ] [Boost] new_master {Boost}{ah2ahfU1TWKXWc4gbZOkxg}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] nodes joined)
[2016-07-03 14:56:13,792][INFO ][http                     ] [Boost] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-07-03 14:56:13,792][INFO ][node                     ] [Boost] started
[2016-07-03 14:56:13,802][INFO ][gateway                  ] [Boost] recovered [0] indices into cluster_state
^C[2016-07-03 14:56:15,267][INFO ][node                     ] [Boost] stopping ...
[2016-07-03 14:56:15,277][INFO ][node                     ] [Boost] stopped
[2016-07-03 14:56:15,277][INFO ][node                     ] [Boost] closing ...
[2016-07-03 14:56:15,284][INFO ][node                     ] [Boost] closed
14:56:15 ⌁35% [jason:~/elasticsearch] 130 $ 
```

&lt;/details&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to netty 3.10.6.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19235</link><project id="" key="" /><description>This upgrades to the latest netty release. @rjernst I ran `gradle check` on the distribution project and nothing failed even when I had not yet upgraded the sha1. Running `gradle dependencyLicenses` did it, maybe something is broken here?
</description><key id="163558711">19235</key><summary>Upgrade to netty 3.10.6.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>review</label><label>upgrade</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-07-03T11:43:02Z</created><updated>2016-07-08T08:02:27Z</updated><resolved>2016-07-05T09:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-03T12:29:34Z" id="230151053">LGTM.
</comment><comment author="rjernst" created="2016-07-03T18:24:47Z" id="230167607">@s1monw I made the same change and it failed for me as expected...

```
[11:22:48][~/Code/elasticsearch/distribution]$ git diff
diff --git a/core/build.gradle b/core/build.gradle
index 88c5cc2..2231880 100644
--- a/core/build.gradle
+++ b/core/build.gradle
@@ -75,7 +75,7 @@ dependencies {
   compile "com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:${versions.jackson}"

   // network stack
-  compile 'io.netty:netty:3.10.5.Final'
+  compile 'io.netty:netty:3.10.6.Final'
   // percentiles aggregation
   compile 'com.tdunning:t-digest:3.0'
   // precentil ranks aggregation
[11:22:29][~/Code/elasticsearch/distribution]$ gradle check
...
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':distribution:dependencyLicenses'.
&gt; Missing SHA for netty-3.10.6.Final.jar. Run 'gradle updateSHAs' to create
```
</comment><comment author="salyh" created="2016-07-06T09:26:52Z" id="230722852">Netty 3 is EOL and 3.10.6.Final would be the last release. Looking into https://github.com/elastic/elasticsearch/issues/3226 it does not look like an update to 4.0 or 4.1 is considered for ES 5?
</comment><comment author="s1monw" created="2016-07-08T08:02:27Z" id="231299442">&gt; Netty 3 is EOL and 3.10.6.Final would be the last release. Looking into #3226 it does not look like an update to 4.0 or 4.1 is considered for ES 5?

There is no issue yet but we are working on the future of networking. In a nutshell the plan is to modularize netty3 (build it as a plugin) and have a second module with netty4 such that we can do an experimental phase with the new networking. It's such a crucial part of the system that we want a safety net for the new version since a rather large portion of netty has changed fundamentally and we first need to learn how it affects ES. As a sideeffect this will allow us to do much better code isolation also in terms of security permissions etc. Yet, netty4 might not come with 5.0GA but I am confident it will come soon after.

There is work under way related to this like #19196 #19129 #19125 #19096 so we are actively working on this. I hope that helps... I will open a modularization issue soon too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchSecurityException.java</file><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRunnable.java</file><file>core/src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/TaskOperationFailure.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/NodeExplanation.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/TransportClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BackoffPolicy.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestHandler.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/Retry.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/AutoCreateIndex.java</file><file>core/src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>core/src/main/java/org/elasticsearch/action/support/DelegatingActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>core/src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/PlainListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportActions.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/BootstrapCheck.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/cli/MultiCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/SettingCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/Terminal.java</file><file>core/src/main/java/org/elasticsearch/cli/UserException.java</file><file>core/src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskConfig.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java</file><file>core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateFilter.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/DelayedAllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/BasePrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/MacAddressProvider.java</file><file>core/src/main/java/org/elasticsearch/common/NamedRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/Randomness.java</file><file>core/src/main/java/org/elasticsearch/common/UUIDGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReferenceStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/CompositeBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ReleasablePagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/component/LifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java</file><file>core/src/main/java/org/elasticsearch/common/compress/DeflateCompressor.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/ShapesAvailability.java</file><file>core/src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/hash/MurmurHash3.java</file><file>core/src/main/java/org/elasticsearch/common/inject/Scope.java</file><file>core/src/main/java/org/elasticsearch/common/inject/State.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java</file><file>core/src/main/java/org/elasticsearch/common/io/Channels.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>core/src/main/java/org/elasticsearch/common/lease/Releasables.java</file><file>core/src/main/java/org/elasticsearch/common/logging/ESLogger.java</file><file>core/src/main/java/org/elasticsearch/common/logging/LoggerMessageFormat.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTokenStream.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/ElasticsearchDirectoryReader.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/netty/KeepFrameDecoder.java</file><file>core/src/main/java/org/elasticsearch/common/network/Cidrs.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/text/Text.java</file><file>core/src/main/java/org/elasticsearch/common/text/UTF8SortedAsUnicodeComparator.java</file><file>core/src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/LocalTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/PortsRange.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/util/ByteArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java</file><file>core/src/main/java/org/elasticsearch/common/util/DoubleArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java</file><file>core/src/main/java/org/elasticsearch/common/util/FloatArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/IntArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/LongArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/AbstractRunnable.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/SuspendableRefContainer.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadContext.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>core/src/main/java/org/elasticsearch/discovery/AckClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file></files><comments><comment>Upgrade to netty 3.10.6.Final (#19235)</comment></comments></commit></commits></item><item><title>Reindex API fails if replicas count is increased during the operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19234</link><project id="" key="" /><description>Reindexing using the Reindex API on 2.3.1, and setting replicas count to &gt; 0 during the reindexing operation failed the operation with the following error:

```
   "failures": [
      {
         "index": "entities-1467533893025",
         "type": "entity",
         "id": "AVDSeSyc-CwqIYtC3lcT",
         "cause": {
            "type": "unavailable_shards_exception",
            "reason": "[entities-1467533893025][0] Not enough active copies to meet write consistency of [QUORUM] (have 1, needed 4). Timeout: [1m], request: [BulkShardRequest to [entities-1467533893025] containing [7] requests]"
         },
         "status": 503
      },
```

Can the reindex API be more tolerant for replication scaling? we used it this way because reindexing to 0 replicas is faster, but wanted to add replicas when nearing completion of the reindexing operation to allow for faster rollout of the new index.
</description><key id="163557305">19234</key><summary>Reindex API fails if replicas count is increased during the operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Reindex API</label><label>discuss</label></labels><created>2016-07-03T11:00:53Z</created><updated>2016-07-13T16:51:27Z</updated><resolved>2016-07-08T09:35:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-07-03T11:10:50Z" id="230147885">Reindexing behaves in the same way as normal indexing. Looking at the code, we currently don't expose the `consistencyLevel` parameter of the underlying indexing request. IMO we should. @nik9000 any opinion here?

Note that long term we have plans (which will hopefully make it into 5.0 - see discussions on #18985 ) to rename the consistencyLevel parameter to `waitForActiveShards` which reflects how it works. This will be set per request, but also have an index level settings to control the default, which will be updatable dynamically giving you what you want in a better way IMO. 
</comment><comment author="synhershko" created="2016-07-03T11:16:46Z" id="230148120">Sounds good. Might be worthy to mention I was jumping from replicas: 0 to 6, but given the batch mode of reindexing - the default IMO should be 100% async replication.
</comment><comment author="bleskes" created="2016-07-03T11:36:34Z" id="230148883">Let’s not conflate async replications with the active shards requirement - it’s a different thing. That said async replication presents a challenge for error and such. It is removed and will therefore unlikely to come back.  If it all, we should invest in running the search of reindex concurrently to the indexing but that is another discussions as well (and it creates coordination complexity which I’m not sure at all is needed here).

&gt; On 03 Jul 2016, at 13:16, Itamar Syn-Hershko notifications@github.com wrote:
&gt; 
&gt; Sounds good. Might be worthy to mention I was jumping from replicas: 0 to 6, but given the batch mode of reindexing - the default IMO should be 100% async replication.
&gt; 
&gt; —
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment><comment author="clintongormley" created="2016-07-08T09:35:29Z" id="231317483">This isn't a problem with reindexing, but with any CRUD request.  If you increase replicas to a point where the quorum is not available, CRUD requests will stall and eventually timeout.  I don't think this is a bug that can be fixed.

Exposing the consistency level (which would have to be done at the start of reindexing) would be a workaround for this situation and we should probably do that.
</comment><comment author="nik9000" created="2016-07-08T16:27:57Z" id="231406285">&gt; Exposing the consistency level (which would have to be done at the start of reindexing) would be a workaround for this situation and we should probably do that.

You can set the consistency level on reindex the same way you set it on any other write action. I just never documented it because I'm a doofus. [Here](https://github.com/elastic/elasticsearch/blob/2.4/modules/reindex/src/test/resources/rest-api-spec/test/reindex/60_consistency.yaml) is the test. It is just a request parameter.

Sorry I didn't notice this five days ago when you opened it....
</comment><comment author="nik9000" created="2016-07-13T16:51:27Z" id="232416696">Ah. I do mention it in the URL Parameters section.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can't map array of [lon, lat] </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19233</link><project id="" key="" /><description>I'm trying to get Kibana to see my location info as a geo_point. I've been running about mapping and how to do it, but I can't seem to figure it out. The data looks like this in mongodb:

`{ "_id" : "3", "loc" : "[-122.0574, 37.41919999999999]", "fs" : "Clean", "name" : "www.googleapis.com", "timestamp" : "2016-07-02T21:02:53.623Z", "destination" : "192.168.79.136", "source" : "216.58.212.138", "vt" : "0" }`

I've tried the following:

```
curl -XPUT 'http://localhost:9200/yapdns/_mapping' -d '
    {
      "mappings": {
        "loc": {
          "properties": {
            "location": {
              "type": "geo_point"
            }
          }
        }
      }
    } '
```

But got this error: 
`{"error":{"root_cause":[{"type":"action_request_validation_exception","reason":"Validation Failed: 1: mapping type is missing;"}],"type":"action_request_validation_exception","reason":"Validation Failed: 1: mapping type is missing;"},"status":400}`

Then I tried:

```
curl -XPUT 'http://localhost:9200/yapdns/_mapping/loc' -d '
{
  "properties": {
    "loc": {
      "type": "geo_point"
    }
  }
}
```

and got this error:

`{"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"mapper [loc] cannot be changed from type [string] to [geo_point]"}],"type":"illegal_argument_exception","reason":"mapper [loc] cannot be changed from type [string] to [geo_point]"},"status":400}`

I'm with struggling with this issue for hours now..

How would I map the values that are stored in the key "loc" as geo_point?
</description><key id="163554567">19233</key><summary>Can't map array of [lon, lat] </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nerotix</reporter><labels /><created>2016-07-03T09:29:05Z</created><updated>2016-07-03T10:08:44Z</updated><resolved>2016-07-03T10:08:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-03T10:08:44Z" id="230145579">Please ask this on discuss.elastic.co

Your mapping is incorrect. Field name is loc and not location IMO.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field stats API gte/lt reversed?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19232</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-field-stats.html) we state;

&gt; min_value - The lowest value in the field.
&gt; max_value - The highest value in the field.

And then in [this example](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-field-stats.html#_field_stats_index_constraints) we have;

```
curl -XPOST "http://localhost:9200/_field_stats?level=indices" -d '{
   "fields" : ["answer_count"] 
   "index_constraints" : { 
      "creation_date" : { 
         "min_value" : { 
            "gte" : "2014-01-01T00:00:00.000Z"
         },
         "max_value" : {
            "lt" : "2015-01-01T00:00:00.000Z"
         }
      }
   }
}
```

But if you test it, it appears that the correct layout is actually in reverse when it comes to using `lt` and `gte`;

```
curl -XPOST "http://localhost:9200/_field_stats?level=indices" -d '{
   "fields" : ["answer_count"] 
   "index_constraints" : { 
      "creation_date" : { 
         "min_value" : { 
            "lt" : "2015-01-01T00:00:00.000Z"
         },
         "max_value" : {
            "gte" : "2014-01-01T00:00:00.000Z"
         }
      }
   }
}
```

Logically it seems that the second way is correct, but I didn't want to just make a PR without checking.
</description><key id="163545111">19232</key><summary>Field stats API gte/lt reversed?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2016-07-03T03:18:56Z</created><updated>2016-07-07T12:57:01Z</updated><resolved>2016-07-07T12:57:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-03T20:44:38Z" id="230174336">I agree the 2nd way makes more sense.
</comment><comment author="markwalkom" created="2016-07-06T00:40:25Z" id="230642870">Any other feedback, this should be an easy fix.
</comment><comment author="clintongormley" created="2016-07-06T12:26:06Z" id="230756652">Actually, the existing format is correct.  We're looking for indices where the earliest creation_date is the same or after 2014-01-01  and the latest creation_date is before 2015-01-01.

Here's an example which demonstrates that this is correct:

```
PUT 2014-01/t/1
{
  "date": "2014/01/01"
}

PUT 2014-02/t/1
{
  "date": "2014/02/01"
}

PUT 2014-03/t/1
{
  "date": "2014/03/01"
}

PUT 2014-04/t/1
{
  "date": "2014/04/01"
}

PUT 2014-05/t/1
{
  "date": "2014/05/01"
}

PUT 2014-06/t/1
{
  "date": "2014/06/01"
}

GET 2014*/_field_stats?level=indices
{
  "fields": [
    "date"
  ],
  "index_constraints": {
    "date": {
      "min_value": {
        "gte": "2014/03/01",
        "format": "yyyy/MM/dd"
      },
      "max_value": {
        "lt": "2014/06/01",
        "format": "yyyy/MM/dd"
      }
    }
  }
}
```
</comment><comment author="markwalkom" created="2016-07-06T21:14:30Z" id="230908973">Yeah, after I re-read it a few times it makes more sense. As does your example.
Thanks :)
</comment><comment author="bleskes" created="2016-07-07T02:03:45Z" id="230959786">I think what people want to know is which indices are relevant to a certain period. Since all index constraints need to be matched, the version posted will return indices that are fully contained within the requested period. However, if indices are partially overlapping that period, they will not be returned. Here is a little tweak that demonstrates this (note the extra doc in the 2014-06 index):

```
PUT 2014-01/t/1
{
  "date": "2014/01/01"
}


PUT 2014-02/t/1
{
  "date": "2014/02/01"
}

PUT 2014-03/t/1
{
  "date": "2014/03/01"
}

PUT 2014-04/t/1
{
  "date": "2014/04/01"
}

PUT 2014-05/t/1
{
  "date": "2014/05/01"
}

PUT 2014-06/t/1
{
  "date": "2014/06/01"
}

PUT 2014-06/t/1
{
  "date": "2014/06/10"
}

PUT 2014-07/t/1
{
  "date": "2014/07/10"
}

## doesn't return 2014-06
GET 2014*/_field_stats?level=indices&amp;filter_path=*.*.fields.date.doc_count
{
  "fields": [
    "date"
  ],
  "index_constraints": {
    "date": {
      "min_value": {
        "gte": "2014/03/01",
        "format": "yyyy/MM/dd"
      },
      "max_value": {
        "lt": "2014/06/03",
        "format": "yyyy/MM/dd"
      }
    }
  }
}

## does return 2014-06
GET 2014*/_field_stats?level=indices&amp;filter_path=*.*.fields.date.doc_count
{
  "fields": [
    "date"
  ],
  "index_constraints": {
    "date": {
      "max_value": {
        "gte": "2014/03/01",
        "format": "yyyy/MM/dd"
      },
      "min_value": {
        "lt": "2014/06/03",
        "format": "yyyy/MM/dd"
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-07-07T12:54:01Z" id="231068741">@bleskes nice one.. will fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update field-stats.asciidoc</comment></comments></commit></commits></item><item><title>Do not catch throwable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19231</link><project id="" key="" /><description>Today throughout the codebase, catch throwable is used with reckless
abandon. This is dangerous because the throwable could be a fatal
virtual machine error resulting from an internal error in the JVM, or an
out of memory error or a stack overflow error that leaves the virtual
machine in an unstable and unpredictable state. This commit removes
catch throwable from the codebase and removes the temptation to use it
by modifying listener APIs to receive instances of Exception instead of
the top-level Throwable.
</description><key id="163542767">19231</key><summary>Do not catch throwable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-03T01:37:06Z</created><updated>2016-08-20T15:15:58Z</updated><resolved>2016-07-04T12:41:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-03T01:38:31Z" id="230130431">This PR is blocked on a release of SecureSM that incorporates elastic/securesm#4, but I think that reviews can start, and the PR also helps clarify the intent of elastic/securesm#4.
</comment><comment author="s1monw" created="2016-07-03T11:10:10Z" id="230147861">I am not sure we should block this aweseom cleanup by the securesm changes. I'm likely -1 on having multiple exit points in the JVM but most of this PR is awesome. Can we detach the two?
</comment><comment author="jasontedor" created="2016-07-03T12:31:11Z" id="230151123">&gt; Can we detach the two?

I've detached the uncaught exception handler changes and will open a second PR on top of master (for discussion) when this PR is integrated.
</comment><comment author="s1monw" created="2016-07-04T12:11:25Z" id="230276469">LGTM lets get this in! I really wonder how we can keep this under control maybe we can add some forbidden API magic?
</comment><comment author="agirbal" created="2016-07-11T15:33:09Z" id="231771303">Can't this change potentially make it harder on operations to deal with ES? I am thinking of case where you get a domino effect of nodes just dying, and the fewer nodes are up the more likely the others will die. Factor in the fact that with some customers it can take many minutes for ES to restart.
Back in the day I was working on a Saas, following many incidents of domino effects, we tried really hard to always salvage the JVM, would it be only to troubleshoot it. Upon certain exceptions (IO error from disk, OOM) it would enter a degraded state and report it to rest of cluster to stop receiving requests or just pass through only (i.e. client mode). From there one could flush its caches, check the instance and potentially put it back in normal state.
</comment><comment author="nik9000" created="2016-07-11T15:36:55Z" id="231772467">If the JVM throws an OOM then salvaging it is likely to put it in an unexpected state. That is scary enough that it is worth crashing I think.

IO errors are still caught and salvaged.

I agree that we need to prevent these OOMs from happening and we do actively work on that.
</comment><comment author="jasontedor" created="2016-07-11T15:51:28Z" id="231777068">&gt; Can't this change potentially make it harder on operations to deal with ES?

I think that it makes it easier because there is no safe recovery from virtual machine errors like OOM. In the past, we would silently discard these fatal errors leaving the JVM in an unpredictable state. In that case, operators should be restarting their instances, but they didn't have a way of reliably knowing if the JVM experienced such an error or not. With this change and #19272, we've taken this burden away from operators and removed a potential source of corruption and other resiliency issues.

&gt; Back in the day I was working on a Saas, following many incidents of domino effects, we tried really hard to always salvage the JVM, would it be only to troubleshoot it.

When the JVM throws an OOM it enters a questionable and unexpected state, the point is that it can not be salvaged.

&gt; From there one could flush its caches, check the instance and potentially put it back in normal state.

We can not reliably do this after the JVM has entered such a state, there are no guarantees that we can safely do anything at this point.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchSecurityException.java</file><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRunnable.java</file><file>core/src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/TaskOperationFailure.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/TransportClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestHandler.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/Retry.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>core/src/main/java/org/elasticsearch/action/support/DelegatingActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/PlainListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportActions.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskConfig.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/DelayedAllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/MacAddressProvider.java</file><file>core/src/main/java/org/elasticsearch/common/Randomness.java</file><file>core/src/main/java/org/elasticsearch/common/geo/ShapesAvailability.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/lease/Releasables.java</file><file>core/src/main/java/org/elasticsearch/common/logging/LoggerMessageFormat.java</file><file>core/src/main/java/org/elasticsearch/common/network/Cidrs.java</file><file>core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/AbstractRunnable.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadContext.java</file><file>core/src/main/java/org/elasticsearch/discovery/AckClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/env/ESFileStore.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java</file><file>core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>core/src/main/java/org/elasticsearch/gateway/MetaStateService.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/index/CompositeIndexEventListener.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexWarmer.java</file><file>core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexOrdinalsFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVOrdinalsIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexingOperationListener.java</file><file>core/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java</file><file>core/src/main/java/org/elasticsearch/index/shard/RefreshListeners.java</file><file>core/src/main/java/org/elasticsearch/index/shard/SearchOperationListener.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/TermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java</file><file>core/src/main/java/org/elasticsearch/monitor/Probes.java</file><file>core/src/main/java/org/elasticsearch/monitor/jvm/JvmGcMonitorService.java</file><file>core/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java</file><file>core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java</file><file>core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>core/src/main/java/org/elasticsearch/repositories/VerificationFailure.java</file><file>core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/head/RestAliasesExistAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/support/RestActionListener.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/search/MultiValueMode.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/action/SearchTransportService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/main/java/org/elasticsearch/tasks/PersistedTaskInfo.java</file><file>core/src/main/java/org/elasticsearch/tasks/Task.java</file><file>core/src/main/java/org/elasticsearch/tasks/TaskManager.java</file><file>core/src/main/java/org/elasticsearch/tasks/TaskPersistenceService.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/main/java/org/elasticsearch/transport/DelegatingTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/PlainTransportFuture.java</file><file>core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java</file><file>core/src/main/java/org/elasticsearch/transport/ResponseHandlerFailureTransportException.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/main/java/org/elasticsearch/watcher/FileWatcher.java</file><file>core/src/test/java/org/elasticsearch/ESExceptionTests.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/ListenerActionIT.java</file><file>core/src/test/java/org/elasticsearch/action/RejectionActionIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/HotThreadsIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/CancellableTasksTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/RetryTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/TransportBulkActionTookTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java</file><file>core/src/test/java/org/elasticsearch/action/main/MainActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ListenableActionFutureTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ReplicationOperationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportWriteActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/MultiTermVectorsIT.java</file><file>core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/AbstractLifecycleRunnableTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/AbstractRunnableTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/EsExecutorsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/ThreadContextTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/ConstructingObjectParserTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingDisabledTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indexing/IndexActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushUtil.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryTargetTests.java</file><file>core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java</file></files><comments><comment>Do not catch throwable</comment></comments></commit></commits></item><item><title>Remove unused methods and classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19230</link><project id="" key="" /><description>This change removes a handful of classes and methods that were simply
unused. Some of the classes were intermediate abstract classes that
added nothing to the base class they extended.
</description><key id="163530697">19230</key><summary>Remove unused methods and classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-02T18:35:42Z</created><updated>2016-07-06T03:46:56Z</updated><resolved>2016-07-06T03:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-04T06:47:37Z" id="230217100">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionListenerResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/common/io/CharSequenceReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/FastStringReader.java</file><file>core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/main/java/org/elasticsearch/transport/BaseTransportResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/FutureTransportResponseHandler.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/bar/BarTestClass.java</file><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file></files><comments><comment>Merge pull request #19230 from rjernst/unused</comment></comments></commit></commits></item><item><title>Unable to install Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19229</link><project id="" key="" /><description>Hi,

I have followed all the instuctions given on this page: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html

The last step mentioned results in a HttpError404: 
sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch

It is not able to find the packages mentioned in this link: https://packages.elastic.co/elasticsearch/2.3.1/debian/dists/stable/main/binary-amd64/Packages

below is the result of me running the installation command

sudo apt-get update &amp;&amp; sudo apt-get install elasticsearchHit http://security.ubuntu.com trusty-security InRelease  
Hit http://security.ubuntu.com trusty-security/main Sources  
Hit http://security.ubuntu.com trusty-security/restricted Sources  
Hit http://security.ubuntu.com trusty-security/universe Sources  
Hit http://security.ubuntu.com trusty-security/multiverse Sources  
Hit http://security.ubuntu.com trusty-security/main amd64 Packages  
Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages  
Hit http://security.ubuntu.com trusty-security/universe amd64 Packages  
Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages  
Hit http://security.ubuntu.com trusty-security/main i386 Packages  
Hit http://security.ubuntu.com trusty-security/restricted i386 Packages  
Hit http://security.ubuntu.com trusty-security/universe i386 Packages  
Hit http://security.ubuntu.com trusty-security/multiverse i386 Packages  
Hit http://security.ubuntu.com trusty-security/main Translation-en  
Hit http://security.ubuntu.com trusty-security/multiverse Translation-en  
Hit http://security.ubuntu.com trusty-security/restricted Translation-en  
Hit http://security.ubuntu.com trusty-security/universe Translation-en  
Ign http://extras.ubuntu.com trusty InRelease  
Ign http://ppa.launchpad.net trusty InRelease  
Ign http://in.archive.ubuntu.com trusty InRelease  
Ign http://ppa.launchpad.net trusty InRelease  
Hit http://in.archive.ubuntu.com trusty-updates InRelease  
Hit http://ppa.launchpad.net trusty InRelease  
Hit http://in.archive.ubuntu.com trusty-backports InRelease  
Get:1 http://extras.ubuntu.com trusty Release.gpg [72 B]  
Ign http://ppa.launchpad.net trusty InRelease  
Hit http://extras.ubuntu.com trusty Release  
Hit http://ppa.launchpad.net trusty InRelease  
Hit http://extras.ubuntu.com trusty/main Sources  
Hit http://extras.ubuntu.com trusty/main amd64 Packages  
Hit http://ppa.launchpad.net trusty InRelease  
Hit http://extras.ubuntu.com trusty/main i386 Packages  
Ign http://packages.elastic.co stable InRelease  
Hit http://ppa.launchpad.net trusty Release.gpg  
Hit http://ppa.launchpad.net trusty Release.gpg  
Hit http://in.archive.ubuntu.com trusty Release.gpg  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/main Sources  
Get:2 https://packages.elastic.co stable InRelease  
Ign https://packages.elastic.co stable InRelease  
Ign https://packages.elastic.co stable InRelease  
Hit http://in.archive.ubuntu.com trusty-updates/restricted Sources  
Hit http://packages.elastic.co stable Release.gpg  
Ign https://packages.elastic.co stable Release.gpg  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Hit http://packages.elastic.co stable Release  
Ign https://packages.elastic.co stable Release.gpg  
Hit http://ppa.launchpad.net trusty/main Translation-en  
Hit http://packages.elastic.co stable/main amd64 Packages  
Ign https://packages.elastic.co stable Release  
Hit http://packages.elastic.co stable/main i386 Packages  
Ign https://packages.elastic.co stable Release  
Hit http://ppa.launchpad.net trusty Release.gpg  
Ign http://extras.ubuntu.com trusty/main Translation-en_IN  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Ign http://extras.ubuntu.com trusty/main Translation-en  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Hit http://ppa.launchpad.net trusty/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-updates/universe Sources  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Hit http://ppa.launchpad.net trusty/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-updates/multiverse Sources  
Hit http://in.archive.ubuntu.com trusty-updates/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/restricted amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/universe amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/multiverse amd64 Packages  
Hit http://ppa.launchpad.net trusty Release  
Hit http://in.archive.ubuntu.com trusty-updates/main i386 Packages  
Hit http://ppa.launchpad.net trusty Release  
Hit http://ppa.launchpad.net trusty Release  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Hit http://ppa.launchpad.net trusty/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-updates/restricted i386 Packages  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/universe i386 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/multiverse i386 Packages  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Ign http://packages.elastic.co stable/main Translation-en_IN  
Hit http://in.archive.ubuntu.com trusty-updates/main Translation-en  
Hit http://ppa.launchpad.net trusty/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-updates/multiverse Translation-en  
Hit http://ppa.launchpad.net trusty/main i386 Packages  
Ign http://packages.elastic.co stable/main Translation-en  
Hit http://ppa.launchpad.net trusty/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-updates/restricted Translation-en  
Hit http://in.archive.ubuntu.com trusty-updates/universe Translation-en  
Hit http://in.archive.ubuntu.com trusty-backports/main Sources  
Hit http://in.archive.ubuntu.com trusty-backports/restricted Sources  
Hit http://in.archive.ubuntu.com trusty-backports/universe Sources  
Hit http://in.archive.ubuntu.com trusty-backports/multiverse Sources  
Ign http://ppa.launchpad.net trusty/main Translation-en_IN  
Hit http://in.archive.ubuntu.com trusty-backports/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/restricted amd64 Packages  
Ign http://ppa.launchpad.net trusty/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-backports/universe amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/multiverse amd64 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/main i386 Packages           ^[[B
Hit http://in.archive.ubuntu.com trusty-backports/restricted i386 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/universe i386 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/multiverse i386 Packages  
Hit http://in.archive.ubuntu.com trusty-backports/main Translation-en  
Hit http://in.archive.ubuntu.com trusty-backports/multiverse Translation-en  
Hit http://in.archive.ubuntu.com trusty-backports/restricted Translation-en  
Hit http://in.archive.ubuntu.com trusty-backports/universe Translation-en  
Hit http://in.archive.ubuntu.com trusty Release  
Ign http://dl.google.com stable InRelease  
Hit http://dl.google.com stable Release.gpg  
Hit http://dl.google.com stable Release  
Hit http://in.archive.ubuntu.com trusty/main Sources  
Hit http://dl.google.com stable/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty/restricted Sources  
Hit http://in.archive.ubuntu.com trusty/universe Sources  
Hit http://in.archive.ubuntu.com trusty/multiverse Sources  
Hit http://in.archive.ubuntu.com trusty/main amd64 Packages  
Hit http://in.archive.ubuntu.com trusty/restricted amd64 Packages  
Hit http://in.archive.ubuntu.com trusty/universe amd64 Packages  
Hit http://in.archive.ubuntu.com trusty/multiverse amd64 Packages  
Hit http://in.archive.ubuntu.com trusty/main i386 Packages  
Err https://packages.elastic.co stable/main amd64 Packages  
  HttpError404
Hit http://in.archive.ubuntu.com trusty/restricted i386 Packages  
Err https://packages.elastic.co stable/main i386 Packages  
  HttpError404
Hit http://in.archive.ubuntu.com trusty/universe i386 Packages  
Ign http://dl.google.com stable/main Translation-en_IN  
Hit http://in.archive.ubuntu.com trusty/multiverse i386 Packages  
Ign https://packages.elastic.co stable/main Translation-en_IN  
Ign http://dl.google.com stable/main Translation-en  
Ign https://packages.elastic.co stable/main Translation-en  
Err https://packages.elastic.co stable/main amd64 Packages  
  HttpError404
Err https://packages.elastic.co stable/main i386 Packages  
  HttpError404
Ign https://packages.elastic.co stable/main Translation-en_IN  
Hit http://in.archive.ubuntu.com trusty/main Translation-en  
Ign https://packages.elastic.co stable/main Translation-en  
Hit http://in.archive.ubuntu.com trusty/multiverse Translation-en  
Hit http://in.archive.ubuntu.com trusty/restricted Translation-en  
Hit http://in.archive.ubuntu.com trusty/universe Translation-en  
Ign http://in.archive.ubuntu.com trusty/main Translation-en_IN  
Ign http://in.archive.ubuntu.com trusty/multiverse Translation-en_IN
Ign http://in.archive.ubuntu.com trusty/restricted Translation-en_IN
Ign http://in.archive.ubuntu.com trusty/universe Translation-en_IN
Fetched 72 B in 53s (1 B/s)
W: Failed to fetch https://packages.elastic.co/elasticsearch/2.3.1/debian/dists/stable/main/binary-amd64/Packages  HttpError404

W: Failed to fetch https://packages.elastic.co/elasticsearch/2.3.1/debian/dists/stable/main/binary-i386/Packages  HttpError404

W: Failed to fetch https://packages.elastic.co/elasticsearch/2.3/debian/dists/stable/main/binary-amd64/Packages  HttpError404

W: Failed to fetch https://packages.elastic.co/elasticsearch/2.3/debian/dists/stable/main/binary-i386/Packages  HttpError404

E: Some index files failed to download. They have been ignored, or old ones used instead.
</description><key id="163525272">19229</key><summary>Unable to install Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">muraliappiness</reporter><labels /><created>2016-07-02T16:21:46Z</created><updated>2016-07-02T20:51:59Z</updated><resolved>2016-07-02T19:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-02T19:50:32Z" id="230118912">I do not think that you literally followed the instructions on that page. I suspect that you replaced "2.x" by "2.3.1" and "2.3" that is leading to the HTTP 404 status codes. For clarity, here are the _exact_ commands that you need to execute:

```
$ wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
OK
$ echo "deb https://packages.elastic.co/elasticsearch/2.x/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list
deb https://packages.elastic.co/elasticsearch/2.x/debian stable main
vagrant@vagrant-ubuntu-trusty-64:~$ sudo apt-get update
Get:1 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
Get:2 http://security.ubuntu.com trusty-security/main Sources [117 kB]         
Ign http://archive.ubuntu.com trusty InRelease          
Get:3 https://packages.elastic.co stable InRelease                    
Ign https://packages.elastic.co stable InRelease                  
Get:4 http://security.ubuntu.com trusty-security/universe Sources [38.0 kB]
Get:5 https://packages.elastic.co stable Release                      
Get:6 http://security.ubuntu.com trusty-security/main amd64 Packages [501 kB]
Get:7 https://packages.elastic.co stable/main amd64 Packages
Get:8 http://archive.ubuntu.com trusty-updates InRelease [65.9 kB]            
Get:9 https://packages.elastic.co stable/main Translation-en_US     
Get:10 http://security.ubuntu.com trusty-security/universe amd64 Packages [130 kB]
Get:11 http://security.ubuntu.com trusty-security/main Translation-en [276 kB] 
Get:12 http://security.ubuntu.com trusty-security/universe Translation-en [77.1 kB]
Hit http://archive.ubuntu.com trusty Release.gpg                               
Get:13 http://archive.ubuntu.com trusty-updates/main Sources [277 kB]
Ign https://packages.elastic.co stable/main Translation-en_US    
Ign https://packages.elastic.co stable/main Translation-en
Get:14 http://archive.ubuntu.com trusty-updates/universe Sources [157 kB]
Get:15 http://archive.ubuntu.com trusty-updates/main amd64 Packages [788 kB]
Get:16 http://archive.ubuntu.com trusty-updates/universe amd64 Packages [362 kB]
Get:17 http://archive.ubuntu.com trusty-updates/main Translation-en [396 kB]
Get:18 http://archive.ubuntu.com trusty-updates/universe Translation-en [190 kB]
Hit http://archive.ubuntu.com trusty Release                           
Get:19 http://archive.ubuntu.com trusty/main Sources [1,064 kB]
Get:20 http://archive.ubuntu.com trusty/universe Sources [6,399 kB]
Hit http://archive.ubuntu.com trusty/main amd64 Packages                       
Hit http://archive.ubuntu.com trusty/universe amd64 Packages                   
Hit http://archive.ubuntu.com trusty/main Translation-en                       
Hit http://archive.ubuntu.com trusty/universe Translation-en                   
Ign http://archive.ubuntu.com trusty/main Translation-en_US                    
Ign http://archive.ubuntu.com trusty/universe Translation-en_US                
Fetched 10.9 MB in 8s (1,222 kB/s)                                             
Reading package lists... Done
$ sudo apt-get install elasticsearch
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 189 not upgraded.
Need to get 27.4 MB of archives.
After this operation, 30.8 MB of additional disk space will be used.
Get:1 https://packages.elastic.co/elasticsearch/2.x/debian/ stable/main elasticsearch all 2.3.3 [27.4 MB]
Fetched 27.4 MB in 10s (2,692 kB/s)                                            
Selecting previously unselected package elasticsearch.
(Reading database ... 86712 files and directories currently installed.)
Preparing to unpack .../elasticsearch_2.3.3_all.deb ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Unpacking elasticsearch (2.3.3) ...
Processing triggers for ureadahead (0.100.0-16) ...
Setting up elasticsearch (2.3.3) ...
Processing triggers for ureadahead (0.100.0-16) ...
$
```

If you do need to install Elasticsearch 2.3.1, you can execute:

```
$ sudo apt-get install elasticsearch=2.3.1
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BulkProcessor doesn't commit the transaction log even if flush() is called</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19228</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8

**OS version**: Mac 10.10.5

**Description of the problem including expected versus actual behavior**:
Expecting the bulkProcessor to commit the transaction log when calling flush()
but that's not the case

**Steps to reproduce**:
run following test code:
1. create an index request 
2. add it to a bulk processor
3. flush the bulk processor
4. refresh the same index
5. an exception will be thrown saying the index could not be found

```
public class testBulkProcessor extends ESIntegTestCase {
    @Test
    public void testBulkProcessor(){
        BulkProcessor bulkProcessor =
                BulkProcessor.builder(client(), new BulkProcessor.Listener() {
                    @Override
                    public void beforeBulk(long executionId, BulkRequest request) {
                    }
                    @Override
                    public void afterBulk(long executionId, BulkRequest request,
                            BulkResponse response) {
                    }
                    @Override
                    public void afterBulk(long executionId, BulkRequest request,
                            Throwable failure) {
                    }
                }).setBulkActions(1000)
                        .setBulkSize(new ByteSizeValue(5, ByteSizeUnit.MB)).setConcurrentRequests(2)
                        .build();
        IndexRequest indexRequest = new IndexRequest("test", "test", "1");
        indexRequest.source(new HashMap());
        bulkProcessor.add(indexRequest);
        bulkProcessor.flush();
        client().admin().indices().prepareRefresh("test").get();
    }
}
```

**Provide logs (if relevant)**:
[test] IndexNotFoundException[no such index]
the exception is thrown at 

```
client().admin().indices().prepareRefresh("test").get();
```
</description><key id="163491547">19228</key><summary>BulkProcessor doesn't commit the transaction log even if flush() is called</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">panfngsugar</reporter><labels /><created>2016-07-01T23:58:36Z</created><updated>2016-07-02T06:42:05Z</updated><resolved>2016-07-02T06:42:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-07-02T06:42:05Z" id="230087214">This is not a bug, just a wrong usage of the API. By setting `setConcurrentRequests(2)` the bulk processor is executing `bulkProcessor.flush()` asynchronously. This means that the index "test" is not created yet while you call refresh on it. Either set `setConcurrentRequests(0)` or wait on the `afterBulk` event using the `BulkProcessor.Listener` before doing a refresh.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Links broken on plugin authors doc page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19227</link><project id="" key="" /><description>URL: https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/plugin-authors.html

All the links on this page that contain `2.x` in them result in 404s. I'm guessing the fix here is to replaces the `2.x` references in this doc with `2.3`?
</description><key id="163490855">19227</key><summary>Links broken on plugin authors doc page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>bug</label><label>docs</label></labels><created>2016-07-01T23:47:51Z</created><updated>2016-07-01T23:50:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add IngestPlugin api for plugins to add ingest processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19226</link><project id="" key="" /><description>This change cuts ingest plugins over the new pull based plugins model.
</description><key id="163489705">19226</key><summary>Add IngestPlugin api for plugins to add ingest processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T23:31:33Z</created><updated>2016-07-06T04:04:44Z</updated><resolved>2016-07-06T04:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-07-03T20:19:26Z" id="230173106">LGTM (left one minor note)
</comment><comment author="rjernst" created="2016-07-06T04:04:39Z" id="230666202">Thanks @martijnvg, I added tests in f49ce8e
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>core/src/main/java/org/elasticsearch/ingest/IngestService.java</file><file>core/src/main/java/org/elasticsearch/ingest/Pipeline.java</file><file>core/src/main/java/org/elasticsearch/ingest/PipelineStore.java</file><file>core/src/main/java/org/elasticsearch/ingest/Processor.java</file><file>core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/NodeModule.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/main/java/org/elasticsearch/plugins/IngestPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/ConfigurationUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestServiceTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/AbstractStringProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/AppendProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ConvertProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/DateIndexNameProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/DateProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/FailProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/GrokProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/GsubProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/IngestCommonPlugin.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/JoinProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/RemoveProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/RenameProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ScriptProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SetProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SortProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SplitProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/AppendProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ConvertProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/DateIndexNameFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/DateProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/FailProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/GrokProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/GsubProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/JoinProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/LowercaseProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/RemoveProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/RenameProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ScriptProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SetProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SplitProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SplitProcessorTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/TrimProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/UppercaseProcessorFactoryTests.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/IngestAttachmentPlugin.java</file><file>plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java</file><file>plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java</file><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/IngestUserAgentPlugin.java</file><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/UserAgentProcessor.java</file><file>plugins/ingest-useragent/src/test/java/org/elasticsearch/ingest/useragent/UserAgentProcessorFactoryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/IngestTestPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java</file></files><comments><comment>Merge pull request #19226 from rjernst/ingest_plugin_api</comment></comments></commit></commits></item><item><title>Internal: Remove generics from LifecycleComponent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19225</link><project id="" key="" /><description>The only reason for LifecycleComponent taking a generic type was so that
it could return that type on its start and stop methods. However, this
chaining has no practical necessity. Instead, start and stop can be
void, and a whole bunch of confusing generics disappear.
</description><key id="163488889">19225</key><summary>Internal: Remove generics from LifecycleComponent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T23:20:24Z</created><updated>2016-07-04T14:24:26Z</updated><resolved>2016-07-01T23:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-07-01T23:22:26Z" id="230067100">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/DelayedAllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/component/LifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServerTransport.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>core/src/main/java/org/elasticsearch/monitor/MonitorService.java</file><file>core/src/main/java/org/elasticsearch/monitor/jvm/JvmGcMonitorService.java</file><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/Transport.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java</file><file>core/src/test/java/org/elasticsearch/cluster/NodeConnectionsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java</file><file>core/src/test/java/org/elasticsearch/test/NoopDiscovery.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/management/AzureComputeServiceImpl.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/management/AzureComputeServiceAbstractMock.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/AwsEc2ServiceMock.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/CapturingTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Merge pull request #19225 from rjernst/we_dont_need_generics</comment></comments></commit></commits></item><item><title>Open file descriptors go up after increasing index.merge.policy.max_merged_segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19224</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.3

**JVM version**: openjdk version "1.8.0_65"
OpenJDK Runtime Environment (build 1.8.0_65-b17)
OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)

**OS version**: Amazon linux 3.14.35-28.38.amzn1.x86_64

**Description of the problem including expected versus actual behavior**:
We have multi gigabyte indices in our cluster, and we started noticing degraded performance over time ( our response times degraded by 20ms-40ms each month ). 

In many of these indices, several segments had about 3gb of data, and percentage of deleted documents was about 1%-2%. I thought the default max size limit of 5gb was possibly getting in the way of merging those segments, so I decided to increase it up to 15G, expecting ES to merge more and decrease segments. But when I increased the max size to 15gb, open file descriptor usage climbed up from 3.4k range to 3.8k-4k range, and degraded our response times. No new merges happened as a result of this change.

When the limit was set back to 2G, merges started happening and open file descriptor usage dropped. Further decreasing the limit to 1G had the same effect. 

**Steps to reproduce**:
1. increase  index.merge.policy.max_merged_segment to 15G. 
2. watch how file descriptor usage goes up...

**Provide logs (if relevant)**:   I could not find anything in the logs to explain the behavior.
</description><key id="163481640">19224</key><summary>Open file descriptors go up after increasing index.merge.policy.max_merged_segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkelkarbv</reporter><labels><label>:Store</label><label>feedback_needed</label></labels><created>2016-07-01T22:04:06Z</created><updated>2017-05-17T09:14:51Z</updated><resolved>2017-05-17T09:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-04T14:23:51Z" id="230301909">&gt; But when I increased the max size to 15gb, open file descriptor usage climbed up from 3.4k range to 3.8k-4k range, and degraded our response times. No new merges happened as a result of this change.

You say that no new merges happened, but how long did you wait? I think the most likely explanation is that it started to merge eg 5 x 3GB shards, which increased the open file handles and put a lot of IO load on the system (thus affecting response times), but perhaps you didn't let it complete?
</comment><comment author="mkelkarbv" created="2016-07-05T15:23:35Z" id="230511214">@clintongormley I waited for an hour for merges to start, but there was no merge activity. the I/O also did not go up. 
</comment><comment author="clintongormley" created="2016-07-06T11:17:14Z" id="230744737">@mikemccand could you shed any light on this?
</comment><comment author="mikemccand" created="2016-07-06T13:12:38Z" id="230766810">Increasing `max_merged_segment` will not cause merges to occur ... there must also be a set of 10 (by default) segments around that same size in order for a merge to be picked, and there must be too many segments in the shard so that a merge is needed.  If you enable `TRACE` logging for `org.elasticsearch.index.engine.lucene.iw` it will give (very verbose!) details about how merges are being selected.

However, this change should not have caused an increase nor decrease in file descriptors.

Were there any other changes done?  Open scroll requests?  Ongoing indexing?  New shards?
</comment><comment author="mkelkarbv" created="2016-07-29T16:47:58Z" id="236232281">Indexing was happening all the time on this cluster. New shards were not being created. 
</comment><comment author="mkelkarbv" created="2016-08-08T19:03:19Z" id="238342359">is there a feedback on this? is there something we can tune?
</comment><comment author="colings86" created="2017-03-31T13:43:28Z" id="290715554">@mkelkarbv are you still seeing this issue on recent releases on Elasticsearch?</comment><comment author="colings86" created="2017-05-17T09:14:51Z" id="302033282">No further feedback, closing</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify TcpTransport interface by reducing send code to a single send method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19223</link><project id="" key="" /><description>Due to some optimization on the netty layer we had quite some code / cruft
added to the TcpTransport to allow for those optimizations. After cleaning
up BytesReference we can now move this optimization into TcpTransport and
have a simple send method on the implementation layer instead. This commit
adds a CompositeBytesReference that also allows message headers to be written
separately which simplify the header code as well since no skips are needed
anymore.
</description><key id="163481228">19223</key><summary>Simplify TcpTransport interface by reducing send code to a single send method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T22:00:24Z</created><updated>2016-07-05T06:33:19Z</updated><resolved>2016-07-05T06:33:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-01T22:01:36Z" id="230057013">@jasontedor can you take a look
</comment><comment author="jasontedor" created="2016-07-04T01:17:58Z" id="230187416">It looks good @s1monw; I left some minor comments.
</comment><comment author="s1monw" created="2016-07-04T09:27:34Z" id="230246725">@jasontedor I pushed updates
</comment><comment author="jasontedor" created="2016-07-04T17:02:53Z" id="230328372">It still looks good, I left a couple more minor comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchSecurityException.java</file><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRunnable.java</file><file>core/src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/TaskOperationFailure.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/NodeExplanation.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/allocation/TransportClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BackoffPolicy.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestHandler.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/Retry.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/AutoCreateIndex.java</file><file>core/src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>core/src/main/java/org/elasticsearch/action/support/DelegatingActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>core/src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/PlainListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportActions.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/BootstrapCheck.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/cli/MultiCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/SettingCommand.java</file><file>core/src/main/java/org/elasticsearch/cli/Terminal.java</file><file>core/src/main/java/org/elasticsearch/cli/UserException.java</file><file>core/src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskConfig.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskListener.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java</file><file>core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateFilter.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/DelayedAllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/BasePrimaryAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/MacAddressProvider.java</file><file>core/src/main/java/org/elasticsearch/common/NamedRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/Randomness.java</file><file>core/src/main/java/org/elasticsearch/common/UUIDGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReferenceStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/CompositeBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ReleasablePagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java</file><file>core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/component/LifecycleComponent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java</file><file>core/src/main/java/org/elasticsearch/common/compress/DeflateCompressor.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/ShapesAvailability.java</file><file>core/src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/hash/MurmurHash3.java</file><file>core/src/main/java/org/elasticsearch/common/inject/Scope.java</file><file>core/src/main/java/org/elasticsearch/common/inject/State.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java</file><file>core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java</file><file>core/src/main/java/org/elasticsearch/common/io/Channels.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>core/src/main/java/org/elasticsearch/common/lease/Releasables.java</file><file>core/src/main/java/org/elasticsearch/common/logging/ESLogger.java</file><file>core/src/main/java/org/elasticsearch/common/logging/LoggerMessageFormat.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTokenStream.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/ElasticsearchDirectoryReader.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/netty/KeepFrameDecoder.java</file><file>core/src/main/java/org/elasticsearch/common/network/Cidrs.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/text/Text.java</file><file>core/src/main/java/org/elasticsearch/common/text/UTF8SortedAsUnicodeComparator.java</file><file>core/src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/LocalTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/PortsRange.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/util/ByteArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java</file><file>core/src/main/java/org/elasticsearch/common/util/DoubleArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java</file><file>core/src/main/java/org/elasticsearch/common/util/FloatArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/IntArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/LongArray.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/AbstractRunnable.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/SuspendableRefContainer.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadContext.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>core/src/main/java/org/elasticsearch/discovery/AckClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReferenceStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/CompositeBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpHeader.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/AbstractBytesReferenceTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/CompositeBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file></files><comments><comment>Simplify TcpTransport interface by reducing send code to a single send method (#19223)</comment></comments></commit></commits></item><item><title>Thread Context should collect unique response headers in async actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19222</link><project id="" key="" /><description>The current implementation of the `ThreadContext` (#17804) works by having the context passed from thread-to-thread, and the last thread to write to it wins. In the case of async (e.g., scatter/gather) request/response schemes, this won't work if anything unique happens along the way.

This covers all current needs, but it could prove to be very useful to have it actually collect unique async response headers.
</description><key id="163480087">19222</key><summary>Thread Context should collect unique response headers in async actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-07-01T21:52:00Z</created><updated>2016-07-01T21:52:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>HistogramTests fail with seed B2FF9DFDD88D8DE6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19221</link><project id="" key="" /><description>They hit a nasty ArrayIndexOutOfBoundException.
</description><key id="163473691">19221</key><summary>HistogramTests fail with seed B2FF9DFDD88D8DE6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T21:03:41Z</created><updated>2016-07-29T11:10:45Z</updated><resolved>2016-07-05T08:01:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T21:03:54Z" id="230046907">This happens about once every 250 times or so.
</comment><comment author="clintongormley" created="2016-07-04T14:18:54Z" id="230300912">@colings86 please could you take a look
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java</file></files><comments><comment>[TEST] fix histogram test when extended bounds overlaps data</comment></comments></commit></commits></item><item><title>Migrate global, filter, and filters aggregation to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19220</link><project id="" key="" /><description>Once all of these are migrated we'll be able to remove aggregation's custom "streams" which function that same as NamedWriteable. It also allows us to make most of the fields on aggregations final which is rather nice.

Also starts to migrate MultiBucketAggregation.Bucket to Writeable, allowing the buckets to have immutable parts.
</description><key id="163472011">19220</key><summary>Migrate global, filter, and filters aggregation to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T20:50:46Z</created><updated>2016-07-04T13:07:29Z</updated><resolved>2016-07-04T13:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T20:50:52Z" id="230044552">@colings86 another one!
</comment><comment author="colings86" created="2016-07-04T08:12:48Z" id="230231193">Left a minor comment but LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate the cardinality, scripted_metric, and geo_centroid aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19219</link><project id="" key="" /><description>This is the last of the remaining "calc" aggregations.

Once all of these are migrated we'll be able to remove aggregation's custom "streams" which function that same as NamedWriteable. It also allows us to make most of the fields on aggregations `final` which is rather nice.
</description><key id="163463269">19219</key><summary>Migrate the cardinality, scripted_metric, and geo_centroid aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T19:50:06Z</created><updated>2016-07-04T12:51:49Z</updated><resolved>2016-07-04T12:51:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T19:50:19Z" id="230033590">@colings86 another one for you.
</comment><comment author="colings86" created="2016-07-04T08:16:27Z" id="230231878">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: Remove cyclic dependency between HttpServer and NodeService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19218</link><project id="" key="" /><description>NodeService has an "service attributes" map, which is only
set by HttpServer on start/stop. But the only thing it puts in this map
is already available as part of the HttpServer info which is added to
node info requests. This change removes the attributes map and removes
the dependency in HttpServer on NodeService.
</description><key id="163434571">19218</key><summary>Internal: Remove cyclic dependency between HttpServer and NodeService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T16:52:08Z</created><updated>2016-07-04T11:02:03Z</updated><resolved>2016-07-01T19:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-07-01T17:17:38Z" id="230001288">@javanna I pushed a change removing the unnecessary bwc.
</comment><comment author="javanna" created="2016-07-01T17:19:34Z" id="230001729">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/NodeInfoStreamingTests.java</file></files><comments><comment>Merge pull request #19218 from rjernst/http_server_without_node_service</comment></comments></commit></commits></item><item><title>Add doc values support to the _size field in the mapper-size plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19217</link><project id="" key="" /><description>This change activates the doc_values on the _size field for indices created after 5.0.0-alpha4.
It also adds a note in the breaking changes that explain the situation and how to get around it.

Closes #18334
</description><key id="163430177">19217</key><summary>Add doc values support to the _size field in the mapper-size plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugin Mapper Size</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T16:26:11Z</created><updated>2016-07-05T12:53:14Z</updated><resolved>2016-07-05T12:53:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T16:34:56Z" id="229991716">It looks like you fixed some line length violations so maybe you should remove lines from `checkstyle_suppressions.xml`?
</comment><comment author="jpountz" created="2016-07-01T21:29:25Z" id="230051336">LGTM, let's just do the version bump before merging it?
</comment><comment author="jimczi" created="2016-07-05T10:23:50Z" id="230442172">Thanks @nik9000 and @jpountz. 
I'll merge after https://github.com/elastic/elasticsearch/pull/19250
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/SizeFieldMapper.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file></files><comments><comment>Merge pull request #19217 from jimferenczi/mapper_size_docvalues2</comment></comments></commit></commits></item><item><title>Add embedded stash key support to rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19216</link><project id="" key="" /><description>This allowes embedding stash keys in string like `t${key}est`. This
allows simple string concatenation like acitons.

The test for this is in `ObjectPathTests` because `Stash` doesn't seem
to have a test on its own and it is simple enough to test embedded
stashes this way. And this is a way I expect them to be used eventually.

Required by #18585
</description><key id="163430155">19216</key><summary>Add embedded stash key support to rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T16:26:01Z</created><updated>2016-07-01T18:11:43Z</updated><resolved>2016-07-01T18:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T16:26:57Z" id="229989999">@javanna and @HonzaKral we discussed this a few hours ago. I'm still pushing all the tests through it but it seems to work.
</comment><comment author="javanna" created="2016-07-01T16:53:54Z" id="229995842">left a few comments, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use a static default precision for the cardinality aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19215</link><project id="" key="" /><description>Today the default precision for the cardinality aggregation depends on how many
parent bucket aggregations it had. The reasoning was that the more parent bucket
aggregations, the more buckets the cardinality had to be computed on. And this
number could be huge depending on what the parent aggregations actually are.

However now that we run terms aggregations in breadth-first mode by default when
there are sub aggregations, it is less likely that we have to run the cardinality
aggregation on kagilions of buckets. So we could use a static default, which will
be less confusing to users.
</description><key id="163418345">19215</key><summary>Use a static default precision for the cardinality aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T15:21:48Z</created><updated>2016-07-18T09:31:09Z</updated><resolved>2016-07-18T09:31:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-07-05T10:22:14Z" id="230441823">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java</file></files><comments><comment>Use a static default precision for the cardinality aggregation. #19215</comment></comments></commit></commits></item><item><title>Enable checkstyle ModifierOrder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19214</link><project id="" key="" /><description>This PR enables the Checkstyle [ModifierOrder](http://checkstyle.sourceforge.net/config_modifier.html) module and fix the codebase.
</description><key id="163407978">19214</key><summary>Enable checkstyle ModifierOrder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T14:31:43Z</created><updated>2016-07-01T14:58:10Z</updated><resolved>2016-07-01T14:58:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-07-01T14:39:32Z" id="229964297">LGTM. I tend to like to hedge the notes I add to `checkstyle.xml` with things like "it is not that the standard is perfect, but having a consistent order makes the code more readable and no other order is compellingly better than the standard".
</comment><comment author="tlrx" created="2016-07-01T14:49:04Z" id="229966681">Thanks @nik9000. I'll add your comment to the notes.
</comment><comment author="s1monw" created="2016-07-01T14:58:10Z" id="229969021">++
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Smoke Tester: Try to compile a plugin as part of tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19213</link><project id="" key="" /><description>In order to test if the repository has been created correctly and dependencies are available, the smoke test should also try to build a plugin. Alternatively there does not need to be any sourcecode or elasticsearch plugin, but just a project that has a fair share of elasticsearch dependencies, so we make sure those are downloaded correctly.

Reasoning is to prevent something like #19205 in the future
</description><key id="163406239">19213</key><summary>Smoke Tester: Try to compile a plugin as part of tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>adoptme</label><label>test</label></labels><created>2016-07-01T14:23:10Z</created><updated>2016-07-01T16:25:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove some unused parameters/methods/fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19212</link><project id="" key="" /><description>This PR removes some unused parameters, methods and private fields here and there. 

Warning: most boring spring-cleaning PR to review
</description><key id="163391015">19212</key><summary>Remove some unused parameters/methods/fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T13:01:44Z</created><updated>2016-07-01T15:09:06Z</updated><resolved>2016-07-01T15:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-01T13:08:16Z" id="229941910">looks great
</comment><comment author="nik9000" created="2016-07-01T13:08:21Z" id="229941929">You may want to have a look at [RedundantModifier](http://checkstyle.sourceforge.net/config_modifier.html#RedundantModifier).
</comment><comment author="tlrx" created="2016-07-01T13:17:41Z" id="229944060">&gt; You may want to have a look at RedundantModifier.

I didn't know about it, thanks. I think we can activate it even if it's just a matter of style.
</comment><comment author="jasontedor" created="2016-07-01T13:43:47Z" id="229950051">&gt; You may want to have a look at [RedundantModifier](http://checkstyle.sourceforge.net/config_modifier.html#RedundantModifier).

+1
</comment><comment author="jpountz" created="2016-07-01T14:55:16Z" id="229968212">LGTM
</comment><comment author="tlrx" created="2016-07-01T15:09:06Z" id="229971759">Thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add doc values support to the _size field, with fallback to fielddata for older index versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19211</link><project id="" key="" /><description>Since fielddata for numerics is not available in ES anymore this change adds a simple LongFieldCacheIndexFieldData that uses the
UninvertingReader to create the FieldCache. This class is only available in the mapper size plugin.

Closes #18334
</description><key id="163371562">19211</key><summary>Add doc values support to the _size field, with fallback to fielddata for older index versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugin Mapper Size</label><label>enhancement</label></labels><created>2016-07-01T10:55:50Z</created><updated>2016-07-01T16:31:03Z</updated><resolved>2016-07-01T16:26:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-07-01T11:51:46Z" id="229927706">Is this an april fools? We definitely shouldn't do this.
</comment><comment author="jimczi" created="2016-07-01T12:10:54Z" id="229930931">@rmuir lol ! Maybe we should not do it but this would mean no aggs or sort on the _size field unless the index has been created after alpha4. @clintongormley WDYT ? 
</comment><comment author="rmuir" created="2016-07-01T12:14:05Z" id="229931482">Yes, please. My concern is definitely use of the lucene fieldcache (UninvertingReader). I cannot think of any possible reason why we'd ever want to do that: simply put, nothing can justify its use. This is not even in lucene's trunk anymore!
</comment><comment author="nik9000" created="2016-07-01T12:24:35Z" id="229933582">As I see it we have a couple of options:
1. Remove the ability to sort and aggregate on size or
2. Have a way of building the appropriate data structures in 2.x so that aggregations and sorting keep working on upgrade to 5.0 or
3. Tell people that if they use aggregations or sorts on size they'll have to reindex once they upgrade or
4. Support the structures we have for indexes built in 2.x.

Am I missing something?
</comment><comment author="rmuir" created="2016-07-01T12:32:38Z" id="229935147">I'm confused why we wouldnt just try to recommend these users move to `ingest-attachments` anyway?

Either way: please, lets not bring UninvertingReader into this. I don't care what the consequences are.
</comment><comment author="clintongormley" created="2016-07-01T14:36:29Z" id="229963510">&gt; I'm confused why we wouldnt just try to recommend these users move to ingest-attachments anyway?

This is not to do with attachments (issue was incorrectly labelled), but with the `_size` metafield, which didn't support doc-values in 2.x.  This renders the field useless when upgrading to 5.x, unless we have a fallback to fielddata for old indices.

Chatting to @jpountz he says we could resurrect fielddata just for this plugin, but it would require a sizeable amount of effort to do so.  This is not a widely used plugin and we're not entirely sure how it is being used, so I suggest that we add doc values to `_size` now (if not there already), and we wait on adding the bwc layer until users ask for it.
</comment><comment author="nik9000" created="2016-07-01T14:43:52Z" id="229965356">&gt; I suggest that we add doc values to _size now (if not there already), and we wait on adding the bwc layer until users ask for it.

+1. So long as we leave a note that it is a breaking change and tell users how to get around it. I think the only option is to reindex.
</comment><comment author="jimczi" created="2016-07-01T16:26:44Z" id="229989953">&gt; I suggest that we add doc values to _size now (if not there already)

I opened a new PR here:
https://github.com/elastic/elasticsearch/pull/19217

&gt; and we wait on adding the bwc layer until users ask for it.

... so I am going to close this one until users ask for it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set the SC and QPC type always to `.percolator` in percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19210</link><project id="" key="" /><description>PR for #19130
</description><key id="163363279">19210</key><summary>Set the SC and QPC type always to `.percolator` in percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>review</label><label>v2.4.0</label></labels><created>2016-07-01T10:06:32Z</created><updated>2016-07-05T07:51:21Z</updated><resolved>2016-07-05T07:51:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-01T14:55:01Z" id="229968142">I need to refresh my acronyms. :)
LGTM
</comment><comment author="martijnvg" created="2016-07-01T14:56:48Z" id="229968655">&gt; I need to refresh my acronyms. 

These acronyms are bad, but otherwise the title would be too long. :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>move to elastic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19209</link><project id="" key="" /><description /><key id="163354256">19209</key><summary>move to elastic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stefansaye</reporter><labels /><created>2016-07-01T09:19:52Z</created><updated>2016-07-01T12:47:37Z</updated><resolved>2016-07-01T12:23:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-01T12:23:38Z" id="229933402">Hi @stefansaye 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>move to elastic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19208</link><project id="" key="" /><description /><key id="163354067">19208</key><summary>move to elastic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stefansaye</reporter><labels /><created>2016-07-01T09:18:55Z</created><updated>2016-07-01T13:48:16Z</updated><resolved>2016-07-01T12:07:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-01T12:07:48Z" id="229930370">Please ask this on discuss.elastic.co.
We can help there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recursive chown in init scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19207</link><project id="" key="" /><description>For some usecases (e.g. mounting existing data into a docker container) recursive chown for the init script is useful.
</description><key id="163347426">19207</key><summary>Recursive chown in init scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2016-07-01T08:43:06Z</created><updated>2016-10-16T18:47:58Z</updated><resolved>2016-10-13T15:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-26T12:56:45Z" id="235258817">LGTM, it's just odd that I see three commits in the PR but in the "files changed" it see only the changes from https://github.com/elastic/elasticsearch/pull/19207/commits/e21bea64cb41e1008db41729b0adbb46a0acec45. Can you please check why?
</comment><comment author="robin13" created="2016-07-26T12:58:50Z" id="235259332">I guess a bad fork-merge path on my part... but the lines changed is what matter eh? :)
</comment><comment author="robin13" created="2016-07-26T13:16:40Z" id="235263857">Jenkins test it
</comment><comment author="jasontedor" created="2016-07-31T15:06:35Z" id="236435019">There's not really anything that CI will catch with this change because tests are limited here, I think that instead a test needs to be added to the packaging tests.
</comment><comment author="jasontedor" created="2016-10-13T15:10:09Z" id="253542213">I think this pull request is broken (I think because you opened the pull request from your master branch and pushed commits to it for another pull request), the diff stats show 1646 commits across 5165 files.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Build: set group for client and sniffer, disable publishing for client-test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19206</link><project id="" key="" /><description>Build: set group for client and sniffer, disable publishing for client-test

Closes #19205
</description><key id="163344425">19206</key><summary>Build: set group for client and sniffer, disable publishing for client-test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T08:25:26Z</created><updated>2016-07-01T12:28:53Z</updated><resolved>2016-07-01T12:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-01T08:27:06Z" id="229887541">It looks good to me (as far as my Gradle knowledge can validate the change :p ). So have another reviewer might be better...
</comment><comment author="javanna" created="2016-07-01T08:27:48Z" id="229887693">@nik9000 can you have a look? I think this should have been part of #19032
</comment><comment author="spinscale" created="2016-07-01T08:49:51Z" id="229892875">Ran `gradle install` locally with this PR, and was put under 

```
~/.m2/repository/org/elasticsearch/client/rest/5.0.0-alpha4-SNAPSHOT
```

thx for fixing!
</comment><comment author="nik9000" created="2016-07-01T11:06:00Z" id="229920479">Lgtm. I should have done it when I moved it.
On Jul 1, 2016 4:49 AM, "Alexander Reelsen" notifications@github.com
wrote:

&gt; Ran gradle install locally with this PR, and was put under
&gt; 
&gt; ~/.m2/repository/org/elasticsearch/client/rest/5.0.0-alpha4-SNAPSHOT
&gt; 
&gt; ++
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19206#issuecomment-229892875,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLorYcUi9GC3X6w5m9LP-oyvXvExp_ks5qRNSzgaJpZM4JC7Hc
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19206 from javanna/fix/client_group_id</comment></comments></commit></commits></item><item><title>REST client is not released with the right groupId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19205</link><project id="" key="" /><description>Apparently we published the REST Client on maven central at https://repo1.maven.org/maven2/org/elasticsearch/rest/5.0.0-alpha4/

Which means that the artifact coordinates are `org.elasticsearch:rest:5.0.0-alpha4`.
But it should be `org.elasticsearch.client:rest:5.0.0-alpha4`.
</description><key id="163340039">19205</key><summary>REST client is not released with the right groupId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>build</label></labels><created>2016-07-01T07:57:28Z</created><updated>2016-07-01T12:29:50Z</updated><resolved>2016-07-01T12:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-01T08:04:26Z" id="229882961">Also reported at https://discuss.elastic.co/t/restclient-in-5-0-0-alpha4/54509
</comment><comment author="javanna" created="2016-07-01T12:29:50Z" id="229934599">This is fixed in our gradle configuration, also the artifacts for 5.0.0-alpha4 were added in the proper locations and the groupId was updated in their corresponding poms.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Build: set group for client and sniffer, disable publishing for client-test</comment></comments></commit></commits></item><item><title>Includes the index UUID in the _cat/indices API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19204</link><project id="" key="" /><description>Includes the index UUID in the _cat/indices API and adds tests for the _cat/indices functionality.

Closes #19132
</description><key id="163324189">19204</key><summary>Includes the index UUID in the _cat/indices API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-07-01T05:50:24Z</created><updated>2016-07-01T18:46:55Z</updated><resolved>2016-07-01T18:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-01T06:41:16Z" id="229866814">LGTM
</comment><comment author="abeyad" created="2016-07-01T13:47:15Z" id="229951048">thanks for the review @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestIndicesActionTests.java</file></files><comments><comment>Includes the index UUID in the _cat/indices API and adds tests</comment><comment>for the _cat/indices functionality.</comment></comments></commit></commits></item><item><title>Is there an api to get stopwords used by an anlyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19203</link><project id="" key="" /><description>Say I'm using http://localhost:9200/_analyze?analyzer=indonesian as an analyzer. Is there an api that I can use to get all the Indonesian stopwords used by this analyzer? If it doesn't exist, can we add one?

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="163290422">19203</key><summary>Is there an api to get stopwords used by an anlyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dzhang22</reporter><labels /><created>2016-06-30T23:43:39Z</created><updated>2016-07-01T08:57:01Z</updated><resolved>2016-07-01T08:57:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-01T08:57:01Z" id="229894424">There is no generic API to get stop words across all ways that analyzers can be constructed, so I doubt we can have such an API. The default list of stop words for indonesian is here if you are interested https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/id/stopwords.txt
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticsearchTimeoutException returning wrong status code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19202</link><project id="" key="" /><description>**Elasticsearch version**:
1.7.3

**Description of the problem**:
ElasticsearchTimeoutException currently does not  override the status() method from ElasticsearchException and because of that it returns INTERNAL_SERVER_ERROR(500) status code. 

Expected behavior is that it returns the REQUEST_TIMEOUT(408) status code.
</description><key id="163281071">19202</key><summary>ElasticsearchTimeoutException returning wrong status code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ramarajesh2112</reporter><labels><label>:Exceptions</label><label>feedback_needed</label></labels><created>2016-06-30T22:31:21Z</created><updated>2016-07-01T11:57:46Z</updated><resolved>2016-07-01T11:56:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-07-01T11:49:50Z" id="229927398">Hi @ramarajesh2112 

Can you give an example of a request where this manifests?
</comment><comment author="jasontedor" created="2016-07-01T11:56:09Z" id="229928418">[400-level status](https://tools.ietf.org/html/rfc7231#section-6.5) codes are for _client_ issues. In this case, a [408 status code](https://tools.ietf.org/html/rfc7231#section-6.5.7) would and does mean that the server timed out waiting for the client. However, [500-level status codes](https://tools.ietf.org/html/rfc7231#section-6.6) are for _server_ issues. The `ElasticsearchTimeoutException` is used internally when an internal operation times out so I think a 500-level status code is correct. As there is not a specific 500-level status code for this situation, the generic [500 status code](https://tools.ietf.org/html/rfc7231#section-6.6.1) seems apt.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent analysis service from eagerly registering default analyzer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19201</link><project id="" key="" /><description>The tests make references to PrebuiltAnalyzers.DEFAULT so tests fail. @s1monw 
</description><key id="163275647">19201</key><summary>Prevent analysis service from eagerly registering default analyzer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mitchswaldman</reporter><labels><label>:Analysis</label><label>Awaiting CLA</label><label>bug</label><label>review</label></labels><created>2016-06-30T21:55:12Z</created><updated>2016-12-20T13:06:58Z</updated><resolved>2016-12-20T13:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-08T08:36:33Z" id="231305634">this might not be relevant - seems like  #19163 isn't a bug, we will see...
</comment><comment author="karmi" created="2016-07-08T08:36:37Z" id="231305646">Hi @mitchswaldman, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/19201.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?
</comment><comment author="s1monw" created="2016-12-20T13:06:58Z" id="268238726">closing for now</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Skip the execution of an empty pipeline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19200</link><project id="" key="" /><description>main optimization: `sourceToMap` is not called, therefore avoiding creation of Map of Maps

Closes #19192.
</description><key id="163273766">19200</key><summary>Skip the execution of an empty pipeline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T21:43:37Z</created><updated>2016-07-01T16:15:06Z</updated><resolved>2016-07-01T16:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-06-30T22:45:26Z" id="229809900">&gt; therefore avoiding creation of Map of Maps

That's the main goal of it, but it is a shame that it can't skip the entire processing of it given that every node has the ability to make that decision.
</comment><comment author="martijnvg" created="2016-07-01T06:28:24Z" id="229864633">LGTM

&gt; That's the main goal of it, but it is a shame that it can't skip the entire processing of it given that every node has the ability to make that decision.

Today each node in the cluster can make this decision, because ingest pipelines are materialized in memory by the PipelineStore. We talked about not doing this if ingest is disabled and when that happens we wouldn't be able to make such a decision any more in a simple way. The cluster state does contain the pipelines but these are not materialized (just arrays of bytes, PipelineConfiguration). The reason why today we materialize the pipelines on all nodes, is to allow the simulate api to run on any node.

So we can work on making this optimization, but if somewhere down to line we only materialize the pipelines on ingest nodes then we may have to drop this optimization.
</comment><comment author="talevy" created="2016-07-01T15:53:47Z" id="229982538">&gt; So we can work on making this optimization, but if somewhere down to line we only materialize the pipelines on ingest nodes then we may have to drop this optimization.

which one should we do _now_? The strategy of attaching an empty pipeline to a workflow may become a common pattern for dealing with schema evolution
</comment><comment author="martijnvg" created="2016-07-01T15:58:15Z" id="229983620">&gt; which one should we do now?

The optimization in this PR. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java</file></files><comments><comment>Skip the execution of an empty pipeline (#19200)</comment></comments></commit></commits></item><item><title>Migrate more aggregations to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19199</link><project id="" key="" /><description>These are the first aggregations with multiple `InternalAggregation`s
backing the same `AggregationBuilder`. This required a change in the
register method's signature.
</description><key id="163268200">19199</key><summary>Migrate more aggregations to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T21:12:13Z</created><updated>2016-07-01T18:49:34Z</updated><resolved>2016-07-01T18:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-30T21:12:27Z" id="229790413">@colings86 another one for you!
</comment><comment author="colings86" created="2016-07-01T09:02:50Z" id="229895887">@nik9000 I left a comment on naming but otherwise its looking good
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate stats and extended stats to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19198</link><project id="" key="" /><description>Migrates the `stats` and `extended_stats` aggregations and pipeline
aggregations from the special purpose aggregations streams to
`NamedWriteable`. These are the first pipeline aggregations so this
adds the infrastructure to support both streams and `NamedWriteable`s
for pipeline aggregations.
</description><key id="163247275">19198</key><summary>Migrate stats and extended stats to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T19:23:42Z</created><updated>2016-07-01T13:20:35Z</updated><resolved>2016-07-01T13:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-30T19:24:17Z" id="229762481">@colings86 this is for you when you get a chance.
</comment><comment author="colings86" created="2016-07-01T08:51:45Z" id="229893215">LGTM
</comment><comment author="nik9000" created="2016-07-01T13:20:35Z" id="229944721">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Registering plugin using an embedded client node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19197</link><project id="" key="" /><description>Hi, we're using the Java client API as an embedded node.  

```
final Settings settings = Settings.settingsBuilder()
                                          .put("plugin.types", DeleteByQueryPlugin.class.getName())
                                          .loadFromPath(findConfigFile())
                                          .build();
        this.node = nodeBuilder().settings(settings).node();
        this.node.start();
        this.client = node.client();
```

We notice that the plugin is never registered.  Is there a way to register the plugin?
</description><key id="163241089">19197</key><summary>Registering plugin using an embedded client node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels /><created>2016-06-30T18:52:16Z</created><updated>2016-06-30T21:33:23Z</updated><resolved>2016-06-30T19:25:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johnament" created="2016-06-30T19:20:03Z" id="229761309">I was able to work around by creating a custom node class to override it, but its odd that plugins aren't loaded in embedded without installing the standalone node

```
    private static class FlerpNode extends Node {

        public FlerpNode(Settings preparedSettings) {
            super(InternalSettingsPreparer.prepareEnvironment(preparedSettings, null), Version.CURRENT, asList(DeleteByQueryPlugin.class));
        }
    }
```
</comment><comment author="jasontedor" created="2016-06-30T19:25:52Z" id="229762862">You did not post what version of Elasticsearch you're on, but `plugin.types` is no longer supported and the workaround that you're using is the only way to achieve your goal of using a plugin with an embedded node.
</comment><comment author="johnament" created="2016-06-30T19:31:27Z" id="229764254">Sorry, this is ElasticSearch 2.3.3, latest GA.

Is this approach future proof?  It looks like for instance 5.0 removes the version param

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/node/Node.java#L183
</comment><comment author="jasontedor" created="2016-06-30T20:04:03Z" id="229772259">Maybe you mean something different by "future proof" than I'm understanding, but I think it's fair to say that nothing in the Java API is future proof, especially across major versions.
</comment><comment author="johnament" created="2016-06-30T20:24:45Z" id="229778187">Ok.  So how do I get this in as a feature request, to register additional plugins via the embedded client?
</comment><comment author="dadoonet" created="2016-06-30T20:40:12Z" id="229782155">You can start a local standard node instance with all plugins you need then use a TransportClient in your application.
</comment><comment author="jasontedor" created="2016-06-30T20:45:21Z" id="229783480">&gt; Ok. So how do I get this in as a feature request, to register additional plugins via the embedded client?

That would duplicate #15107 and #15927 (and further links therein), which have been closed. You will see that the discussion there points you to the method that you discovered here.
</comment><comment author="johnament" created="2016-06-30T21:33:23Z" id="229795439">Ok, thanks for linking to this tickets.  I think I understand a bit better.  Would be a good thing to consider switching then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clean up BytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19196</link><project id="" key="" /><description>BytesReference should be a really simple interface, yet it has a gazillion
ways to achieve the same this. Methods like `#hasArray`, `#toBytesArray`, `#copyBytesArray`
`#toBytesRef` `#bytes` are all really duplicates. This change simplifies the interface
dramatically and makes implementations of it much simpler. All array access has been removed
and is streamlined through a single `#toBytesRef` method. Utility methods to materialize a
compact byte array has been added too for convenience.
</description><key id="163235456">19196</key><summary>Clean up BytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T18:41:21Z</created><updated>2016-07-29T11:43:43Z</updated><resolved>2016-07-01T14:09:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-30T18:42:19Z" id="229751293">@jpountz @rjernst can you take a look I think this needs good reviews
</comment><comment author="rjernst" created="2016-06-30T18:55:54Z" id="229755041">LGTM, much simpler!
</comment><comment author="jpountz" created="2016-07-01T06:52:01Z" id="229868501">I left some comments but I like the simplification a lot. This was much needed.
</comment><comment author="jpountz" created="2016-07-01T06:52:24Z" id="229868565">Why is it labeled as breaking?
</comment><comment author="s1monw" created="2016-07-01T07:36:43Z" id="229878019">&gt; Why is it labeled as breaking?

well it's an internal API break on the java level?
</comment><comment author="s1monw" created="2016-07-01T08:30:19Z" id="229888191">@jpountz I pushed new commits addressing all your comments 
</comment><comment author="jpountz" created="2016-07-01T08:40:28Z" id="229890508">LGTM
</comment><comment author="s1monw" created="2016-07-01T10:12:14Z" id="229910935">@jpountz I also removed the equals / hashcode impls for PagedBytesReference since they are as efficient as the defaults now.
</comment><comment author="jpountz" created="2016-07-01T10:17:10Z" id="229911874">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/text/Text.java</file><file>core/src/main/java/org/elasticsearch/common/text/UTF8SortedAsUnicodeComparator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/index/get/GetResult.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SourceToParse.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/put/RestPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/restore/RestRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/Template.java</file><file>core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion2x/CompletionSuggestion.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferBytesReference.java</file><file>core/src/test/java/org/elasticsearch/ESExceptionTests.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplanationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/storedscripts/GetStoredScriptRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/flush/SyncedFlushUnitTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/WritePipelineResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java</file><file>core/src/test/java/org/elasticsearch/action/main/MainActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexGraveyardTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexMetaDataTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>core/src/test/java/org/elasticsearch/common/ChannelsTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/AbstractBytesReferenceTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/BytesArrayTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/BytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/DeflateCompressedXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/GeoDistanceTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/ShapeRelationTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/SpatialStrategyTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/builders/AbstractShapeBuilderTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/AbstractWriteableEnumTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/common/transport/BoundTransportAddressTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/ThreadContextTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/cbor/JsonVsCborTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/smile/JsonVsSmileTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilterPathGeneratorFilteringTests.java</file><file>core/src/test/java/org/elasticsearch/deps/jackson/JacksonLocationTests.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexRequestBuilderIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/CombineFunctionTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/OperatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/ScoreModeTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FieldValueFactorFunctionModifierTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesRequestCacheTests.java</file><file>core/src/test/java/org/elasticsearch/indices/TermsLookupTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryTargetTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestStatsTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/NodeInfoStreamingTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/main/RestMainActionTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/support/RestTableTests.java</file><file>core/src/test/java/org/elasticsearch/search/DocValueFormatTests.java</file><file>core/src/test/java/org/elasticsearch/search/MultiValueModeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/internal/InternalSearchHitTests.java</file><file>core/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/searchafter/SearchAfterBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/slice/SliceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SortOrderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/AbstractSuggestionBuilderTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/WritableTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/phrase/DirectCandidateGeneratorTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/phrase/SmoothingModelTestCase.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotTests.java</file><file>core/src/test/java/org/elasticsearch/tasks/PersistedTaskInfoTests.java</file><file>core/src/test/java/org/elasticsearch/tasks/TaskIdTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/ChannelBufferBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateNoopIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestSearchTemplateAction.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/action/search/template/MultiSearchTemplateIT.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolateQuery.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorFieldMapper.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java</file><file>test/framework/src/main/java/org/elasticsearch/common/bytes/ByteBufferBytesReference.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/StreamsUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>test/framework/src/main/java/org/elasticsearch/test/store/MockFSDirectoryService.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Clean up BytesReference (#19196)</comment></comments></commit></commits></item><item><title> Add ranking evaluation API to Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19195</link><project id="" key="" /><description>This relates to #18798.

This effort tries to help downstream users figure out whether the ranking function they are using produces good enough search result rankings.

To validate our API does the right thing we focus on annotation based quality metrics only (Prec@, MRR, ERR, see also https://en.wikipedia.org/wiki/Mean_reciprocal_rank and https://en.wikipedia.org/wiki/Learning_to_rank#Evaluation_measures).

The API takes a set of queries, each with a set of search results, and their relevance wrt. to the query. We focus on binary, maybe graded relevancy for now, it's explicitly not a goal to have very generic ratings per result.

For simplicity reasons those evaluation datasets will be supplied at query time instead of being stored and accessible in a dedicated index. Supporting to resume quality computations is something we decided to care about once users start asking for it.

What we leave to the user:
- Logging queries. We assume that what most users want to do with that API is to figure out how well their mapping from user supplied query and all sorts of additional information works for ranking search results. As a result logging plain elastic search queries isn't very helpful. What might be helpful is logging query parameters, like e.g. in template requests. We keep that in mind as future work.
- Creating annotations. This step is left to the downstream user, who need to figure out how to assign some quality level to search results given a query. It's also left to the downstream user to build a query annotation UI.

As a second step we envision enabling users to automatically learn which weights to assign to query parameters.

As a third step we envision downstream users being able to monitor their ranking quality over time. This involves building a QA specific UI that helps talk to the API outlined above, maybe integrating watcher for constant monitoring.
</description><key id="163233979">19195</key><summary> Add ranking evaluation API to Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Rank Evaluation</label><label>feature</label></labels><created>2016-06-30T18:33:57Z</created><updated>2016-09-20T15:23:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-06-30T18:46:16Z" id="229752392">As a bit of real world validation of the above suggestion here's what Erik Bernhardson from Wikimedia, thanks @nik9000 for the introduction, told us about how Wikimedia's search team is doing ranking QA:

&gt; Real world data sets are a bit hard. We do collect a reasonable (0.5%) amount of click through 
&gt; and dwell time data from which to try and derive SAT (satisfied) clicks. Unfortunately the 
&gt; individual queries are considered [PII](https://en.wikipedia.org/wiki/Personally_identifiable_information) which means the data isn't able to be shared publicly. 
&gt; This data gets fed into our [relevance forge](https://github.com/wikimedia/wikimedia-discovery-relevanceforge) software, currently we use a calculation nominally 
&gt; called the [Paul Score](https://github.com/wikimedia/wikimedia-discovery-relevanceForge/blob/master/engineScore.py#L176-L208) after [Paul Nelson from Search Technologies](https://www.elastic.co/elasticon/conf/2016/sf/engine-scoring-and-predictive-analytics-to-boost-search-accuracy) that presented the idea at 
&gt; elasticon.
&gt; 
&gt; The basic concept is to take all queries and clicks from a user session and consider any click to 
&gt; be a vote for all queries in that session. A form of DCG is then calculated per-session based on 
&gt; this, and all the session scores are averaged together. This is a very basic method of handling 
&gt; query reformulation, along with not over weighting heavy users. It is somewhat useful, but has a 
&gt; very heavy preference for links that already show up in the top of the search rankings. For us we 
&gt; already see ~85% of clicks are to the top 3 [results](https://phabricator.wikimedia.org/T126522#2027738), so it's really only optimizing for the other 
&gt; 15% of clicks. We haven't started using this with autocomplete but it might be more useful there 
&gt; as there are more reformulations to work with.
&gt; 
&gt; The only thing i could offer up is the results from our labeling platform ([discernatron](https://discernatron.wmflabs.org/), [see also](https://github.com/wikimedia/wikimedia-discovery-discernatron) In 
&gt; Discernatron results sourced from our own engine, google, bing and duck duck go are labeled on 
&gt; a 4 point scale by human graders. This is a brand new platform that has very minimal usage so 
&gt; the data set is currently too small to be much use to anyone. The queries used here have been 
&gt; hand filtered to remove PII (10 to 20% of queries end up getting filtered) so are possible to be 
&gt; released as a public dataset. Sadly it only has ~70 sets of scores against 32 unique queries as of 
&gt; today. We are still iterating on the code behind the labeling platform before really pushing for it's 
&gt; usage. If the data might be interesting a relatively recent TSV export is [available](https://phabricator.wikimedia.org/P3255), but i doubt 
&gt; it's enough for you to do much. Once we get a reasonable amount of data here we will be using it 
&gt; for nDCG calculations in relevance forge, and possibly other algorithms (Yandex's [pFound](http://romip.ru/romip2009/15_yandex.pdf) is 
&gt; an interesting algo i've been looking at recently). In our ideal world we would be getting around 
&gt; 500 new queries graded this way every month, but we are a ways away from that reality.
&gt; 
&gt; I don't know if you are aware, but the presenter from NY Times at elasticon spent about 5 
&gt; minutes of his presentation [complaining that elastic didn't currently have anything like this built in](https://www.elastic.co/elasticon/conf/2016/sf/all-the-data-thats-fit-to-find-search-at-the-new-york-times). 
&gt; His name escapes me at the moment, but might be someone worth looking up and pinging as 
&gt; well.

 What metrics are you currently using?

&gt; Currently using Paul score and nDCG as mentioned. Considering pFound, but it's mostly an 
&gt; nDCG reformulation using binary judgements (probably click throughs) that takes into account 
&gt; how likely a user is to continue progressing down the search results. We will possibly find more 
&gt; as we have time to look around and evaluate things. There are plenty of different ways to 
&gt; generate a score, but most are quite similar to what we already calculate. Will have to figure out 
&gt; what it is we want to measure that isn't covered by the above three. We've only done initial steps 
&gt; with our relevance forge project so far, but we finally got some dedicated hardware to clone 
&gt; elasticsearch indicies to and run it on. Will be using it more over the coming months as we 
&gt; evaluate the change from tf/idf to bm25. If we come up with new requirements that lead us to 
&gt; investigating more algo's I'll ping you.

Can I make our conversation public on github so it gets tracked with the rank evaluation ticket?

&gt; No restrictions, it can all be replicated to the public issue.
&gt; 
&gt; One thing that would be important, at least for us, is for any kind of API to not just take search 
&gt; terms with a templated query, but entire generated queries. We have a decent bit of pre-
&gt; processing that happens on user queries before they get sent off to elasticsearch. Mostly things 
&gt; like custom boost's, but also special grammar, feature escaping (ex. we don't allow user specified 
&gt; boosts with ^, or grouping with parens), etc. Perhaps not directly useful to you, but our raw 
&gt; generated queries are available by attaching a cirrusDumpQuery query parameter to any [fulltext](https://en.wikipedia.org/wiki/Special:Search?search=morelike:California&amp;fulltext=1&amp;cirrusDumpQuery) 
&gt; [search](https://en.wikipedia.org/wiki/Special:Search?search=elasticsearch&amp;fulltext=1&amp;cirrusDumpQuery).
</comment><comment author="MaineC" created="2016-09-19T11:05:02Z" id="247965004">Keeping here for the record: After giving a talk on how to build personalisation features step by step starting with basic ES features last Thursday at BEDCon which included talks on how to evaluate search results there was quite some interest in this functionality from attendees. Was great being able to point them to the feature branch. Though unlikely it would be great to get some early feedback.
</comment><comment author="softwaredoug" created="2016-09-20T01:54:36Z" id="248180435">I have a question, in part because I'm trying to figure out how a product of mine ([Quepid](http://quepid.com)) would integrate with this API

My understanding is the following
- This feature logs query keyword strings (whatever is plugged in from a search template)
- This feature logs basically per-doc judgements or quality metrics, probably based on analytics. But also possibly based on manual human judgments.
- Per query, I can have a historic sense of NDCG, MRR, and other classic relevance metrics

Is it planned that I could use this feature to do test-driven relevancy? Let's say I want to tweak my query, analysis, (or really anything) and measure the impact by computing a new NDCG over the new results. Will that be a supported feature? ie a worbench mode? Or is this more about logging and reporting (and tweaking boosts automatically)? (I ask because the workbench thing is what Quepid does, and anything that can help me what's planned here would be helpful to me).
</comment><comment author="MaineC" created="2016-09-20T15:23:30Z" id="248335399">Hi Doug,

thanks for getting into this conversation. As this API really is in the early stages we are very much looking for input on how to improve it and make it work better with software like Quepid.

&gt; This feature logs query keyword strings (whatever is plugged in from a search template)

Logging is currently left to the user.

&gt; This feature logs basically per-doc judgements or quality metrics, probably based on analytics. But also possibly based on manual human judgments.

In it's current stage we assume that the user of this API provides human judgments. Reading through the Quepid tour I would imagine that this is equal to the query ratings you collect from your users.

&gt; Per query, I can have a historic sense of NDCG, MRR, and other classic relevance metrics

You'd execute a request against an Elasticsearch index that provides information on the queries to run for testing as well as the quality level of documents returned. As a result you'd get a relevance score. This would give you a sense of NDCG/MRR/etc. wrt. specific queries and documents that have been annotated. The way I would imagine someone to use this API would be to first execute the queries as they are against the index to get a sense of current NDCG/MRR/etc. Then change the queries executed (either by using another template but still the same parameters, or in a very simple setup generating new queries with existing parameters (semi-)automatically and re-executing these. Together with the quality level the API would give you a list of documents returned by your search queries that don't have any ratings yet so you can easily ship them off to manual inspection.

You can take a look at the current API request/response format here: https://github.com/elastic/elasticsearch/blob/feature/rank-eval/modules/rank-eval/src/main/java/org/elasticsearch/index/rankeval/RestRankEvalAction.java#L46

Hope this helps clear some confusion. Please don't hesitate to ask for further details. Also if you have any input on how to improve the API, on which other extensions would be helpful for you let us know. Would be great to receive feedback from you as soon as you start playing with this. If you want to get your hands dirty feel free to check out [our rank-eval module on the feature/rank-eval branch](https://github.com/elastic/elasticsearch/tree/feature/rank-eval/modules/rank-eval/src/main/java/org/elasticsearch/index/rankeval).

Again thanks for your feedback,
Isabel
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate sum, min, and max aggregations over to NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19194</link><project id="" key="" /><description>Migrate sum, min, and max aggregations over to NamedWriteable.
</description><key id="163233389">19194</key><summary>Migrate sum, min, and max aggregations over to NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T18:31:03Z</created><updated>2016-07-01T13:49:35Z</updated><resolved>2016-07-01T13:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-30T19:24:21Z" id="229762491">@colings86 this is for you when you get a chance.
</comment><comment author="colings86" created="2016-07-01T08:58:58Z" id="229894893">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested query against documents with multiple nested fields with same second-level field names only finds hits for the first-defined nested field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19193</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.3

**JVM version**: 1.7.0_101

**OS version**: Ubuntu 14.04.4 LTS

**Description of the problem including expected versus actual behavior**:
I create a document with several nested fields on it. Then I added second-level fields of the same name on each of those nested fields. Then I tried a nested query against each of the paths, and found that only the first-defined path succeeded

**Expected:** Performing a nested query when a document has multiple nested fields with the same second-level field names succeeds

**Actual:** I only get hits for matches on the nested field that was defined first.

**Steps to reproduce**:
1. Create an index and document type with nested field mappings. Add a document which adds a field of the same name to each nested field.

```
PUT /test_index

PUT /test_index/_mappings/test_doc
{
  "properties": {
    "field1": {
      "type": "nested"
    },
    "field2": {
      "type": "nested"
    },
    "field3": {
      "type": "nested"
    }
  }
}

PUT /test_index/test_doc/1
{
  "field1": {
    "value": "100"
  },
  "field2": {
    "value": "100"
  },
  "field3": {
    "value": "100"
  }
}
```

2 . Check that nested mappings are defined correctly

```
GET /test_index/_mappings/test_doc
```

```
{
   "test_index": {
      "mappings": {
         "test_doc": {
            "properties": {
               "field1": {
                  "type": "nested",
                  "properties": {
                     "value": {
                        "type": "string"
                     }
                  }
               },
               "field2": {
                  "type": "nested",
                  "properties": {
                     "value": {
                        "type": "string"
                     }
                  }
               },
               "field3": {
                  "type": "nested",
                  "properties": {
                     "value": {
                        "type": "string"
                     }
                  }
               }
            }
         }
      }
   }
}
```

3 . Try performing a nested filter query against the first-defined nested field. See that it correctly gets a hit.

```
GET /test_index/_search
{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "field1",
          "filter": {
              "term": { "value": "100" }
          }
        }
      }
    }
  }
}
```

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test_index",
            "_type": "test_doc",
            "_id": "1",
            "_score": 1,
            "_source": {
               "field1": {
                  "value": "100"
               },
               "field2": {
                  "value": "100"
               },
               "field3": {
                  "value": "100"
               }
            }
         }
      ]
   }
}
```

4 . Try performing a nested filter query against the the later-defined nested fields. See that they do not get hits.

```
GET /test_index/_search
{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "field2",
          "filter": {
              "term": { "value": "100" }
          }
        }
      }
    }
  }
}
```

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}

```

**Provide logs (if relevant)**:
</description><key id="163216991">19193</key><summary>Nested query against documents with multiple nested fields with same second-level field names only finds hits for the first-defined nested field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaimemarijke</reporter><labels /><created>2016-06-30T17:11:19Z</created><updated>2016-06-30T20:38:59Z</updated><resolved>2016-06-30T20:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-30T20:38:59Z" id="229781832">This was a known issue in 1.x, and was fixed in 2.x. See #8870
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ingest: Skip Empty Pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19192</link><project id="" key="" /><description>The idea here is to allow indexing to supply a _known_ pipeline that can be empty. If the pipeline is empty, then just skip it. That's it.

---

For example:

``` http
PUT /_ingest/pipeline/upgrade-my-api-2
{
  "description" : "Placeholder to upgrade from API v2 to current",
  "processors" : [ ]
}
```

While API v2 is the current version, the pipeline naturally does nothing, so the ingest pipeline should literally be skipped. But eventually, you can create an API v3, update the pipeline and then new indexing can be adjusted as needed (including index renames).

``` http
PUT /my-index-2/type/123?pipeline=upgrade-my-api-2
{
  "field" : "blah"
}
```

After v3 rolls out, the pipeline is replaced with something like (and it could be further modified for v4 to allow v2 to v4, for as long as a version needs to survive):

``` http
PUT /_ingest/pipeline/upgrade-my-api-2
{
  "description" : "Upgrade from API v2 to v3",
  "processors" : [
    {
      "rename": {
        "field": "field",
        "target_field": "another_name"
      }
    },
    {
      "set": {
        "field": "_index",
        "value": "my-index-3"
      }
    }
  ]
}
```

This enables secondary systems to be upgraded in a rolling fashion to enable temporary pipeline usage without any overhead burden while it's not needed.
</description><key id="163205896">19192</key><summary>Ingest: Skip Empty Pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2016-06-30T16:16:50Z</created><updated>2016-07-01T16:15:05Z</updated><resolved>2016-07-01T16:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-06-30T21:45:39Z" id="229798192">@pickypg, I've opened a PR ^^ to implement this edge-case optimization. It is as minimal as it can be. Basically, it avoids the creation of the Map of Maps for the IngestDocument (which is probably the most costly thing). It does not avoid the Ingest Service, so index requests still get routed to ingest nodes and all that comes with that.
</comment><comment author="clintongormley" created="2016-07-01T11:46:37Z" id="229926877">@talevy If the coordinating node has `node.ingest: true`, will ingest happen on that node, or will it be sent to some ingest node in round-robin fashion?  Just wondering if we should support a `pipeline_preference`  option, like we do for search.
</comment><comment author="talevy" created="2016-07-01T15:50:53Z" id="229981785">@clintongormley iirc, ingest will happen on the coordinating node of the index request. This may or may not be the same as the coordinating node of a bulk request. So, I believe it tags along to whatever preference behavior the regular index request adopts
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java</file></files><comments><comment>Skip the execution of an empty pipeline (#19200)</comment></comments></commit></commits></item><item><title>Add support for extracting ranges from percolator queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19191</link><project id="" key="" /><description>Also stop support percolator queries containing `range` queries with ranges based on `now`.
</description><key id="163197581">19191</key><summary>Add support for extracting ranges from percolator queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking</label></labels><created>2016-06-30T15:40:09Z</created><updated>2016-07-08T11:01:41Z</updated><resolved>2016-07-08T11:01:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-05T16:26:32Z" id="230529252">I don't understand how the percolator can correlate the ranges of values with the name of the field. For instance if I index a query on a `foo` field and try to percolate a document that contains a `bar` field, how does it know it should not try ot use the value of `bar` in the approximation query since it is a different field name? Similarly how does it work if a query has ranges on multiple field names?
</comment><comment author="martijnvg" created="2016-07-07T06:12:06Z" id="230987813">&gt; I don't understand how the percolator can correlate the ranges of values with the name of the field. For instance if I index a query on a foo field and try to percolate a document that contains a bar field, how does it know it should not try ot use the value of bar in the approximation query since it is a different field name?

So it doesn't know and in the case of your example it the approximation query will match and then we the MemoryIndex verifies that match it will not verify it and so it will never match.

The goal here is to do better for range queries than we do right now (in many cases marking the entire percolator query as failed). The should clause with range queries that we add to the approximation query will yield more false matches, then the should clause we add for evaluating the percolator queries that failed query extraction. So I think in general it will be better?

&gt; Similarly how does it work if a query has ranges on multiple field names?

If the document has at least one range that matches (not necessarily matching with the field) with what we recorded for the percolator query at index time the approximation query will match it. Depending if the range matches with the field and if it needs to match with the other ranges too (depending if ranges are part of a conjunction or disjunction) it will be become a match after it has been verified by the MemoryIndex. 

So in the previous approach we did record with what field a range belongs to, so it was less likely to be a false positive. The big down side is that we didn't have control over the amount of fields the `percolator` field type created, because for each number type + range field name combination we would introduce a two new field (from and to fields.

With this approach we will always have 12 fields for range queries. In retrospect I'm still not happy with the facts that we add 12 fields (it is still a lot) (field per number type) and that the approximation query is less accurate. I don't feel there is rush to get this in, there must be a better approach the handle range queries. 
</comment><comment author="jpountz" created="2016-07-07T16:31:41Z" id="231133754">Thanks for the explanation, it makes sense.

&gt; With this approach we will always have 12 fields for range queries. In retrospect I'm still not happy with the facts that we add 12 fields (it is still a lot) (field per number type) and that the approximation query is less accurate.

I have mixed feelings about it too. On the one hand I don't want to have one field per query field in order to index range queries, but on the other hand, using a field per field type introduces fuzziness that makes me a bit worried, both because it could be a nest for bugs, and also because there could certainly be patterns of range queries and/or documents that defeat this optimization by making queries always returned by the approximation.

I think we should put it on hold for now.
</comment><comment author="martijnvg" created="2016-07-07T19:11:03Z" id="231177758">@jpountz I do want to extract the `now` range validation from `PercolatorFieldMapper` and some of the refactoring here (mainly the move of `ExtractQueryTermsService#extractQueryTerms` to PercolatorFieldMapper) and put it into a new pr. Does that make sense?
</comment><comment author="jpountz" created="2016-07-07T21:49:15Z" id="231217804">+1
</comment><comment author="martijnvg" created="2016-07-08T11:01:41Z" id="231333253">Closing this PR, we should look into this optimization another time. Opened #19327 for the `now` range validation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for `teardown` section in REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19190</link><project id="" key="" /><description>This commits adds support for a `teardown` section that can be defined in REST tests to
clean up any items that may have been created by the test and are not cleaned up by
deletion of indices and templates.
</description><key id="163196340">19190</key><summary>Add support for `teardown` section in REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T15:35:11Z</created><updated>2016-07-02T13:12:41Z</updated><resolved>2016-06-30T15:51:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-06-30T15:46:39Z" id="229700618">LGTM
</comment><comment author="javanna" created="2016-07-01T07:44:26Z" id="229879407">Is the new teardown section already supported by the clients? Or do we have to declare it as a feature and add a skip section to any test that makes use of it (see https://github.com/elastic/elasticsearch/blob/master/test/framework/src/main/java/org/elasticsearch/test/rest/support/Features.java)?
</comment><comment author="jaymode" created="2016-07-01T11:34:32Z" id="229924996">I know @karmi has implemented support for it and I believe the other clients are also adding support for it. I don't know if adding it as a feature with skip is worth it since the clients are adding support for it if they haven't already
</comment><comment author="javanna" created="2016-07-01T11:42:01Z" id="229926153">ok sure if it doesn't cause any test failure and everybody is implementing it we should be good.
</comment><comment author="karmi" created="2016-07-02T13:12:41Z" id="230101131">@javanna, yes, among the language clients we decided that the best way forward would be to implement a proper xUnit style `teardown` to solve the problems that tests don't clean up after they run. I think @jaymode already works on implementing that in context of X-Pack I think.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/RestTestCandidate.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/TeardownSectionParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/RestTestSuite.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/TeardownSection.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/TeardownSectionParserTests.java</file></files><comments><comment>Merge pull request #19190 from jaymode/rest_teardown</comment></comments></commit></commits></item><item><title>Expose task information from NodeClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19189</link><project id="" key="" /><description>This exposes a method to start an action and return a task from
`NodeClient`. This allows reindex to use the injected `Client` rather
than require injecting `TransportAction`s
</description><key id="163186386">19189</key><summary>Expose task information from NodeClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T14:56:27Z</created><updated>2016-07-07T22:28:23Z</updated><resolved>2016-07-07T22:28:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-30T14:58:03Z" id="229684602">This should wait on #19170 because it allows us to drop the casts.

I'm not sure whether to mark this "plugins" or "internal". It is a step in allowing us to have a clean interface for plugins to create RestHandlers.
</comment><comment author="nik9000" created="2016-06-30T19:47:54Z" id="229768258">@rjernst this fixed reindex to not need the actions.
</comment><comment author="nik9000" created="2016-07-04T12:20:37Z" id="230278029">@rjernst, do you want to review this?
</comment><comment author="rjernst" created="2016-07-06T03:40:22Z" id="230663806">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>non-binary gender option in term aggr. example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19188</link><project id="" key="" /><description>Include ar least an 'other' category in the gender example for terms aggregations.
</description><key id="163160942">19188</key><summary>non-binary gender option in term aggr. example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lnwdr</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2016-06-30T13:07:19Z</created><updated>2016-07-01T12:54:25Z</updated><resolved>2016-07-01T12:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lnwdr" created="2016-06-30T13:12:15Z" id="229653515">I have singed the CLA. Is there a way to re-check it?
</comment><comment author="clintongormley" created="2016-06-30T13:43:32Z" id="229661923">Hi @lnwdr 

Thanks for the PR.  The change concerns me because it could easily be interpreted to mean that terms aggs now support an automatic `other` bucket, a much requested feature (see https://github.com/elastic/elasticsearch/issues/12411).

Given that this change doesn't add clarity (and actually introduces some confusion) I'm going to close it.  Thanks anyway
</comment><comment author="clintongormley" created="2016-06-30T13:49:30Z" id="229663567">If you'd like to update the PR using a name other than `other`, then I'd be happy to merge it.
</comment><comment author="s1monw" created="2016-06-30T13:53:52Z" id="229664866">@lnwdr another option is to change the example altogether something that is non gender? I mean we can use something like `genre` and we use `rock`, `folk`, `electronic` etc for the values? WDYT
</comment><comment author="lnwdr" created="2016-06-30T15:28:18Z" id="229694970">@clintongormley fair point. @s1monw's suggestion of using music genres is probably the way to go here since alternative terms to 'other' in a gender context would probably have the same problem. I'll update the PR accordingly, if that's ok.
</comment><comment author="s1monw" created="2016-06-30T17:03:10Z" id="229722586">&gt; @clintongormley fair point. @s1monw's suggestion of using music genres is probably the way to got here since alternative terms to 'other' in a gender context would probably also have the same problem. I'll update the PR accordingly, if that's ok.

++ thanks
</comment><comment author="clintongormley" created="2016-07-01T12:54:12Z" id="229939157">LGTM, thanks @lnwdr 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>non-binary gender option in term aggr. example (#19188)</comment></comments></commit></commits></item><item><title>Nested RemoteTransportExceptions flood the logs and fill the disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19187</link><project id="" key="" /><description>It happened during a rolling restart needed for a security upgrade. The cluster is running elastic 2.3.3.
All nodes are running the same JVM version (OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)).

A RemoteTransportException seemed to "loop?" between 2 nodes causing elastic to log bigger and bigger exception traces as a new RemoteException exception seemed to be created with the previous one carrying all its causes.

The first trace was (on elastic1045) : 

```
[2016-06-30 08:34:20,553][WARN ][org.elasticsearch        ] Exception cause unwrapping ran for 10 levels...
RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s][p]]]; nested: IllegalIndexShardStateException[CurrentState[POST_RECOVERY] operation only allowed when started/recovering, origin [PRIMARY]];
[[11 lines of Caused by: RemoteTransportException]]
Caused by: [itwiki_general_1415230945][[itwiki_general_1415230945][2]] IllegalIndexShardStateException[CurrentState[POST_RECOVERY] operation only allowed when started/recovering, origin [PRIMARY]]
        at org.elasticsearch.index.shard.IndexShard.ensureWriteAllowed(IndexShard.java:1062)
        at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:593)
        at org.elasticsearch.index.engine.Engine$Index.execute(Engine.java:836)
        at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:237)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:389)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:191)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

The second one (same root cause) appeared few ms after with also 12 causes.
The third and fourth ones had 14 causes, fifth and sixth 16 causes and so on...
The last one I've seen had 1982 chained causes.

The logs were nearly the same on elastic1036 (master) generating 27gig of logs in few minutes on both nodes.

Surprisingly the cluster was still performing relatively well with higher gc activity on these nodes.

Then (maybe 1 hour after the first trace) elastic1045 was dropped from the cluster:

```
[2016-06-30 09:48:25,953][INFO ][discovery.zen            ] [elastic1045] master_left [{elastic1036}{DUOG0aGqQ3Gajr_wcFTOyw}{10.64.16.45}{10.64.16.45:9300}{rack=B3, row=B, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
```

It was immediately re-added and the log flood stopped.

I'll comment on this ticket if it happens again.
</description><key id="163160298">19187</key><summary>Nested RemoteTransportExceptions flood the logs and fill the disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nomoa</reporter><labels><label>:Recovery</label><label>bug</label><label>feedback_needed</label><label>v2.4.0</label></labels><created>2016-06-30T13:04:04Z</created><updated>2016-08-22T14:31:24Z</updated><resolved>2016-08-22T14:31:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-30T15:16:26Z" id="229690872">I think that this is a manifestation of #12573. It can happen when the target node of a primary relocation takes a long time to apply the cluster state that contains the information that it has the new primary. It is fixed in the upcoming v5.0.0 (#16274). The question is why the node took so long to apply a cluster state update. Is there anything else in the logs that might indicate this? What is the time stamp of the last log entry that has one of those huge exception traces?
</comment><comment author="nomoa" created="2016-06-30T16:30:55Z" id="229713551">@ywelsch this is still unclear, the first trace was 08:34:20,553 and the last one was a StackOverflow at 09:48:19,128 : 

```
[2016-06-30 09:48:19,128][WARN ][action.bulk              ] [elastic1036] Failed to send response for indices:data/write/bulk[s]
java.lang.StackOverflowError
        at java.lang.Exception.&lt;init&gt;(Exception.java:84)
        at java.lang.RuntimeException.&lt;init&gt;(RuntimeException.java:80)
        at org.elasticsearch.ElasticsearchException.&lt;init&gt;(ElasticsearchException.java:84)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.&lt;init&gt;(NotSerializableExceptionWrapper.java:41)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:560)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.writeTo(NotSerializableExceptionWrapper.java:65)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:564)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.writeTo(NotSerializableExceptionWrapper.java:65)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:564)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.writeTo(NotSerializableExceptionWrapper.java:65)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:564)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.writeTo(NotSerializableExceptionWrapper.java:65)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:564)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)
        at org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.writeTo(NotSerializableExceptionWrapper.java:65)
        at org.elasticsearch.common.io.stream.StreamOutput.writeThrowable(StreamOutput.java:564)
        at org.elasticsearch.ElasticsearchException.writeTo(ElasticsearchException.java:226)

```

Note that It's happening again right now with two other nodes elastic1021 and elastic1036 (still master).

Unfortunately keeping the logs is difficult (disk full).
</comment><comment author="ywelsch" created="2016-06-30T17:55:20Z" id="229737835">It is tricky to verify that this is indeed #12573 (If so, we could think about backporting #16274). Once the exceptions start bubbling up, the nodes have up-to-date cluster states (i.e. the node with the primary relocation target now has the cluster state where primary relocation is completed). It’s just by unwinding the deep call stack of the recursive calls between the nodes where the exceptions are stacked on top of each other.

Is the rolling restart of the cluster completed by now? If so, are there any shard relocations in progress?

@bleskes thoughts?
</comment><comment author="nomoa" created="2016-06-30T18:43:36Z" id="229751653">Note that I'm pretty sure that the second time it happened the cluster was green but certainly with shards relocating.

I've restarted the master to schedule a new election, we'll monitor the cluster state and comment this ticket with any new relevant info.

I agree with you, I'm not sure that exception timestamp in the logs are relevant because it seems to be a recursion problem and most of the logs where generated by the circuit breaker in ExceptionsHelper#unwrapCause (I wonder if the same kind of circuit breaker should be added to the logger itself to avoid writing bazillions of Caused By lines).
</comment><comment author="jasontedor" created="2016-07-06T15:29:46Z" id="230808382">Can you share a pair of subsequent log messages, one with `n` causes, and the next with `n + 2` causes showing the full stack trace for each log entry?
</comment><comment author="nomoa" created="2016-07-06T17:51:06Z" id="230851520">It happened again today.
What seems to be clear is that it happens when the cluster goes back to green after a node restart thus when rebalacing starts.
Cluster settings:
- node_concurrent_recoveries: 3
- node_initial_primaries_recoveries: 3
- cluster_concurrent_rebalance: 16
- max_bytes_per_sec is pretty low at 20mb with 3 concurrent_streams (we encounter latency issues if we set more)

It's unclear to me how node_concurrent_recoveries and cluster_concurrent_rebalance interacts together. What happens if the cluster decide to rebalance more than 3 shards to the same node will node_concurrent_recoveries prevent this from happening?

I think that what saves us from OOM is a StackOverflow when the huge exception is serialized.

@jasontedor here is the first 4 log entries : (https://gist.github.com/nomoa/2ee1f8bb44a4c6c01c400787d66bc383)

Here the pattern seems to be 2 with 13 cause, 2 with 15 causes and so on...
The last one I've seen in the log before filling up the disk seemed to have 1085 causes. This single log entry was 54m of text...
</comment><comment author="ywelsch" created="2016-07-07T15:30:04Z" id="231114252">@nomoa I've back-ported the fix in #12573 to 2.4 (#19296). All information so far indicates that it is this issue you're experiencing. Unfortunately my back-port was too late to make it for 2.3.4. You will have to wait for 2.4.0 to test it out. In the meantime, I wonder if dedicated master nodes would help here. If I understand correctly, this issue appeared only when a primary shard on the master node was involved. As cluster states are first applied on all the other nodes before it's applied on the master node, and if cluster state application is slow (due to large number of indices / shards etc.), having dedicated master nodes might decrease the time in which cluster states are out of sync on the nodes holding the primary relocation source and target.

Might also be interesting to increase logging level of "org.elasticsearch.cluster.service" to DEBUG to see how long nodes take to apply the cluster state (messages of the form "processing [{}]: took {} done applying updated cluster_state").
</comment><comment author="nomoa" created="2016-07-07T15:49:55Z" id="231121279">@ywelsch awesome, thanks for the backport.

Yes it always happened on shards where the master was involved, and if I understood correctly this specific issue could happen between two data nodes. Note that It's not the first time we suspect the master being too busy to act properly.

Moving to a dedicated master node is on our todo list, thanks for the suggestions.
</comment><comment author="rohit01" created="2016-08-22T06:29:22Z" id="241324784">Happening for me as well. I have disabled logging for the time being. Waiting for ES 2.4.0 :)
</comment><comment author="clintongormley" created="2016-08-22T14:31:19Z" id="241431413">Closed by https://github.com/elastic/elasticsearch/pull/19296

Once 2.4.0 is out, please ping on this ticket if you're still seeing the same issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate discovery-azure and rename it to discovery-azure-classic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19186</link><project id="" key="" /><description>As discussed at https://github.com/elastic/elasticsearch-cloud-azure/issues/91#issuecomment-229113595, we know that the current `discovery-azure` plugin only works with Azure Classic VMs / Services (which is somehow Legacy now).

The proposal here is to rename `discovery-azure` to `discovery-azure-classic` in case some users are using it.
And deprecate it for 5.0.

Closes #19144.
</description><key id="163156341">19186</key><summary>Deprecate discovery-azure and rename it to discovery-azure-classic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>deprecation</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T12:44:26Z</created><updated>2016-06-30T14:40:36Z</updated><resolved>2016-06-30T13:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-06-30T13:06:46Z" id="229652179">Left two comments, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix explain in function_score if no function filter matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19185</link><project id="" key="" /><description>When each function in function_score has a filter but none of them matches
we always assume 1 for the combined functions and then combine that with the
sub query score.
But the explanation did not reflect that because in case no function matched
we did not even use the actual score that was computed in the explanation.
</description><key id="163153918">19185</key><summary>fix explain in function_score if no function filter matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T12:31:39Z</created><updated>2016-07-28T11:14:09Z</updated><resolved>2016-07-28T11:14:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-07-26T13:12:27Z" id="235262714">Left two very minor comments. Otherwise LGTM.
</comment><comment author="brwe" created="2016-07-27T17:05:54Z" id="235652268">@danielmitterdorfer thanks a lot for the review! addressed all comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java</file></files><comments><comment>fix explain in function_score if no function filter matches (#19185)</comment></comments></commit></commits></item><item><title>Also support query term extract for queries wrapped inside a FunctionScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19184</link><project id="" key="" /><description>Additionally for highlighting percolator hits, also extract percolator query from FunctionScoreQuery and DisjunctionMaxQuery
</description><key id="163153210">19184</key><summary>Also support query term extract for queries wrapped inside a FunctionScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T12:27:52Z</created><updated>2016-07-08T08:56:01Z</updated><resolved>2016-07-08T08:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-08T08:32:58Z" id="231304936">Left one suggestion, looks good otherwise.
</comment><comment author="martijnvg" created="2016-07-08T08:56:01Z" id="231309394">Pushed via: https://github.com/elastic/elasticsearch/commit/7b8ae54f0fa25e53c7953a057d863b7a571a6ec6
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filebeat exclude_lines prior to multiline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19183</link><project id="" key="" /><description>Filebeat (1.2.3) feature request to drop lines prior to being included in a multiline message. According to the docs exclude_lines is done after the multiline is combined.
</description><key id="163140401">19183</key><summary>Filebeat exclude_lines prior to multiline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ptrlv</reporter><labels /><created>2016-06-30T11:12:59Z</created><updated>2016-06-30T11:33:18Z</updated><resolved>2016-06-30T11:33:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-30T11:33:18Z" id="229633320">The [Beats repository](https://github.com/elastic/beats/) is distinct from the Elasticsearch repository. You'll have better luck if you open an issue there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Not able to install plugin. &lt;java.lang.classcastException&gt;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19182</link><project id="" key="" /><description>**Elasticsearch version**: 1.0.2

**JVM version**: java 7

**OS version**:  Windows 7
**Issue**
I am trying to install plugin (which is just a custom Analyzer) its giving this error

 com.analyzer.AnalyzerPlugin cannot be cast to org.elasticsearch.plugins.Plugin   at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:505)

what could be the possible reason of it?

**logs**:

[2016-06-30 16:03:22,796][INFO ][node                     ] [Whirlwind] version[1.0.2], pid[17104], build[4fc0da7/2014-03-25T15:41:15Z]
[2016-06-30 16:03:22,796][INFO ][node                     ] [Whirlwind] initializing ...
[2016-06-30 16:03:22,804][WARN ][plugins                  ] [Whirlwind] failed to load plugin from [file:/C:/Users/user/Downloads/elasticsearch-1.0.2/plugins/analyzer/es-plugin.properties]
org.elasticsearch.ElasticsearchException: Failed to load plugin class [com.analyzer.AnalyzerPlugin]
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:516)
        at org.elasticsearch.plugins.PluginsService.loadPluginsFromClasspath(PluginsService.java:397)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:100)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:143)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:68)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:201)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.ClassCastException: com.analyzer.AnalyzerPlugin cannot be cast to org.elasticsearch.plugins.Plugin
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:505)
        ... 7 more
[2016-06-30 16:03:22,808][INFO ][plugins                  ] [Whirlwind] loaded [], sites []
[2016-06-30 16:03:25,507][INFO ][node                     ] [Whirlwind] initialized
[2016-06-30 16:03:25,507][INFO ][node                     ] [Whirlwind] starting ...
[2016-06-30 16:03:25,602][INFO ][transport                ] [Whirlwind] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.224.242.131:9300]
}
[2016-06-30 16:03:28,765][INFO ][cluster.service          ] [Whirlwind] new_master [Whirlwind][oCo73HnaTNiLzpu-M7YsHw][WBHATVW7HYD][inet[/10.224.242.131:9300]],
 reason: zen-disco-join (elected_as_master)
[2016-06-30 16:03:28,816][INFO ][discovery                ] [Whirlwind] elasticsearch/oCo73HnaTNiLzpu-M7YsHw
[2016-06-30 16:03:28,849][INFO ][gateway                  ] [Whirlwind] recovered [0] indices into cluster_state
[2016-06-30 16:03:28,870][INFO ][http                     ] [Whirlwind] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.224.242.131:9200]
</description><key id="163137731">19182</key><summary>Not able to install plugin. &lt;java.lang.classcastException&gt;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vinay992002</reporter><labels /><created>2016-06-30T10:57:11Z</created><updated>2016-06-30T11:18:50Z</updated><resolved>2016-06-30T11:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-30T11:18:24Z" id="229630720">Your plugin is incorrect probably.

Ask this on discuss.elastic.co please. We'll help you there.
</comment><comment author="jasontedor" created="2016-06-30T11:18:50Z" id="229630800">Presumably because the plugin does not conform to the plugin API for the version of Elasticsearch that you're trying to install on. Either way, the version of Elasticsearch that you're running is woefully old and is unsupported. You should upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to free  up space once you encounter: ][INFO ][cluster.routing.allocation.decider] [usdrst05] low disk watermark [15%] exceeded on [9KjGeluVg][usdnrst05] free: 146.4gb[14.5%], replicas will not be assigned to this node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19181</link><project id="" key="" /><description>How do you reclaim space once you encounter 

][INFO ][cluster.routing.allocation.decider] [usdrst05] low disk watermark [15%] exceeded on [9KjGelExXuVg][usdrst05] free: 146.4gb[14.5%], replicas will not be assigned to this node

  1008G  811G  147G  85% /elasticsearch/data

On all my other nodes the indices take up the following:

@usnst03:/elasticsearch/data/prod/nodes/0/indices$ du -sh 
99G     report_us3e_v2
@usnrst03:/elasticsearch/data/prod/nodes/0/indices$ du -sh 
322G    report_us3_v2
@usnrst03:/elasticsearch/data/prod/nodes/0/indices$ du -sh 
41G     report_log_us3e_v2
@usnrst03:/elasticsearch/data/prod/nodes/0/indices$

On this node the indicies space is as follows:

@usnst05:/elasticsearch/data/prod/nodes/0/indices# du -sh 
629G    report__us3_v2
root@usnres05:/elasticsearch/data/prod_us_nres/nodes/0/indices# du -sh 
53G     report_us3e_v2
root@usnres05:/elasticsearch/data/prod_us_nres/nodes/0/indices# du -sh 
130G    report_us3e_v2

I was able to stop and start the service to get things back to allocating however how do I free up space ?
</description><key id="163135804">19181</key><summary>How to free  up space once you encounter: ][INFO ][cluster.routing.allocation.decider] [usdrst05] low disk watermark [15%] exceeded on [9KjGeluVg][usdnrst05] free: 146.4gb[14.5%], replicas will not be assigned to this node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">choll33</reporter><labels /><created>2016-06-30T10:46:04Z</created><updated>2016-06-30T11:06:16Z</updated><resolved>2016-06-30T11:06:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-30T11:06:16Z" id="229628597">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Doc] Document Update/Delete-By-Query with version number zero</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19180</link><project id="" key="" /><description>Update-By-Query and Delete-By-Query use internal versioning to update/delete documents. But documents can have a version number equal to zero using the external versioning... making the UBQ/DBQ request fail because zero is not a valid version number and they only support internal versioning for now. Sequence numbers might help to solve this issue in the future.

Related to #16654 and  #18750
</description><key id="163135357">19180</key><summary>[Doc] Document Update/Delete-By-Query with version number zero</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Reindex API</label><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T10:43:17Z</created><updated>2016-06-30T13:52:09Z</updated><resolved>2016-06-30T13:51:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T13:27:50Z" id="229657557">Some minor comments, but LGTM
</comment><comment author="tlrx" created="2016-06-30T13:52:02Z" id="229664349">Thanks @clintongormley ! I merged with your comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19180 from tlrx/doc-version-number-zero-with-dbq-and-ubq</comment></comments></commit></commits></item><item><title>Not able to install plugin. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19179</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="163134833">19179</key><summary>Not able to install plugin. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vinay992002</reporter><labels /><created>2016-06-30T10:40:13Z</created><updated>2016-06-30T10:52:20Z</updated><resolved>2016-06-30T10:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-30T10:52:20Z" id="229625919">Closing since there is no context information about any potential bug here. If you want to ask general questions about using Elasticsearch please use the forum at https://discuss.elastic.co/.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename Search Template REST spec names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19178</link><project id="" key="" /><description>Many REST API specs are named according to their domain of competence: `snapshot.*.json`, `indices.*.json`, `tasks.*.json` etc

This commit renames Search Template specs so that they now start with `search.template.` prefix. It also removes a duplicate `search_template.json` (replaced by `search.template.json`)
</description><key id="163124680">19178</key><summary>Rename Search Template REST spec names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T09:46:17Z</created><updated>2016-07-04T08:10:00Z</updated><resolved>2016-07-04T08:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T13:16:50Z" id="229654641">@tlrx I wouldn't do this. The dotted portions of the name represent namespaces in the client. By changing `search_template` to `search.template` we now need to add yet another namespace, which contains very few methods. 
</comment><comment author="tlrx" created="2016-06-30T13:33:11Z" id="229659022">I wanted to improve the naming because files like `get_template.json` actually refers to /_search/template/ endpoint and I think it's not obvious at the first sight.

There's still a duplicate between `search_template.json` and `template.search.json` so if I follow your point I should remove the latter, as well as renaming `template.msearch.json` to `msearch_template.json`... right?
</comment><comment author="clintongormley" created="2016-06-30T15:12:20Z" id="229689612">OK I've just looked through all of the various template APIs.  I think we leave get/put/delete_template as they are, leave search_template as it is, and rename template.msearch -&gt; msearch_template
</comment><comment author="tlrx" created="2016-06-30T15:38:30Z" id="229698148">Thanks @clintongormley . I updated the tests and specs according to your comment.
</comment><comment author="clintongormley" created="2016-07-01T11:33:28Z" id="229924833">LGTM - I would just rename the `template.msearch` test folder to `msearch_template`
</comment><comment author="tlrx" created="2016-07-04T08:10:00Z" id="230230662">Thanks @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove generics from ingest Processor.Factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19177</link><project id="" key="" /><description>The factory for ingest processor is generic, but that is only for the
return type of the create method. However, the actual consumer of the
factories only cares about Processor, so generics are not needed.

This change removes the generic type from the factory. It also removes
AbstractProcessorFactory which only existed in order pull the optional
tag from config. This functionality is moved to the caller of the
factories in ConfigurationUtil, and the create method now takes the tag.
</description><key id="163122800">19177</key><summary>Remove generics from ingest Processor.Factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T09:36:52Z</created><updated>2016-07-01T11:32:22Z</updated><resolved>2016-06-30T15:08:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-30T09:40:22Z" id="229611614">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/ingest/AbstractProcessorFactory.java</file><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>core/src/main/java/org/elasticsearch/ingest/Processor.java</file><file>core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java</file><file>core/src/main/java/org/elasticsearch/node/NodeModule.java</file><file>core/src/test/java/org/elasticsearch/ingest/ConfigurationUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/AbstractStringProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/AppendProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ConvertProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/DateIndexNameProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/DateProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/FailProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ForEachProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/GrokProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/GsubProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/JoinProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/LowercaseProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/RemoveProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/RenameProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ScriptProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SetProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SortProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/SplitProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/TrimProcessor.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/UppercaseProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/AppendProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ConvertProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/DateIndexNameFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/DateProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/FailProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ForEachProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/GrokProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/GsubProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/JoinProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/LowercaseProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/RemoveProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/RenameProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ScriptProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SetProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SplitProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/SplitProcessorTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/TrimProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/UppercaseProcessorFactoryTests.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java</file><file>plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java</file><file>plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/IngestTestPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java</file></files><comments><comment>Merge pull request #19177 from rjernst/ingest_factory_generic</comment></comments></commit></commits></item><item><title>Fix NPE when GCE region is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19176</link><project id="" key="" /><description>When GCE region is empty we get back from the API something like:

```
{
  "id": "dummy"
}
```

instead of:

```
{
  "id": "dummy",
  "items":[ ]
}
```

This generates a NPE when we aggregate all the lists into a single one.

Closes #16967.
</description><key id="163118678">19176</key><summary>Fix NPE when GCE region is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery GCE</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T09:15:51Z</created><updated>2016-06-30T09:44:55Z</updated><resolved>2016-06-30T09:44:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-30T09:23:44Z" id="229607950">Left minor comment, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix NPE when GCE region is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19175</link><project id="" key="" /><description>When GCE region is empty we get back from the API something like:

```
{
  "id": "dummy"
}
```

instead of:

```
{
  "id": "dummy",
  "items":[ ]
}
```

This generates a NPE when we aggregate all the lists into a single one.

Closes #16967.
</description><key id="163118560">19175</key><summary>Fix NPE when GCE region is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>bug</label><label>review</label><label>v2.4.0</label></labels><created>2016-06-30T09:15:11Z</created><updated>2016-06-30T09:40:17Z</updated><resolved>2016-06-30T09:40:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-30T09:17:04Z" id="229606430">Also created PR for master branch with #19176 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update missing-query.asciidoc remove semicolon at line 10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19174</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?

remove semicolon from line 10
</description><key id="163109801">19174</key><summary>Update missing-query.asciidoc remove semicolon at line 10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lostsquirrel</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-06-30T08:30:40Z</created><updated>2016-09-12T22:06:29Z</updated><resolved>2016-09-12T22:06:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T13:10:39Z" id="229653114">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-09-12T22:06:29Z" id="246510270">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature request]add a key "description" when create a index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19173</link><project id="" key="" /><description>**Describe the feature**:
currently when u create a index, u can specify num of shards, num of replicas and mapping. 
I'd like suggest that we can add a key called "description" which is used to add some comments when create index and it is used this way:

curl -XPUT 'http://localhost:9200/twitter/' -d '{
    "description" : "this is book info index etc"
}'
</description><key id="163091903">19173</key><summary>[feature request]add a key "description" when create a index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-06-30T06:40:31Z</created><updated>2016-06-30T07:27:14Z</updated><resolved>2016-06-30T07:27:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-30T06:47:33Z" id="229574265">have you considered using the meta field: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-meta-field.html ? It's not exactly what you ask as it is per type and not per index, but it might get you what you need.
</comment><comment author="makeyang" created="2016-06-30T06:51:39Z" id="229574885">it could be used. thanks
</comment><comment author="s1monw" created="2016-06-30T07:27:14Z" id="229580923">there seems to be a solution - if it won't work please reopen thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a BridgePartition to be used by testAckedIndexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19172</link><project id="" key="" /><description>We have long worked to capture different partitioning scenarios in our testing infra. This PR adds a new variant, inspired by the Jepsen blogs, which was forgotten far - namely a partition where one node can still see and be seen by all other nodes. It also updates the resiliency page to better reflect all the work that was done in this area.
</description><key id="163091196">19172</key><summary>Add a BridgePartition to be used by testAckedIndexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>resiliency</label><label>test</label></labels><created>2016-06-30T06:34:35Z</created><updated>2016-06-30T15:58:12Z</updated><resolved>2016-06-30T15:58:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T08:16:40Z" id="229592822">Couple of minor comments, otherwise LGTM
</comment><comment author="jasontedor" created="2016-06-30T11:46:45Z" id="229635706">@bleskes The code looks good and I left some suggestions on the docs.
</comment><comment author="bleskes" created="2016-06-30T15:20:12Z" id="229692387">@jasontedor I pushed another update, can you take a look?
</comment><comment author="jasontedor" created="2016-06-30T15:21:38Z" id="229692941">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/common/BooleansTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/ReindexScriptTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/disruption/BridgePartition.java</file></files><comments><comment>Add a BridgePartition to be used by testAckedIndexing (#19172)</comment></comments></commit></commits></item><item><title>Not Able to Upload File in ElasticSearch through REST API using AngularJS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19171</link><project id="" key="" /><description>I want to insert data in ElasticSearch through it's REST API through AngularJS but it's giving error:
`Malformed Data must start with an object` when sending through POST

And when sending through PUT it's showing error:
`Http  Put doesn't allow Preflight Response`

**My code in controller is:**

```
 var f =document.getElementById('file')
           angular.forEach(f.files,function(file,index){
              console.log(file.name +" RECEIVED")

              var createIndexUrl='http://'+$rootScope.elasticSearch.url+':9200/'+$rootScope.elasticSearch.indexName+'/'+$rootScope.elasticSearch.typeName+'/'+$rootScope.elasticSearch.id+'?pretty'
              r = new FileReader();
              r.readAsArrayBuffer(file);
              r.onloadend = function(e){
                  var data = e.target.result;
                  var jsonData=JSON.stringify(data)
               console.log(createIndexUrl);



                  /* $http.put(createIndexUrl,data,{headers: {'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',"Access-Control-Allow-Methods" : "POST, GET, PUT, DELETE, OPTIONS"}})*/
                  $http.put(createIndexUrl,data,{headers: {'Content-Type': undefined}})
                   .success(function(data) {

                   console.log("SUCCESSFULL "+data)
               }).error(function(data) {
                console.log("ERROR "+data);
               });
                }
             });
         };
```

**On Html Page:**
`&lt;input type="file" id="file" multiple&gt;`
</description><key id="163081756">19171</key><summary>Not Able to Upload File in ElasticSearch through REST API using AngularJS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SushantVarshney</reporter><labels /><created>2016-06-30T05:07:10Z</created><updated>2016-06-30T12:48:24Z</updated><resolved>2016-06-30T12:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T12:48:23Z" id="229648023">It sounds like you need to configure `http.cors.allow-methods` and maybe some other CORS parameters:
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-http.html#_settings_2

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed rest handler interface to take NodeClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19170</link><project id="" key="" /><description>Previously all rest handlers would take Client in their injected ctor.
However, it was only to hold the client around for runtime. Instead,
this can be done just once in the HttpService which handles rest
requests, and passed along through the handleRequest method. It also
should always be a NodeClient, and other types of Clients (eg a
TransportClient) would not work anyways (and some handlers can be
simplified in follow ups like reindex by taking NodeClient).
</description><key id="163060276">19170</key><summary>Changed rest handler interface to take NodeClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-30T01:04:22Z</created><updated>2016-06-30T18:00:09Z</updated><resolved>2016-06-30T18:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-30T07:30:14Z" id="229581529">LGTM!!! thanks 
</comment><comment author="nik9000" created="2016-06-30T15:00:58Z" id="229685519">LGTM too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/RestFilter.java</file><file>core/src/main/java/org/elasticsearch/rest/RestFilterChain.java</file><file>core/src/main/java/org/elasticsearch/rest/RestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/allocation/RestClusterAllocationExplainAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/delete/RestDeleteRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/get/RestGetRepositoriesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/put/RestPutRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/verify/RestVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/reroute/RestClusterRerouteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/shards/RestClusterSearchShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/delete/RestDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/get/RestGetSnapshotsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/restore/RestRestoreSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/status/RestSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/storedscripts/RestDeleteStoredScriptAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/storedscripts/RestGetStoredScriptAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/storedscripts/RestPutStoredScriptAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/tasks/RestPendingClusterTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestRolloverIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestShrinkIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/delete/RestIndexDeleteAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/head/RestAliasesExistAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/put/RestIndexPutAliasAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/close/RestCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/exists/indices/RestIndicesExistsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/exists/types/RestTypesExistsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestSyncedFlushAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/forcemerge/RestForceMergeAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/open/RestOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/recovery/RestRecoveryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/shards/RestIndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/template/get/RestGetIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/template/head/RestHeadIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/AbstractCatAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestCatAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestFielddataAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestMasterAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestPendingClusterTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestRepositoriesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestSegmentsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/fieldstats/RestFieldStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestHeadAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/termvectors/RestMultiTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/termvectors/RestTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>core/src/test/java/org/elasticsearch/action/ActionModuleTests.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/responseheader/TestResponseHeaderRestAction.java</file><file>core/src/test/java/org/elasticsearch/rest/RestControllerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/RestFilterChainTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestRecoveryActionTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestDeleteSearchTemplateAction.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestGetSearchTemplateAction.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestMultiSearchTemplateAction.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestPutSearchTemplateAction.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestRenderSearchTemplateAction.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/rest/action/search/template/RestSearchTemplateAction.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/RestMultiPercolateAction.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/RestPercolateAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBaseReindexRestHandler.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByQueryRestHandler.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestDeleteByQueryAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestReindexAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestRethrottleAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/RestUpdateByQueryAction.java</file><file>plugins/jvm-example/src/main/java/org/elasticsearch/plugin/example/ExampleCatAction.java</file></files><comments><comment>Merge pull request #19170 from rjernst/rest_handler_client</comment></comments></commit></commits></item><item><title>Remove some TransportActions from RestActions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19169</link><project id="" key="" /><description>Instead use the client. This will help us build the actions more
easily in the future.
</description><key id="163043962">19169</key><summary>Remove some TransportActions from RestActions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T22:43:30Z</created><updated>2016-06-30T13:37:35Z</updated><resolved>2016-06-30T13:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-29T23:14:50Z" id="229517312">Thanks! LGTM
</comment><comment author="nik9000" created="2016-06-30T13:37:35Z" id="229660236">Awesome! Thanks @rjernst . I'll have a look at removing the remaining ones.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Applied skeletal implementation to interface migration refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19168</link><project id="" key="" /><description>This is a semantics-preserving refactoring.
- We are evaluating a research prototype automated refactoring Eclipse plug-in called [Migrate Skeletal Implementation to Interface](https://github.com/khatchad/Migrate-Skeletal-Implementation-to-Interface-Refactoring). We have applied the tool to your project in the hopes of receiving feedback.
- The approach is very conservative and you should not observe many source code changes. We only migrated methods declared in abstract classes with the hopes of such methods being suitable default methods in corresponding interfaces.
- The source code should be semantically equivalent to the original.
- We have run tests prior to applying to the tool and following the application. All tests pass.

Thank you for your help in this evaluation! Any feedback you can provide would be very helpful.
</description><key id="163027953">19168</key><summary>Applied skeletal implementation to interface migration refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">khatchad</reporter><labels /><created>2016-06-29T21:13:17Z</created><updated>2016-06-30T15:25:56Z</updated><resolved>2016-06-29T22:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-29T22:05:32Z" id="229503496">Thanks for the PR @khatchad, but I disagree with the migration of many of these implementations to `default` methods on interfaces. Please see discussion on #18468.
</comment><comment author="khatchad" created="2016-06-30T15:25:56Z" id="229694251">Thank you for the feedback, @jasontedor. That is very interesting that an attempt to perform a migration to default methods was done previously!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pagination and/or filtering for  GET /_snapshot/&lt;repo&gt;/_all endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19167</link><project id="" key="" /><description>Some repository plugins can have high latency (in my case, [Wikimedia's Swift Repository Plugin](https://github.com/wikimedia/search-repository-swift)), or simply very large listings of snapshots. Pagination is essential to prevent timeouts, reduce payload size and in general reduce the workload for the plugin and upstream services.

In most cases a date filter would suffice, but it's not clear the plugin would be able to apply such filtering efficiently.
</description><key id="163027827">19167</key><summary>Pagination and/or filtering for  GET /_snapshot/&lt;repo&gt;/_all endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bgagnon</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2016-06-29T21:12:44Z</created><updated>2017-03-31T13:46:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bgagnon" created="2016-06-29T21:16:15Z" id="229491397">See the other issue I opened at wikimedia/search-repository-swift#24
</comment><comment author="s1monw" created="2016-07-01T09:10:43Z" id="229897777">I am not sure if pagination is the right term here, are you interested in parital results based on a timeout? ie. if a repo won't respond in time you get the listing that have already arrived? Something like this?
</comment><comment author="bgagnon" created="2016-08-21T05:06:35Z" id="241239321">Hmm, no, in this case I would not be interested in partial results due to timing out. I am interested in limiting the amount of listing I am requesting from the repo to give it a chance to respond within reasonable time. A timeout would still be considered a failure in this case.

I'm not sure if all or any of the repo implementations use a fast-access index for listing purposes. It might just be that the Swift one is doing an exhaustive listing of objects in the container where it could have done something less expensive.

Maybe our usage of snapshots is a little bizarre, but we can have several hundreds snapshot points over a long period of time. A way to filter by date range would be quite handy to maintain responsiveness in a list/select/restore UI.

In any case, having a way to specify parameters on  `GET /_snapshot/&lt;repo&gt;/...` seems like a useful addition to a repo plugin's API.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get all parents if ALL/NONE of children match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19166</link><project id="" key="" /><description>Hi, I'm not sure I can do this with ES:

Having a parent doc_type A and children doc_type B. Is there a way to query "all the A documents where NONE/ALL of their Bs children match some filters"?

Having:
- A1
  - B1.foo = 200
  - B2.foo = 100
- A2
  - B3. foo = 100

I tried with the query below, but with this I get A1 since one of their children doesn't have `foo: 100`.

``` json
{
  "query": {
    "has_child": {
      "type": "b",
      "query": {
        "filtered": {
          "filter": {
            "bool": {
              "must_not": { "term": { "foo": 100 }}
            }
          }
        }
      }
    }
  }
}
```

In summary, can we get parents for those which _all_/_none_ of their children match the given filters?
</description><key id="163025958">19166</key><summary>Get all parents if ALL/NONE of children match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jondeandres</reporter><labels /><created>2016-06-29T21:03:19Z</created><updated>2016-07-01T12:34:15Z</updated><resolved>2016-06-30T12:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-30T12:11:43Z" id="229640301">Not in any efficient way.  You have to count all children and compare that to how many children match.  The following will return all parents where all children match but it is a horrible inefficient solution and I would never recommend using it in practice:

```
PUT t 
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT t/parent/1
{}

PUT t/parent/2
{}

PUT t/parent/3
{}

PUT t/child/3?parent=1
{
  "tag": "foo"
}

PUT t/child/4?parent=1
{
  "tag": "bar"
}

PUT t/child/5?parent=1
{
  "tag": "bar"
}

PUT t/child/6?parent=2
{
  "tag": "foo"
}

PUT t/child/7?parent=3
{
  "tag": "bar"
}



GET t/parent/_search
{
  "query": {
    "has_child": {
      "type": "child",
      "score_mode": "sum",
      "query": {
        "function_score": {
          "query": {
            "match_all": {}
          },
          "functions": [
            {
              "weight": -1
            },
            {
              "filter": {
                "match": {
                  "tag": "foo"
                }
              },
              "weight": 1
            }
          ],
          "score_mode": "sum",
          "boost_mode": "replace"
        }
      }
    }
  },
  "min_score": 0
}
```
</comment><comment author="jondeandres" created="2016-07-01T12:34:15Z" id="229935462">Thanks @clintongormley for the response! We'll see what we can do.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move RestHandler registration to ActionModule and ActionPlugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19165</link><project id="" key="" /><description>`RestHandler`s are highly tied to actions so registering them in the
same place makes sense.

Removes the need to for plugins to check if they are in transport client
mode before registering a RestHandler - `getRestHandlers` isn't called
at all in transport client mode.

This caused guice to throw a massive fit about the circular dependency
between NodeClient and the allocation deciders. I broke the circular
dependency by registering the actions map with the node client after
instantiation.
</description><key id="163009625">19165</key><summary>Move RestHandler registration to ActionModule and ActionPlugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T19:42:00Z</created><updated>2016-07-29T11:43:42Z</updated><resolved>2016-06-29T22:34:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-29T19:43:48Z" id="229466362">@rjernst we talked about this kind of thing a few days ago and I finally got to it.

I had trouble with guice for unknown reasons so I decided to break the circular dependency it was claiming was a problem. I mean, without this PR the same circular dependency exists and guice didn't care, but meh. We have to break the circular dependency some time any way.
</comment><comment author="rjernst" created="2016-06-29T20:50:49Z" id="229484492">I left a couple comments. Looks ok to me for now. I have some follow ups I'd like to do to simplify how handlers work once you get this in. Eventually I think these handlers should be constructed in the getter rather than passing black `Class` objects...but we are a long way from there...
</comment><comment author="nik9000" created="2016-06-29T22:05:33Z" id="229503502">@rjernst I pushed fixed for most things. I'm keeping my static import if you don't mind.
</comment><comment author="rjernst" created="2016-06-29T22:29:38Z" id="229508701">LGTM
</comment><comment author="nik9000" created="2016-06-29T22:35:04Z" id="229509775">Merged. Thanks for reviewing @rjernst !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Entries repeatedly logged with sub-second frequency filling up disk space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19164</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

5.0.0-alpha3

**JVM version**:

The docker image inherits from the official java:8-jre image.

**OS version**:

Official docker image (https://hub.docker.com/_/elasticsearch/, Debian Jessie) running on a Fedora 23 host.

**Description of the problem including expected versus actual behavior**:

Certain log entries are repeatedly added to the log file with sub-second frequency, resulting in filling up the disk, which can trigger shard migration and possibly node failure. In a matter of hours the log file can be several gigabytes. Ironically, one of the log entries exhibiting such behaviour is warning about low disk space. The log records I have seen added with such frequency are given below.

Expected behaviour is to log entries with less frequency. I certainly don't need to be notified 2 times per second that my disk usage is over 90%. Once every 15 or 30 minutes would suffice.

Would it be an option to allow users to configure how often certain entries would be logged? That would require the program to be intelligent enough and know when an entry was already logged.  Not sure how this could be accomplished - one idea could be to store in memory the timestamp of the last entry of a type and take it into account when a new entry of the same type is about to be logged.

**Steps to reproduce**:
1. Run elasticsearch on a node with more than 90% usage.
2. Observe the log file, the related entry is added 2 times per second.

I do not know how to reproduce the second log entry referenced below, I will file a separate issue if it seems to be a bug.

**Provide logs (if relevant)**:

The following log entry is written up to 7 times per second.

```
{"log":"[2016-06-27 16:49:55,313][WARN ][cluster.routing.allocation.decider] [Sabretooth] high disk watermark [90%] exceeded on [ExDIO2orQJm6a5XHEvxxgg][Sabretooth][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.6gb[7.7%], shards will be relocated away from this node\n","stream":"stdout","time":"2016-06-27T16:49:55.314948647Z"}
{"log":"[2016-06-27 16:49:55,314][INFO ][cluster.routing.allocation.decider] [Sabretooth] rerouting shards: [high disk watermark exceeded on one or more nodes]\n","stream":"stdout","time":"2016-06-27T16:49:55.315047615Z"}
```

The following log entry is written 1 or 2 times per second.

```
{"log":"[2016-06-29 14:55:30,597][WARN ][cluster.action.shard     ] [Thor] [.monitoring-data-2][0] received shard failed for target shard [[.monitoring-data-2][0], node[zUY_PHA0SPet5YLXwPZKSA], [P], s[INITIALIZING], a[id=0xG_VO4iQYC-9EArmgdU1w], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-06-29T14:55:30.398Z], failed_attempts[14], details[failed recovery, failure RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Thor}{zUY_PHA0SPet5YLXwPZKSA}{172.17.0.3}{172.17.0.3:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]]], source shard [[.monitoring-data-2][0], node[zUY_PHA0SPet5YLXwPZKSA], [P], s[INITIALIZING], a[id=0xG_VO4iQYC-9EArmgdU1w], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-06-29T14:55:30.398Z], failed_attempts[14], details[failed recovery, failure RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Thor}{zUY_PHA0SPet5YLXwPZKSA}{172.17.0.3}{172.17.0.3:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]]], message [failed recovery], failure [RecoveryFailedException[[.monitoring-data-2][0]: Recovery failed from null into {Thor}{zUY_PHA0SPet5YLXwPZKSA}{172.17.0.3}{172.17.0.3:9300}]; nested: IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]\n","stream":"stdout","time":"2016-06-29T14:55:30.597899825Z"}
```
</description><key id="163008351">19164</key><summary>Entries repeatedly logged with sub-second frequency filling up disk space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">krystalcode</reporter><labels><label>:Exceptions</label><label>bug</label></labels><created>2016-06-29T19:35:47Z</created><updated>2017-05-09T08:14:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="krystalcode" created="2016-07-05T22:05:57Z" id="230617036">The situation corresponding to the second log entry can be reproduced as described here: https://github.com/elastic/elasticsearch/issues/19275
</comment><comment author="clintongormley" created="2016-07-06T11:42:56Z" id="230748919">I think the disk watermark message was logged every time it tried to assign a shard.  Given that the shard is corrupt, this happens very frequently.  @dakrone is this the case?

Note: https://github.com/elastic/elasticsearch/pull/18467 should have stopped trying to reallocate the shard after five attempts, which would have stopped this excessive logging i think?
</comment><comment author="ywelsch" created="2016-07-06T12:11:35Z" id="230753966">The reason that #18467 does not stop reallocating this shard (failed_attempts[14]) is because it's a primary shard. For primary shards we currently force allocation if there are only shard copies on nodes where the deciders say no. The reason for this is to ensure for example that a primary is still allocated even if the node is above the low watermark. This does not play nicely with the allocation decider that checks the number of allocation attempts, however. We should rethink how allocation deciders work in case of primary shard allocation.
</comment><comment author="dakrone" created="2016-07-06T22:08:21Z" id="230922132">&gt; I think the disk watermark message was logged every time it tried to assign a shard. @dakrone is this the case?

This message (the exceeded message) should only occur when a new cluster info is retrieved and the node is still over the limit. By default this is every 30 seconds, or whenever a new data node has joined. I just tested this locally and I get the message every 30 seconds, as expected.
</comment><comment author="krystalcode" created="2016-07-07T00:38:11Z" id="230948134">@clintongormley unfortunately I have not kept these logs. However, from what I remember, the disk watermark message was present without the failed shard recovery message and could be unrelated. The failed shard recovery message appeared later on the log, due to https://github.com/elastic/elasticsearch/issues/19275, after a restart. By that time, I had probably cleaned up some disk space and I believe I was not getting the high watermark message any more.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES 5 Doesn't allow registering default analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19163</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha3

**JVM version**: 1.8.0_73-b02

**OS version**: CentOS Linux release 7.2.1511

**Description of the problem including expected versus actual behavior**:
I'm trying to upgrade an ES plugin from 2.3 to 5.0.0-alpha3, and a template we previously had working is now causing an issue. The template attempts to set some default analyzers for an index, but when the index is created it throws an error saying there's already an analyzer with the default name. 

Here's the template I'm using:
`{
  "order": 0, 
  "settings": {
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.alias": "default",
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.type": "fairhair-index-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.type": "fairhair-tokenizing-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.alias": "default_search"
  },
"mappings" : {
   //some mappings
},
"template" : "document.*"
}`

**Provide logs (if relevant)**:
Here's the error stack trace I get:
`elasticsearch_1  | [2016-06-28 23:40:43,871][DEBUG][action.admin.indices.create] [Iridia] [document.document-20151106] failed to create
elasticsearch_1  | java.lang.IllegalStateException: already registered analyzer with name: default
elasticsearch_1  |  at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:109)
elasticsearch_1  |  at org.elasticsearch.index.analysis.AnalysisRegistry.build(AnalysisRegistry.java:161)
elasticsearch_1  |  at org.elasticsearch.index.IndexService.&lt;init&gt;(IndexService.java:138)
elasticsearch_1  |  at org.elasticsearch.index.IndexModule.newIndexService(IndexModule.java:328)
elasticsearch_1  |  at org.elasticsearch.indices.IndicesService.createIndexService(IndicesService.java:398)
elasticsearch_1  |  at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:363)
`

I've looked at the AnalysisService class constructor, and I see it does a check for analyzers with the name "default" in the passed in analyzer mappings. I believe these conditional statements fire:

`if (!analyzerProviders.containsKey("default")) {
            analyzerProviders.put("default", new StandardAnalyzerProvider(indexSettings, null, "default", Settings.Builder.EMPTY_SETTINGS));
        }
        if (!analyzerProviders.containsKey("default_search")) {
            analyzerProviders.put("default_search", analyzerProviders.get("default"));
        }
        if (!analyzerProviders.containsKey("default_search_quoted")) {
            analyzerProviders.put("default_search_quoted", analyzerProviders.get("default_search"));
        }`

So it registers the standard analyzer, but while iterating through the passed in analyzers, this check also fires:

`if (analyzers.containsKey(name)) { //Grabs the name from each entry in analyzer map
                throw new IllegalStateException("already registered analyzer with name: " + name);
            }`

I'm confused as to how both these conditions are firing. This check to see if the current map of analyzers already contains the key "name" was introduced in ES 5. 

EDIT: I know why the illegal state exception is being thrown. The Analysis Registry automatically adds the entry: "default" =&gt; StandardAnalyzer. So from what I can tell, it's actually impossible to register your own default index analyzer, since there will always be the standard analyzer. I think the check for whether a name is already in the analyzer mappings is a bug and should be removed. 
</description><key id="163001610">19163</key><summary>ES 5 Doesn't allow registering default analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mitchswaldman</reporter><labels><label>:Analysis</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T19:02:27Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-07-21T07:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-30T07:25:16Z" id="229580558">that is a bug - I think we should not register a "default" analyzer in our `AnalysisModule` since `AnalysisService` does it for us if there is no default specified. 
</comment><comment author="mitchswaldman" created="2016-06-30T16:31:14Z" id="229713630">I think throwing an illegal state exception when encountering a duplicate analyzer name should be removed. If user defined analyzers are deterministically placed into the analyzer mappings after the built in analyzers, then I think it makes sense to simply overwrite them. Maybe I'm not clear on how all this stuff get's wired up, but I'm actually a little confused as to how it's even possible to trigger duplicate analyzer named form within the constructor of AnalysisService. A Map of (analyzer name =&gt; analyzer) is passed into the constructor, which by definition has no duplicate keys. So how is it that while iterating through this map that we even encounter the same key twice? 
</comment><comment author="s1monw" created="2016-06-30T17:02:37Z" id="229722428">&gt; I think throwing an illegal state exception when encountering a duplicate analyzer name should be removed. If user defined analyzers are deterministically placed into the analyzer mappings after the built in analyzers, then I think it makes sense to simply overwrite them. Maybe I'm not clear on how all this stuff get's wired up, but I'm actually a little confused as to how it's even possible to trigger duplicate analyzer named form within the constructor of AnalysisService. A Map of (analyzer name =&gt; analyzer) is passed into the constructor, which by definition has no duplicate keys. So how is it that while iterating through this map that we even encounter the same key twice?

-1 default is a special one that's fine but nobody should override `english` analyzer etc. ever, you can define your own that's fine
</comment><comment author="mitchswaldman" created="2016-06-30T17:21:43Z" id="229727969">True. Could you enlighten me on how it's even possible to encounter duplicate keys from the passed in Map? I think I'm missing some crucial understanding of the issue. 
</comment><comment author="mitchswaldman" created="2016-06-30T19:42:21Z" id="229766955">Ah I found the issue. The initial mapping of analyzer providers that comes in will contain a default mapping to the standard analyzer provider. And although I have defined an alias for my custom analyzer to be the default analyzer, this alias isn't picked up until AFTER the initial check to see if "default" is in the provider mapping. When I was debugging ES I found that that my custom analyzer was the first to be iterated over. So the mapping of analyzers was first updated to include my analyzer, then the aliases of my custom analyzer were added to the mapping of analyzers. So after this first iteration, there was a mapping of "default" =&gt; CustomAnalyzer. When we finally iterate over the (default =&gt; standard analyzer provider) entry, that's where we hit the illegal state exception. So maybe to fix the issue, we move the initial check to see if "default" is missing from the passed in analyzer providers after we iterate through the providers.
</comment><comment author="s1monw" created="2016-06-30T21:02:39Z" id="229787941">what about just never register a default in the AnalysisModule - that should fix the problem
</comment><comment author="mitchswaldman" created="2016-06-30T21:23:19Z" id="229792984">I tried doing that, and the default analyzer snuck in still via the PreBuiltAnalyzers. So I deleted the DEFAULT entry in the PrebuildAnalyzers enumeration, but it still snuck in because the AnalysisService does a check for it in the providers but doesn't look at the possible aliases. So I finally got around it by moving that check after the for loop. Because by then, all the aliases have been registered.
</comment><comment author="s1monw" created="2016-06-30T21:25:11Z" id="229793462">let's look at some code, can you open a PR?
</comment><comment author="mitchswaldman" created="2016-06-30T21:26:57Z" id="229793937">Sure! 
</comment><comment author="s1monw" created="2016-07-08T08:34:32Z" id="231305255">I added a testcase to master just to make sure it actually doesn't work but it does. I can reproduce your issue if I alias the default analyzer twice in the mapping, I wonder if you have an issue in your mapping that 5.x detects? Se my test here: https://github.com/elastic/elasticsearch/commit/1cb1373722b97a23f7f551e9827bbedea97b1424
</comment><comment author="mitchswaldman" created="2016-07-08T16:05:59Z" id="231400504">Hmm interesting. I don't think there's anything wrong with my mapping. Here's the settings portion:
"settings": {
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.alias": "default",
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.type": "fairhair-index-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.type": "fairhair-tokenizing-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.alias": "default_search"
  }
</comment><comment author="s1monw" created="2016-07-08T19:08:41Z" id="231446930">wait, what is `fairhair-index-analyzer` and where is is defined?
</comment><comment author="mitchswaldman" created="2016-07-08T19:50:04Z" id="231455783">It's a custom analyzer defined in a plugin. It's registered with the AnalysisModule via the `onModule(AnalysisModule module)` method:

`module.registerAnalyzer(INDEX_ANALYZER_NAME, FairhairIndexAnalyzerProvider::new);
        module.registerAnalyzer(TOKENIZING_ANALYZER_NAME, FairhairTokenizingAnalyzerProvider::new);`

INDEX_ANALYZER_NAME = "fairhair-index-analyzer"
TOKENIZING_ANALYZER_NAME = "fairhair-tokenizing-analyzer"
</comment><comment author="s1monw" created="2016-07-08T20:37:55Z" id="231466442">one more question, do you send settings with the create index request since your setting here come from a template?
</comment><comment author="mitchswaldman" created="2016-07-08T20:42:06Z" id="231467404">The settings are being posted to ES once it's up and running. I'm able to see the template with `GET /_template/document-template.json` before there's any indices in ES. Once I try to post data to the index, that's when the "failed to create index exception" happens
</comment><comment author="s1monw" created="2016-07-11T10:09:51Z" id="231694439">@mitchswaldman can you provide a full re-creation of the issue like with curl statements and the bodies of the templates etc? 
</comment><comment author="s1monw" created="2016-07-18T14:32:07Z" id="233346004">@mitchswaldman ping
</comment><comment author="mitchswaldman" created="2016-07-18T17:10:11Z" id="233393340">Oh shoot my bad! Of course. 

1) We are running Elasticsearch in Docker so here are the relevant parts of the Dockerfile for installing elasticsearch.

`Dockerfile:`

# 

ENV PATH /usr/share/elasticsearch/bin:$PATH

```
ENV ES_JAVA_OPTS="$ES_JAVA_OPTS -Xmx1g -Xms1g"

RUN curl -O https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/$ELASTICSEARCH_VERSION/elasticsearch-$ELASTICSEARCH_VERSION.tar.gz
RUN tar -zxvf elasticsearch-$ELASTICSEARCH_VERSION.tar.gz

RUN useradd elasticsearch

RUN mv -f elasticsearch-$ELASTICSEARCH_VERSION/ /usr/share/elasticsearch

RUN set -ex \
  &amp;&amp; for path in \
    /usr/share/elasticsearch/data \
    /usr/share/elasticsearch/logs \
    /usr/share/elasticsearch/config \
    /usr/share/elasticsearch/config/scripts \
  ; do \
    mkdir -p "$path"; \
    chown -R elasticsearch:elasticsearch "$path"; \
  done


COPY config/logging.yml /usr/share/elasticsearch/config/

VOLUME /usr/share/elasticsearch/data

COPY docker-entrypoint.sh /
COPY startup.sh /
COPY startup-wrapper.sh /

ENTRYPOINT ["/docker-entrypoint.sh"]

EXPOSE 9200 9300

CMD ["/startup-wrapper.sh"]


ADD plugins /usr/share/elasticsearch/plugins_source


RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install analysis-icu
RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install analysis-phonetic
RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install x-pack
RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install file:////usr/share/elasticsearch/plugins_source/fairhair-elasticsearch-queries-1.19.0_es20/fairhair-elasticsearch-queries-1.19.0-SNAPSHOT.zip
```

# 

That last plugin is the custom plugin we wrote which registers `FairhairTokenizer` and the other Fairhair analyzers.

2) On start up is when we post the templates to ES.

`startup-wrapper.sh` (referenced in Dockerfile)

# 

```
set -m

elasticsearch &amp;

while [ $(curl -s -o /dev/null -w "%{http_code}" localhost:9200) != 200 ]; do echo "Waiting for elasticsearch to be ready before posting templates..."; sleep 5; done

for template in $(ls -1 /usr/share/elasticsearch/config/templates); do
  echo -e "\nPosting template: $template"
  curl -s -XPUT "localhost:9200/_template/$template" -d"@/usr/share/elasticsearch/config/templates/$template"
done

echo -e "\nAttaching to elasticsearch process" 
fg %1
```

# 

3) Here is the template in question. I've removed many of the mappings since they are somewhat redundant.  
`document-template.json`

# 

```
{
  "order": 0,
  "settings": {
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.alias": "default",
    "index.analysis.analyzer.fairhairIndexAnalyzerv3.type": "fairhair-index-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.type": "fairhair-tokenizing-analyzer",
    "index.analysis.analyzer.fairhairTokenizingAnalyzer.alias": "default_search"
  },
  "mappings": {
    "docnews": {
      "_source": {
        "enabled": true
      },
      "dynamic": "false",
      "_timestamp": {
        "enabled": true
      },
      "_all": {
        "enabled": false
      },
      "properties": {
        "body": {
          "dynamic": false,
          "properties": {
            "byLine": {
              "dynamic": false,
              "properties": {
                "text": {
                  "type": "text",
                  "store": "no",
                  "index": "analyzed",
                  "norms": "false",
                  "index_options": "positions",
                  "doc_values": "false",
                  "analyzer": "fairhairIndexAnalyzerv3"
                }
              }
            },
            "content": {
              "dynamic": false,
              "properties": {
                "text": {
                  "type": "text",
                  "store": "no",
                  "index": "analyzed",
                  "norms": "false",
                  "index_options": "positions",
                  "doc_values": "false",
                  "analyzer": "fairhairIndexAnalyzerv3"
                }
              }
            },
            "ingress": {
              "dynamic": false,
              "properties": {
                "text": {
                  "type": "text",
                  "store": "no",
                  "index": "analyzed",
                  "norms": "false",
                  "index_options": "positions",
                  "doc_values": "false",
                  "analyzer": "fairhairIndexAnalyzerv3"
                }
              }
            },
            "publishDate": {
              "dynamic": false,
              "properties": {
                "date": {
                  "type": "date",
                  "doc_values": "true"
                }
              }
            },
            "title": {
              "dynamic": false,
              "properties": {
                "text": {
                  "type": "text",
                  "store": "no",
                  "index": "analyzed",
                  "norms": "false",
                  "index_options": "positions",
                  "doc_values": "false",
                  "analyzer": "fairhairIndexAnalyzerv3"
                }
              }
            },
            "contentTags": {
              "type": "keyword",
              "index": "not_analyzed",
              "norms": "false",
              "index_options": "docs",
              "doc_values": "true"
            },
            "links": {
              "dynamic": false,
              "type": "nested",
              "properties": {
                "url": {
                  "type": "keyword",
                  "index": "not_analyzed",
                  "norms": "false",
                  "index_options": "docs",
                  "doc_values": "true"
                }
              }
            }
          }
        }
   },
  "template": "document.*"
}
```

# 

4) The final step in creating the error is to simply POST some data to an index such as `document.document-20160716`. Upon index creation, ES will fail and throw the aforementioned error. 

Let me know if there's anything I need to clear up or add on. For instance, is there a part in the custom queries plugin you need to see?
</comment><comment author="clintongormley" created="2016-07-19T12:47:15Z" id="233621159">Out of interest, why are you doing this:

```
"settings": {
  "index.analysis.analyzer.fairhairIndexAnalyzerv3.alias": "default",
  "index.analysis.analyzer.fairhairIndexAnalyzerv3.type": "fairhair-index-analyzer",
  "index.analysis.analyzer.fairhairTokenizingAnalyzer.type": "fairhair-tokenizing-analyzer",
  "index.analysis.analyzer.fairhairTokenizingAnalyzer.alias": "default_search"
},
```

Why not just do this:

```
"settings": {
  "analysis": {
    "analyzer": {
      "default": {
        "type": "fairhair-index-analyzer"
      },
      "default_search": {
        "type": "fairhair-tokenizing-analyzer"
      }
    }
  }
}
```
</comment><comment author="mitchswaldman" created="2016-07-19T16:03:11Z" id="233681273">This template was given to me. So the settings part just came like that. 
</comment><comment author="s1monw" created="2016-07-19T19:44:49Z" id="233743993">@mitchswaldman @clintongormley I opened a fix for this issue in #19506 @mitchswaldman would you be able to test this fix if it solves your issue?
</comment><comment author="s1monw" created="2016-07-19T19:47:47Z" id="233744787">@mitchswaldman on another note, I think you should move to the mapping @clintongormley suggested. The entire alias feature seems pretty esoteric and it has lots of caveats like the late binding it does can cause tricky problems. ie.

```
"index.analysis.analyzer.foobar.alias" : "default"
"index.analysis.analyzer.foobar.type", "german"
"index.analysis.analyzer.foobar_search.alias": "default_search"
"index.analysis.analyzer.foobar_search.type": "default"
```

won't work as you expect, it uses the `standard` analyzer as `default_search` but `german` for indexing which is, err not ideal :) - I think one way or the other we will remove this ability unless I understand it's real purpose at some point :)... 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TextFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/indices/analysis/AnalysisModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file></files><comments><comment>Fix analyzer alias processing (#19506)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TextFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/indices/analysis/AnalysisModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file></files><comments><comment>Fix analyzer alias processing (#19506)</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/indices/analysis/AnalysisModuleTests.java</file></files><comments><comment>[TEST] Test analyzer alias works</comment></comments></commit></commits></item><item><title>Resolve scripts on coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19162</link><project id="" key="" /><description>While #19161 will help with search scroll requests having consistent scripts through the life of a scroll, an issue still exists where stored scripts can be updated an a search request could execute different versions depending on the order/quickness that shards get updated cluster state.

One possible solution is to do the script resolution on the coordinating node, so that the same scripts are passed along to each shard. A caveat to this is figuring out how to do this without opening up a path to sidestepping the controls on script execution for inline vs stored vs file scripts (ie, rewriting to an inline script would not work).
</description><key id="162996958">19162</key><summary>Resolve scripts on coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2016-06-29T18:39:59Z</created><updated>2016-06-30T11:41:46Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripts should be stored in QueryShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19161</link><project id="" key="" /><description>Currently the apis for compiling scripts take the ClusterState to support stored scripts. This is going away in #19136 since most users of the ScriptService should not need to know or care about where the script comes from, it should be an impl detail of the ScriptService. The one place that needs this api is doing a search scroll. There, the QueryShardContext keeps the cluster state at the time the scroll was created. However, a simpler approach is to have the context keep any scripts it uses. This should be fairly simple as it appears from a cursory glance that anywhere in a search request that accesses a script peeks into the ScriptService of the query shard context. Instead, a wrapper method can be used that will cache the script in the context.
</description><key id="162995806">19161</key><summary>Scripts should be stored in QueryShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label></labels><created>2016-06-29T18:34:12Z</created><updated>2016-06-30T11:41:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-29T18:35:49Z" id="229448012">Also note that this should solve the same problem with file scripts, which could currently be changed during a scroll. With this they will be cached like any other script in the context.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate ExceptionsHelper.detailedMessage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19160</link><project id="" key="" /><description>This is a trappy "helper" and only hurts.
See #19069
</description><key id="162991044">19160</key><summary>Deprecate ExceptionsHelper.detailedMessage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T18:11:12Z</created><updated>2016-06-30T11:37:55Z</updated><resolved>2016-06-29T18:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-29T18:12:33Z" id="229441166">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file></files><comments><comment>Merge pull request #19160 from rjernst/deprecate_exception_hurter</comment></comments></commit></commits></item><item><title>Clarify time units usage in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19159</link><project id="" key="" /><description>This commit clarifies the distinction between supported time units for
durations and supported time units for durations in the docs.

Relates #19102
</description><key id="162975913">19159</key><summary>Clarify time units usage in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T16:55:50Z</created><updated>2016-06-29T21:02:15Z</updated><resolved>2016-06-29T21:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-29T17:06:23Z" id="229422258">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Clarify time units usage in docs</comment></comments></commit></commits></item><item><title>Clearer error when handling fractional time values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19158</link><project id="" key="" /><description>In 2f638b5a23597967a98b1ced1deac91d64af5a44, support for fractional time
values was removed. While this change is documented, the error message
presented does not give an indication that fractional inputs are not
supported. This commit fixes this by detecting when the input is a time
value that would successfully parse as a double but will not parse as a
long and presenting a clear error message that fractional time values
are not supported.

Relates #19102
</description><key id="162971664">19158</key><summary>Clearer error when handling fractional time values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T16:35:39Z</created><updated>2016-06-29T17:36:11Z</updated><resolved>2016-06-29T17:36:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-29T17:02:35Z" id="229421260">Left a suggestion. LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file></files><comments><comment>Clearer error when handling fractional time values</comment></comments></commit></commits></item><item><title>[TEST] fix vagrant tests for seed with format ABC:DEF</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19157</link><project id="" key="" /><description>Otherwise one gets an error message when passing
-Dtests.seed=ABC:DEF
to any test run.
</description><key id="162966532">19157</key><summary>[TEST] fix vagrant tests for seed with format ABC:DEF</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label></labels><created>2016-06-29T16:11:58Z</created><updated>2016-06-29T18:18:26Z</updated><resolved>2016-06-29T18:18:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-29T17:01:57Z" id="229421088">@brwe I'm curious where this came up? The seeds produced by the packaging tests are just the simple format (without the child seed).
</comment><comment author="brwe" created="2016-06-29T17:35:08Z" id="229430283">It came up when I ran 

`gradle :core:integTest -Dtests.seed=C00A0BEE785BC700:7D0E9FFE8FD94C3E -Dtests.class=org.elasticsearch.snapshots.DedicatedClusterSnapshotRestoreIT`

to reproduce a different test failure. 

Error message was : 

```
...
:buildSrc:build
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.13
  OS Info               : Linux 3.13.0-39-generic (amd64)
  JDK Version           : Oracle Corporation 1.8.0_65 [Java HotSpot(TM) 64-Bit Server VM 25.65-b01]
  JAVA_HOME             : /usr/java/jdk1.8.0_65

FAILURE: Build failed with an exception.

* Where:
Build file '/home/britta/elasticsearch/qa/vagrant/build.gradle' line: 67

* What went wrong:
A problem occurred evaluating project ':qa:vagrant'.
&gt; For input string: "C700:7D"

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

```
</comment><comment author="jasontedor" created="2016-06-29T17:37:40Z" id="229431020">Oh, that makes sense. That's an unfortunate consequence of how we have to randomize the version to upgrade from before we can specify the dependencies for the Vagrant tests, and those are eagerly evaluated. Thanks for fixing this @brwe!
</comment><comment author="brwe" created="2016-06-29T17:38:52Z" id="229431374">pushed another commit
</comment><comment author="brwe" created="2016-06-29T18:04:18Z" id="229438743">pushed another commit to change the input to BigInteger back to 16.
</comment><comment author="jasontedor" created="2016-06-29T18:14:42Z" id="229441746">LGTM.
</comment><comment author="brwe" created="2016-06-29T18:18:01Z" id="229442720">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] fix vagrant tests for seed with format ABC:DEF (#19157)</comment></comments></commit></commits></item><item><title>Highlighting problem when mixing a match query and filtering by geo_distance in query context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19156</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_92

**OS version**: Ubuntu 12.04.5 LTS

**Description of the problem including expected versus actual behavior**:

Say that we have a mapping with two properties:
- description: analyzed string
- location: geo_point

If I combine a match query on description and a geo_distance filter on location using a boolean query (that's why I say "filtering in query context"), Elasticsearch raises an error while highlighting. If I move the geo_distance filter to "post_filter context" it works fine.

I checked a bit the code and it seems to me that when creating the highlighting context [in HighlightPhase](https://github.com/elastic/elasticsearch/blob/v2.3.2/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java#L120),  it uses description as field and geo_point as the type of the fieldMapper. 

If you confirm that this is a bug I can look further and help with it.

**Steps to reproduce**:

```
PUT /test
{
    "mappings": {
        "test":{
            "dynamic": "false",
            "properties": {
                "description" :{
                    "type": "string"
                },
                "location": {
                  "type": "geo_point"
                }
            }
        }
    }
}

PUT /test/test/1
{
    "description": "Hello world",
    "location": {"lat": 1.0, "lon": 1.0}
}

POST /test/test/_search
{
  "from": 0,
  "size": 10,
  "query": {
    "bool": {
      "must": {
        "match": {
          "description": "hello"
        }
      },
      "filter": {
        "geo_distance": {
          "location": [
            1.0,
            1.0
          ],
          "distance": "10km"
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "description": {
      }
    }
  }
}
```

**Provide logs (if relevant)**:

This is the exception:

```
[2016-06-29 16:08:18,057][DEBUG][action.search            ] [Bella Donna] [44] Failed to execute fetch phase
RemoteTransportException[[Bella Donna][127.0.0.1:9300][indices:data/read/search[phase/fetch/id]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [description]]]; nested: NumberFormatException[Invalid shift value (111) in prefixCoded bytes (is encoded value really a geo point?)];
Caused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [description]]]; nested: NumberFormatException[Invalid shift value (111) in prefixCoded bytes (is encoded value really a geo point?)];
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)
    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:592)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: Invalid shift value (111) in prefixCoded bytes (is encoded value really a geo point?)
    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)
    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)
    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)
    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)
    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)
    ... 12 more
```

Thanks!
</description><key id="162948144">19156</key><summary>Highlighting problem when mixing a match query and filtering by geo_distance in query context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moliware</reporter><labels /><created>2016-06-29T14:59:45Z</created><updated>2016-06-29T15:07:20Z</updated><resolved>2016-06-29T15:01:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T15:01:31Z" id="229384297">Hi @moliware - this will be fixed in 2.3.4 by https://github.com/elastic/elasticsearch/pull/18495
</comment><comment author="moliware" created="2016-06-29T15:07:20Z" id="229386123">Thanks @clintongormley I did a couple of searches and couldn't find it. Sorry!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Vagrant boxes before running packaging test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19155</link><project id="" key="" /><description>This commit adds an execution of a Vagrant box update task before
bringing a Vagrant box up for running packaging tests.

Closes #19145
</description><key id="162944854">19155</key><summary>Update Vagrant boxes before running packaging test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T14:47:40Z</created><updated>2016-06-29T15:04:54Z</updated><resolved>2016-06-29T15:04:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-29T14:58:01Z" id="229383167">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update Vagrant boxes before running packaging test</comment></comments></commit></commits></item><item><title>[doc] explain avg in function_score better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19154</link><project id="" key="" /><description>relates to #19068
</description><key id="162937881">19154</key><summary>[doc] explain avg in function_score better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>docs</label></labels><created>2016-06-29T14:20:45Z</created><updated>2016-06-30T09:52:54Z</updated><resolved>2016-06-30T09:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T14:42:09Z" id="229377712">Couple of typos otherwise LGTM
</comment><comment author="brwe" created="2016-06-29T15:51:14Z" id="229400255">@clintongormley thanks for the review! pushed a commit.
</comment><comment author="clintongormley" created="2016-06-30T08:57:51Z" id="229602086">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[doc] explain avg in function_score better (#19154)</comment></comments></commit></commits></item><item><title> Mustache: Add util functions to render JSON and join array values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19153</link><project id="" key="" /><description>Backport of #18856 on 2.4
</description><key id="162936941">19153</key><summary> Mustache: Add util functions to render JSON and join array values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.4.0</label></labels><created>2016-06-29T14:17:03Z</created><updated>2016-06-30T07:11:40Z</updated><resolved>2016-06-30T07:11:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-06-29T14:52:07Z" id="229381112">LGTM
</comment><comment author="tlrx" created="2016-06-30T07:11:40Z" id="229578260">Thanks @spinscale 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implement HEAD for index private properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19152</link><project id="" key="" /><description>When you `curl ip:9200/name/_mappings` (or `_settings`, for example), you get a response with the JSON object.

But when you do  `curl -I ip:9200/name/_mappings`, you get 404.

Why? Because `-I` is not a GET operation, but a HEAD one, and it's not implemented.

I think this is strange behaviour. According to [HTTP protocol spec](https://tools.ietf.org/html/rfc2616#section-9.4):

&gt;  The HEAD method is identical to GET except that the server MUST NOT
&gt;    return a message-body in the response. The metainformation contained
&gt;    in the HTTP headers in response to a HEAD request SHOULD be identical
&gt;    to the information sent in response to a GET request.

So would be nice to have the HEAD acting the same as GET here (without body response), instead of returning 404. I wasted some time trying to figure out what was wrong until i removed `-I` :confused: 
</description><key id="162934416">19152</key><summary>Implement HEAD for index private properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felixsanz</reporter><labels /><created>2016-06-29T14:06:42Z</created><updated>2016-06-29T14:39:35Z</updated><resolved>2016-06-29T14:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T14:39:35Z" id="229376835">Given that support for HEAD requests is not required (and doesn't make much sense with this request) I think the correct behaviour would be to return 405 method not allowed.  As such, i'm closing as a duplicate of #15335
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>LuceneTests.testAsSequentialAccessBits fails frequently and reproducible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19151</link><project id="" key="" /><description>Failure is : 

```
java.lang.AssertionError: The top-reader used to create Weight (org.apache.lucene.index.CompositeReaderContext@622216af) is not the same as the current reader's top-reader (org.apache.lucene.index.CompositeReaderContext@474b5998
    at __randomizedtesting.SeedInfo.seed([8C29ADADD27E8824:8EBEA1349D13476A]:0)
    at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:100)
    at org.elasticsearch.common.lucene.LuceneTests.testAsSequentialAccessBits(LuceneTests.java:381)
```

To reproduce: 

```
gradle :core:test -Dtests.seed=8C29ADADD27E8824 -Dtests.class=org.elasticsearch.common.lucene.LuceneTests -Dtests.method="testAsSequentialAccessBits" -Dtests.security.manager=true -Dtests.locale=sq -Dtests.timezone=Africa/Libreville
```

See for example: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/916/consoleText
</description><key id="162930811">19151</key><summary>LuceneTests.testAsSequentialAccessBits fails frequently and reproducible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-06-29T13:51:54Z</created><updated>2016-06-30T09:11:21Z</updated><resolved>2016-06-30T09:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-06-29T13:55:28Z" id="229363084">I muted the test for now.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java</file></files><comments><comment>test: use the reader from the searcher (newSearcher(...) method may change the reader) instead of the reader we create in the test</comment></comments></commit></commits></item><item><title>Fix docs example for the _id field, the field is not accessible in scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19150</link><project id="" key="" /><description>This change fix the example in the docs that tries to load the _id field in a script. 
</description><key id="162925406">19150</key><summary>Fix docs example for the _id field, the field is not accessible in scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T13:28:32Z</created><updated>2016-07-29T11:13:01Z</updated><resolved>2016-06-29T13:30:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T13:30:19Z" id="229355825">LGTM
</comment><comment author="jimczi" created="2016-06-29T13:30:55Z" id="229355977">Thanks @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19150 from jimferenczi/id_field_docs</comment></comments></commit></commits></item><item><title>Docs should say something about dots in field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19149</link><project id="" key="" /><description>Maybe I'm missing it, but I couldn't find much about dots in field names in the docs. I think we should probably have something in the main docs about how Elasticsearch handles dots in field names and something in the migration guide pointing to those docs.
</description><key id="162922438">19149</key><summary>Docs should say something about dots in field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v5.0.0-beta1</label></labels><created>2016-06-29T13:14:54Z</created><updated>2016-09-14T14:43:23Z</updated><resolved>2016-09-02T10:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-07-03T02:51:43Z" id="230132312">Can we also explain why this was removed and then added back in as well please?
</comment><comment author="clintongormley" created="2016-09-02T10:02:06Z" id="244335414">Done
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GPU support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19148</link><project id="" key="" /><description>I would be nice if Elasticsearch/Lucene could support GPUs.  I just saw an article http://tech.marksblogg.com/billion-nyc-taxi-rides-nvidia-tesla-mapd.html and I would love to see something approaching that performance with elasticsearch.

Seems nvidia has some support in this direction:
http://on-demand.gputechconf.com/gtc/2014/presentations/S4506-indexing-documents-on-gpu-web-rt.pdf
http://www.jcuda.org/

While this might really something for lucene project, I thought perhaps it's of more interest for Elasticsearch might have different needs than the generic lucene project.  (could be a good paid ad-on).
</description><key id="162918220">19148</key><summary>GPU support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yehosef</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2016-06-29T12:56:07Z</created><updated>2017-03-27T12:11:36Z</updated><resolved>2016-07-01T09:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T13:37:11Z" id="229357731">By the sounds of http://stackoverflow.com/a/22868938/819598 GPU wouldn't be a great fit for most of the Elasticsearch workload.
</comment><comment author="yehosef" created="2016-06-29T15:42:18Z" id="229397399">Thats an interesting link - thanks.  The pdf document I linked shows as a good fit for indexing data - perhaps because it's easily parallelized.  That would be good for text/logs, less for regular analytics.

But it is interesting to think about how MapD is doing it because that is largely the same problem it seems (I found marks blog because of his reference to using elasticsearch to analyze the NY taxi data - it performed really well considering it was on a single machine and most of the other tests were in cloud clusters).

But thanks for the investigation / consideration.
</comment><comment author="nikonyrh" created="2016-06-29T18:25:03Z" id="229444829">In my understanding Elasticsearch has four distinct workloads:
- Indexing new documents (parse, tokenize, stemming, what have you) + updating datastructures
- Filtering results (yes/no filter)
- Searching results (ranking by relevance)
- Aggregating results

Did you @yehosef have a specific workload in mind? If all data doesn't fit to GPU's memory there are inherent problems with moving data and results back and forth, unless you do "sufficiently much" computation / unit of data.

Also algorithms with high branching factor aren't ideal workload, it is interesting how MapD achieves its performance. Note that the taxi data benchmark was done on a machine which had 32000 USD worth of GPUs.
</comment><comment author="yehosef" created="2016-06-29T22:09:56Z" id="229504479">I didn't have specific workload in mind - all are important. It might be valuable even if it only helps one or two of the workloads. I don't know enough about lucene internals to know whether or where GPUs would help.  But I'll speculate:

1) - the paper mentioned indexing so that seems reasonable.  You have different documents that have to be tokenized, stemmed, etc.  The same, relatively simple work happening over and over again with relatively little interaction between them.
2) merging indices - you have two indices and you need to merge them - index A for the word "foo" and Index B for "foo" need to be combined.  Likewise, the two indices for "bar" need to be merged, etc.
3) I don't know how the aggregations work - seems like the parallelization might help.. different cores dealing with different elements of the agg but I'm not sure.
4) could be searching also  - I have indices for "the", "quick", "brown" and "fox" which might reference millions of documents. I need to break those down to find which documents are in the majority of these terms - and include the tf/idf relationship.  Different cores might take a subset of the document/references and process it and then pass along the results.

So - these are just speculations based on my limited understanding of how Elasticsearch and Lucene work.  Just as Elasticsearch works by breaking down different lucene shards on different machines and bringing the results together - that may also work inside a machine with multiple cores instead of across machines.  

thanks again for thinking about it.
</comment><comment author="s1monw" created="2016-07-01T09:11:54Z" id="229898040">I think this is a too high hanging fruit even for labeling it that way. We discussed it internally and we don't see this happening in the next couple of years.
</comment><comment author="yehosef" created="2016-07-02T22:12:20Z" id="230124474">So I supposed I shouldn't open a ticket to port Elasticsearch to http://www.seastar-project.org/ to get 10x performance boosts like http://www.scylladb.com/ over cassandra?  
</comment><comment author="yehosef" created="2017-03-27T12:11:36Z" id="289434996">For anyone interested - this is being explored in Lucene https://issues.apache.org/jira/browse/LUCENE-7745</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion of empty strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19147</link><project id="" key="" /><description>This feature request is for the completion suggester, to enable the completion of empty text.
There is currently no way to complete empty text, it returns an empty result list (tested on ES 2.2.1).

Use case scenario:
I need to auto-complete the name of cities given a postal code. Usually only one city matches a postal code, but some postal codes have many cities. For example postal_code='75000' has only one city='Paris', whereas postal_code='51300' has 46 cities.

Of course a prefix query could do the job, but the auto-completion should do the job.
</description><key id="162911336">19147</key><summary>Completion of empty strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">gathanase</reporter><labels><label>:Suggesters</label><label>enhancement</label></labels><created>2016-06-29T12:21:07Z</created><updated>2016-07-05T11:05:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T13:21:31Z" id="229353563">@areek what do you think?
</comment><comment author="s1monw" created="2016-07-01T09:15:41Z" id="229898841">I am not sure if we should do that... we don't really have suggestions for the empty string, if you as a caller of the API realize that the query is empty you can just run a plain query to get whatever you need? I don't see what we can suggest on the empty query, it's likely something static or heavily user dependent?
</comment><comment author="clintongormley" created="2016-07-01T12:20:15Z" id="229932747">A query wouldn't work given that the candidates are in the completion FST.  We could just provide the first candidates, ranked by whatever weight is specified in the FST.  This is more useful than it would appear at first glance given that the user is using a context which will narrow down the possible candidates already.
</comment><comment author="gathanase" created="2016-07-04T13:19:58Z" id="230288996">Exactly, the context narrows down the candidates.
In my use case, there are 38000 french cities, and usually only 1 (up to 50) for any given postal code.
</comment><comment author="s1monw" created="2016-07-04T13:39:44Z" id="230293078">@areek I am not sure what the cost is when we traverse the FST by weight from the root arcs can you tell if that is feasible?
</comment><comment author="areek" created="2016-07-04T16:18:03Z" id="230322182">One way to accomplish this with completion suggester in v5 is to use a regex prefix (set `regex: "."` instead of setting `prefix`). Maybe we should default to this, when no prefix or regex is specified in the completion query? With this approach, all unique first characters would be queued upon FST intersection and only the highest `5` (default size is 5) paths would be traversed. WDYT?
</comment><comment author="clintongormley" created="2016-07-04T16:34:17Z" id="230324582">I like the idea and, from your description, it sounds pretty efficient.  +1
</comment><comment author="s1monw" created="2016-07-05T06:43:42Z" id="230398952">sounds good to me @areek 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for Azure Ressource Manager (ARM)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19146</link><project id="" key="" /><description>As explained in #19144 we could create a new plugin named `discovery-azure-arm` which is based on the [newest versions of the Azure SDK](https://github.com/Azure/azure-sdk-for-java/wiki/Azure-SDK-for-Java-Features).

Classic was based on private key / password which was bad for what we want to achieve with such a plugin.
ARM uses now key / secrets which are really much more flexible and safer.

Basically you can now write code like [this example](https://github.com/Azure/azure-sdk-for-java/wiki/Getting-Started#client-creation):

``` java
ComputeManagementClient client;

// Default environment, no credentials
client = new ComputeManagementClientImpl();

// Custom environment with AzureChinaCloud endpoint
String chinaCloudEndpoint = "https://management.chinacloudapi.cn/";
client = new ComputeManagementClientImpl(chinaCloudEndpoint);

// Default environment with credentials
ApplicationTokenCredentials credentials = new ApplicationTokenCredentials(
        "client-id", "tenant-id", "secret", null);
client = new ComputeManagementClientImpl(credentials);

// Custom environment with credentials
client = new ComputeManagementClientImpl(chinaCloudEndpoint, credentials);

// Fully customized, with HTTP client and Retrofit modifiable
client = new ComputeManagementClientImpl(
        chinaCloudEndpoint,
        credentials,
        new OkHttpClient.Builder(),
        new Retrofit.Builder());
```
</description><key id="162897534">19146</key><summary>Add support for Azure Ressource Manager (ARM)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure ARM</label><label>PR sent</label></labels><created>2016-06-29T10:58:30Z</created><updated>2017-06-07T13:41:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-07-01T09:18:17Z" id="229899410">Looks like we should do it. @dadoonet is there anything to discuss here (it has the `discuss` label)?
</comment><comment author="aslanbekirov" created="2016-07-12T07:54:47Z" id="231965444">@dadoonet It seems that  you already started working on it. Is there a planned date for release?
</comment><comment author="dadoonet" created="2016-07-12T08:08:55Z" id="231968316">No date sorry. I just can tell that this will happen only with 5.0 series or later. So won't be available for 2.x series.
</comment><comment author="aslanbekirov" created="2016-07-12T08:26:03Z" id="231972020">We would like to contribute, can you publish the branch that you are working on,so we may continue with that?
</comment><comment author="ejsmith" created="2016-09-04T16:54:39Z" id="244614322">Will this support using an Azure Scale Sets to discover member nodes?
</comment><comment author="justinbarias" created="2017-01-10T01:35:43Z" id="271461514">Hello, any update on this piece of work? Or at least a placeholder plugin for the "azure-discovery-arm" plugin?
</comment><comment author="JamieCressey" created="2017-01-10T13:18:37Z" id="271573224">+1 for this plugin</comment><comment author="synhershko" created="2017-06-07T08:55:44Z" id="306733477">@dadoonet what is the status of this? was this released and can be used with 5.x? Classic plugin has a deprecated warning but no ARM plugin seems to be available at this point?</comment><comment author="dadoonet" created="2017-06-07T12:25:48Z" id="306778885">@synhershko I made some progress recently on this at #22679.
Not ready yet. My plan is to have something ready for 6.x series.

In the meantime, if you want to use azure discovery you still need to use the classic version of the plugin even though it's deprecated.</comment><comment author="synhershko" created="2017-06-07T12:57:16Z" id="306786437">Problem is its useless if you use ARM for your deployment. Any chance this
can be backported (and finished soon :) ) ?

On Jun 7, 2017 3:26 PM, "David Pilato" &lt;notifications@github.com&gt; wrote:

&gt; @synhershko &lt;https://github.com/synhershko&gt; I made some progress recently
&gt; on this at #22679 &lt;https://github.com/elastic/elasticsearch/pull/22679&gt;.
&gt; Not ready yet. My plan is to have something ready for 6.x series.
&gt;
&gt; In the meantime, if you want to use azure discovery you still need to use
&gt; the classic version of the plugin even though it's deprecated.
&gt;
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/19146#issuecomment-306778885&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AAM9HNhhxMdrPxzyv9SdxFT8tYEgF14Uks5sBpb-gaJpZM4JBA0C&gt;
&gt; .
&gt;
</comment><comment author="dadoonet" created="2017-06-07T13:41:59Z" id="306798280">I don't think it will be ever backported when finished.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>vagrant tests should run `vagrant box update` too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19145</link><project id="" key="" /><description>Otherwise everyone needs to run this manually every now and then with is a PITA. See also: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+packaging-tests/826/consoleText

**Elasticsearch version**: master
</description><key id="162896691">19145</key><summary>vagrant tests should run `vagrant box update` too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2016-06-29T10:53:06Z</created><updated>2016-06-29T15:04:54Z</updated><resolved>2016-06-29T15:04:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dliappis" created="2016-06-29T11:31:43Z" id="229330233">+1 on this one. vagrant boxes are versioned and usually bring important improvements or fixes for repos that may become unavailable.

Running `vagrant box update` under the root of the checked out repo, where the `Vagrantfile` resides, will update the boxes referencee in the machine definitions and run pretty fast is everything if already up to date.

Additionally it will avoid people scratching their heads with packagingtest failures due to having downloaded an older version of an elastic box at some point in the past and unnecessarily creating issues.

I think it is as simple as adding it before: https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/build.gradle#L226
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate discovery-azure and rename it to discovery-azure-classic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19144</link><project id="" key="" /><description>As discussed at https://github.com/elastic/elasticsearch-cloud-azure/issues/91#issuecomment-229113595, we know that the current `discovery-azure` plugin only works with Azure Classic VMs / Services (which is somehow Legacy now).

The proposal here is to rename `discovery-azure` to `discovery-azure-classic` in case some users are using it.
And deprecate it for 5.0. 

Note that the renaming already happened from 2.x to 5.0 because we split `cloud-azure` in 2 plugins:
- `discovery-azure`
- `repository-azure`

We can then work on a new `discovery-azure-arm` plugin if there is a need.

Another option could be to provide classic and ARM in the same `discovery-azure` plugin but this will mean that we will depend on multiple Azure SDK jars. Read https://github.com/Azure/azure-sdk-for-java/wiki/Azure-SDK-for-Java-Features
But I don't think we should do it as it could potentially lead us to some JAR conflicts.
</description><key id="162895232">19144</key><summary>Deprecate discovery-azure and rename it to discovery-azure-classic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>discuss</label></labels><created>2016-06-29T10:44:21Z</created><updated>2016-06-30T13:44:58Z</updated><resolved>2016-06-30T13:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2016-06-29T10:49:21Z" id="229322668">Would definitely prefer a clean split so we can easily phase out `discovery-azure-classic` when at such it time it makes sense to do so. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/AzureDiscoveryModule.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/AzureServiceDisableException.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/AzureServiceRemoteException.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/management/AzureComputeService.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/cloud/azure/classic/management/AzureComputeServiceImpl.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/discovery/azure/classic/AzureUnicastHostsProvider.java</file><file>plugins/discovery-azure-classic/src/main/java/org/elasticsearch/plugin/discovery/azure/classic/AzureDiscoveryPlugin.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/AbstractAzureComputeServiceTestCase.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/AzureComputeServiceSimpleMock.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/AzureComputeServiceTwoNodesMock.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/cloud/azure/classic/management/AzureComputeServiceAbstractMock.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureDiscoveryClusterFormationTests.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureDiscoveryRestIT.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureMinimumMasterNodesTests.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureSimpleTests.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureTwoStartedNodesTests.java</file></files><comments><comment>Deprecate discovery-azure and rename it to discovery-azure-classic</comment></comments></commit></commits></item><item><title>Combine query with completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19143</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 16.04

**Description of the problem including expected versus actual behavior**:

Hi, I'm trying to combine query with completion suggester. I made an example available on found.no:
https://www.found.no/play/gist/667e4d974fe0f0045e5c0b5e44e47b06

Basically I would like to filter completion suggester by some criteria (like organization tenancy, etc.). 
I saw that a context extension is available on completion query.

I would like to know if it's a bug or if completion suggester doesn't take in account query, filter, post_filter, etc.? Should we use context to filter suggestions?

I tried on different versions, especially Elasticsearch 2.3.3.

**Steps to reproduce**:
https://www.found.no/play/gist/667e4d974fe0f0045e5c0b5e44e47b06
</description><key id="162876103">19143</key><summary>Combine query with completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeremyb</reporter><labels /><created>2016-06-29T09:08:46Z</created><updated>2016-07-11T08:29:59Z</updated><resolved>2016-06-29T13:00:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T13:00:11Z" id="229348245">Hi @jeremyb 

The completion suggester uses a completely separate mechanism from search, and so can't be combined with queries.  This is documented.
</comment><comment author="jeremyb" created="2016-06-29T13:09:42Z" id="229350547">Thanks for your reply @clintongormley. I suspected something like this.

IMHO the fact that "completion suggester uses a completely separate mechanism from search" is not very clear in the documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html. And the first example with a mix of query and suggest can be confusing.
</comment><comment author="squinker" created="2016-07-11T08:29:59Z" id="231672252">Yes, I made the same mistake here, it could really use a bit of highlighting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>inner_hits breaks from version 1.7 to 2.3 upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19142</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.7.0_95

**OS version**: Red Hat Enterprise Linux Workstation release 6.7 (Santiago)

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 I have described the issue on community at https://discuss.elastic.co/t/inner-hits-queries-break-from-1-7-to-2-3-in-elasticsearch/54242 .

I had an inner_hits query which was working fine in version 1.7 but it stopped working in version 2.3.3. Following is the query :

```
"query": {
    "filtered": {
        "filter": {
            "and": [
                {
                    "range": {
                        "@timestamp": {
                             "gte": "2015-01-01||/d",
                             "lte": "2016-01-01||/d"
                        }
                    }
                },
                {
                    "or": [
                        {
                            "has_child": {
                                "type": "TypeName",
                                "filter": {
                                    "term": {
                                        "eventType.raw": "some-event"
                                    }
                                },
                                "inner_hits": {
                                    "_source": "@timestamp"
                                }
                            }
                        },
                        {
                            "not": {
                                "has_child": {
                                    "type": "TypeName",
                                    "filter": {
                                        "term": {
                                            "eventType.raw": "some-event"
                                        }
                                    },
                                    "inner_hits": {
                                        "_source": "@timestamp"
                                    }
                                }
                            }
                        }
                    ]
                }
            ]
        }
    }
}
```

With version 2.3.3 I get the following response for it :

```
"reason": {
   "type": "illegal_argument_exception",
   "reason": "inner_hit definition with the name [TypeName] already exists. Use a different inner_hit name"
}
```

**Provide logs (if relevant)**:
Following is the error stack

```
nested: IllegalArgumentException[inner_hit definition with the name [TypeName] already exists. Use a different inner_hit name];
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:855)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:654)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:620)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:371)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: inner_hit definition with the name [TypeName] already exists. Use a different inner_hit name
    at org.elasticsearch.search.fetch.innerhits.InnerHitsContext.addInnerHitDefinition(InnerHitsContext.java:69)
    at org.elasticsearch.index.query.QueryParseContext.addInnerHits(QueryParseContext.java:218)
    at org.elasticsearch.index.query.HasChildQueryParser.parse(HasChildQueryParser.java:155)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:277)
    at org.elasticsearch.index.query.NotQueryParser.parse(NotQueryParser.java:69)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:263)
    at org.elasticsearch.index.query.OrQueryParser.parse(OrQueryParser.java:69)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:263)
    at org.elasticsearch.index.query.AndQueryParser.parse(AndQueryParser.java:69)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:263)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:79)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:838)
    ... 12 more



```
</description><key id="162875303">19142</key><summary>inner_hits breaks from version 1.7 to 2.3 upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">siddharthgoel88</reporter><labels><label>adoptme</label><label>low hanging fruit</label><label>non-issue</label></labels><created>2016-06-29T09:04:35Z</created><updated>2016-08-18T20:31:16Z</updated><resolved>2016-08-18T20:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="siddharthgoel88" created="2016-06-29T09:22:35Z" id="229303940">May be https://github.com/elastic/elasticsearch/issues/16218 could give some more insight into this.
</comment><comment author="dadoonet" created="2016-06-29T09:54:20Z" id="229311328">@martijnvg suggested that we could come with a better error message: https://discuss.elastic.co/t/inner-hits-queries-break-from-1-7-to-2-3-in-elasticsearch/54242/4

Marking as "non issue"
</comment><comment author="clintongormley" created="2016-06-29T13:01:33Z" id="229348578">I think the exception is pretty clear already.  No idea about how to improve it.  Any suggestions?
</comment><comment author="dadoonet" created="2016-06-29T13:11:17Z" id="229350907">The thing is that the user did not explicitly define a name. It was inherited from the has_child query. So may be we could say something like

&gt; inner_hit definition with the name [TypeName] already exists. Use a different inner_hit name **or define one explicitly**.
</comment><comment author="clintongormley" created="2016-06-29T13:28:46Z" id="229355442">ah right, gotcha
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Extract AbstractBytesReferenceTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19141</link><project id="" key="" /><description>We have a ton of tests for PagedBytesReference but not really many for the other
implementation of BytesReference. This change factors out a basic AbstractBytesReferenceTestCase
that simplifies testing other implementations. It also caught a couple of bug here and there like
a missing mask when reading bytes as ints in PagedBytesReference.

@jpountz can you take a look
</description><key id="162872109">19141</key><summary>Extract AbstractBytesReferenceTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T08:48:54Z</created><updated>2016-06-29T12:45:54Z</updated><resolved>2016-06-29T12:45:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-29T10:33:55Z" id="229319787">LGTM, it'd just be nice to cut over the try/fail/catch/success patter to expectThrows.
</comment><comment author="s1monw" created="2016-06-29T12:39:19Z" id="229343525">@jpountz I pushed an update
</comment><comment author="jpountz" created="2016-06-29T12:45:10Z" id="229344775">LGTTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/netty/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferStreamInput.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/AbstractBytesReferenceTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/BytesArrayTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/netty/ChannelBufferBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/netty/NettyUtilsTests.java</file></files><comments><comment>Extract AbstractBytesReferenceTestCase (#19141)</comment></comments></commit></commits></item><item><title>Persistent Node Ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19140</link><project id="" key="" /><description>Node IDs are currently randomly generated during node startup. That means they change every time the node is restarted. While this doesn't matter for ES proper, it makes it hard for external services to track nodes. Another, more minor, side effect is that indexing the output of, say, the node stats API results in creating new fields due to node ID being used as keys.

The first approach I considered was to use the node's published address as the base for the id. We already [treat nodes with the same address as the same](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java#L387) so this is a simple change (see [here](https://github.com/elastic/elasticsearch/compare/master...bleskes:node_persistent_id_based_on_address)). While this is simple and it works for probably most cases, it is not perfect. For example, if after a node restart, the node is not able to bind to the same port (because it's not yet freed by the OS), it will cause the node to still change identity. Also in environments where the host IP can change due to a host restart, identity will not be the same. 

Due to those limitation, I opted to go with a different approach where the node id will be persisted in the node's data folder. This has the upside of connecting the id to the nodes data. It also means that the host can be adapted in any way (replace network cards, attach storage to a new VM). I

It does however also have downsides - we now run the risk of two nodes having the same id, if someone copies clones a data folder from one node to another. To mitigate this I changed the semantics of the protection against multiple nodes with the same address to be stricter - it will now reject the incoming join if a node exists with the same id but a different address. Note that if the existing node doesn't respond to pings (i.e., it's not alive) it will be removed and the new node will be accepted when it tries another join.

Last, and most importantly, this change requires that _all_ nodes persist data to disk. This is a change from current behavior where only data &amp; master nodes store local files. This is the main reason for marking this PR as breaking.

Other less important notes:
- DummyTransportAddress is removed as we need a unique network address per node. Use `LocalTransportAddress.buildUnique()` instead.
- I renamed `node.add_lid_to_custom_path` to `node.add_lock_id_to_custom_path` to avoid confusion with the node ID which is now part of the `NodeEnvironment` logic.
- I removed the `version` paramater from `MetaDataStateFormat#write` , it wasn't really used and was just in the way :)
- TribeNodes are special in the sense that they do start multiple sub-nodes (previously known as client nodes). Those sub-nodes do not store local files but derive their ID from the parent node id, so they are generated consistently.

This PR supersedes #17811, and changes it by adding an ephimeralID field to DiscoveryNode that maintains the current id semantics, i.e., it changes with each node restart. This allows to keep the same semantics and use it for node equality.
</description><key id="162870965">19140</key><summary>Persistent Node Ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-29T08:42:54Z</created><updated>2016-07-04T19:09:25Z</updated><resolved>2016-07-04T19:09:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-30T10:14:56Z" id="229618842">@bleskes Left some comments on the PR but looks good overall. Can you add documentation as well? (breaking change). I'm ok if you want to address tribe nodes as well in this PR (I don't think it's too big of a change, see #17987).
</comment><comment author="bleskes" created="2016-07-03T20:23:19Z" id="230173296">@ywelsch Thanks. I pushed a commit addressing your comments. I'm not sure about the tribe nodes - will reach out to discuss more.
</comment><comment author="bleskes" created="2016-07-04T13:35:59Z" id="230292314">@ywelsch I pushed another commit with the tribe node change we discussed.
</comment><comment author="ywelsch" created="2016-07-04T15:30:46Z" id="230314590">LGTM. Can you also add something to the migration docs? (see e.g. https://github.com/elastic/elasticsearch/pull/17987/files#diff-2d50fb3821a6a54aedf97e29135d711aR13 )
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/node/NodeTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/IndexSettingsModule.java</file></files><comments><comment>Persistent Node Names (#19456)</comment></comments></commit><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/common/hash/MurmurHash3.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/LocalTransportAddress.java</file><file>core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeMetaData.java</file><file>core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>core/src/main/java/org/elasticsearch/gateway/MetaStateService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplanationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterChangedEventTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/NodeConnectionsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/IndexFolderUpgraderTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryTargetTests.java</file><file>core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/ConfigurationUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/NodeInfoStreamingTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeServiceTests.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java</file><file>test/framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ClusterServiceUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Persistent Node Ids (#19140)</comment></comments></commit></commits></item><item><title>Roadmap for Elasticsearch 5 release candidate?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19139</link><project id="" key="" /><description>The latest version of ES is [elasticsearch-5-0-0-alpha3](https://github.com/elastic/elasticsearch/releases/tag/v5.0.0-alpha3)

Doese someone know about roadmap for Elasticsearch 5 release candidate?
Thank you. 
</description><key id="162858337">19139</key><summary>Roadmap for Elasticsearch 5 release candidate?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phathvu</reporter><labels /><created>2016-06-29T07:26:45Z</created><updated>2016-10-06T09:51:22Z</updated><resolved>2016-06-29T08:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-29T08:27:55Z" id="229290593">Please ask questions on discuss.elastic.co.

You will probably find an already answered question there BTW.
</comment><comment author="satterly" created="2016-10-06T09:51:22Z" id="251916743">See https://discuss.elastic.co/t/elastic-5-0-0-release-date/61382
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>High CPU usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19138</link><project id="" key="" /><description>Hi,  I encountered a problem when I insert data, the following specific information.
(Elasticsearch 2.3.1, JVM version 1.7.0_80x, CentOS release 6.6 (Final))

100.6% (502.7ms out of 500ms) cpu usage by thread 'elasticsearch[Pressure][index][T#8]'
     3/10 snapshots sharing following 30 elements
       java.lang.invoke.LambdaForm$DMH/1697384030.invokeSpecial_LL_L(LambdaForm$DMH)
       java.lang.invoke.LambdaForm$BMH/1674119038.reinvoke(LambdaForm$BMH)
       java.lang.invoke.LambdaForm$MH/5369010.invokeExact_MT(LambdaForm$MH)
       java.lang.invoke.MethodHandleImpl$GuardWithCatch.invoke_L1(MethodHandleImpl.java:619)
       java.lang.invoke.LambdaForm$DMH/1697384030.invokeSpecial_LL_L(LambdaForm$DMH)
       java.lang.invoke.LambdaForm$BMH/1674119038.reinvoke(LambdaForm$BMH)
       java.lang.invoke.LambdaForm$DMH/1697384030.invokeSpecial_LL_L(LambdaForm$DMH)
       java.lang.invoke.LambdaForm$MH/688130558.guard(LambdaForm$MH)
       java.lang.invoke.LambdaForm$DMH/1697384030.invokeSpecial_LL_L(LambdaForm$DMH)
       java.lang.invoke.LambdaForm$MH/1319810003.guard(LambdaForm$MH)
       java.lang.invoke.LambdaForm$MH/303157806.linkToCallSite(LambdaForm$MH)
       2057dd62e69d7b398105d17365a0e4fffae8cc18.run(2057dd62e69d7b398105d17365a0e4fffae8cc18:1)
       org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:313)
       java.security.AccessController.doPrivileged(Native Method)
       org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:310)
       org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:252)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:197)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:80)
       org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:174)
       org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:168)
       org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:66)
       org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:244)
       org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:240)
       org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
       org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

**This is the code:** 

`public void run() {
        while (true) {
            final long start = System.currentTimeMillis();
            int max = 0;
            for (int j = 0; j &lt; 1024; j++) {
                max = ATOMIC.incrementAndGet();
                final String id = Integer.toString(max);
                final IndexTest entity = new IndexTest();
                entity.setId(id);
                repository.save("index_test", "type", id, id, entity, false);
            }
            final long end = System.currentTimeMillis();
            long cost = end - start;
            long avgCost = cost / 1024;
            LOGGER.info("thread index: {},\t run: {},\t cost: {}ms,\t average cost: {}ms, maxValue: {}", this.index,
                    1024, cost, avgCost, max);
        }
    }`

**logger**
2016-06-29 04:01:58 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 6,    run: 1024,  cost: 9971ms,   average cost: 9ms, maxValue: 29784947
2016-06-29 04:01:58 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 3,    run: 1024,  cost: 9833ms,   average cost: 9ms, maxValue: 29785724
2016-06-29 04:01:59 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 2,    run: 1024,  cost: 9844ms,   average cost: 9ms, maxValue: 29786615
2016-06-29 04:02:01 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 7,    run: 1024,  cost: 9754ms,   average cost: 9ms, maxValue: 29787929
2016-06-29 04:02:02 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 8,    run: 1024,  cost: 9655ms,   average cost: 9ms, maxValue: 29789595
2016-06-29 04:02:02 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 4,    run: 1024,  cost: 9641ms,   average cost: 9ms, maxValue: 29789704
2016-06-29 04:02:03 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 5,    run: 1024,  cost: 9759ms,   average cost: 9ms, maxValue: 29790382
2016-06-29 08:48:06 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 9,    run: 1024,  cost: 17170973ms,   average cost: 16768ms, maxValue: 29792325
2016-06-29 08:48:06 INFO [com.chinatime.datacenter.test.elastic.InsertES2]:71 Line - thread index: 3,    run: 1024,  cost: 17167821ms,   average cost: 16765ms, maxValue: 29796080
Exception in thread "pool-1-thread-6" ElasticsearchTimeoutException[Timeout waiting for task.]
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:70)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:62)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:52)
    at com.chinatime.common.elasticsearch.utils.AbstractElasticRepository.save(AbstractElasticRepository.java:120)
    at com.chinatime.datacenter.test.elastic.InsertES2.run(InsertES2.java:60)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Work
</description><key id="162851082">19138</key><summary>High CPU usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iokays</reporter><labels /><created>2016-06-29T06:35:47Z</created><updated>2016-06-29T12:54:22Z</updated><resolved>2016-06-29T06:53:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="iokays" created="2016-06-29T06:40:59Z" id="229269857">CUP INFO:

Cpu0  : 99.7%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st
Cpu1  :  0.3%us,  0.0%sy,  0.0%ni, 99.3%id,  0.3%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu2  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu3  :  1.3%us,  0.7%sy,  0.0%ni, 98.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu4  :  0.7%us,  0.7%sy,  0.0%ni, 98.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu5  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu6  :  0.7%us,  0.0%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu7  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:  32827372k total, 19282824k used, 13544548k free,   492640k buffers
Swap: 20479996k total,        0k used, 20479996k free,  7979768k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND  
20324 public    20   0 6870m 1.4g  43m S 403.0  4.3   6235:34 java       
</comment><comment author="jpountz" created="2016-06-29T06:53:25Z" id="229271839">Most cpu time seems to be spent running your update script. You might want to simplify it or apply the update on client side if you really need to make it faster, but otherwise there is nothing wrong with having high CPU usage given that you are trying to index at full throttle.
</comment><comment author="iokays" created="2016-06-29T07:05:34Z" id="229273916">@jpountz  Thank you, but I stopped writing this is now the state of the CPU data after 7 hours, has not come down. hot_threads too.
</comment><comment author="jpountz" created="2016-06-29T07:38:53Z" id="229280067">Is it possible that you have an infinite loop in your update scripts?
</comment><comment author="iokays" created="2016-06-29T07:55:47Z" id="229283543">I now have the method to script the update of total abolition, and then test to see test results.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make plugins closeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19137</link><project id="" key="" /><description>This change allows Plugin implementions to implement Closeable when they
have resources that should be released. As a first example of how this
can be used, I switched over ingest plugins, which just had the geoip
processor. The ingest framework had chains of closeable to support this,
which is now removed.
</description><key id="162808802">19137</key><summary>Make plugins closeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T23:17:30Z</created><updated>2016-06-29T20:54:20Z</updated><resolved>2016-06-29T20:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-29T06:32:42Z" id="229268633">LGTM
</comment><comment author="martijnvg" created="2016-06-29T06:33:27Z" id="229268733">Nice cleanup! LGTM2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ingest/IngestService.java</file><file>core/src/main/java/org/elasticsearch/ingest/PipelineStore.java</file><file>core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java</file></files><comments><comment>Merge pull request #19137 from rjernst/closeable_plugins</comment></comments></commit></commits></item><item><title>Remove ClusterState from compile api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19136</link><project id="" key="" /><description>Stored scripts are pulled from the cluster state, and the current api
requires passing the ClusterState on each call to compile. However, this
means every user of the ScriptService needs to depend on the
ClusterService. Instead, this change makes the ScriptService a
ClusterStateListener. It also simplifies tests a lot, as they no longer
need to create fake cluster states (except when testing stored scripts, which apparently we don't do because tests with this change pass...I'll work on adding those).
</description><key id="162791055">19136</key><summary>Remove ClusterState from compile api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T21:24:13Z</created><updated>2016-06-30T13:04:59Z</updated><resolved>2016-06-30T05:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-29T04:16:47Z" id="229252196">I'm not sure, but does this break scrolling with a script? Namely, we hold the cluster state in the search context so it's constant during the life of the scroll but this change opens such scrolls to concurrent modification issues (in particular, modifying the script used in the scroll).  
</comment><comment author="martijnvg" created="2016-06-29T06:40:40Z" id="229269805">The `ClusterState` parameter isn't pretty, but it ensures that during the lifespan of a search context the the same stored scripts are used (so scripts don't disappear or get modified during the execution of a (scroll) search request). I think we want to keep this behaviour?
</comment><comment author="jpountz" created="2016-06-29T10:43:18Z" id="229321588">Isn't it a bit broken already today? For instance if you perform modifications of the scripts concurrently with search requests then shards might have different versions of the script depending on cluster state propagation delays?

Regardless of this, removing the ClusterState parameter from this method is appealing to me. I am not sure the benefits are worth the cost, I would rather add warnings to the documentation that performing script modifications concurrently with search requests leads to undefined behaviours.
</comment><comment author="jasontedor" created="2016-06-29T11:13:26Z" id="229326999">&gt; Isn't it a bit broken already today? For instance if you perform modifications of the scripts concurrently with search requests then shards might have different versions of the script depending on cluster state propagation delays?

This is an inherent problem with extracting the state on multiple nodes; this problem will still exist with this PR as-is.

&gt; Regardless of this, removing the ClusterState parameter from this method is appealing to me. I am not sure the benefits are worth the cost, I would rather add warnings to the documentation that performing script modifications concurrently with search requests leads to undefined behaviors.

I agree that removing the `ClusterState` parameter is appealing, but I disagree with the conclusion. Scrolls can be long-running and I think this would be a limitation.

I think that we can solve both of these problems if we extract the script on the coordinating node and attach it to the query before pushing it to the executing shards?
</comment><comment author="jasontedor" created="2016-06-29T14:03:28Z" id="229365462">&gt; &gt; Regardless of this, removing the ClusterState parameter from this method is appealing to me. I am not sure the benefits are worth the cost, I would rather add warnings to the documentation that performing script modifications concurrently with search requests leads to undefined behaviors.
&gt; 
&gt; I agree that removing the ClusterState parameter is appealing, but I disagree with the conclusion. Scrolls can be long-running and I think this would be a limitation.

One other thought on this: another issue that I have with it is that the indices can be updated during a scroll, so it would be a confusing UX for the script to not be updatable during a scroll.
</comment><comment author="rjernst" created="2016-06-29T18:42:53Z" id="229449985">I spoke with @jpountz and @jasontedor and opened an issue to store scripts in the query context (#19161) and an issue to resolve scripts on the coordinating node (#19162). I think this issue should go in as is, since stored scripts were only added recently (and this behavior would have already been broken with their predecessor, indexed scripts), and this is clearly the right move from the ScriptService api. I will follow up on the other issues to get back the consistent scroll behavior.
</comment><comment author="jasontedor" created="2016-06-29T18:57:32Z" id="229454159">&gt;  I think this issue should go in as is

Agree, this is a good step forward.

&gt; this is clearly the right move from the ScriptService api

Modulo removing the `ClusterStateListener` implementation as the two issues that you opened will move us to.
</comment><comment author="jasontedor" created="2016-06-30T02:54:44Z" id="229546794">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/ScriptScoreFunctionBuilder.java</file><file>core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/InternalScriptedMetric.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketselector/BucketSelectorPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java</file><file>core/src/test/java/org/elasticsearch/script/FileScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/IngestCommonPlugin.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/ScriptProcessor.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ScriptProcessorFactoryTests.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/ScriptProcessorTests.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/action/search/template/TransportSearchTemplateAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkIndexByScrollAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportReindexAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportUpdateByQueryAction.java</file></files><comments><comment>Merge pull request #19136 from rjernst/script_service_deps</comment></comments></commit></commits></item><item><title>Reindex: Skip fetching version if we aren't going to use it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19135</link><project id="" key="" /><description>The `_reindex` API sometimes uses the `_version` of the documents that it is reindex and sometimes doesn't, but it **always** fetches the version. It should really only fetch it if the request has `version_type=external`.
</description><key id="162773114">19135</key><summary>Reindex: Skip fetching version if we aren't going to use it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T20:00:30Z</created><updated>2016-08-04T09:12:46Z</updated><resolved>2016-07-29T21:14:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequest.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/ScrollableHitSource.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportDeleteByQueryAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportReindexAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportUpdateByQueryAction.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteResponseParsers.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/AsyncBulkByScrollActionTests.java</file></files><comments><comment>Reindex: Only ask for _version we need it</comment></comments></commit></commits></item><item><title>Running elasticsearch-5.0.0-alpha3 on Ubuntu 14.04 seems to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19134</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha3

**JVM version**: 1.8.0_25

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

elasticsearch doesn't seem to be doing any thing. In particular, I don't see any log messages about binding to IP and port. Even "netstat -anp" doesn't show any bindings.

**Steps to reproduce**:
1. cd elasticsearch-5.0.0-alpha3
2. ./bin/elasticsearch
</description><key id="162772887">19134</key><summary>Running elasticsearch-5.0.0-alpha3 on Ubuntu 14.04 seems to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">draghuram</reporter><labels /><created>2016-06-28T19:59:16Z</created><updated>2017-03-29T11:42:08Z</updated><resolved>2016-06-28T20:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-28T20:01:31Z" id="229165110">Sorry that you're having trouble, but we'll need more detail than this to debug. What log messages do you see? Can you take a stack dump with `jstack` when it's apparently hanging?
</comment><comment author="draghuram" created="2016-06-28T20:06:45Z" id="229166628">I see no log messages on STDOUT. Is there a log file I can check?

I am attaching the jstack output.
[es.txt](https://github.com/elastic/elasticsearch/files/337939/es.txt)
</comment><comment author="jasontedor" created="2016-06-28T20:10:42Z" id="229167743">From the stack trace I can tell you that there will not have been any log messages at this point.

```
"main" #1 prio=5 os_prio=0 tid=0x00007f84d000b000 nid=0x296b runnable [0x00007f84d95e2000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.fs.UnixNativeDispatcher.stat0(Native Method)
    at sun.nio.fs.UnixNativeDispatcher.stat(UnixNativeDispatcher.java:286)
    at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:70)
    at sun.nio.fs.UnixFileStore.devFor(UnixFileStore.java:55)
    at sun.nio.fs.UnixFileStore.&lt;init&gt;(UnixFileStore.java:70)
    at sun.nio.fs.LinuxFileStore.&lt;init&gt;(LinuxFileStore.java:48)
    at sun.nio.fs.LinuxFileSystem.getFileStore(LinuxFileSystem.java:112)
    at sun.nio.fs.UnixFileSystem$FileStoreIterator.readNext(UnixFileSystem.java:213)
    at sun.nio.fs.UnixFileSystem$FileStoreIterator.hasNext(UnixFileSystem.java:224)
    - locked &lt;0x00000000807cc8e8&gt; (a sun.nio.fs.UnixFileSystem$FileStoreIterator)
    at org.apache.lucene.util.IOUtils.getFileStore(IOUtils.java:543)
    at org.apache.lucene.util.IOUtils.spinsLinux(IOUtils.java:487)
    at org.apache.lucene.util.IOUtils.spins(IOUtils.java:476)
    at org.elasticsearch.env.ESFileStore.&lt;init&gt;(ESFileStore.java:60)
    at org.elasticsearch.env.Environment.&lt;clinit&gt;(Environment.java:105)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:98)
    at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:194)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:234)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
    at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
```

Is that always where it's hung? Maybe take `jstack` a few more times to be sure?

That's where it's trying to detect if you have an SSD. I think this is environmental; is something wrong with your filesystem?
</comment><comment author="draghuram" created="2016-06-28T20:16:33Z" id="229169346">The latest jstack output shows identical trace as above. I do have a hanging NFS mount and when I unmounted it, ES did proceed and there were some log messages about IP and port number. So the bad NFS mount was the problem. I am curious why ES is going through all the mount points, however.

Thanks for quick help any way.
</comment><comment author="jasontedor" created="2016-06-28T20:21:54Z" id="229170788">NFS is what's wrong with your filesystem. 😛

&gt; I am curious why ES is going through all the mount points, however.

We cache them on startup (expensive) to look up the store paths later.

&gt; Thanks for quick help any way.

You're welcome.
</comment><comment author="draghuram" created="2016-06-28T20:34:55Z" id="229174573">If I may suggest, it would be helpful if ES logs messages as it looks up (could be DEBUG level) paths. These messages can be shown with "-v" option and that will help in debugging.
</comment><comment author="nik9000" created="2016-06-28T20:37:48Z" id="229175416">We could almost certainly log a warning if the process takes more than a second....
</comment><comment author="jasontedor" created="2016-06-28T20:40:03Z" id="229176040">Responding to comment from @draghuram:

&gt; If I may suggest, it would be helpful if ES logs messages as it looks up (could be DEBUG level) paths. These messages can be shown with "-v" option and that will help in debugging.

and @nik9000:

&gt; We could almost certainly log a warning if the process takes more than a second....

This is happening in Lucene, before we've even bootstrapped. Yes, we can fork a thread and so on, but I think that's overkill here?
</comment><comment author="s1monw" created="2016-06-28T20:47:29Z" id="229178162">&gt; This is happening in Lucene, before we've even bootstrapped. Yes, we can fork a thread and so on, but I think that's overkill here?

I agree - what if the NFS mount hangs at a later stage we are just pushing the problem out. 
</comment><comment author="draghuram" created="2016-06-28T20:48:14Z" id="229178386">Though I don't know internals of ES, forking a thread just for this purpose does sound overkill.
</comment><comment author="NeilRickards" created="2017-03-29T11:42:08Z" id="290064386">Does ES actually have anything on that NFS mount - data, binaries, config, etc.?
If so, then failing when the mount is unavailable sounds reasonable (though ideally with more logging).
If, on the other hand, ES (or Lucene) is just enumerating all the mounts on the system and hanging, then that sounds like undesirable behaviour.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added documentation for aggregation profiling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19133</link><project id="" key="" /><description /><key id="162749512">19133</key><summary>Added documentation for aggregation profiling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T18:07:07Z</created><updated>2016-06-28T18:33:55Z</updated><resolved>2016-06-28T18:33:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-28T18:16:47Z" id="229135813">LGTM other than a couple of minor typoes
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>#19133 Added documentation for aggregation profiling</comment></comments></commit></commits></item><item><title>Show UUID in cat indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19132</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
5.x

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
In 5.x, index directory names on disk are named with UUID instead of index name. It would be very helpful to show the UUID in `_cat/indices` as opposed to only fetching it from index settings. 
</description><key id="162746772">19132</key><summary>Show UUID in cat indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">inqueue</reporter><labels><label>:CAT API</label><label>adoptme</label><label>feature</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T17:54:35Z</created><updated>2016-07-01T18:46:55Z</updated><resolved>2016-07-01T18:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/cat/RestIndicesActionTests.java</file></files><comments><comment>Includes the index UUID in the _cat/indices API and adds tests</comment><comment>for the _cat/indices functionality.</comment></comments></commit></commits></item><item><title>Proof of concept: Build actions "without" guice </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19131</link><project id="" key="" /><description>Instead of building actions with guice this instead builds them using
a simple helper around reflective constructor invocation. Guice is
still used for resolving services, but those that are available
outside of guice are used instead. This provides a "path away from
guice": we work to resolve all of the services without guice. Once
we've done so we should be 90% of the way to removing guice.
</description><key id="162733829">19131</key><summary>Proof of concept: Build actions "without" guice </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>discuss</label><label>review</label></labels><created>2016-06-28T16:50:24Z</created><updated>2016-07-04T11:06:18Z</updated><resolved>2016-07-02T11:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-28T16:50:58Z" id="229110435">I started playing with this last night. I'm not sure that it is even close to the right way for us to proceed but it gives something to talk about so I opened the PR.
</comment><comment author="nik9000" created="2016-06-28T16:51:33Z" id="229110584">I didn't go through all the actions and remove the `@Inject` annotation. I don't think that helps demonstrate the idea and that is the whole point of this PR.
</comment><comment author="s1monw" created="2016-06-28T18:39:51Z" id="229142724">wait so this builds a poor mans guice to remove guice? I think that's the wrong way to approach this. On a high level we have to figure out a way to create these actions in a different way! If we can't move to a model where we call ctors without reflection I think we can just stick with guice it's not a single bit better though. The next steps IMO are to separate out the TransportService  from guice which requires some work downstream I think on plugins but from there on once we have TransportService created without guice there are lots of things we can do / create without guice. I also think if we go down that route we will have better ideas later how to fix this?
</comment><comment author="nik9000" created="2016-06-28T20:17:12Z" id="229169533">I think not having a strategy to build the actions other than guice is holding back how we build the plugin interfaces and remove guice from other spots. We keep binding things to guice because actions need it.

We could certainly do something similar without reflection using a huge context object. It'd need a thing for actions and some strategy for dealing with services declared by plugins (closure in the plugin?). And it'd have all the stuff that I was cramming into the poor man's guice - unless we remove lots of those dependencies but I don't think we'll be able to squash too many. The advantage of the context object is that a plugin author knows all the things they have access to. We'd have to go away from registering classes and to registering constructors - which is much better any way.

I could certainly try and work something up like that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[BUG] Not expected behaviour with filter on percolate's _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19130</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3.3

**JVM version**:
java version "1.7.0_101"
OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

**OS version**:
Ubuntu 14.04.4

**Description of the problem including expected versus actual behavior**:
I've done a percolation on my index and i've applied a term filter on percolate's `_id` field, but the response is not the expected.

request

```
GET index/type/1/_percolate
{
  "filter": {
    "term": {
      "_id": "query_1"
    }
  }
}
```

expected response

```
{
  "took": 41,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "total": 1,
  "matches": [
    {
      "_index": "index",
      "_id": "query_1"
    }
  ]
}
```

actual response

```
{
  "took": 14,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "total": 0,
  "matches": []
}
```

**Steps to reproduce**:
1. Create an index
2. Store a query in percolator
3. And do a percolation with filter on _id
</description><key id="162719993">19130</key><summary>[BUG] Not expected behaviour with filter on percolate's _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">giovannialbero1992</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-28T15:48:06Z</created><updated>2016-07-05T07:53:57Z</updated><resolved>2016-07-05T07:53:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T12:03:14Z" id="229336167">Yes, for some reason the percolator filter doesn't support the `_id` lookup via `_uid`.  You can work around this by filtering on `_uid` instead:

```
{
  "filter": {
    "term": {
      "_uid": ".percolator#query_1"
    }
  }
}
```

Note: percolation has changed completely in 5.0.  Instead of the percolate API we now have the percolate query, which is just part of the query DSL.  As such, the rest of the query DSL will work as expected.
</comment><comment author="martijnvg" created="2016-07-05T07:53:57Z" id="230410453">Closed via #19210
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Factor out ChannelBuffer from BytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19129</link><project id="" key="" /><description>The ChannelBuffer interface today leaks into the BytesReference abstraction
which causes a hard dependency on netty across the board. This chance moves
this dependency and all BytesReference -&gt; ChannelBuffer conversion into
NettyUtlis and removes the abstraction leak on BytesReference.
This change also removes unused methods on the BytesReference interface
and simplifies access to internal pages.
</description><key id="162717049">19129</key><summary>Factor out ChannelBuffer from BytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T15:36:13Z</created><updated>2016-06-29T08:45:06Z</updated><resolved>2016-06-29T08:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-28T18:40:57Z" id="229143040">@jasontedor, @rmuir or @rjernst can anybody take a look at this
</comment><comment author="rjernst" created="2016-06-28T19:45:24Z" id="229160637">This looks ok to me. The one comment I left I think indicates the special case for slice in the iterator is not necessary? The randomized test looks correct to me. I think without the special case, it just means it is still going by page, but it is not the same pages of bytes you would get if you started without an offset (to me that is ok, I don't think page alignment matters there, but maybe that was the intent and I am wrong).
</comment><comment author="s1monw" created="2016-06-29T08:01:42Z" id="229284867">@rjernst I simplified the iteration and added assertions. The good news is that the test failed immediately after I added the assertions. I will push this in a bit
</comment><comment author="jpountz" created="2016-06-29T08:07:33Z" id="229286035">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/BytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/bytes/ReleasablePagedBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/netty/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/common/netty/NettyUtils.java</file><file>core/src/main/java/org/elasticsearch/common/util/ByteArray.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferStreamInput.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransportChannel.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/netty/NettyUtilsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/common/bytes/ByteBufferBytesReference.java</file></files><comments><comment>Factor out ChannelBuffer from BytesReference (#19129)</comment></comments></commit></commits></item><item><title>Add missing permission to repository-s3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19128</link><project id="" key="" /><description>Note: PR against master branch, 2.4 is #19121

S3 repository needs a special permission to work because when no region is explicitly set the AWS SDK will load a JSON file that contain all Amazon's endpoints and will map the content of this file to plain old Java objects. To do that, it uses Jackson's databinding and reflection that require the java.lang.reflect.ReflectPermission "suppressAccessChecks" permission.

This issue only occur if no region is set in the repository setting and in the elasticsearch.yml file.

closes #18539
</description><key id="162710175">19128</key><summary>Add missing permission to repository-s3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Repository S3</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T15:09:00Z</created><updated>2016-07-01T08:44:18Z</updated><resolved>2016-07-01T08:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-28T16:40:12Z" id="229107365">The explanation isn't correct: its not jackson's fault. Its the fault of how they are using it, the access modifiers on their classes is simply wrong. This isn't necessary.
</comment><comment author="clintongormley" created="2016-06-29T12:12:35Z" id="229337985">Perhaps the easiest solution here is to make the region required
</comment><comment author="dadoonet" created="2016-06-29T12:14:17Z" id="229338319">Region or endpoint. Or as I wrote in the other PR make us-west (or east) as default 
</comment><comment author="tlrx" created="2016-06-29T12:42:26Z" id="229344174">&gt; Perhaps the easiest solution here is to make the region required

I think we must change things so that either endpoint or region (or both) are required when registering a S3 repository and the values must be double checked using AWS SDK utils provided in the latest versions (but not the one we currently use).

With the current version AWS SDK version 1.10.69, the issue #18539 happens when the default endpoint `s3.amazonaws.com` is used (or region `us-east` or `us-east-1`because for these regions we try to be lenient and set endpoint to the default endpoint too).

This PR is just a move to fix things, but the long term solution would be:
- endpoint or region required when registering S3 repository
- update AWS SDK to a version that works with the security manager
- remove leniency around region &amp; endpoint from the plugin

&gt; Or as I wrote in the other PR make us-west (or east) as default

We could default to a user chosen default value (in settings file) for endpoint or region, yes, but I don't think it's up to us to choose a default value.
</comment><comment author="clintongormley" created="2016-06-29T13:25:47Z" id="229354703">&gt; We could default to a user chosen default value (in settings file) for endpoint or region, yes, but I don't think it's up to us to choose a default value.

it's easy enough to set inline, i don't see the need for a default setting
</comment><comment author="tlrx" created="2016-06-29T13:28:52Z" id="229355465">Good for me, it's already the "Russian dolls of settings" there.
</comment><comment author="tlrx" created="2016-07-01T08:44:18Z" id="229891468">Thanks for the reviews. I consider it had a LGTM [here](https://github.com/elastic/elasticsearch/pull/19121#issuecomment-229709587)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade httpclient dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19127</link><project id="" key="" /><description>From 4.4.4 to 4.4.5:
https://issues.apache.org/jira/browse/HTTPCORE/fixforversion/12334619/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel

Mentioned in https://github.com/elastic/elasticsearch/pull/18585/files/f0b0c694a7fce557494b3ede9ca6d9e1bf62b390#r68774945
</description><key id="162709850">19127</key><summary>Upgrade httpclient dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Java REST Client</label><label>build</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T15:07:47Z</created><updated>2016-07-27T15:05:42Z</updated><resolved>2016-07-22T20:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Build: upgrade httpcore version to 4.4.5</comment></comments></commit></commits></item><item><title>Reenable logger usage checker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19126</link><project id="" key="" /><description>The logger usage checker (#16707) which ensures proper usage of `log.debug|info|warn|error|...` was inadvertently disabled when it was added. 

The first commit of this PR reenables the logger usage checker and makes it less naggy when encountering logging usages of the form `logger.info(someStringBuilder)`. Previously it would fail with the error message _First argument must be a string constant so that we can statically ensure proper place holder usage_. Now it will only fail in case any arguments are provided as well, for example `logger.info(someStringBuilder, 42)`.

The second commit cleans up some logging usages in the code.
</description><key id="162699842">19126</key><summary>Reenable logger usage checker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label><label>test</label></labels><created>2016-06-28T14:28:04Z</created><updated>2016-06-28T14:52:35Z</updated><resolved>2016-06-28T14:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-28T14:36:35Z" id="229068665">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/NodeConnectionsService.java</file><file>core/src/main/java/org/elasticsearch/common/logging/ESLogger.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/MaxMapCountCheckTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryTermsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/AbstractLifecycleRunnableTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java</file><file>test/logger-usage/src/main/java/org/elasticsearch/test/loggerusage/ESLoggerUsageChecker.java</file><file>test/logger-usage/src/test/java/org/elasticsearch/test/loggerusage/ESLoggerUsageTests.java</file></files><comments><comment>Merge pull request #19126 from ywelsch/fix/reenable-loggerusagechecker</comment></comments></commit></commits></item><item><title>Cleanup Compressor interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19125</link><project id="" key="" /><description>Today we have several deprecated methods, leaking netty interfaces, support for
multiple compressors on the compressor interface. The netty inteface can simply
be replaced by BytesReference which we already have an implementaion for, all the
others are not used and are removed in this commit.
</description><key id="162692848">19125</key><summary>Cleanup Compressor interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T13:59:18Z</created><updated>2016-06-28T16:20:32Z</updated><resolved>2016-06-28T15:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-28T13:59:43Z" id="229057368">@jasontedor  can you take a look
</comment><comment author="jasontedor" created="2016-06-28T14:05:03Z" id="229059019">LGTM.
</comment><comment author="jpountz" created="2016-06-28T16:20:32Z" id="229101857">&lt;3
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java</file><file>core/src/main/java/org/elasticsearch/common/compress/DeflateCompressor.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransportChannel.java</file><file>core/src/test/java/org/elasticsearch/common/compress/DeflateCompressTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/DeflateCompressedXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatIT.java</file></files><comments><comment>Cleanup Compressor interface (#19125)</comment></comments></commit></commits></item><item><title>Valid IPs giving "not a valid ip address" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19124</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:  2.3.3

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

@jpountz I think I might have found a problem as a result of the quick fix from [issue #18740](https://github.com/elastic/elasticsearch/issues/18740). It doesn't give me the "Mixing up field types" error anymore but it tells me that the IPs are not valid:

**Provide logs (if relevant)**:
`source[{"@version":"1","@timestamp":"2016-06-27T21:16:52.085Z","domain_name":"2667500489.MARS.ORDERBOX-DNS.COM.","type":"A","ip_address":"162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253","domain_name_exact":"2667500489.MARS.ORDERBOX-DNS.COM.","domain_name_length":33,"ip_address_exact":"162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253","domain_name_segmented":["2667500489mars","order","box","dns"],"last_registered":"2016-05-01T10:00:00.000-0500"}]} MapperParsingException[failed to parse [ip_address]]; nested: IllegalArgumentException[failed to parse ip [162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253], not a valid ip address]; at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:329) at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309) at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:436) at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262) at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122) at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309) at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:580) at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:559) at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:212) at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224) at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326) at org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:389) at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:191) at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68) at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639) at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279) at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271) at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75) at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376) at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: failed to parse ip [162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253], not a valid ip address at org.elasticsearch.index.mapper.ip.IpFieldMapper.ipToLong(IpFieldMapper.java:84) at org.elasticsearch.index.mapper.ip.IpFieldMapper.innerParseCreateField(IpFieldMapper.java:351) at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:241) at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:321) ... 23 more
`
Here's the mapping:

```
{
    "settings": {
        "number_of_shards": {{ num_shards }},
        "number_of_replicas": 2,
        "index":{
            "analysis":{
               "analyzer":{
                  "keylower":{
                     "tokenizer":"keyword",
                     "filter":"lowercase"
                  }
               }
            }
        }
    },
    "mappings": {
        "_default_": {
            "properties": {
                "ip_address":{
                    "type":"ip"
                }
            }
        },
        "NS": {
            "properties": {
                "domain_name": {
                    "type": "string",
                    "index": "analyzed",
                    "analyzer":"keylower"
                },
                "domain_name_exact": {
                    "type": "string",
                    "index":"not_analyzed"
                },
                "domain_name_length": {
                    "type": "integer"
                },
                "type": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "is_registered": {
                    "type": "boolean"
                },
                "last_registered": {
                    "type":   "date"
                },
                "last_deregistered": {
                    "type":   "date"
                },
                "registered_history": {
                    "type":   "date"
                },
                "deregistered_history": {
                    "type":   "date"
                },
                "name_servers": {
                    "type": "string",
                    "index": "analyzed"
                },
                "name_servers_exact": {
                    "type": "string",
                    "index":"not_analyzed"
                },
                "past_name_servers": {
                    "type": "string",
                    "index": "analyzed"
                },
                "past_name_servers_exact": {
                    "type": "string",
                    "index":"not_analyzed"
                },
                "domain_name_segmented": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "domain_name_exact_pronounceability": {
                    "type": "integer"
                },
                "domain_name_segmented_pronounceability": {
                    "type": "integer"
                }
            }
        },
        "A": {
            "properties": {
                "domain_name": {
                    "type": "string",
                    "index": "analyzed",
                    "analyzer":"keylower"
                },
                "domain_name_exact": {
                    "type": "string",
                    "index":"not_analyzed"
                },
                "domain_name_length": {
                    "type": "integer"
                },
                "type": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "ip_address": {
                    "type": "ip"
                },
                "ip_address_exact": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}
```

I'm not sure if this is unrelated but I've exhausted the docs and discussion at this point. I'm dealing with 20 shards(also 20 nodes), 2 replicas, 16GB of ram(8GB to heap), mlockall set to true, ran "sudo swapoff -a" on all the machines and still have problems with the GB not doing its job and nodes becoming unresponsive. Looking at the logs, I was getting errors like the one above before the GC warnings appeared, can they be related?
</description><key id="162680479">19124</key><summary>Valid IPs giving "not a valid ip address" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">esamudio</reporter><labels /><created>2016-06-28T13:03:04Z</created><updated>2016-06-28T15:30:02Z</updated><resolved>2016-06-28T15:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-28T13:25:22Z" id="229047602">`162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253` is not a valid ip address. If you are trying to assign multiple IP addresses to the `ip_address` field, you must provide them as an array of values such as:

```
{
    "ip_address": [ "162.251.82.124" , "162.251.82.125" , "162.251.82.252" , "162.251.82.253" ]
}
```
</comment><comment author="esamudio" created="2016-06-28T14:06:06Z" id="229059330">@clintongormley @colings86 

Thanks for the quick response!

I should have made this clearer. I have the document in elasticsearch already and that happens when I perform an update call to insert a completely unrelated field. I believe that when the document is reindexed, its not being parsed properly.
</comment><comment author="jpountz" created="2016-06-28T14:18:14Z" id="229063018">Can you provide us with a simple recreation of the issue so that we can look into it?
</comment><comment author="esamudio" created="2016-06-28T15:15:27Z" id="229081248">I think I finally understood where the issue originates but I'm not sure of how to easily replicate it. I'm not familiar enough with github issues to know if I can move this one or if I have to duplicate it because it seems to be a logstash problem. The elasticsearch plugin in logstash seems to be the cause of the problem. Whenever I insert or update anything using "action =&gt; update" in logstash, the IPs give problems because it doesn't properly parse the array of IPs, triggering the error mentioned by @colings86. For example, below is a new document being indexed with logstash using update. This is an excerpt of the logstash log:

`{:timestamp=&gt;"2016-06-28T02:48:01.215000+0000", :message=&gt;"Failed action. ", :status=&gt;400, :action=&gt;["update", {:_id=&gt;"2667500489.MARS.ORDERBOX-DNS.COM.", :_index=&gt;"full", :_type=&gt;"A", :_routing=&gt;nil, :_retry_on_conflict=&gt;1}, #&lt;LogStash::Event:0x3b5cf2c6 @metadata_accessors=#&lt;LogStash::Util::Accessors:0x4053d7b9 @store={}, @lut={}&gt;, @cancelled=false, @data={"@version"=&gt;"1", "@timestamp"=&gt;"2016-06-28T02:47:08.358Z", "zone"=&gt;"com", "sign"=&gt;"+", "domain_name"=&gt;"2667500489.MARS.ORDERBOX-DNS.COM.", "type"=&gt;"A", "ip_address"=&gt;["162.251.82.124", "162.251.82.125", "162.251.82.252", "162.251.82.253"], "domain_name_exact"=&gt;"2667500489.MARS.ORDERBOX-DNS.COM.", "domain_name_length"=&gt;33, "is_registered"=&gt;true, "last_registered"=&gt;"2016-05-01T10:00:00.000-0500", "registered_history"=&gt;["2016-05-01T10:00:00.000-0500"], "deregistered_history"=&gt;[], "ip_address_exact"=&gt;["162.251.82.124", "162.251.82.125", "162.251.82.252", "162.251.82.253"]}, @metadata={}, ...[omitted for brevity]... :response=&gt;{"update"=&gt;{"_index"=&gt;"full", "_type"=&gt;"A", "_id"=&gt;"2667500489.MARS.ORDERBOX-DNS.COM.", "status"=&gt;400, "error"=&gt;{"type"=&gt;"mapper_parsing_exception", "reason"=&gt;"failed to parse [ip_address]", "caused_by"=&gt;{"type"=&gt;"illegal_argument_exception", "reason"=&gt;"failed to parse ip [162.251.82.124,162.251.82.125,162.251.82.252,162.251.82.253], not a valid ip address"}}}}, :level=&gt;:warn}`
</comment><comment author="jpountz" created="2016-06-28T15:28:13Z" id="229085349">I don't think Github has any workflow to move issue, so you'd have to open another one on the logstash repo: https://github.com/elastic/logstash. I am closing this one in the meantime.
</comment><comment author="esamudio" created="2016-06-28T15:30:02Z" id="229085958">Thanks! I really appreciate the effort and quick response you guys have whenever an issue is opened
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CLI tool to force create an empty translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19123</link><project id="" key="" /><description>While the translog has been greatly improved, it is still possible that there are lurking bugs which can cause corruption, eg see #18972. When this occurs, there is currently no way for the user to recover the data in the index. 

We need a command line tool, for expert use only, which will write a fresh transaction log (losing data in the process) to allow users to recover what data they can.
</description><key id="162657268">19123</key><summary>CLI tool to force create an empty translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Index APIs</label><label>feature</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T10:52:49Z</created><updated>2016-07-26T15:53:05Z</updated><resolved>2016-07-26T15:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T15:34:08Z" id="229087324">Should it integrate with the existing `index.shard.check_on_startup: fix`?
</comment><comment author="s1monw" created="2016-06-28T15:39:02Z" id="229088975">yeah that is a good idea @jpountz 
</comment><comment author="bleskes" created="2016-06-28T15:54:59Z" id="229094170">&gt; Should it integrate with the existing index.shard.check_on_startup: fix?

@jpountz can you elaborate?
</comment><comment author="jpountz" created="2016-06-28T16:18:42Z" id="229101278">We already have a hook for removing the corrupted parts of a shard, even if this might cause data loss. So I think it would be nicer to integrate with it instead of having something completely different for the translog and the Lucene index. We might not like the fact it is a setting, but once it is abstracted as a single thing, we could consider exposing it as a command-line tool too if we like it better. For the record, `index.shard.check_on_startup: fix` removes all segments from the shard that appear as corrupted.
</comment><comment author="dakrone" created="2016-06-28T16:29:51Z" id="229104576">Perhaps something like: `index.shard.check_on_startup: truncate_translog` ? (I dislike the word "fix" because it makes it sound like there isn't data loss, even though there is)
</comment><comment author="bleskes" created="2016-06-28T16:32:18Z" id="229105252">OK. I agree this are related. But I think we're saying we want to move away from this being a setting (which can linger around and be dangerous) and create a command line tool that does both lucene level fixes and translog level fixes. Correct?
</comment><comment author="jpountz" created="2016-06-28T16:32:34Z" id="229105339">I think a single option should take care of both the index and the translog? I am good with renaming `fix`, I agree we need something that better carries the fact that there will be data loss.
</comment><comment author="dakrone" created="2016-06-28T16:36:49Z" id="229106499">I dunno, the more I think about it, the more I like the CLI tool, because we can warn users that their will be data loss and require confirmation.

Also, I was hoping in the future to be able to dump some diagnostic information from a translog using the CLI, or even dump the data itself in the case someone lost or deleted the shard but has a translog with some documents in it
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogToolCli.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TruncateTranslogCommand.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TruncateTranslogIT.java</file></files><comments><comment>Add 'elasticsearch-translog' CLI tool with 'translog' command</comment></comments></commit></commits></item><item><title>Validates new dynamic settings from the current state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19122</link><project id="" key="" /><description>Thanks to https://github.com/elastic/elasticsearch/pull/19088 the settings are now validated against dynamic updaters on the master.
Though only the new settings are applied to the IndexService created for the validation.
Because of this we cannot check the transition from one value to another in a dynamic updaters.
This change creates the IndexService from the current settings and validates that the new dynamic settings can replace the current settings.
This change also removes the validation of dynamic settings when an index is opened.
The validation should have occurred when the settings have been updated.

Relates to #19046 
</description><key id="162651257">19122</key><summary>Validates new dynamic settings from the current state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T10:18:16Z</created><updated>2016-06-28T13:36:09Z</updated><resolved>2016-06-28T13:36:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-28T10:23:34Z" id="229010877">@s1monw this is a follow up of #19088 
Sorry for missing this yesterday but I need to know the current setting in order to validate the new value. I hope it makes sense.
</comment><comment author="s1monw" created="2016-06-28T10:27:56Z" id="229011751">left some comments - thanks for catching this
</comment><comment author="jimczi" created="2016-06-28T11:06:07Z" id="229018867">Thanks @s1monw, I pushed a change to address your comment about the signature. 
</comment><comment author="s1monw" created="2016-06-28T13:15:58Z" id="229045276">LGTM thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file></files><comments><comment>Merge pull request #19122 from jimferenczi/validate_settings_bis</comment></comments></commit></commits></item><item><title>Add missing permission for S3 repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19121</link><project id="" key="" /><description>Note: PR against 2.4 branch, master will follow.

S3 repository needs a special permission to work because when no region is explicitly set the AWS SDK will load a JSON file that contain all Amazon's endpoints and will map the content of this file to plain old Java objects. To do that, it uses Jackson's databinding and reflection that require the  `java.lang.reflect.ReflectPermission "suppressAccessChecks"` permission.

This issue only occur if no region is set in the repository setting and in the elasticsearch.yml file.

closes #18539
</description><key id="162650625">19121</key><summary>Add missing permission for S3 repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Cloud AWS</label><label>blocker</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-28T10:14:50Z</created><updated>2016-07-01T07:55:47Z</updated><resolved>2016-07-01T07:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T10:17:42Z" id="229009672">It looks good to me.
</comment><comment author="dadoonet" created="2016-06-28T10:18:56Z" id="229009920">That being said, I wonder if we should better ourself fall back setting to a default region than giving permission to the SM
</comment><comment author="tlrx" created="2016-06-28T12:22:24Z" id="229033065">We could fall back to default US region, but I think that some users also use this plugin with their own custom endpoint and enforcing a AWS default region here might be problematic?
</comment><comment author="dadoonet" created="2016-06-28T12:24:36Z" id="229033530">IIRC setting the endpoint has precedence.
</comment><comment author="tlrx" created="2016-06-28T13:33:48Z" id="229049824">I think defaulting to a region, whatever it is, is too trappy. I think that the current way endpoint &amp; region settings are managed in the plugin is not fully coherent with the AWS SDK.

For example, this does not work:

```
PUT /_snapshot/my_s3_repository '{
  "type": "s3",
  "settings": {
    "bucket": "cloud-aws-test", 
    "region": "us-east" 
  }
}'
```

because we set the default endpoint to `s3.amazonaws.com` for regions "us-east" and "us-east-1". 

I think we must review the way region override endpoints but for now I'm just fixing things so that it works. 

So I'm +1 on adding the special permission for now.
</comment><comment author="rmuir" created="2016-06-28T16:45:38Z" id="229108923">Same comment as on #19128 

My objection is with the explanation: it makes it seem as if this is "justified", it is not. It is simply shitty code AWS code: they need to fix their access modifiers. Its not necessary.

Sorry but, if we explain it like we currently do, it makes it sound like they are doing nothing wrong, and nobody will ever fix it. The truth is you can submit a PR to AWS adding a missing `public` and the bug goes away.
</comment><comment author="s1monw" created="2016-06-28T20:29:44Z" id="229173007">&gt; Sorry but, if we explain it like we currently do, it makes it sound like they are doing nothing wrong, and nobody will ever fix it. The truth is you can submit a PR to AWS adding a missing public and the bug goes away.

@tlrx can you open an issue with them to fix this?
</comment><comment author="rmuir" created="2016-06-28T20:35:46Z" id="229174824">I think its ok to give the permission for now, before pushing I just want the comment to be correct so we know its a fixable situation. They have fixed this problem before in another part of the code (their configuration uses the same serialization).
</comment><comment author="tlrx" created="2016-06-30T08:37:59Z" id="229597482">&gt; My objection is with the explanation: it makes it seem as if this is "justified", it is not. It is simply shitty code AWS code: they need to fix their access modifiers. Its not necessary.

I agree, my explanation is misleading, sorry. This is all my brain was able to produce in English after having spent so much time debugging ec2/s3 stuff.

I pointed to Jackson's because the stacktrace in #18539  shows that `ClassUtil.checkAndFixAccess()` method throws the exception when it tries to call `setAccessible(true)` on a public constructor without even checking the modifiers first, which seemed strange to me. Since the AWS `Partitions` class looked good to me (I also suspected a missing `public` on ctor or setter... did I miss something?) I suspected that Jackson's should check the modifiers first like it does [in more recent versions](https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/util/ClassUtil.java#L864-L865).

I stopped there, hoping that a more recent version of AWS SDK will use a more recent version of Jackon Databinding that has more checks and options to configure object bindings.

&gt; Sorry but, if we explain it like we currently do, it makes it sound like they are doing nothing wrong, and nobody will ever fix it. 

I agree, I updated my comment. Thanks for your feedback, please let me know if that's better now.

&gt; The truth is you can submit a PR to AWS adding a missing public and the bug goes away.

That was my first guess too but I think now that the issue can only be fixed with an update of the version of Jackson used by AWS SDK + a better configuration of Jackson's object mapper used by AWS SDK (like disabling MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS / OVERRIDE_PUBLIC_ACCESS_MODIFIERS / ALLOW_FINAL_FIELDS_AS_MUTATORS). So many things just to load a JSON config file...

I'll create an issue in the aws sdk java GitHub repository to track this.

Edit: Finally found https://github.com/aws/aws-sdk-java/issues/528 and created https://github.com/aws/aws-sdk-java/issues/766 to track this
</comment><comment author="rmuir" created="2016-06-30T16:16:40Z" id="229709587">+1
</comment><comment author="tlrx" created="2016-07-01T07:55:47Z" id="229881403">Merged in ef1bbe46c11423f1e400d643059d4ab52d1b88be
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[reference doc update request]concurrent update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19120</link><project id="" key="" /><description>```
let's assume concurrent update operation is A, B
let's assume each shard has one replica, then P and R stands for primary and R stands for replica
let's assume update operation without specifying version and therefore use internal versiont type
```

so it could happen different things:
1. P execute A, B, R execute A, B
2. P execute A, B, R execute B, A
3. P execute A, B, R execute only A, B has failed on R due to version conflict
4. P execute only A and B failed due to version conflit. R execute A

all of the above has the possibility to happen, right?
if so, better update this part into reference doc and let users to get it well.
</description><key id="162643088">19120</key><summary>[reference doc update request]concurrent update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-06-28T09:37:08Z</created><updated>2016-06-29T01:43:28Z</updated><resolved>2016-06-28T09:43:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-28T09:43:12Z" id="229001989">This should be a topic on the [Elastic Discourse forum](https://discuss.elastic.co), which I see you already [opened](https://discuss.elastic.co/t/es-concurrent-update-question/54136). We appreciate your active involvement in the community, but it would be helpful if you followed the repository guidelines (in the issue template) and also did not cross post. 
</comment><comment author="clintongormley" created="2016-06-28T10:22:55Z" id="229010748">You'll find it all clearly documented here: https://www.elastic.co/guide/en/elasticsearch/guide/current/_partial_updates_to_a_document.html
</comment><comment author="makeyang" created="2016-06-29T01:43:28Z" id="229234012">@clintongormley clear enough. thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>arrays not rendering properly in search templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19119</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**:1.8.0_25

**OS version**: Mac OSX

**Description of the problem including expected versus actual behavior**:

When an array like  `my_ids` with value `[90, 80, 74]` is passed to the search template which includes the following block,

```
                  "must_not":[
                    {
                      "ids" : {
                          "type" : "xxx",
                          "values" : {{my_ids}}
                      }
                    }
                  ]
```

 it renders as 

```
         "must_not":[
                    {
                      "ids" : {
                          "type" : "talents",
                          "values" : {0=90, 1=80, 2=74}
                      }
                    }
              ]
```

which used to be rendered as expected with `"values": [90, 80, 74]` in ES 2.1.1.
</description><key id="162642132">19119</key><summary>arrays not rendering properly in search templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asldevi</reporter><labels><label>:Search Templates</label><label>discuss</label></labels><created>2016-06-28T09:32:22Z</created><updated>2016-06-30T07:12:55Z</updated><resolved>2016-06-30T07:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-29T11:42:06Z" id="229332132">So this is an interesting one... yes the behaviour has changed, but this was never supported behaviour.  it happened to work in this case, but in other cases it fails.  For instance, if `my_ids` contained `["foo", "bar"]` it would get rendered as `[foo, bar]` which is not legal JSON.

Instead of fixing this issue, i think we should backport https://github.com/elastic/elasticsearch/pull/18856 instead, which gives proper support for rendering JSON arrays and objects.
</comment><comment author="asldevi" created="2016-06-30T05:11:17Z" id="229560899">#18856 looks just great. With that upcoming, I agree that this doesn't need to be fixed. Thanks.
</comment><comment author="tlrx" created="2016-06-30T07:12:55Z" id="229578437">&gt; #18856 looks just great. With that upcoming, I agree that this doesn't need to be fixed. Thanks.

Great! The feature has been backported to 2.4 in #19153. I'm closing this issue then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate nested docs with RamDirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19118</link><project id="" key="" /><description>Use RamDirectory for percolating nested documents instead of using multiple MemoryIndex instances with SlowCompositeReaderWrapper workaround.
</description><key id="162640372">19118</key><summary>Percolate nested docs with RamDirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T09:22:57Z</created><updated>2016-06-29T07:00:33Z</updated><resolved>2016-06-29T07:00:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T16:08:35Z" id="229098248">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Release code(v2.3.3) complie Error:Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.3.2:junit4 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19117</link><project id="" key="" /><description>**Elasticsearch version**:release code(v2.3.3)

**JVM version**:1.8.0_60

**OS version**: 32~14.04.1-Ubuntu

**Description of the problem including expected versus actual behavior**:
yes, I download the release code(v2.3.3)  and put it on an CI environment. When execute the command 'mvn test',  most of the test suites failed, most of whose log turn out to be "Throwable #1: java.lang.NoClassDefFoundError: org.elasticsearch.test.ESTestCase". Detail information is as follows:

**Steps to reproduce**:
 1.mvn install
 2.mvn clean test

**Provide logs (if relevant)**:
part of the log:
Suite: org.elasticsearch.test.rest.test.TestSectionParserTests
ERROR   0.00s J0 | TestSectionParserTests.initializationError &lt;&lt;&lt;

&gt; Throwable #1: java.lang.NoClassDefFoundError: org.elasticsearch.test.ESTestCase
&gt;    at java.lang.Class.getDeclaredMethods0(Native Method)
&gt;    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
&gt;    at java.lang.Class.getDeclaredMethods(Class.java:1975)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:215)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:212)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$ModelBuilder.build(ClassModel.java:85)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel.methodsModel(ClassModel.java:212)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel.&lt;init&gt;(ClassModel.java:207)
&gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
&gt; Completed [554/558 (549!)] on J0 in 0.00s, 1 test, 1 error &lt;&lt;&lt; FAILURES!

part of the log:
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.3.2:junit4 (tests) on project elasticsearch: There were test failures: 558 suites, 583 tests, 7 suite-level errors, 546 errors [seed: 9A962C70CAEC565A] -&gt; [Help 1]
</description><key id="162635051">19117</key><summary>Release code(v2.3.3) complie Error:Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.3.2:junit4 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wenpos</reporter><labels /><created>2016-06-28T08:55:57Z</created><updated>2016-06-28T10:22:54Z</updated><resolved>2016-06-28T09:47:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T09:37:29Z" id="229000749">When does this fail? When you execute the first command or the second one?
</comment><comment author="dadoonet" created="2016-06-28T09:47:39Z" id="229003030">I tried this on my laptop on 2.3.3 tag:

``` sh
mvn clean install
```

And everything is fine.

You probably have an error in your setup.
</comment><comment author="wenpos" created="2016-06-28T10:19:05Z" id="229009947">thank you, what do you mean my setup?
</comment><comment author="dadoonet" created="2016-06-28T10:19:42Z" id="229010073">I meant your "CI environment".
</comment><comment author="wenpos" created="2016-06-28T10:22:54Z" id="229010745">thank you, I'll have a look
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Make discovery-azure work again on 2.4 branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19116</link><project id="" key="" /><description>Backport of #19062 on 2.4 branch
Closes #18637
</description><key id="162630792">19116</key><summary> Make discovery-azure work again on 2.4 branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Cloud Azure</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-28T08:33:31Z</created><updated>2016-08-26T13:31:35Z</updated><resolved>2016-06-28T08:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T08:37:35Z" id="228987206">Went through the code. I did not test it as I did for #19062 but it looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mvn test throws errors like this 'NoClassDefFoundError org.elasticsearch.test.ESTestCase'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19115</link><project id="" key="" /><description>**Elasticsearch version**:release code(v2.3.3)

**JVM version**:1.8.0_60

**OS version**: 32~14.04.1-Ubuntu

**Description of the problem including expected versus actual behavior**:
yes, I download the release code(v2.3.3)  and put it on an CI environment. When execute the command 'mvn test',  most of the test suites failed, most of whose log turn out to be "Throwable #1: java.lang.NoClassDefFoundError: org.elasticsearch.test.ESTestCase". Detail information is as follows:

**Steps to reproduce**:
 1.mvn install
 2.mvn clean test

**Provide logs (if relevant)**:
part of the log:
Suite: org.elasticsearch.test.rest.test.TestSectionParserTests
ERROR   0.00s J0 | TestSectionParserTests.initializationError &lt;&lt;&lt;

&gt; Throwable #1: java.lang.NoClassDefFoundError: org.elasticsearch.test.ESTestCase
&gt;    at java.lang.Class.getDeclaredMethods0(Native Method)
&gt;    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
&gt;    at java.lang.Class.getDeclaredMethods(Class.java:1975)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:215)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:212)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel$ModelBuilder.build(ClassModel.java:85)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel.methodsModel(ClassModel.java:212)
&gt;    at com.carrotsearch.randomizedtesting.ClassModel.&lt;init&gt;(ClassModel.java:207)
&gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
&gt; Completed [554/558 (549!)] on J0 in 0.00s, 1 test, 1 error &lt;&lt;&lt; FAILURES!

part of the log:
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.3.2:junit4 (tests) on project elasticsearch: There were test failures: 558 suites, 583 tests, 7 suite-level errors, 546 errors [seed: 9A962C70CAEC565A] -&gt; [Help 1]
**Describe the feature**:
</description><key id="162627908">19115</key><summary>mvn test throws errors like this 'NoClassDefFoundError org.elasticsearch.test.ESTestCase'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wenpos</reporter><labels /><created>2016-06-28T08:17:17Z</created><updated>2016-06-28T08:49:25Z</updated><resolved>2016-06-28T08:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-28T08:23:45Z" id="228984137">Please ask questions like this in our forum discuss.elastic.co. When opening a question there it would help to know more about what you are trying to achieve, what your setup ist (are you running from command line or from ide etc...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`index.checksum_on_merge` should default to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19114</link><project id="" key="" /><description>This was lost in a refactoring and should default to true to ensure we catch corruptions
where they happen.

Note this only applies to the 1.7 branch
</description><key id="162621967">19114</key><summary>`index.checksum_on_merge` should default to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>:Settings</label><label>bug</label><label>resiliency</label><label>review</label><label>v1.7.6</label></labels><created>2016-06-28T07:41:18Z</created><updated>2016-06-28T10:39:47Z</updated><resolved>2016-06-28T10:39:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T07:59:26Z" id="228978941">It looks good
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Insertion Bulk data not work. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19113</link><project id="" key="" /><description>Hi all, 
so basically im trying new things. this time with elastic search.
i already install elastic and kibana, and sense plugin of course.
i try all basic command like create index, mapping, adding document etc.
everything goes well until i try bulk insertion.

```
POST /ecommerce/product/_bulk
{"index":{"_id":"1002"}}
{"name":"SWA magazine", "price":"90.000", "description":"swa magazine description",
"status":"active", "quantity":3, "categories":[{"name":"magazine"}], 
"tags":["business", "magazine", "sales", "news"]}
{"index":{"_id":"1003"}}
{"name":"SWA magazine", "price":"90.000", "description":"swa magazine description",
"status":"active", "quantity":3, "categories":[{"name":"magazine"}], 
"tags":["business", "magazine", "sales", "news"]}
```

i dont even know what is wrong with my code.
i always got this error :  

![error bulk elastic](https://cloud.githubusercontent.com/assets/1556126/16406725/0b1504ea-3d3a-11e6-91d2-3eabf66b8dc2.png)

pls help me to fix this out.
many thanks in advance.
</description><key id="162617282">19113</key><summary>Insertion Bulk data not work. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gutasaputra</reporter><labels /><created>2016-06-28T07:10:38Z</created><updated>2016-06-29T10:19:07Z</updated><resolved>2016-06-28T07:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-06-28T07:27:45Z" id="228972614">Please use the [forum](http://discuss.elastic.co/) for this kind of questions.

you need ensure, that there are no new lines in your JSON body of the document you are trying to index. bulk indexing is very picky about this. See the [bulk API docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)
</comment><comment author="clintongormley" created="2016-06-29T10:19:07Z" id="229316873">You can't have new lines in your bulk body.  In Sense you can just use Auto Indent to format it as follows:

```
POST /ecommerce/product/_bulk
{"index":{"_id":"1002"}}
{"name":"SWA magazine","price":"90.000","description":"swa magazine description","status":"active","quantity":3,"categories":[{"name":"magazine"}],"tags":["business","magazine","sales","news"]}
{"index":{"_id":"1003"}}
{"name":"SWA magazine","price":"90.000","description":"swa magazine description","status":"active","quantity":3,"categories":[{"name":"magazine"}],"tags":["business","magazine","sales","news"]}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support new Asia Pacific (Mumbai) ap-south-1 AWS region</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19112</link><project id="" key="" /><description>AWS [announced](http://www.allthingsdistributed.com/2016/06/introducing-aws-asia-pacific-mumbai-region.html) a new region: Asia Pacific (Mumbai)    `ap-south-1`.

We need to support it for:
- repository-s3: s3.ap-south-1.amazonaws.com or s3-ap-south-1.amazonaws.com
- discovery-ec2: ec2.ap-south-1.amazonaws.com

For reference: http://docs.aws.amazon.com/general/latest/gr/rande.html

Closes #19110.
</description><key id="162614738">19112</key><summary>Support new Asia Pacific (Mumbai) ap-south-1 AWS region</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T06:52:03Z</created><updated>2016-08-29T09:13:15Z</updated><resolved>2016-06-28T08:01:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T06:55:08Z" id="228966388">@tlrx Could you review it please?

@clintongormley should I backport that to 2.4 branch? Knowing that users can always use `cloud.aws.ec2.endpoint` or `cloud.aws.s3.endpoint` to set the AWS new endpoint manually. What about 1.7 series?
</comment><comment author="tlrx" created="2016-06-28T08:00:07Z" id="228979084">LGTM
</comment><comment author="vanga" created="2016-08-29T07:23:01Z" id="243050686">&gt; should I backport that to 2.4 branch? Knowing that users can always use cloud.aws.ec2.endpoint or cloud.aws.s3.endpoint to set the AWS new endpoint manually

@dadoonet can you please tell me if this change is going to 2.4?
</comment><comment author="dadoonet" created="2016-08-29T08:52:11Z" id="243068862">No. It's not in 2.4 branch.
</comment><comment author="vanga" created="2016-08-29T09:13:15Z" id="243073483">Thanks, just wanted to confirm, will set the endpoints manually.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Testing infra - stablize data folder usage and clean up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19111</link><project id="" key="" /><description>The plan for persistent node ids ( #17811 ) is to tie the node identity to a file stored in it's data folders. As such it becomes important that nodes in our testing infra have better affinity with their data folders and that their data folders are not cleaned underneath them. The first is important because we fix the random seed used for node id generation (for reproducibility) and allowing the same node to use two different data folders causes two separate nodes to have the same id, which prevents the cluster from forming. The second is important, for example, where a full cluster restart / single node restart need to maintain node identity and wiping the data folders at the wrong moment prevents this.

Concretely this PR does the following:
1) Remove previous attempts to have data folder per role using a prefix. This wasn't effective as it was using the data paths settings which are only used for part of the runs. An attempt to completely separate the paths via the home dir failed due to assumptions made by index custom path about node data folder ordinal uniqueness  (see #19076)
2) Change full cluster restarts to start up nodes in the same order their were first created in, only randomly swapping nodes with the same roles.
3) Change test cluster reset methods to first shutdown the unneeded nodes and then re-start the shared nodes that were shut down, so they'll reclaim their data folders.
4) Improve data folder wiping logic and make sure it wipes only folders of "offline" nodes.
5) Add some very basic tests
</description><key id="162613317">19111</key><summary>Testing infra - stablize data folder usage and clean up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-28T06:40:38Z</created><updated>2016-06-28T14:39:20Z</updated><resolved>2016-06-28T14:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-28T10:19:03Z" id="229009936">nice, left a small comment. I think eventually we want data dirs that should be reused to be explicit. but that can be another change
</comment><comment author="bleskes" created="2016-06-28T14:39:20Z" id="229069505">thanks @s1monw I pushed a change with your request. tass pass and I merged it in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Testing infra - stablize data folder usage and clean up (#19111)</comment></comments></commit></commits></item><item><title>Support new Asia Pacific (Mumbai) ap-south-1 AWS region</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19110</link><project id="" key="" /><description>AWS [announced](http://www.allthingsdistributed.com/2016/06/introducing-aws-asia-pacific-mumbai-region.html) a new region: Asia Pacific (Mumbai)    `ap-south-1`.

We need to support it for:
- repository-s3: s3.ap-south-1.amazonaws.com or s3-ap-south-1.amazonaws.com
- discovery-ec2: ec2.ap-south-1.amazonaws.com

For reference: http://docs.aws.amazon.com/general/latest/gr/rande.html 
</description><key id="162611639">19110</key><summary>Support new Asia Pacific (Mumbai) ap-south-1 AWS region</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>adoptme</label><label>enhancement</label></labels><created>2016-06-28T06:26:51Z</created><updated>2016-06-28T08:01:38Z</updated><resolved>2016-06-28T08:01:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file></files><comments><comment>Support new Asia Pacific (Mumbai) ap-south-1 AWS region</comment></comments></commit></commits></item><item><title>[TEST] refactor search yaml tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19109</link><project id="" key="" /><description>Refactored oned of the yaml tests for more visibility.
</description><key id="162572108">19109</key><summary>[TEST] refactor search yaml tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T23:38:49Z</created><updated>2016-06-28T00:05:22Z</updated><resolved>2016-06-27T23:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T23:49:46Z" id="228909850">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] refactor search yaml tests (#19109)</comment></comments></commit></commits></item><item><title>Pull actions from plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19108</link><project id="" key="" /><description>Instead of implementing `onModule(ActionModule)` to register actions, this has plugins implement `ActionPlugin` to declare actions. This is yet another step in cleaning up the plugin infrastructure.

While I was in there I switched `AutoCreateIndex` and `DestructiveOperations` to be eagerly constructed which makes them easier to use when de-`guice`-ing the code base.
</description><key id="162566649">19108</key><summary>Pull actions from plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T22:54:13Z</created><updated>2016-07-29T11:43:42Z</updated><resolved>2016-06-28T13:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-28T07:54:34Z" id="228977922">looks great left a minor! thanks for doing this
</comment><comment author="nik9000" created="2016-06-28T13:18:15Z" id="229045843">Merged as fa4844c3f47ebc56478ef1bb7913d73a962fe945
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[API] separate tasks.list and tasks.get APIs in the json definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19107</link><project id="" key="" /><description>All variable parts of the URL should be documented.
</description><key id="162565824">19107</key><summary>[API] separate tasks.list and tasks.get APIs in the json definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T22:48:18Z</created><updated>2016-06-28T11:15:01Z</updated><resolved>2016-06-28T00:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T23:18:32Z" id="228904551">Actually the bug is that `"/_tasks/{task_id}"` isn't part of the tasks list API any more - it is the task get API now.
</comment><comment author="HonzaKral" created="2016-06-27T23:46:13Z" id="228909295">Thanks for the note. I was looking for `tasks.get` and didn't find it so I assumed it's just an option for list.

@nik9000 is there any reason why the API is `task.get` as opposed to `tasks.get`? We typically have everything in the same namespace and using plural even for get, like `indices.get`.

Updated the PR to remove this option from `tasks.list`
</comment><comment author="nik9000" created="2016-06-27T23:50:32Z" id="228909991">LGTM.

I just called the task `task.get` instead of `tasks.get`. I have no problem renaming it.
</comment><comment author="HonzaKral" created="2016-06-27T23:53:26Z" id="228910475">Thanks! Added a commit with the rename
</comment><comment author="nik9000" created="2016-06-28T00:05:06Z" id="228912176">LGTM
</comment><comment author="spinscale" created="2016-06-28T07:23:35Z" id="228971818">I am confused by this change (or maybe the YAML spec). The `RestGetTaskAction` states at https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestGetTaskAction.java#L40 that an URL parameter to specify the task id is expected. Now the spec has been changed that the `taskId` specified in the YAML spec always is a HTTP parameter? (This works in ES because if the parameter is not part of the URL, the HTTP parameters are tried)
</comment><comment author="HonzaKral" created="2016-06-28T11:09:48Z" id="229019531">@spinscale the specs for `tasks.get` haven't changed at all, we only removed the option for `tasks.list` to get a `task_id` in the URL to align it with the spec at https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestListTasksAction.java#L48

Sorry if the wording of the PR was a bit confusing
</comment><comment author="spinscale" created="2016-06-28T11:15:01Z" id="229020477">uh, misread the diff, yaml tests/spec are fine. thx for the clarification
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Tests: Rename task.get to tasks.get</comment></comments></commit><commit><files /><comments><comment>[API] separate tasks.list and tasks.get APIs in the json definition (#19107)</comment></comments></commit></commits></item><item><title>Expose the ClusterInfo object in the allocation explain output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19106</link><project id="" key="" /><description>This adds an optional parameter to the cluster allocation explain API
that will return the cluster info object, `include_disk_info`, the
output looks like:

```
GET /_cluster/allocation/explain?include_disk_info -d'
{"index": "i", "shard": 0, "primary": false}'

{
  ... other info ...

  "cluster_info" : {
    "nodes" : {
      "7Uws-vL7R6WVm3ZwQA1n5A" : {
        "node_name" : "Kraven the Hunter",
        "least_available" : {
          "path" : "/path/to/data1",
          "total_bytes" : 165999570944,
          "used_bytes" : 118180614144,
          "free_bytes" : 47818956800,
          "free_disk_percent" : 28.8,
          "used_disk_percent" : 71.2
        },
        "most_available" : {
          "path" : "/path/to/data2",
          "total_bytes" : 165999570944,
          "used_bytes" : 118180614144,
          "free_bytes" : 47818956800,
          "free_disk_percent" : 28.8,
          "used_disk_percent" : 71.2
        }
      }
    },
    "shard_sizes" : {
      "[i][2][p]_bytes" : 0,
      "[i][4][p]_bytes" : 130,
      "[i][1][p]_bytes" : 0,
      "[i][3][p]_bytes" : 0,
      "[i][0][p]_bytes" : 130
    },
    "shard_paths" : {
      "[i][3], node[7Uws-vL7R6WVm3ZwQA1n5A], [P], s[STARTED], a[id=LegZLDniTVaw0Y1urv7s3g]" : "/path/to/data1/nodes/0",
      "[i][1], node[7Uws-vL7R6WVm3ZwQA1n5A], [P], s[STARTED], a[id=lAU_4vf_SKmoRdtg0ACnjQ]" : "/path/to/data1/nodes/0",
      "[i][2], node[7Uws-vL7R6WVm3ZwQA1n5A], [P], s[STARTED], a[id=Aurpeuj7SeGeyPDDpCtRgg]" : "/path/to/data1/nodes/0",
      "[i][0], node[7Uws-vL7R6WVm3ZwQA1n5A], [P], s[STARTED], a[id=Vgg8GlQTQ82C2j6HYBq8DQ]" : "/path/to/data1/nodes/0",
      "[i][4], node[7Uws-vL7R6WVm3ZwQA1n5A], [P], s[STARTED], a[id=t8hQlVSxQe-58fSeaXcAqg]" : "/path/to/data1/nodes/0"
    }
  }
}
```

Resolves #14405
</description><key id="162561384">19106</key><summary>Expose the ClusterInfo object in the allocation explain output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T22:18:46Z</created><updated>2016-07-12T22:26:59Z</updated><resolved>2016-07-12T22:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-07-12T20:57:58Z" id="232178994">Modulo some nits, LGTM. Fire at will.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify write failure handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19105</link><project id="" key="" /><description>Currently, any write (e.g. `index`, `delete`) operation failure can be categorized as:
- request failure (e.g. analysis, parsing error, version conflict) 
- transient operation failure (e.g. due to shard initializing, relocation)
- environment failure (e.g. out of disk, corruption, lucene tragic event)

The main motivation of the PR is to handle these failure types appropriately for a 
write request. Each failure type needs to be handled differently:
- request failure (being request specific) should be replicated and then failed
- transient failure should be retried (eventually succeeding)
- environment failure (persistent primary shard failure) should fail the request 
  immediately.

Currently, transient operation failures are retried in replication action but no distinction
is made between request and environment failures, both fails write request immediately. 

In this PR, we distinguish between request and environment failures for a write operation. 
In case of environment failures, the exception is bubbled up failing the request and in case 
of request failures, the exception is captured and replication continues (we ignore performing 
on replicas when such failures occur in primary). Transient operation failures are bubbled up 
to be retried by the replication operation, as before. 

Note: https://github.com/elastic/elasticsearch/issues/20109 simplifies bulk execution code, which should clean up error handling for shard bulk requests.
</description><key id="162548350">19105</key><summary>Simplify write failure handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v6.0.0-alpha1</label></labels><created>2016-06-27T21:06:31Z</created><updated>2017-05-05T09:59:01Z</updated><resolved>2016-11-01T22:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-06-29T21:28:11Z" id="229494438">After discussions with @bleskes, I changed the scope of this PR. Now, the PR focuses on making primary write operation failures a valid write result, so replication operation can handle them explicitly. We can add operation failure replication, as needed in the `feature/seq_no` branch
</comment><comment author="areek" created="2016-08-03T22:11:11Z" id="237391128">@bleskes I updated the PR to only capture operation level write failures in write results for primary and replica operations that do not fail the engine and are not dwelt with (retried) by the replication action.  Could you take a look?
</comment><comment author="bleskes" created="2016-08-04T16:23:13Z" id="237605804">Thx @areek . This is getting in good shape. I left some more suggestions. It's probably good to revisit the PR title and description. It has deviated a bit from our current thinking.
</comment><comment author="bleskes" created="2016-08-04T16:28:26Z" id="237607277">one more thing - master moved slightly further in a way that might be useful here. I suggest you merge master into your PR branch.
</comment><comment author="areek" created="2016-09-01T05:39:41Z" id="243981177">Thanks @bleskes for the review! sorry for the lateness here. I have updated the PR and the description, would love to get your feedback on this.
</comment><comment author="areek" created="2016-10-14T22:05:32Z" id="253931066">@bleskes Thanks for the feedback! I rewrote the PR against latest master (and updated PR description). Now instead of having a generic `execute` method for write operations in the engine level, I added a generic `execute` method in `IndexShard`, keeping the changes to `InternalEngine` minimal. Would love to get a review on this :)
</comment><comment author="bleskes" created="2016-10-17T15:46:57Z" id="254247003">I did a first sweep and I like the simplification of having `IndexShard` work on the operation level. I will do another careful sweep tomorrow and I want to think some more about the higher level replication logic. Also, if we end up merging the exception types - we need a BWC layer :)
</comment><comment author="areek" created="2016-10-21T05:11:21Z" id="255294984">Thanks @bleskes for your feedback, I updated the PR simplifying `TransportWriteAction` by decoupling write operations from write responses, though this added complexity to `TransportShardBulkAction`. I will update the PR with simple unit tests for transporting write failures and work on the BWC for merging the exception types.
</comment><comment author="areek" created="2016-10-25T15:33:51Z" id="256070967">@bleskes I have updated the PR, making the index/delete operations immutable by introducing a operation result class, among other things we discussed. Would be awesome if you could do a review.
</comment><comment author="areek" created="2016-10-27T04:28:10Z" id="256543760">Thanks @bleskes for the feedback. I updated the PR, addressing all your comments, including adding tests for failure handling in `TransportWriteAction` and `InternalEngine`
</comment><comment author="areek" created="2016-11-01T04:16:07Z" id="257485241">@bleskes thanks again for the review, I addressed all the comments and had 
one question regarding bwc for the removed exceptions in https://github.com/elastic/elasticsearch/pull/19105#discussion_r85872981
</comment><comment author="bleskes" created="2016-11-01T09:29:21Z" id="257523022">@areek because the github ui sucks, I'm responding here so it will be easy to see:

&gt; &gt;  if we care about rolling restarts from 5.x

We care! and yeah, I think we can just keep them in there and doc that they can be removed in 7.0 (assertion?)

&gt; The index/delete operation failures were communicated as exceptions, so do we even need a bwc for these failures for serialization/deserialization or can we just rely on generic exception serialization/deserialization like we currently do for persistent engine failures during index/delete operations?

I  don't think we can rely on generic exceptions for incoming responses from old nodes. That will ask for a specific exception ID and we won't have it -&gt; boom.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java</file><file>core/src/main/java/org/elasticsearch/index/IndexingSlowLog.java</file><file>core/src/main/java/org/elasticsearch/index/engine/DeleteFailedEngineException.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/IndexFailedEngineException.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexingOperationListener.java</file><file>core/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportWriteActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file></files><comments><comment>Merge pull request #19105 from areek/enhancement/replicate_primary_write_failures</comment></comments></commit></commits></item><item><title>Add the integ test node's configuration directory as a system property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19104</link><project id="" key="" /><description /><key id="162533250">19104</key><summary>Add the integ test node's configuration directory as a system property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>non-issue</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T19:50:22Z</created><updated>2016-06-27T19:52:29Z</updated><resolved>2016-06-27T19:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-27T19:51:46Z" id="228855427">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19104 from dakrone/add-confdir-prop</comment></comments></commit></commits></item><item><title>Bulk update with script / upsert broken in 1.7.5 with different JSON key order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19103</link><project id="" key="" /><description>If, in the JSON object of a bulk update with scripted upsert the keys "upsert" and "scripted_upsert" do not appear as the first keys the whole thing fails. 

This is ONLY in 1.x. This works in 2.x and 5.x

This works

``````
POST /test/type/_bulk
{"update":{"_id":"5","_retry_on_conflict":1}}
{"upsert":{},"scripted_upsert":true,"script":{"script":"ctx._source['@timestamp'] = event['@timestamp']; ctx._source['@version'] = event['@version']","params":{"event":{"@timestamp":"2016-06-24T22:35:46.992Z","@version":"1","message":"sample message here"}}}}```
``````

This does not

```
POST /test/type/_bulk
{"update":{"_id":"5","_retry_on_conflict":1}}
{"script":{"script":"ctx._source['@timestamp'] = event['@timestamp']; ctx._source['@version'] = event['@version']","params":{"event":{"@timestamp":"2016-06-24T22:35:46.992Z","@version":"1","message":"sample message here"}}},"upsert":{},"scripted_upsert":true}
```

Thanks to @pickypg for pinning down the source of this which seems to be [here in UpdateHelper.java](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L83)
</description><key id="162501938">19103</key><summary>Bulk update with script / upsert broken in 1.7.5 with different JSON key order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewvc</reporter><labels><label>:Bulk</label><label>adoptme</label><label>bug</label><label>v1.7.5</label></labels><created>2016-06-27T17:11:48Z</created><updated>2016-06-29T09:59:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Keep input time unit when parsing TimeValues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19102</link><project id="" key="" /><description>This commit modifies TimeValue parsing to keep the input time unit. This
enables round-trip parsing from instances of String to instances of
TimeValue and vice-versa. With this, this commit removes support for the
unit "w" representing weeks, and also removes support for fractional
values of units (e.g., 0.5s).
</description><key id="162498179">19102</key><summary>Keep input time unit when parsing TimeValues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T16:52:17Z</created><updated>2016-06-29T16:56:09Z</updated><resolved>2016-06-27T22:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T18:47:01Z" id="228838374">Just to throw a wrench in this: TimeValue throws out its units when it is streamed. It is always read as nanos.
</comment><comment author="jasontedor" created="2016-06-27T19:53:52Z" id="228855954">&gt; Just to throw a wrench in this: TimeValue throws out its units when it is streamed. It is always read as nanos.

I pushed 25ce3b3fe1b0974825706a9bd6c2f6008ca4db63 to modify the serialization of `TimeValue` so that this is not the case.
</comment><comment author="nik9000" created="2016-06-27T22:22:55Z" id="228894044">LGTM
</comment><comment author="jasontedor" created="2016-06-27T22:41:24Z" id="228897691">Thanks @nik9000!
</comment><comment author="clintongormley" created="2016-06-29T10:09:59Z" id="229315049">@jasontedor out of interest, why is `w` no longer supported, given that it has a fixed duration?
</comment><comment author="clintongormley" created="2016-06-29T10:12:18Z" id="229315532">Also, this page needs updating: https://www.elastic.co/guide/en/elasticsearch/reference/master/common-options.html#time-units as does this https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-diversified-sampler-aggregation.html#_no_specialized_syntax_for_geo_date_fields
</comment><comment author="jasontedor" created="2016-06-29T10:16:08Z" id="229316275">&gt; out of interest, why is w no longer supported, given that it has a fixed duration?

@clintongormley This is due to the fact that we internally use the Java `TimeUnit` type to represent the time unit on a `TimeValue` instance and `TimeUnit` does not have support for weeks. Previously we converted everything internally to milliseconds and used `TimeUnit.MILLISECONDS` internally after parsing a user-input string but with this change we now parse the user-input time unit into the correct value of `TimeUnit`. To support weeks with `TimeValue` we would have to move away from the internal `TimeUnit` representation and use our own type to represent the unit. I saw the costs of this outweighing the benefit of supporting weeks. Let me know if you think otherwise?
</comment><comment author="jasontedor" created="2016-06-29T10:16:30Z" id="229316357">@clintongormley Thanks; I will update the docs accordingly.
</comment><comment author="clintongormley" created="2016-06-29T13:04:54Z" id="229349395">Weeks is easy to express in days, so I'm good with that
</comment><comment author="jasontedor" created="2016-06-29T13:44:14Z" id="229359683">&gt; Weeks is easy to express in days, so I'm good with that

Yes, that was in my thinking as well. Thanks @clintongormley. I'll open the PR for the docs soon. 
</comment><comment author="jasontedor" created="2016-06-29T16:56:09Z" id="229419405">@clintongormley I opened #19159 to update the docs.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/SearchSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Keep input time unit when parsing TimeValues</comment></comments></commit></commits></item><item><title>Support requests_per_second=-1 to mean no throttling in reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19101</link><project id="" key="" /><description>This is entirely on the REST level, Float.POSITIVE_INFINITY is still
how you get no throttling over the transport api.

Closes #19089
</description><key id="162496968">19101</key><summary>Support requests_per_second=-1 to mean no throttling in reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T16:46:15Z</created><updated>2016-07-18T17:09:16Z</updated><resolved>2016-07-18T17:09:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T05:33:48Z" id="228952611">My understanding of #19089 is that we should support `-1` but also remove support for `unlimited` to be consistent with other APIs?
</comment><comment author="nik9000" created="2016-07-13T17:45:22Z" id="232432428">I've removed support for `unlimited` now it only supports -1.
</comment><comment author="jpountz" created="2016-07-18T09:02:12Z" id="233278562">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move the build vagrant build listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19100</link><project id="" key="" /><description>That way it doesn't register until we actually try and set up
the vagrant test root. We don't need it all the time.
</description><key id="162494762">19100</key><summary>Move the build vagrant build listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T16:34:59Z</created><updated>2016-06-27T18:40:05Z</updated><resolved>2016-06-27T18:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T16:35:18Z" id="228800557">Relates to https://github.com/elastic/elasticsearch/pull/19033
</comment><comment author="jasontedor" created="2016-06-27T16:53:41Z" id="228805660">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>State default sort order on missing values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19099</link><project id="" key="" /><description>According to https://github.com/elastic/elasticsearch/blob/baa2d51e599bdb901e472d15cc0251e6712c1d25/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java#L143 the default value is `_last`.
</description><key id="162491029">19099</key><summary>State default sort order on missing values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maciejkula</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-06-27T16:16:42Z</created><updated>2017-01-13T16:06:10Z</updated><resolved>2017-01-13T16:06:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="maciejkula" created="2016-06-27T16:17:59Z" id="228795589">I'd love to sign the CLA but the site doesn't work (Chrome/Firefox on Ubuntu). Lots of red in the JS console.
</comment><comment author="jesushernandez" created="2016-06-27T16:22:37Z" id="228796958">👍 
</comment><comment author="clintongormley" created="2016-06-29T09:59:06Z" id="229312450">Hi @maciejkula  - thanks for the PR.  The CLA link is working again: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-09-12T22:05:59Z" id="246510161">@maciejkula were you able to sign the CLA se we can merge this in?
</comment><comment author="clintongormley" created="2017-01-13T16:05:24Z" id="272479738">CLA not signed - treating as bug report</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>State default sort order on missing values</comment></comments></commit></commits></item><item><title>Fixing typo for path.conf location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19098</link><project id="" key="" /><description>Changing -Ees.path.conf to -Epath.conf
</description><key id="162488439">19098</key><summary>Fixing typo for path.conf location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jalvar08</reporter><labels><label>docs</label></labels><created>2016-06-27T16:03:58Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-06-30T14:42:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-27T16:16:09Z" id="228795084">Thanks @jalvar08, can you sign the CLA?
</comment><comment author="jalvar08" created="2016-06-27T17:34:26Z" id="228816876">Hi Jason,

I've tried to sign the page here:
https://www.elastic.co/contributor-agreement

I choose individual contributor and press the sign agreement button and all
it does is refresh the page.

On Mon, Jun 27, 2016 at 12:17 PM Jason Tedor notifications@github.com
wrote:

&gt; Thanks @jalvar08 https://github.com/jalvar08, can you sign the CLA?
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19098#issuecomment-228795084,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AHCEPTbu5zcRSm6rGjpqoYjol97WPsUoks5qP_eEgaJpZM4I_QAd
&gt; .
</comment><comment author="jasontedor" created="2016-06-27T17:47:43Z" id="228820776">@jalvar08 I'm sorry for the trouble, I can reproduce your issue. I will follow-up with someone to see if we can get it fixed.
</comment><comment author="jasontedor" created="2016-06-30T10:22:01Z" id="229620256">@jalvar08 I believe the issues with CLA have been addressed if you're interested in trying again?
</comment><comment author="jalvar08" created="2016-06-30T14:34:29Z" id="229676967">Yes it worked this time. I signed it.

On Thu, Jun 30, 2016 at 6:23 AM Jason Tedor notifications@github.com
wrote:

&gt; @jalvar08 https://github.com/jalvar08 I believe the issues with CLA
&gt; have been addressed if you're interested in trying again?
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19098#issuecomment-229620256,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AHCEPW7PabsU1MPl6FId8w8fhXEKguIAks5qQ5kEgaJpZM4I_QAd
&gt; .
</comment><comment author="clintongormley" created="2016-06-30T14:42:04Z" id="229679331">thanks @jalvar08 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fixing typo for path.conf location (#19098)</comment></comments></commit></commits></item><item><title>Start migration away from aggregation streams</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19097</link><project id="" key="" /><description>We'll migrate to NamedWriteable so we can share code with the rest
of the system. So we can work on this in multiple pull requests without
breaking Elasticsearch in between the commits this change supports
_both_ old style `InternalAggregations.stream` serialization and
`NamedWriteable` style serialization. As such it creates about a
half dozen `// NORELEASE` comments that will have to be removed
once the migration is complete.

This also introduces a boolean `transportClient` flag to `SearchModule`
which is used to skip inappropriate registrations for for the
transport client while still registering the things it needs. In
this case that means that the `InternalAggregation` subclasses are
registered with the `NamedWriteableRegistry` but the `AggregationBuilder`
subclasses are not.

Finally, this moves aggregation registration from guice configuration
time to `SearchModule` construction time. This will make it simpler to
work with in the future as we further clean up Elasticsearch's
extension points.
</description><key id="162483265">19097</key><summary>Start migration away from aggregation streams</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T15:41:25Z</created><updated>2016-06-30T17:08:03Z</updated><resolved>2016-06-30T17:08:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T05:41:56Z" id="228953655">This looks good to me, but please give @colings86 a chance to have a look before merging.
</comment><comment author="colings86" created="2016-06-28T12:52:08Z" id="229039613">@nik9000 I left one minor comment but LGTM so feel free to push with or without the change.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Factor out abstract TCPTransport* classes to reduce the netty footprint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19096</link><project id="" key="" /><description>Today we have a ton of logic inside the NettyTransport\* codebase. The footprint
of the code that has a direct netty dependency is large and alternative implementations
are pretty hard today since they need to know all about our proticol etc.
This change moves most of the code into TCPTransport\* baseclasses and moves all
the protocol send code together. The base classes now contain the majority of the logic
while NettyTransport\* classes remain to implement the glue code, configuration and optimization.
</description><key id="162480830">19096</key><summary>Factor out abstract TCPTransport* classes to reduce the netty footprint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T15:31:13Z</created><updated>2016-07-29T11:43:42Z</updated><resolved>2016-06-30T11:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-28T11:19:53Z" id="229021418">looks good. It reduces NettyTransport to the netty specifics (size cut in half). Personally i do not understand the generics, but this is typical for ES code for me. Maybe we can reduce the generics as a followup.
</comment><comment author="jasontedor" created="2016-06-28T13:47:45Z" id="229053830">Just two comments:
1. This still leaks Netty all over the place, for example `#toChannelBuffer` on the `BytesReference` interface.
2. Bikeshed: Maybe `TCP` -&gt; `Tcp` throughout?
</comment><comment author="s1monw" created="2016-06-28T18:42:21Z" id="229143477">&gt; This still leaks Netty all over the place, for example #toChannelBuffer on the BytesReference interface.

that's right, we also have http but that's not part of this PR. I opened #19129 and #19125 to fix this.

&gt; Bikeshed: Maybe TCP -&gt; Tcp throughout?

sure why not
</comment><comment author="s1monw" created="2016-06-29T12:40:30Z" id="229343768">@jasontedor I merged up with master and fixed all the other leaks, I think you can take another look.. I am renamind the TCP stuff now
</comment><comment author="s1monw" created="2016-06-30T07:02:24Z" id="229576620">I will push this today unless anybody objects
</comment><comment author="jasontedor" created="2016-06-30T11:48:41Z" id="229636065">&gt; I will push this today unless anybody objects

+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/transport/NetworkExceptionHelper.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java</file><file>test/framework/src/main/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file></files><comments><comment>Verify lower level transport exceptions don't bubble up on disconnects (#19518)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/common/io/Channels.java</file><file>core/src/main/java/org/elasticsearch/common/netty/KeepFrameDecoder.java</file><file>core/src/main/java/org/elasticsearch/common/netty/ReleaseChannelFutureListener.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/http/netty/ESHttpResponseEncoder.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java</file><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpHeader.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/TcpTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/Transport.java</file><file>core/src/main/java/org/elasticsearch/transport/Transports.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferBytesReference.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferStreamInput.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/ChannelBufferStreamInputFactory.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyHeader.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyInternalESLogger.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyInternalESLoggerFactory.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyMessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransportChannel.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyUtils.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/OpenChannelsHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/SizeHeaderFrameDecoder.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/common/ChannelsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/KeyedLockTests.java</file><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file><file>core/src/test/java/org/elasticsearch/transport/PublishPortTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TCPTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/ChannelBufferBytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyUtilsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Factor out abstract TCPTransport* classes to reduce the netty footprint (#19096)</comment></comments></commit></commits></item><item><title>Add test to new bbox method in GeoHashUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19095</link><project id="" key="" /><description>This PR adds test for the utility method introduced in #18668, which should be merged first. 
</description><key id="162472962">19095</key><summary>Add test to new bbox method in GeoHashUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>review</label><label>test</label></labels><created>2016-06-27T14:58:34Z</created><updated>2016-07-11T15:53:17Z</updated><resolved>2016-07-11T15:49:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-07-11T15:49:53Z" id="231776564">Thanks for the test @cbuescher. I merged it with #18668 so I'm going to close this PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default value of boolean is always false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19094</link><project id="" key="" /><description>No need to explicitly specifying boolean settingsFileFound=false; because by default in java boolean value is false
</description><key id="162465460">19094</key><summary>Default value of boolean is always false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vcharmcaster</reporter><labels /><created>2016-06-27T14:28:36Z</created><updated>2016-06-27T14:49:18Z</updated><resolved>2016-06-27T14:49:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-27T14:49:17Z" id="228768385">Thanks for the PR @vcharmcaster but this will not compile; javac will give errors about `settingsFileFound` not having been initialized.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Java API doc for cluster health</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19093</link><project id="" key="" /><description>In 995e4eda08be99f72ef56052b3f78ceef9100885 we changed the cluster health Java API.
We need to also change the documentation.

Same as #19067 but for 2.4 branch.
</description><key id="162465276">19093</key><summary>Update Java API doc for cluster health</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2016-06-27T14:27:55Z</created><updated>2016-06-28T08:13:59Z</updated><resolved>2016-06-28T08:07:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-06-28T08:06:18Z" id="228980401">LGTM
</comment><comment author="dadoonet" created="2016-06-28T08:13:59Z" id="228982089">Also added in master with b9e8ec1938147a467399f76fe4e978f746cecfc9
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update Java API doc for cluster health</comment></comments></commit></commits></item><item><title>cutover some docs to painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19092</link><project id="" key="" /><description>I didn't cut over any complex ones, just a bunch of the easy examples.

I noticed some code using groovy `for in` loop. This is easy enough to add. 

Also to improve debugging, CheckClassAdapter is used when `picky` is set (tests).
</description><key id="162458377">19092</key><summary>cutover some docs to painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label></labels><created>2016-06-27T13:58:25Z</created><updated>2016-06-28T17:40:25Z</updated><resolved>2016-06-28T17:40:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T05:46:05Z" id="228954206">The doc changes look good.
</comment><comment author="jdconrad" created="2016-06-28T17:25:36Z" id="229120299">LGTM!  Thanks for all the doc updates as well @rmuir.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Locals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/SimpleChecksAdapter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessLexer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/LambdaTests.java</file></files><comments><comment>Merge pull request #19092 from rmuir/more_painless_docs</comment></comments></commit></commits></item><item><title>Consolidate docs snippets testing in our REST test infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19091</link><project id="" key="" /><description>This PR contains a bunch of commits to clean up our REST tests infra. The main goal was to make docs snippets testing less of a special case. Specifically, these are the main changes:

1) `ResponseBodyAssertion` that was introduced to test docs snippets is now replaced with the existing `MatchAssertion`. The useful bits around nice error messages that were introduced with `ResponseBodyAssertion` have been moved to `MatchAssertion` so that all of the REST tests can benefit from it. Rather than taking out response bodies as strings and parse them as json objects, they are now part of their ancestor, as yaml supports embedding json objects in it (no need to parse the response separately then). 
2) remove special `stashResponse` method from `Stash` which was introduced specifically for docs snippets testing. Instead the whole stash can be represented as a navigable object and resolve expressions like `$body.took` natively.

To get there, and while at it, other changes were made:
- clarify `Stash` methods naming: `unstashMap` becomes `replaceStashedValues` and `unstashValue` becomes `getValue`
- make `JsonPath` independent from the data format and rename to `ObjectPath`
- `ObjectPath` to support yaml or json that contain an array as root and can't be represented as a map. this is the format that our cat apis use when returning json or yaml format
- eagerly parse response bodies into `ObjectPath` as it will always happen anyways, no room for optimizations given by lazy parsing
- parse responses through `ObjectPath` based on Content-Type response header and adapt REST tests that used to check yaml content using regexes. We used to automatically detect the xcontent type based on content but that fails with json if it only contains an array. Also no need to auto-detect if we know upfront what content type we have
- fix line length problems in o.e.test.rest package

I realize these are quite some changes all at once but they are tightly related to each other, so one PR should make sense, and I left every single step as a separate commit to ease the review process.
</description><key id="162446208">19091</key><summary>Consolidate docs snippets testing in our REST test infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">javanna</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T13:00:06Z</created><updated>2016-07-01T10:08:52Z</updated><resolved>2016-07-01T10:08:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-06-27T13:00:19Z" id="228738391">@nik9000 this one is for you ;)
</comment><comment author="nik9000" created="2016-06-28T15:05:48Z" id="229078261">Awesome! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/rest/ObjectPath.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/RestTestExecutionContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/Stash.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestResponse.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/GreaterThanEqualToParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/GreaterThanParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/LessThanOrEqualToParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/LessThanParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/Assertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/GreaterThanEqualToAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/LengthAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/LessThanAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/LessThanOrEqualToAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/MatchAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/ResponseBodyAssertion.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/support/Features.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/JsonPathTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/ObjectPathTests.java</file><file>test/framework/src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java</file></files><comments><comment>Merge pull request #19091 from javanna/test/nuke_response_body_assertion</comment></comments></commit></commits></item><item><title>Fix documentation typo in How-To docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19090</link><project id="" key="" /><description>Fix Typos
</description><key id="162444256">19090</key><summary>Fix documentation typo in How-To docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>docs</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T12:50:22Z</created><updated>2016-06-27T13:44:08Z</updated><resolved>2016-06-27T13:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-27T13:37:10Z" id="228747374">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19090 from tlrx/fix-typos-in-how-to-docs</comment></comments></commit></commits></item><item><title>Change `unlimited` throttling to use `-1` instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19089</link><project id="" key="" /><description>We only use `unlimited` in the reindex API for throttling, where we use `-1` everywhere else to represent "disabled".  We should make the reindex API consistent and switch to using `-1` instead.

See https://github.com/elastic/elasticsearch/pull/18791#issuecomment-228297229 for the FixItFriday conclusion. 
</description><key id="162440980">19089</key><summary>Change `unlimited` throttling to use `-1` instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Reindex API</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T12:32:47Z</created><updated>2016-07-18T17:09:16Z</updated><resolved>2016-07-18T17:09:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBaseReindexRestHandler.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/BulkByScrollTask.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/BulkByScrollTaskTests.java</file></files><comments><comment>Support requests_per_second=-1 to mean no throttling in reindex</comment></comments></commit></commits></item><item><title>Validate settings against dynamic updaters on the master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19088</link><project id="" key="" /><description>Today all settings are only validated against their validators
that are available when settings are registered. Yet, some settings updaters
have validators that are dynamic ie. their validation depends on other variables
that are only available at runtime. We do not run those validators when settings
are updated causing index updates to fail on the data nodes instead of on the master.

Relates to #19046
</description><key id="162432334">19088</key><summary>Validate settings against dynamic updaters on the master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T11:40:06Z</created><updated>2016-06-27T15:18:27Z</updated><resolved>2016-06-27T15:18:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-27T11:40:15Z" id="228722465">@jimferenczi FYI
</comment><comment author="jimczi" created="2016-06-27T12:07:27Z" id="228727506">LGTM, thanks for fixing this @s1monw 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/indices/settings/UpdateSettingsIT.java</file></files><comments><comment>Validate settings against dynamic updaters on the master (#19088)</comment></comments></commit></commits></item><item><title>How to make cache available for index which is bulk indexing data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19087</link><project id="" key="" /><description>How to make cache available for index which is bulk indexing data.

I use es in logging scenario, the alerts need to query es every 10 seconds, but no cache hits event if I set 
 curl -XPUT 'localhost:9200/es_index_logrouter60_2016-06-17/_settings' -d'{ "index.cache.query.enable": true}'

Could anyone help me?
</description><key id="162428783">19087</key><summary>How to make cache available for index which is bulk indexing data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">9zdata</reporter><labels /><created>2016-06-27T11:18:47Z</created><updated>2016-06-27T13:21:02Z</updated><resolved>2016-06-27T13:21:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T13:21:02Z" id="228743187">This is a question best asked on `discuss.elastic.co`. We try to keep github for bugs and features.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Register group setting for repository-azure accounts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19086</link><project id="" key="" /><description>Since the Settings infrastructure has been improved, a group setting must be registered by the repository-azure plugin to allow settings like `cloud.azure.storage.my_account.account` to be coherent with Azure plugin documentation.

Without this change, a user will hit the following error:

```
[2016-06-27 12:01:15,666][INFO ][plugins                  ] [Darkhawk] modules [], plugins [repository-azure]
|    Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [cloud.azure.storage.my_account_test.account] please check that any required plugins are installed, or check the breaking changes documentation for removed settings
```

This PR also adds a simple integration test for this.
</description><key id="162417018">19086</key><summary>Register group setting for repository-azure accounts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Repository Azure</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T10:07:46Z</created><updated>2016-06-27T11:45:52Z</updated><resolved>2016-06-27T10:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-27T10:14:33Z" id="228706630">LGTM
</comment><comment author="s1monw" created="2016-06-27T11:42:39Z" id="228722909">LGTM 2
</comment><comment author="tlrx" created="2016-06-27T11:45:52Z" id="228723457">Thanks @dadoonet @s1monw.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pass resolved extended bounds to unmapped histogram aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19085</link><project id="" key="" /><description>Previous to this change the unresolved extended bounds was passed into the histogram aggregator which meant extendedbounds.min and extendedbounds.max was passed through as null. This had two effects on the histogram aggregator:
1. If the histogram aggregator was unmapped across all shards, the reduce phase would not add buckets for the extended bounds and the response would contain zero buckets
2. If the histogram aggregator was not unmapped in some shards, the reduce phase might sometimes chose to reduce based on the unmapped shard response and therefore the extended bounds would be ignored.

This change resolves the extended bounds in the unmapped case and solves the above two issues.

Closes #19009
</description><key id="162413862">19085</key><summary>Pass resolved extended bounds to unmapped histogram aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-06-27T09:51:39Z</created><updated>2016-06-27T14:42:11Z</updated><resolved>2016-06-27T13:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-27T12:45:50Z" id="228735193">LGTM
</comment><comment author="clintongormley" created="2016-06-27T14:29:21Z" id="228762150">@colings86 can this be backported to 2.4?
</comment><comment author="colings86" created="2016-06-27T14:42:00Z" id="228766083">It couldn't be cherry-picked but since it was a simple change I have made the same change in 2.4 in this commit: https://github.com/elastic/elasticsearch/commit/376a14d50308e0a2c37c8a13cbebea065cac634f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to enable Slow Logs for Suggest query ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19084</link><project id="" key="" /><description>**Elasticsearch version**:Any

**Describe the feature**: I want to enable Slow Logs logging for _suggest query. I have already enable index.search.slowlog.\* logs , I can see only search logs.

Is there any way to log _suggest query ? 
</description><key id="162388456">19084</key><summary>How to enable Slow Logs for Suggest query ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ishara</reporter><labels><label>:Logging</label><label>:Search</label></labels><created>2016-06-27T07:21:16Z</created><updated>2016-10-18T08:50:50Z</updated><resolved>2016-07-01T09:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-27T14:04:04Z" id="228754853">~~Currently there isn't a way to do this~~ 

Actually I'm not sure about this.  Just setting the timeouts low enough should work I think?
</comment><comment author="ishara" created="2016-06-28T08:27:55Z" id="228985079">No luck , _suggest query not log.
</comment><comment author="s1monw" created="2016-07-01T09:21:06Z" id="229900009">this is fixed in 5.0 since we are going through the search phase with suggestions too. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>modify messy form</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19083</link><project id="" key="" /><description /><key id="162379565">19083</key><summary>modify messy form</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keweiliu6</reporter><labels><label>Awaiting CLA</label><label>docs</label><label>v5.0.0-beta1</label></labels><created>2016-06-27T06:02:02Z</created><updated>2016-09-12T22:05:36Z</updated><resolved>2016-09-12T22:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-27T13:10:45Z" id="228740786">Seems right. Can you sign the CLA reply to the ticket? If you've already signed it please just reply to the ticket telling us and we'll figure out why the checker thinks you haven't.
</comment><comment author="dakrone" created="2016-09-12T22:05:35Z" id="246510060">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>gateway.recover_after_nodes should support dynamic setting by api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19082</link><project id="" key="" /><description>**Describe the feature**:

I write a elasticsearch service, support user dynamic add or remove elasticsearch node from cluster by web console. Elasticsearch cluster's zen discovery is good for this situation, new node join cluster did not need to restart old node for reloading config, cluster setting such as 'discovery.zen.minimum_master_nodes' support dynamic updating by api. But 'gateway.recover_after_nodes' is static, and only used by the master node, when the cluster scale_in and scale_out, how i change the setting? I have to restart old nodes for loading this config.

So I think gateway.recover_after_nodes should be a cluster setting and support dynamic setting by api.
</description><key id="162373024">19082</key><summary>gateway.recover_after_nodes should support dynamic setting by api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jolestar</reporter><labels /><created>2016-06-27T04:40:59Z</created><updated>2016-07-01T05:24:49Z</updated><resolved>2016-06-27T13:02:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2016-06-27T09:21:02Z" id="228694753">+1
</comment><comment author="bleskes" created="2016-06-27T13:02:44Z" id="228738957">sadly there is a catch 22 here. Unlike min_master_node (which is used when ever nodes join and leave the cluster), gateway.recover\* settings are only used during the initial cluster state recovery, when a cluster is formed for the first time. You should use these setting to prevent the master from starting to allocate shards before all nodes have joined, creating unneeded chatter. This means we can't store it in the cluster state because at that point it's not yet recovered (we have long terms plan to change things in that area, but for now this is how it works).  How ever, it is worth while to go and change the setting in the elasticsearch.yml files on the master nodes when the cluster size changes - it will be used in the next cluster outage. 

I'm closing this for now. Feel free to reopen if you feel something is not covered.
</comment><comment author="jolestar" created="2016-07-01T05:24:49Z" id="229856613">Ok, i got it. Now, i change the recover\* setting in elasticsearch.yam when cluster size change, but not restart  the node.  thanks. @bleskes 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elastic Search node using the complete swap.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19081</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7

**JVM version**: 1.7.0_45-b18

**OS version**: Linux 2.6.18-406.el5 #1 SMP Wed Jun 3 05:51:45 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

I am running 5 nodes ES cluster. Each node is having 16GB RAM and 8 CPUs.
Xms configured 8GB.
No other processes are running on these nodes.
Number of documents per day 300 million
Total documents in cluster : 3 billion
After few days, when I check the swap usage, it is getting used completely.
I am not able to find out the root cause why swap is getting used?
Can you please let me know what is causing swap usage on my ES nodes?
If you need anyf urther information to debug this issue, please let me know.

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="162346038">19081</key><summary>Elastic Search node using the complete swap.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ravitandur</reporter><labels /><created>2016-06-26T20:37:30Z</created><updated>2016-06-26T20:42:50Z</updated><resolved>2016-06-26T20:42:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ravitandur" created="2016-06-26T20:38:59Z" id="228621426">free -m
             total       used       free     shared    buffers     cached
Mem:         15922      15518        404          0         46       6489
-/+ buffers/cache:       8981       6941
Swap:         2047       **2036**         10
</comment><comment author="jasontedor" created="2016-06-26T20:42:50Z" id="228621603">For some background on Elasticsearch, and swapping, please see my [comments](https://github.com/elastic/elasticsearch/issues/17605#issuecomment-207149090) on #17065. If you have follow-up questions, please open a topic on the [Elastic Discourse forum](https://discuss.elastic.co) as Elastic reserves GitHub for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename plainless into painless in migration doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19080</link><project id="" key="" /><description>The scripting language was wrongly named.

I searched for other occurrences of `plainless` and this was the only ones. 

Cheers :beers: 
</description><key id="162333113">19080</key><summary>Rename plainless into painless in migration doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienalexandre</reporter><labels><label>docs</label></labels><created>2016-06-26T15:42:56Z</created><updated>2016-06-26T15:44:17Z</updated><resolved>2016-06-26T15:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-26T15:44:17Z" id="228607489">Thanks! :) 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19080 from damienalexandre/patch-4</comment></comments></commit></commits></item><item><title>An incredibly arcane missing header in response to a POST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19079</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: 
openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-0ubuntu4~16.04.1-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:
Linux jeff-desktop 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
(Ubuntu 16.04)

**Description of the problem including expected versus actual behavior**:
This is going to sound incredibly arcane.  There is a missing header in the response to a successful PUT.  The following curl commands work exactly as they are supposed to, there is no problem there.

```
jeffs@jeff-desktop:~$ curl -i -XPOST 'jeffsilverm.ddns.net:9200/customer/external?pretty' -d '
{
  "name": "Jane Eerickson"
}'
HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Content-Length: 201

{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLfkcV0A40HdQsPZ72",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}

jeffs@jeff-desktop:~$ curl -i -XGET 'jeffsilverm.ddns.net:9200/customer/external/AVWLfkcV0A40HdQsPZ72?pretty'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 173

{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLfkcV0A40HdQsPZ72",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "Jane Eerickson"
  }
}jeffs@jeff-desktop:~$ curl -i -XGET 'jeffsilverm.ddns.net:9200/customer/external/AVWLn7k00A40HdQsPZ78?pretty'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 162

{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLn7k00A40HdQsPZ78",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "Bob"
  }
}
jeffs@jeff-desktop:~$ 
jeffs@jeff-desktop:~$ 
```

The problem is that, according to  RFC 2616 section 10.2.2, (https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) the Location header should be included in the 201 response:

&gt; 10.2.2 201 Created
&gt; 
&gt; The request has been fulfilled and resulted in a new resource being created. The newly created resource can be referenced by the URI(s) returned in the entity of the response, with the most specific URI for the resource given by a **Location** header field. The response SHOULD include an entity containing a list of resource characteristics and location(s) from which the user or user agent can choose the one most appropriate. The entity format is specified by the media type given in the Content-Type header field. The origin server MUST create the resource before returning the 201 status code. If the action cannot be carried out immediately, the server SHOULD respond with 202 (Accepted) response instead.
&gt; 
&gt; A 201 response MAY contain an ETag response header field indicating the current value of the entity tag for the requested variant just created, see section 14.19.

**Steps to reproduce**:
 1.`curl -i -XPOST 'jeffsilverm.ddns.net:9200/customer/external?pretty' -d ' {   "name": "Bob" }' | fgrep -i Location`
 returns nothing.
 3.

**Describe the feature**:
There should be a Location header.  In the example above, the URL 'jeffsilverm.ddns.net:9200/customer/external/AVWLn7k00A40HdQsPZ78?pretty' works flawlessly.

```
jeffs@jeff-desktop:~$ curl -i -XGET 'jeffsilverm.ddns.net:9200/customer/external/AVWLn7k00A40HdQsPZ78?pretty'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 162

{jeffs@jeff-desktop:~$ curl -i -XGET 'jeffsilverm.ddns.net:9200/customer/external/AVWLn7k00A40HdQsPZ78?pretty'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 162

{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLn7k00A40HdQsPZ78",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "Bob"
  }
}
jeffs@jeff-desktop:~$ 
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLn7k00A40HdQsPZ78",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "Bob"
  }
}
jeffs@jeff-desktop:~$ 
```

The only problem is the missing Location field in the returned HTTP header when doing the POST.  Note that since the GET returns status code 200 (correctly), no Location header is required by RFC 2616.  The POST returns status code 201 (correctly), and that is where the Location header is required.

The work around is to parse the response and look for the _id field, which can be done using jq or the python json.loads method.  So I believe that this is a very low priority issue.
</description><key id="162316600">19079</key><summary>An incredibly arcane missing header in response to a POST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jeffsilverm</reporter><labels><label>:REST</label><label>low hanging fruit</label></labels><created>2016-06-26T07:39:47Z</created><updated>2016-07-25T21:57:37Z</updated><resolved>2016-07-25T21:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-07-01T09:23:08Z" id="229900435">lets fix it - thanks for reporting.. the location should just be the `GET` URL I guess?
</comment><comment author="jeffsilverm" created="2016-07-10T16:49:07Z" id="231598637">In the case of

jeffsilverm.ddns.net:9200/customer/external?pretty' -d '
{
  "name": "Jane Eerickson"
}'

I my example, elasticsearch returns

HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Content-Length: 201

{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "AVWLfkcV0A40HdQsPZ72",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}

I believe the resulting header should be

HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Location: jeffsilverm.ddns.net:9200/customer/_AVWLfkcV0A40HdQsPZ72_Content-Length:
SOMETHING
{  "_index" : "customer",  "_type" : "external",  "_id" :
"AVWLfkcV0A40HdQsPZ72",  "_version" : 1,  "_shards" : {    "total" :
2,    "successful" : 1,    "failed" : 0  },  "created" : true}

The use case is the ability to create a list of pointers to documents in a
client of elasticsearch.

Thank you

Jeff

On Fri, Jul 1, 2016 at 2:24 AM, Simon Willnauer notifications@github.com
wrote:

&gt; lets fix it - thanks for reporting.. the location should just be the GET
&gt; URL I guess?
&gt; 
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/19079#issuecomment-229900435,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AAhhh1TWE2ilECXZDuyBrJisTyLIDkcmks5qRNy3gaJpZM4I-gTq
&gt; .

## 

Jeff Silverman, linux sysadmin
nine two four   twentieth avenue east
Seattle, WA, nine eight one one two -3507
(253) 459-2318
jeffsilverm@gmail.c0m (note the zero!)
http://www.commercialventvac.com
See my portfolio of writings and talks
http://www.commercialventvac.com/portfolio.html
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/DocWriteResponse.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/StatusToXContent.java</file><file>core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/support/RestStatusToXContentListener.java</file><file>core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>core/src/test/java/org/elasticsearch/action/DocWriteResponseTests.java</file><file>distribution/deb/src/test/java/org/elasticsearch/test/rest/RestIT.java</file><file>distribution/integ-test-zip/src/test/java/org/elasticsearch/test/rest/CreatedLocationHeaderIT.java</file><file>distribution/integ-test-zip/src/test/java/org/elasticsearch/test/rest/RestIT.java</file><file>distribution/rpm/src/test/java/org/elasticsearch/test/rest/RestIT.java</file><file>distribution/tar/src/test/java/org/elasticsearch/test/rest/RestIT.java</file><file>distribution/zip/src/test/java/org/elasticsearch/test/rest/RestIT.java</file><file>docs/src/test/java/org/elasticsearch/smoketest/SmokeTestDocsIT.java</file><file>modules/aggs-matrix-stats/src/test/java/org/elasticsearch/search/aggregations/matrix/MatrixAggregationRestIT.java</file><file>modules/ingest-common/src/test/java/org/elasticsearch/ingest/common/IngestCommonRestIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionRestIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyRestIT.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/PainlessRestIT.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorRestIT.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/ReindexRestIT.java</file><file>modules/transport-netty3/src/test/java/org/elasticsearch/http/netty3/Netty3RestIT.java</file><file>modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4RestIT.java</file><file>plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/AnalysisICURestIT.java</file><file>plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/AnalysisKuromojiRestIT.java</file><file>plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/AnalysisPhoneticRestIT.java</file><file>plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/AnalysisSmartChineseRestIT.java</file><file>plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/AnalysisPolishRestIT.java</file><file>plugins/discovery-azure-classic/src/test/java/org/elasticsearch/discovery/azure/classic/AzureDiscoveryRestIT.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/DiscoveryEc2RestIT.java</file><file>plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java</file><file>plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/IngestAttachmentRestIT.java</file><file>plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java</file><file>plugins/ingest-user-agent/src/test/java/org/elasticsearch/ingest/useragent/UserAgentRestIT.java</file><file>plugins/jvm-example/src/test/java/org/elasticsearch/plugin/example/JvmExampleRestIT.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/LangJavaScriptRestIT.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/LangPythonScriptRestIT.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MapperAttachmentsRestIT.java</file><file>plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/MapperMurmur3RestIT.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/MapperSizeRestIT.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureRepositoryRestIT.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepositoryRestIT.java</file><file>plugins/repository-hdfs/src/test/java/org/elasticsearch/repositories/hdfs/HdfsRepositoryRestIT.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/RepositoryS3RestIT.java</file><file>plugins/store-smb/src/test/java/org/elasticsearch/index/store/SMBStoreRestIT.java</file><file>qa/backwards-5.0/src/test/java/org/elasticsearch/backwards/MultiNodeBackwardsIT.java</file><file>qa/smoke-test-ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java</file><file>qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/smoketest/IngestWithDependenciesIT.java</file><file>qa/smoke-test-multinode/src/test/java/org/elasticsearch/smoketest/SmokeTestMultiIT.java</file><file>qa/smoke-test-plugins/src/test/java/org/elasticsearch/smoketest/SmokeTestPluginsIT.java</file><file>qa/smoke-test-reindex-with-painless/src/test/java/org/elasticsearch/smoketest/SmokeTestReindexWithPainlessIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESClientYamlSuiteTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/RestTestExecutionContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/support/Features.java</file></files><comments><comment>Add Location header and improve REST testing</comment></comments></commit></commits></item><item><title>read timeout for ec2 discovery plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19078</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.8

**OS version**: Linux ip-10-0-1-106 4.1.13-19.31.amzn1.x86_64 #1 SMP Wed Jan 20 00:25:47 UTC 2016 x86_64 x86_64 x86_64 GNU/Linu

**Description of the problem including expected versus actual behavior**:
It seems with a large number of ec2 instances within a region, the ec2 discovery plugin will timeout

``` bash
[2016-06-25 03:48:09,987][INFO ][discovery.ec2            ] [Zartra] Exception while retrieving instance list from AWS API: Unable to unmarshall response (ParseError at [row,col]:[174,26]
Message: Read timed out). Response Code: 200, Response Text: OK
[2016-06-25 03:49:21,390][INFO ][discovery.ec2            ] [Zartra] Exception while retrieving instance list from AWS API: Unable to unmarshall response (java.net.SocketTimeoutException: Read timed out). Response Code: 200, Response Text: OK
```

It would be great if we can configure this timeout either through environmental variables of system properties. 
</description><key id="162313952">19078</key><summary>read timeout for ec2 discovery plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dat-vikash</reporter><labels><label>:Plugin Discovery EC2</label><label>adoptme</label><label>enhancement</label></labels><created>2016-06-26T05:42:32Z</created><updated>2016-06-26T06:13:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Require timeout units when parsing query body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19077</link><project id="" key="" /><description>Today when parsing the timeout field in a query body, if time units are
supplied the parser throws a NumberFormatException. Addtionally, the
parsing allows the timeout field to not specify units (it assumes
milliseconds). This commit fixes this behavior by not only allowing time
units to be specified but requires time units to be specified. This is
consistent with the documented behavior and the behavior in 2.x.

Closes #19075
</description><key id="162293163">19077</key><summary>Require timeout units when parsing query body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-25T18:13:56Z</created><updated>2016-06-26T10:35:59Z</updated><resolved>2016-06-25T20:18:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-25T19:27:23Z" id="228566340">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file></files><comments><comment>Require timeout units when parsing query body</comment></comments></commit></commits></item><item><title>InternalTestCluster: Fix data path separation for different node roles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19076</link><project id="" key="" /><description>#18514 introduced dedicate masters in our testing runs. It also added logic to make sure master and data nodes use different data paths as starting data node on a master path (and vice versa) could lead to tricky to debug failures. Sadly the PR only worked in the case where data paths were explicitly set - which we only do in 20% of the runs. This commit fixes this by working on the base dir level instead. It also adds a test so we know things work :)
</description><key id="162284367">19076</key><summary>InternalTestCluster: Fix data path separation for different node roles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-25T14:47:15Z</created><updated>2016-06-29T10:16:32Z</updated><resolved>2016-06-28T06:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-25T14:47:27Z" id="228547387">@javanna care to take a look?
</comment><comment author="bleskes" created="2016-06-25T17:55:36Z" id="228562134">@javanna hold back on this one. The whole concept of separated data path for master/data nodes breaks the assumption of using node lock ordinals to separate node data when using indices with custom paths  - the data path separation makes the lock ordinals not unique anymore. That's the gist - it's more subtle in practice. I'll pick this up monday :)
</comment><comment author="bleskes" created="2016-06-28T06:40:55Z" id="228963032">closing in favor of #19111
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java</file></files><comments><comment>Testing infra - stablize data folder usage and clean up (#19111)</comment></comments></commit></commits></item><item><title>Timeout values like `1s` result in number_format_exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19075</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Environment**

ES 5.0.0-alpha3
Linux/4.4.13-moby/amd64
Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_91/25.91-b14
running inside Docker.

**Problem**

I cannot issue "human" values for `timeout`any more, e.g. `timeout=1s`.

**Steps to reproduce**:

Notice the `"timeout":"1s"` here:

```
$ curl 'localhost:9200/_search' -d '{"query":{"match_all":{}},"timeout":"1s"}'
{"error":{"root_cause":[{"type":"number_format_exception","reason":"For input string: \"1s\""}],"type":"number_format_exception","reason":"For input string: \"1s\""},"status":400}
```

Notice the `"timeout":"1000"` here (same with `1000`, without the quotes btw):

```
$ curl 'localhost:9200/_search' -d '{"query":{"match_all":{}},"timeout":"1000"}'
{"took":1,"timed_out":false,"_shards":{"total":2,"successful":2,"failed":0},"hits":{"total":3,"max_score":1.0,"hits":[{"_index":"elastic-test","_type":"tweet","_id":"1","_score":1.0,"_timestamp":1466863101867,"_source":{"user":"olivere","message":"Welcome to Golang and Elasticsearch.","retweets":108,"created":"2012-12-12T17:38:34Z"}},{"_index":"elastic-test","_type":"tweet","_id":"2","_score":1.0,"_timestamp":1466863101879,"_source":{"user":"olivere","message":"Another unrelated topic.","retweets":0,"created":"2012-10-10T08:12:03Z"}},{"_index":"elastic-test","_type":"tweet","_id":"3","_score":1.0,"_timestamp":1466863101882,"_source":{"user":"sandrae","message":"Cycling is fun.","retweets":12,"created":"2011-11-11T10:58:12Z"}}]}}
```

Cannot find a deprecation notice. Is this expected behavior?
</description><key id="162282701">19075</key><summary>Timeout values like `1s` result in number_format_exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">olivere</reporter><labels><label>bug</label></labels><created>2016-06-25T14:07:23Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-06-25T20:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-25T16:01:44Z" id="228552406">I don't think it's expected. I would actually expect the opposite: reject time values without unit.
</comment><comment author="jasontedor" created="2016-06-25T18:15:27Z" id="228563062">&gt; Cannot find a deprecation notice. Is this expected behavior?

This is a mistake, thanks for reporting. We require units on time values, so the behavior is completely the opposite of the documented behavior and the behavior in the 2.x series of Elasticsearch. I have opened #19077 to fix. Thanks for reporting! I have marked you as eligible for the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program).
</comment><comment author="jasontedor" created="2016-06-25T20:18:58Z" id="228568650">Closed by #19077
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ingest-useragent plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19074</link><project id="" key="" /><description>This PR adds an `ingest-useragent` plugin, meant to replicate the functionality of `logstash-filter-useragent`.

This is the last ingest plugin missing to be able to replicate a standard pipeline for web logs with just the Ingest Node (e.g. as in the [example for Apache logs](https://github.com/elastic/examples/tree/master/ELK_apache)).

The code is based on [logstash-filter-useragent](https://github.com/logstash-plugins/logstash-filter-useragent), the Ruby library it uses ([uap-ruby](https://github.com/ua-parser/uap-ruby)) and the Java equivalent, [uap-java](https://github.com/ua-parser/uap-java). However, since the code in uap-java is kind of overengineered for this purpose (esp. way too many classes) I wrote the whole thing from scratch, so there shouldn't be any significant code share.

`regexes.yaml`, the file containing regular expressions for useragent, operating system and device detection is copied from [uap-core](https://github.com/ua-parser/uap-core), I added a license header to it. Doing it this way keeps this dependency in the code, but will require it to be updated manually from time to time. If a user requires a newer or even custom version of this file, they can copy it into the `config/` directory of the plugin and specify the `regex_file` option (similar to `database_file` in `ingest-geoip`).

Some points that may be worth discussing:
1. Should `regexes.yaml` be part of this repo? `ingest-geoip` depends on some external database files that are in a separate repo under the elastic org and uploaded to Maven Central. I shied away from this because A) in this case it's a text file, not binary B) it's more straightforward having it in this repo, thereby being able to easily PR new versions, seeing all changes made to it in the git history, etc.
2. This plugin has one dependency: `commons-collections`. It's not a unique new one, `repository-hdfs` already depends on it. However, here only one class is used, `LRUMap`. It's important, as it dramatically improves performance (in my tests, it reduces processing time by almost 80%, would likely be even higher if measured without grok running in front). If an LRU cache already exists in Elasticsearch or one of its existing dependencies using that one might be better.
3. In `UseragentParser` there's two classes with public members and no methods, used solely as containers to be passed to `UseragentProcessor`. I'm happy to put getters and setters in place, I thought they'd unnecessarily clutter the code without adding any real value.
4. I tried running `gradle check` as per `CONTRIBUTING.MD`, however it fails due to `_ttl` no longer being available in 5.0. Since this happens because of tests outside of my code and I can see that those jobs are currently disabled on build.elastic.co I figured I'm fine. `gradle assemble` and `gradle test` on just this plugin as well as `gradle assemble` on elasticsearch/ works fine, and I tested the deployed plugin with real data.

Hope I didn't forget anything, let me know if I have or if I should change something.
</description><key id="162238730">19074</key><summary>ingest-useragent plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>:Ingest</label><label>feature</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T22:00:29Z</created><updated>2016-07-05T09:39:17Z</updated><resolved>2016-07-01T13:49:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-24T22:51:35Z" id="228482257">&gt; If an LRU cache already exists in Elasticsearch or one of its existing dependencies using that one might be better.

We have one, [`org.elasticsearch.common.cache.Cache`](https://github.com/elastic/elasticsearch/blob/7c87d39f0c62d5e9f84d9f199ebc7ba15191bead/core/src/main/java/org/elasticsearch/common/cache/Cache.java).
</comment><comment author="cwurm" created="2016-06-24T23:30:36Z" id="228488463">Indeed, thanks. It can't be configured to only hold a certain number of elements though, can it? High-cardinality data would lead to higher memory usage then.

The cardinality in my smallish dataset of 300K web requests is about 5K different agent strings, in my tests an LRU cache with 1K elements is practically as good as one with 10K.

I tested it out, performance is as good as with `LRUMap`, so that's not a concern.
</comment><comment author="jasontedor" created="2016-06-24T23:33:27Z" id="228488888">&gt; It can't be configured to only hold a certain number of elements though, can it?

Yes, it can. Set the maximum weight to the maximum number of elements that you want and set the weight function to map every element to weight 1.
</comment><comment author="martijnvg" created="2016-06-27T06:19:19Z" id="228662668">@cwurm Nice work!

I wonder if this processor can be placed in the `ingest-common` module instead of as separate plugin? Especially if this processor can handle its resource like the grok processor? The reasons that geoip and attachment are provided as plugin is because of the size its dependencies and the fact that these plugins require additional security permissions.
</comment><comment author="cwurm" created="2016-06-27T16:02:43Z" id="228791387">@martijnvg I like the idea.

However, ingest-common doesn't have its own directory in `config/`. As you suggested earlier, we could load the default regex file from the classpath, but what about when a user wants to provide a custom regex file?

The user could put it into the top-level config directory, together with elasticsearch.yml and other standard files. Is there another feature that does that already? Would doing this be alright?

I guess we could have the user create a new subdirectory, yet it feels error-prone.
</comment><comment author="s1monw" created="2016-06-27T16:06:23Z" id="228792454">&gt; The user could put it into the top-level config directory, together with elasticsearch.yml and other standard files. Is there another feature that does that already? Would doing this be alright?

yes that is the place to put things like this. you can specify a directory  via some setting and that directory can be wherever. You are not bound to anything as long as the security manager allows to read from it. stuff like this must be a node setting but must not be part of the module/plugin
</comment><comment author="martijnvg" created="2016-06-27T16:11:45Z" id="228793880">&gt; The user could put it into the top-level config directory, together with elasticsearch.yml and other standard files. Is there another feature that does that already? Would doing this be alright?

I think we should pass custom regexes via the processor configuration in the pipeline definition? This is also now how custom grok patterns are specified. So right now this doesn't allow sharing of custom regexes between pipelines, but we could add infrastructure for that in ingest, in a general manner, so that grok patterns can be shared too between pipelines. If we have this then managing custom regexes is much easier as it is available on all nodes via the cluster state.
</comment><comment author="cwurm" created="2016-06-27T16:40:01Z" id="228801893">&gt; I think we should pass custom regexes via the processor configuration in the pipeline definition? This is also now how custom grok patterns are specified.

For grok this makes sense, but I don't think that is practical here.

We would provide a standard set of 800+ regexes. I'm pretty sure there's user agents it doesn't cover and going forward, this will only get worse as the release ages.

At the moment, in `logstash-filter-useragent` we allow the user to provide a custom file containing new patterns (similar in `ingest-geoip` for IPs). In practice, this will be a more current version of the same file (on average, it [gets several updates per month](https://github.com/ua-parser/uap-core/commits/master/regexes.yaml)).

I don't think pasting all of that into Sense / Console makes sense. There might be cases where somebody wants to add a specific user agent (e.g. some internal crawler or something), but even then it's impractical to add it separately, as order matters (first matching regex wins).
</comment><comment author="martijnvg" created="2016-06-28T07:00:24Z" id="228967687">&gt;  as order matters (first matching regex wins).

Ok, I didn't realise this. Then for now lets allow adding custom files via `config/ingest-useragent` directory and the out of the box file should then be provided via the jar file.
</comment><comment author="cwurm" created="2016-06-29T17:28:37Z" id="229428399">Is anything still outstanding after my last commits?
</comment><comment author="martijnvg" created="2016-06-30T08:01:01Z" id="229589339">@cwurm The PR is looking good. I left some more comments. Also when running `gradle clean check` there are some checkstyle failure (some lines are too long).
</comment><comment author="martijnvg" created="2016-06-30T08:31:49Z" id="229596068">We also need an integration test that tests using a custom regex file. 
1. Add a custom test regex file to the plugin root directory.
2. Add the following snippet to the `build.gradle` file:

```
integTest {
  cluster {
    extraConfigFile 'ingest-useragent/test-regexes.yaml', 'test-regexes.yaml'
  }
}
```
1. Add a test case that creates a pipeline that sets `regex_file` setting to `test-regexes.yaml`.
</comment><comment author="rjernst" created="2016-06-30T10:00:38Z" id="229615983">I would not put that file under src/test/resources because then it would be on the classpath, and the point would be to ensure finding it in the filesystem works (yes the proposed gradle config will add it to the config dir, but having it in both places leaves us unsure of whether it works bc of the classpath or filesystem).
</comment><comment author="martijnvg" created="2016-06-30T10:16:34Z" id="229619196">@rjernst Makes sense, then that file should be added to the plugin root directory.
</comment><comment author="cwurm" created="2016-06-30T11:20:07Z" id="229631040">@martijnvg `UserAgentProcessorFactoryTests` creates a second file in the file system and tests using that in `testBuildRegexFile`. Would that not be enough?
</comment><comment author="martijnvg" created="2016-06-30T11:26:15Z" id="229632026">@cwurm No, it is should be separate file that doesn't endup on the classpath. This file should be simple and just contain a single line (dummy regex), just to prove that if a custom file specified things work as expected.
</comment><comment author="cwurm" created="2016-06-30T15:39:04Z" id="229698312">@martijnvg Last few commits should have implemented remaining suggestions. The optional directory one is outstanding, I didn't quite understand that
</comment><comment author="martijnvg" created="2016-06-30T15:47:24Z" id="229700841">@cwurm The `config/ingest-useragent` only exists if custom regex files are added, right? The logic in `IngestUserAgentPlugin.createUserAgentParsers(...)` could just be:

```
if (directoryExists) {
  // load the parsers based on custom files
}
```
</comment><comment author="cwurm" created="2016-06-30T16:26:34Z" id="229712328">@martijnvg Right. Implemented in https://github.com/elastic/elasticsearch/pull/19074/commits/d4cf76d59c1078296ece7753cd0ae8f8f5d33f89
</comment><comment author="martijnvg" created="2016-06-30T20:23:20Z" id="229777839">@cwurm This looks great! The only thing I'm wondering now if we should move this processor in the `ingest-common` module. What do you think? 

By default we don't rely on any additional config that needs to be packaged, the default regex file is part of jar file itself. Also this plugin doesn't depend an additional dependencies or require additional security permission (either both is usually a reason the package a feature as a plugin) and by moving this processor to the `ingest-common` module it becomes out-of-the-box available.
</comment><comment author="cwurm" created="2016-06-30T21:34:39Z" id="229795759">@martijnvg I'm a little hesitant, because all the other plugins in `ingest-common` so far are very general-purpose, whereas this one is just applicable if you have user agent strings in your data, which most data doesn't. I'd be happy to have it in common though, so if you think that's a good idea let's do it.
</comment><comment author="martijnvg" created="2016-07-01T06:11:03Z" id="229862099">@cwurm We can always move it to `ingest-commen` if we find out that a lot of times this plugin ends up being installed. LGTM!
</comment><comment author="cwurm" created="2016-07-01T11:35:52Z" id="229925184">@martijnvg Thanks. I've made two more commits to what has proved to be the most "fragile" piece of the code during development and testing. Still looking good? Can I merge?
</comment><comment author="martijnvg" created="2016-07-01T11:40:08Z" id="229925834">@cwurm 👍  yes, still looking great.
</comment><comment author="jasontedor" created="2016-07-05T09:39:17Z" id="230432818">@cwurm Thanks for the great work in this PR.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/IngestUserAgentPlugin.java</file><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/UserAgentCache.java</file><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/UserAgentParser.java</file><file>plugins/ingest-useragent/src/main/java/org/elasticsearch/ingest/useragent/UserAgentProcessor.java</file><file>plugins/ingest-useragent/src/test/java/org/elasticsearch/ingest/useragent/UserAgentProcessorFactoryTests.java</file><file>plugins/ingest-useragent/src/test/java/org/elasticsearch/ingest/useragent/UserAgentProcessorTests.java</file><file>plugins/ingest-useragent/src/test/java/org/elasticsearch/ingest/useragent/UserAgentRestIT.java</file></files><comments><comment>Add ingest-useragent plugin (#19074)</comment></comments></commit></commits></item><item><title>Switch analysis from push to pull</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19073</link><project id="" key="" /><description>Instead of plugins calling `registerTokenizer` and friends to add custom analysis components they now instead have to implement `AnalysisPlugin` and override `getTokenizer` and friends. This lines up extending plugins in with extending scripts and allows AnalysisModule to construct the AnalysisRegistry
immediately as part of its constructor which makes testing analysis **much** simpler.

This also moves the default analysis configuration into AnalysisModule which is how search is setup.

And it stops binding `HunspellService` in guice and reworks its test so that is no longer required.

Finally this removed `extends AbstractModule` from `AnalysisService`. `AnalysisService` is now only responsible for building the `AnalysisRegistry` and `Node` binds that with guice directly.
</description><key id="162236537">19073</key><summary>Switch analysis from push to pull</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T21:44:19Z</created><updated>2016-07-29T11:43:42Z</updated><resolved>2016-06-26T11:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-24T21:46:38Z" id="228469926">@s1monw: this is another startup cleanup - it doesn't actually remove any `@Inject` annotations but it makes the `AnalysisRegistry` much simpler to access during startup which seems like the first step to removing it from guice.
</comment><comment author="s1monw" created="2016-06-25T18:58:06Z" id="228565029">Great LGTM left some minors
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_score is always 1 when running query with saved query template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19072</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.0

**JVM version**: 
java version "1.7.0_95"
OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

**OS version**:
ubuntu0.14.04.1

**Description of the problem including expected versus actual behavior**:
Query ran from saved query template always returns _score of 1

Where as query ran without using the saved query template returns the proper _score

**Steps to reproduce**:
1. Create query with saved query template
2. Run query with the template and params
3. See that all the _score have been set to 1
4. Run the same query without the template
5. See that all the _score have been populated correctly

**Provide logs (if relevant)**:

Not logs but:

```
{
   "query":{
      "bool":{
         "should":[
            {
               "bool":{
                  "should":[
                     {
                        "bool":{
                           "must":[
                              {
                                 "exists":{
                                    "field":"productions_name_raw"
                                 }
                              },
                              {
                                 "term":{
                                    "productions_name_raw":{
                                       "value":"{{ 0_Title }}",
                                       "boost":5
                                    }
                                 }
                              }
                           ]
                        }
                     },
                     {
                        "bool":{
                           "must":[
                              {
                                 "exists":{
                                    "field":"productions_name_raw"
                                 }
                              },
                              {
                                 "prefix":{
                                    "productions_name_raw":{
                                       "value":"{{ 0_Title }}",
                                       "boost":5
                                    }
                                 }
                              }
                           ]
                        }
                     },
                     {
                        "bool":{
                           "must":[
                              {
                                 "exists":{
                                    "field":"productions_name"
                                 }
                              },
                              {
                                 "match_phrase":{
                                    "productions_name":{
                                       "query":"{{ 0_Title }}",
                                       "slop":0,
                                       "boost":1
                                    }
                                 }
                              }
                           ]
                        }
                     },
                     {
                        "bool":{
                           "must":[
                              {
                                 "exists":{
                                    "field":"productions_name"
                                 }
                              },
                              {
                                 "term":{
                                    "productions_name":"{{ 0_Title }}"
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            },
            {
               "bool":{
                  "must":[
                     {
                        "exists":{
                           "field":"productions_production_year"
                        }
                     },
                     {
                        "term":{
                           "productions_production_year":"{{ 1_year }}"
                        }
                     }
                  ]
               }
            }
         ]
      }
   }
}
```
</description><key id="162214796">19072</key><summary>_score is always 1 when running query with saved query template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">billxinli</reporter><labels /><created>2016-06-24T19:28:45Z</created><updated>2016-06-27T13:35:03Z</updated><resolved>2016-06-27T13:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-27T13:35:02Z" id="228746817">Hi @billxinli 

You haven't provided a recreation, just a query.  You can see from this simple example that search templates work just fine, and give the correct score:

```
PUT /_search/template/foo
{
  "query": {
    "match": {
      "title": "{{title}}"
    }
  }
}

PUT t/t/1
{
  "title": "quick brown fox"
}

GET _search/template
{
  "id": "foo",
  "params": {
    "title": "quick fox"
  }
}
```

Your query is seriously broken though.  For once, you don't need all of those `exists` clauses.  Then you're using `tem` queries on analyzed fields.  

You need some advice (and to read the docs). I suggest asking questions like these in the forum instead: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Revert slow tests heartbeat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19071</link><project id="" key="" /><description>This commit reverts the slow tests heartbeat added in
b6fbd18e091de7b22ba8020241d01ce5dfd8accc. The heartbeat has not served
its stated purpose of drawing attention to slow tests, and the heartbeat
can kill builds with SELinux enforcing enabled. While the heartbeats can
be disabled via the environment variable PULSE_SERVER, and SELinux
policy files can be changed, since the heartbeats are not accomplishing
their intended purpose they should be removed.

Relates #15322
</description><key id="162204164">19071</key><summary>Revert slow tests heartbeat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-24T18:29:14Z</created><updated>2016-06-26T10:35:59Z</updated><resolved>2016-06-24T18:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-24T18:30:16Z" id="228424115">LGTM.

Sad to see it gone, but it was always annoying.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Revert slow tests heartbeat</comment></comments></commit></commits></item><item><title>Add replaceAll and replaceFirst</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19070</link><project id="" key="" /><description>These are useful methods in groovy that give you control over
the replacements used:

```
'the quick brown fox'.replaceAll(/[aeiou]/,
        m -&gt; m.group().toUpperCase(Locale.ROOT))
```

They mimick two of the augments groovy has for regexes.
</description><key id="162200041">19070</key><summary>Add replaceAll and replaceFirst</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T18:04:48Z</created><updated>2016-06-29T12:54:30Z</updated><resolved>2016-06-28T20:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-24T18:34:14Z" id="228425198">Why not use the official loop for replacements in Matcher? https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html#appendReplacement-java.lang.StringBuffer-java.lang.String-
</comment><comment author="rmuir" created="2016-06-24T20:54:17Z" id="228459405">I don't know enough about java's regex to know if Uwe's suggestion is strictly equivalent. But I like the PR.
</comment><comment author="nik9000" created="2016-06-24T21:22:56Z" id="228465281">&gt; Why not use the official loop for replacements in Matcher?

I figured those were less than desirable because they were StringBuffer instead of StringBuilder.
</comment><comment author="uschindler" created="2016-06-24T22:12:15Z" id="228475005">&gt; I figured those were less than desirable because they were StringBuffer instead of StringBuilder.

That's not an issue anymore. StringBuilder and StringBuffer are as fast in Java 6+. The introduction of StringBuilder was only needed in older Java versions. This is the reason why the APIs of Matcher were not changed afterwards (issues in OpenJDK closed).

So please don't reimplement the code and be safe and use the Matcher methods! If you look at `String.replaceAll(String, String)` (the regex one) and `Matcher.replaceAll()` source code you see the same loop: http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/java/util/regex/Matcher.java#Matcher.replaceAll%28java.lang.String%29

In addition, the Java APIs allow to use `\1` and `\0` for groups, and can be "disabled" by `quoteReplacement`
</comment><comment author="nik9000" created="2016-06-26T12:42:39Z" id="228599563">&gt; That's not an issue anymore

Looks like it. Uncontended synchronization elimination works as expected. Another old habit gone.

&gt; In addition, the Java APIs allow to use \1 and \0 for groups, and can be "disabled" by quoteReplacement

I'll look into it.
</comment><comment author="nik9000" created="2016-06-26T12:51:21Z" id="228599907">&gt; I'll look into it.

And done. It seems silly that they didn't make `quoteReplacement` a boolean parameter but I guess this way covers slightly more cases.
</comment><comment author="uschindler" created="2016-06-26T15:47:33Z" id="228607638">Thanks.

Shouldn't we maybe allow `\1` or `$1` to allow references to groups (means remove `quoteReplacement`)? To me this is a major use-case or search/replace.
</comment><comment author="nik9000" created="2016-06-26T16:03:11Z" id="228608364">Not for the closure form. You can get them from the matcher. You can
certainly use replaceAll on the matcher if you want them.
On Jun 26, 2016 11:47 AM, "Uwe Schindler" notifications@github.com wrote:

&gt; Thanks.
&gt; 
&gt; Shouldn't we maybe allow \1 or $1 to allow references to groups (means
&gt; remove quoteReplacement)? To me this is a major use-case or
&gt; search/replace.
&gt; 
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/19070#issuecomment-228607638,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLoq0sGr2e7u5zmZKBX3hQukhvdfI7ks5qPp8agaJpZM4I9_j3
&gt; .
</comment><comment author="nik9000" created="2016-06-27T02:12:29Z" id="228640345">My reasoning is that `\1` makes sense in `Matcher.replaceAll` because you are sending it a string - but this version you send it a `Function` so if you want a capturing group you can just call `m.group` on it and have the capturing group. `\1` feels like it'd just get in the way and be surprising. I could see someone having the opposite opinion though. For what it is worth, the method this is imitating in groovy doesn't support `\1` either though the javadocs don't go into why.
</comment><comment author="clintongormley" created="2016-06-27T13:48:03Z" id="228750318">@nik9000 i want to be able to do things like:

```
/\b(?:\d{12}(\d{4}))\b/.replaceAll("...$1")
```

I have no idea what this would look like with your syntax?
</comment><comment author="nik9000" created="2016-06-27T13:52:06Z" id="228751442">&gt; I have no idea what this would look like with your syntax?

That is already there with us whitelisting `Matcher#replaceAll`

```
/\b(?:\d{12}(\d{4}))\b/.matcher("string").replaceAll("stuff$1")
```

This proposed syntax if for when you need to manipulate the captures more than just moving them around.
</comment><comment author="rmuir" created="2016-06-27T14:04:54Z" id="228755088">&gt; For what it is worth, the method this is imitating in groovy doesn't support \1 either though the javadocs don't go into why.

I think it would be confusing to diverge from that behavior. For a lot of reasons consistency with whatever it is doing will be easiest on everyone if we can have it.
</comment><comment author="nik9000" created="2016-06-27T14:08:04Z" id="228755918">@clintongormley I can add another example to the docs calling out `Matcher#replaceAll`'s existence and `$1` support and to describe this feature as for when you need "yet more control".
</comment><comment author="nik9000" created="2016-06-27T16:17:21Z" id="228795414">&gt; I can add another example to the docs calling out Matcher#replaceAll's existence and $1 support and to describe this feature as for when you need "yet more control".

I just added that to this PR.
</comment><comment author="rmuir" created="2016-06-28T17:42:10Z" id="229125446">What is the status here? this change looks good to me.
</comment><comment author="nik9000" created="2016-06-28T20:18:09Z" id="229169750">&gt; What is the status here? this change looks good to me.

I'm not sure.... I think at this point I'll merge it and we can rework it later if it needs it?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove uses of `ExceptionHelper.detailedMessage`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19069</link><project id="" key="" /><description>This method tries to summarize an exception in a single string. But the entire point of the structured exception work was to get real stack traces. This method just hurts, and does not help. For example, if you have a `NullPointerException`, nothing will help except for the stack trace, but detailedMessage will only emit the exception name.  We should remove all uses of this (there are over 50 right now).
</description><key id="162195705">19069</key><summary>Remove uses of `ExceptionHelper.detailedMessage`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Exceptions</label><label>PITA</label></labels><created>2016-06-24T17:39:35Z</created><updated>2016-06-29T19:40:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-28T15:37:00Z" id="229088318">+1 this annoyed me multiple times already
</comment><comment author="rmuir" created="2016-06-29T16:16:51Z" id="229407968">&gt; This method just hurts, and does not help.

Then lets rename it to `ExceptionsHurter`, and deprecate it to at least stop the bleeding. Otherwise the task will get bigger over time.
</comment><comment author="rjernst" created="2016-06-29T19:40:59Z" id="229465622">&gt; deprecate it to at least stop the bleeding

Done in #19160
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ExceptionsHelper.java</file></files><comments><comment>Internal: Deprecate ExceptionsHelper.detailedMessage</comment></comments></commit></commits></item><item><title>Function score: score_mode avg looks broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19068</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

The function score with `score_mode: avg` looks broken:

```
PUT /foo/bar/1
{
  "name": "a b"
}

PUT /foo/bar/2
{
  "name": "b"
}

PUT /foo/bar/3
{
  "name": "a"
}

GET foo/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "filter": {
            "term": {
              "name": "a"
            }
          },
          "weight": 20
        },
        {
          "filter": {
            "term": {
              "name": "b"
            }
          },
          "weight": 10
        }
      ],
      "score_mode": "avg",
      "boost_mode": "replace"
    }
  }
}
```

All the scores are now `1`

if you change to `"score_mode": "sum",` everything works as expected, with 10, 20 and 30 being the score.
</description><key id="162186811">19068</key><summary>Function score: score_mode avg looks broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Search</label><label>docs</label></labels><created>2016-06-24T16:48:35Z</created><updated>2017-03-31T09:36:41Z</updated><resolved>2017-03-31T09:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-06-24T17:17:13Z" id="228405707">Well, I see the problem, but I don't know enough to understand why it's being done this way.  Perhaps not for a real reason and it's just incorrect?

When a `WeightFactorFunction` is scored, it multiplies the score (`1` in this case, since it's a filter) [by it's own weight](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java#L61).  But when averaging the function scores, we [explicitly check if the weight is a `WeightFactorFunction`](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java#L312-L316), and if it is, divide by the weight.  So net result is that each function with a weight divides out to a score of 1, which is why it isn't averaging the weights correctly.

I'm not sure why the `WeightFactorFunction` is special cased, but I think we can just remove the special case and increment the counter as normal?
</comment><comment author="clintongormley" created="2016-06-27T13:10:25Z" id="228740730">@brwe What do you think?
</comment><comment author="brwe" created="2016-06-28T12:27:06Z" id="229034078">`avg` computes the weighted average of the functions, see https://github.com/elastic/elasticsearch/issues/8992 and https://github.com/elastic/elasticsearch/issues/13732 for more explanation. You should be able to accomplish the averages of the values by having them returned by a script instead of using them as a weight. 
</comment><comment author="polyfractal" created="2016-06-28T13:49:05Z" id="229054199">Ah, I see.  Perhaps we should re-add a basic, unweighted average as `avg`, and rename the existing `avg` to `weighted_avg`?  Or vice versa: add an `unweighted_avg` and leave the current one as `avg`?

&gt;  You should be able to accomplish the averages of the values by having them returned by a script instead of using them as a weight.

I'm not sure this would work when you want to use more complex queries/filters?  E.g. if docA matches `(xyz AND abc) OR 123` , return a non-weighted score of 25.
</comment><comment author="jpountz" created="2016-06-28T16:02:31Z" id="229096528">I think Britta meant using a `script_score` function when she talked about using a script, rather than a script query. So something like this (not tested):

```
GET foo/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "filter": {
            "term": {
              "name": "a"
            }
          },
          "script_score": {
            "script": "20"
          }
        },
        {
          "filter": {
            "term": {
              "name": "b"
            }
          },
          "script_score": {
            "script": "10"
          }
        }
      ],
      "score_mode": "avg",
      "boost_mode": "replace"
    }
  }
}
```
</comment><comment author="brwe" created="2016-06-28T16:17:50Z" id="229101034">&gt; I think Britta meant using a script_score

yes

&gt;   Perhaps we should re-add a basic, unweighted average as avg, and rename the existing avg to weighted_avg? Or vice versa: add an unweighted_avg and leave the current one as `avg`

If we find we need an unweighted average then I'd prefer the latter option.
</comment><comment author="polyfractal" created="2016-06-28T16:23:01Z" id="229102621">Ah, I see.  Thanks for the explanation :)
</comment><comment author="clintongormley" created="2016-06-29T12:10:52Z" id="229337652">This isn't the first time this has come up, perhaps it should be better explained in the docs? https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-function-score-query.html
</comment><comment author="brwe" created="2016-06-29T14:21:03Z" id="229370902">yes, I opened https://github.com/elastic/elasticsearch/pull/19154
</comment><comment author="colings86" created="2017-03-31T09:36:41Z" id="290666688">It seems like this was addressed in documentation in #19154 so I'll close this issue. If there is more to be done please reopen</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Java API doc for cluster health</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19067</link><project id="" key="" /><description>In 995e4eda08be99f72ef56052b3f78ceef9100885 we changed the cluster health Java API.
We need to also change the documentation.
</description><key id="162181924">19067</key><summary>Update Java API doc for cluster health</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>review</label></labels><created>2016-06-24T16:19:49Z</created><updated>2017-01-20T14:04:08Z</updated><resolved>2016-06-27T13:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-24T16:20:04Z" id="228391061">@danielmitterdorfer Could you look at this change please?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for synonym query to percolator query term extraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19066</link><project id="" key="" /><description /><key id="162180872">19066</key><summary>Add support for synonym query to percolator query term extraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T16:14:16Z</created><updated>2016-06-27T05:46:54Z</updated><resolved>2016-06-27T05:46:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-25T13:05:40Z" id="228539590">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bring painless docs closer to reality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19065</link><project id="" key="" /><description>These docs are woefully out of date. Let's remove the inaccuracies.

This patch does not add a lot of content or fancy examples. If we want to do that, lets do it as another issue.

The goal here is just to remove inaccuracies and keep things minimal.

It also adds `instanceof` operator (it made documenting easier, as we then support all of java's ops). It works for primitive types too, as we don't make any distinction there.
</description><key id="162180133">19065</key><summary>Bring painless docs closer to reality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-06-24T16:10:43Z</created><updated>2016-06-26T10:35:59Z</updated><resolved>2016-06-24T16:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-24T16:41:40Z" id="228396569">LGTM!  @rmuir Thanks for adding this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Augmentation.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessLexer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EInstanceof.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/package-info.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AugmentationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ComparisonTests.java</file></files><comments><comment>Merge pull request #19065 from rmuir/help_painless_docs</comment></comments></commit></commits></item><item><title>Java API MultiGet does not provide high level source filtering and field inclusion per the REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19064</link><project id="" key="" /><description>**Describe the feature**:
The links below are for ES 1.7, but the request also applies to 2.3.

Per the [Multi Get REST documentation](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/docs-multi-get.html#mget-source-filtering) top level source filtering and field inclusion are supported.  

Parity is missing between the MultiGet Java API and the MultiGet REST API.
</description><key id="162176389">19064</key><summary>Java API MultiGet does not provide high level source filtering and field inclusion per the REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arciisine</reporter><labels><label>:CRUD</label><label>discuss</label><label>enhancement</label><label>feedback_needed</label></labels><created>2016-06-24T15:51:34Z</created><updated>2016-07-01T13:02:13Z</updated><resolved>2016-07-01T13:02:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-24T15:59:06Z" id="228385335">Are you looking for something like the following?

``` java
client.prepareGet("index", "type", "id").setFetchSource("include", "exclude").get()
```

I agree that it's not super obvious but I think this is what you are looking for.
</comment><comment author="arciisine" created="2016-06-24T16:00:24Z" id="228385680">I'm looking for

``` java
client.prepareMultiGet("index", null, ids).setFetchSource("include", "exclude").get()
```

Which is not supported by the Java API, but is supported by the Rest API
</comment><comment author="dadoonet" created="2016-06-24T16:01:53Z" id="228386107">This? 

``` java
client.prepareMultiGet().add(new MultiGetRequest.Item("index", "type", "id").fetchSourceContext(new FetchSourceContext("include", "exclude"))).get();
```
</comment><comment author="arciisine" created="2016-06-24T16:12:29Z" id="228389098">Yeah, I found that, and ended up doing this:

``` java
        MultiGetRequestBuilder builder = ...
        List&lt;String&gt; idsToFind = ...
        String[] includes = ...
        String[] excludes = ...

        if (includes != null || excludes != null) {
            for (String id : idsToFind) {
                MultiGetRequest.Item item = new MultiGetRequest.Item(index, null, id)
                if (includes != null) {
                    item.fetchSourceContext().includes(includes)
                }
                if (excludes != null) {
                    item.fetchSourceContext().excludes(excludes)
                }
                builder.add(item);
            }
        } else {
            builder.add(index, null, idsToFind)
        }
```

This doesn't seem to match the high level source filtering indicated in the REST API, but it does work.  
</comment><comment author="arciisine" created="2016-06-24T16:15:01Z" id="228389733">I updated the bug to be specifically about the high level source/field management versus the inability to actually get filtering working. 

I'm not sure if there is a performance difference between adding it on a per item basis or adding it to the entire request.
</comment><comment author="dadoonet" created="2016-06-24T16:33:54Z" id="228394526">Behind the scene we are doing the exact same thing:

``` java
    public MultiGetRequestBuilder add(String index, @Nullable String type, Iterable&lt;String&gt; ids) {
        for (String id : ids) {
            request.add(index, type, id);
        }
        return this;
    }
```

And 

``` java
    public MultiGetRequest add(String index, @Nullable String type, String id) {
        items.add(new Item(index, type, id));
        return this;
    }
```

I don't know what will be the plan with the new REST Client but for the TransportClient (or NodeClient) I don't think we need to add this as we can always do it as you wrote, which ends up with the same data structure in memory.
</comment><comment author="arciisine" created="2016-06-24T16:38:46Z" id="228395801">I'm less concerned about the Java API and more concerned about how the high level filtering works via the REST API.  I'm wondering if it can do something more efficient on the Elasticsearch side if I can declare a consistent set of include/exclude versus a per document configuration.
</comment><comment author="s1monw" created="2016-07-01T09:29:52Z" id="229901824">@arciisine I am confused, you want a default exclude/include for all GET requests on the REST API, right? I really wonder if we should do this since where would you stop if you start adding these APIs? I would rather keep these API clean and don't overload anything except of index and type?
</comment><comment author="arciisine" created="2016-07-01T12:09:38Z" id="229930716">@s1monw This is primarily looking for parity with the REST api which does provide the high level include/exclude.  The reasoning behind the request is not one of convenience (though that does hold some merit), but one of potential performance.  I do not know how the REST API interprets the high level include/exclude.  If all it does is convert the high level include/exclude into item specific includes/excludes then there is nothing to gain (apart from ease of use). 

The Java API docs seem to indicate that it subsumes everything from the REST API, and the REST API docs indicate:

&gt; You can also use the url parameters _source,_source_include &amp; _source_exclude to specify defaults, which will be used when there are no per-document instructions.
</comment><comment author="clintongormley" created="2016-07-01T13:02:13Z" id="229940698">It is for convenience only, and won't benefit performance so i think we're good to close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Asciidoc for java update-by-query api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19063</link><project id="" key="" /><description /><key id="162168860">19063</key><summary>Asciidoc for java update-by-query api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T15:14:03Z</created><updated>2016-06-29T15:12:15Z</updated><resolved>2016-06-29T15:10:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-28T13:56:56Z" id="229056525">@clintongormley and/or @palecur, can you review this?
</comment><comment author="palecur" created="2016-06-28T17:59:58Z" id="229130739">On it.
</comment><comment author="palecur" created="2016-06-28T20:58:52Z" id="229181435">Opened a PR into Nik's repo to modify this PR.
</comment><comment author="nik9000" created="2016-06-29T15:12:15Z" id="229387749">Since @palecur and I looked to be happy with the docs I've merged.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make discovery-azure plugin work again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19062</link><project id="" key="" /><description>This `discovery-azure` plugin is broken since 2.x. This commit fixes the plugin so that it can work with the security manager and uses the right classloader when loading its Azure services.

It's just a fix because testing these things on Azure are very time consuming, but we **really really** need to automatically test this correctly. It's a shame it's been broken for so long.

Note: 2.x fix is similar and will follow soon.

Related to #18637, #15630
</description><key id="162165169">19062</key><summary>Make discovery-azure plugin work again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-06-24T14:55:48Z</created><updated>2016-06-28T08:11:28Z</updated><resolved>2016-06-28T08:06:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-27T07:20:59Z" id="228671060">@dadoonet Thanks for the review. I updated the code, can you please test it on your side?
</comment><comment author="dadoonet" created="2016-06-27T07:31:40Z" id="228672884">The change looks good to me.
</comment><comment author="dadoonet" created="2016-06-27T15:24:29Z" id="228779590">@tlrx I ran a test today:
- Started an azure instance (Ubuntu)
- Installed OpenJDK 8
- Downloaded Elasticsearch from Maven: https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/distribution/zip/elasticsearch/5.0.0-alpha4-SNAPSHOT/elasticsearch-5.0.0-alpha4-20160627.132643-136.zip
- Uploaded discovery-azure plugin I built locally from your branch
- Changed `elasticsearch.yml`:

``` yml
cloud:
    azure:
        management:
            keystore:
                path: /home/elasticsearch/elasticsearch-5.0.0-alpha4-SNAPSHOT/config/azure/azurekeystore.pkcs12
                password: MYPASSWORD
            subscription.id: MYID
            cloud.service.name: MYPROJECT

discovery:
    type: azure
```
- Changed `logging.yml`:

``` yml
  discovery.azure: TRACE
  cloud.azure: TRACE
```
- Started elasticsearch and looked at logs:

```
[2016-06-27 15:17:43,704][DEBUG][cloud.azure              ] [Cyclops] starting azure services
[2016-06-27 15:17:43,704][TRACE][cloud.azure              ] [Cyclops] All required properties for Azure discovery are set!
[2016-06-27 15:17:43,704][DEBUG][cloud.azure              ] [Cyclops] starting azure discovery service
[2016-06-27 15:17:50,198][TRACE][cloud.azure.management   ] [Cyclops] creating new Azure client for [MYID], [MYPROJECT]
[2016-06-27 15:17:51,560][DEBUG][cloud.azure.management   ] [Cyclops] creating new Azure client for [MYID], [MYPROJECT]
[2016-06-27 15:17:57,336][INFO ][node                     ] [Cyclops] initialized
[2016-06-27 15:17:57,336][INFO ][node                     ] [Cyclops] starting ...
[2016-06-27 15:17:58,595][INFO ][transport                ] [Cyclops] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2016-06-27 15:17:58,897][DEBUG][discovery.azure          ] [Cyclops] start building nodes list using Azure API
[2016-06-27 15:18:07,019][TRACE][discovery.azure          ] [Cyclops] ip of current node: [/127.0.0.1]
[2016-06-27 15:18:07,038][TRACE][discovery.azure          ] [Cyclops] adding 10.1.0.4, transport_address 10.1.0.4:9300
[2016-06-27 15:18:07,048][DEBUG][discovery.azure          ] [Cyclops] 1 node(s) added
```

To me everything is OK. +1 to merge your PR.

Thanks!
</comment><comment author="tlrx" created="2016-06-28T08:11:28Z" id="228981560">Unfortunately this bug has not been caught by the `AzureDiscoveryClusterFormationTests` test which has been made to test the discovery-azure plugin with the security manager.

I suspect that a) the test is correct and place the keystore in an accessible place and b) when executed using Gradle and the test framework, the testing classpath contains all the necessary JARs files for the `ServiceLoader` to load the Azure services implementations. I think that using a REST based integration test would have caught the issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wrong Long vlaue when store long value in es</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19061</link><project id="" key="" /><description>today i met one long value convert issue, 
for clear seeing this issue, i capture one image, as following:
&lt;img src="https://cdn.discourse.org/elastic/uploads/default/optimized/2X/4/4aa9e966be69c70fe9ef2ffed1bb41c10f6a6f94_1_690x265.png" width="690" height="265"&gt;

**logstash: 2.1.1**
**elasticsearch: 2.1.1**
**completely logstash config file is shown as following,** 
input {
  stdin { }
}
filter {
  grok {
    match =&gt; {
      "message" =&gt; "%{TIMESTAMP_ISO8601:logging_time}\t%{INT:biz_type}\t%{GREEDYDATA:data}\t%{IP:local_ip}"
    }
  }
  if [biz_type] {
    if ( [data] and [data] =~ /(._){(.+)}(._)/ ) {
      json {
        source =&gt; "data"
        remove_field =&gt; [ "data" ]
      }
    }
  }
}
output {
  stdout {
    codec =&gt; rubydebug
  }
  elasticsearch {
    hosts =&gt; ["localhost:9200"]
    document_type =&gt; "biz_log_record_%{biz_type}"
    index =&gt; "bizlog-%{+YYYY-MM-dd}"
    template_name =&gt; "template_revised"
    template =&gt; "/programs/server/logstash-2.1.1/template/dada-template.json"
    template_overwrite =&gt; true
    manage_template =&gt; true
  }
}

**detail logstash info and es info , as following,**
2016-06-24 15:54:33 10050   {"transporter_id_list": [605603], "push_count": 1, "device_type": 1, "request_id": 1466754873939567710, "is_success": true, "push_id": null, "area_id": 10, "time_type": "test"}    10.10.183.101
{
               "@version" =&gt; "1",
             "@timestamp" =&gt; "2016-06-24T10:31:43.531Z",
                   "host" =&gt; "cookorg.local",
           "logging_time" =&gt; "2016-06-24 15:54:33",
               "biz_type" =&gt; "10050",
               "local_ip" =&gt; "10.10.183.101",
    "transporter_id_list" =&gt; [
        [0] 605603
    ],
             "push_count" =&gt; 1,
            "device_type" =&gt; 1,
             **"request_id" =&gt; 1466754873939567710,**
             "is_success" =&gt; true,
                "push_id" =&gt; nil,
                "area_id" =&gt; 10,
              "time_type" =&gt; "test"
}

**in the es storage, i see different  request_id value, as following:**
{
"_index": "bizlog-2016-06-24",
"_type": "biz_log_record_10050",
"_id": "AVWB9x3-qfZHqOoPMgIt",
"_version": 1,
"_score": 1,
"_source": {
"@version": "1",
"@timestamp": "2016-06-24T10:31:43.531Z",
"host": "cookorg.local",
"logging_time": "2016-06-24 15:54:33",
"biz_type": "10050",
"local_ip": "10.10.183.101",
"transporter_id_list": [
605603
],
"push_count": 1,
"device_type": 1,
**"request_id": 1466754873939567600,**
"is_success": true,
"push_id": null,
"area_id": 10,
"time_type": "test"
}
}

does someone met this issue?
</description><key id="162163630">19061</key><summary>Wrong Long vlaue when store long value in es</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yzhang226</reporter><labels><label>:Mapping</label><label>feedback_needed</label></labels><created>2016-06-24T14:48:42Z</created><updated>2016-07-07T10:27:20Z</updated><resolved>2016-07-07T10:27:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-24T15:13:29Z" id="228373122">What's your mapping for `request_id`? It looks like it's getting treated as a `double` somewhere maybe and that's losing the precision?
</comment><comment author="bleskes" created="2016-06-24T16:23:13Z" id="228391857">Can you try getting the document via curl? JavaScript (which head uses) treats all numbers as double and thus can not represent the full range of longs. I suspect all is fine on the ES side of things
</comment><comment author="jasontedor" created="2016-07-07T10:27:20Z" id="231041388">No additional feedback, closing. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>when i use pattern with analyzer then fire agggrigation query then it split data </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19060</link><project id="" key="" /><description>## my index

PUT /whatever
{
  "settings": {
    "analysis": {
        "filter" : {
            "code" : {
               "type" : "pattern_capture",
               "preserve_original" : 1,
               "patterns" :"[\w\s\/]+"
            }
         },
   "analyzer" : {
            "code" : {
               "tokenizer" : "pattern",
               "filter" : [ "code", "lowercase" ]
            }
         }

```
}
```

  },
  "mappings": {
    "type": {
      "properties": {
        "name": {
          "type": "string",
          "analyzer": "code"
        }
      }
    }
  }
}
## insert record

PUT whatever/d/1
{

```
"name":"Ajay Rathod"
```

}
## i fire query 

GET whatever/_search
{

```
"query": {
    "query_string": {

       "query": "name:Ajay"

    }
},
"aggs":
{
    "demo1":{
        "terms":{
            "field":"name"
        }
    }
}
```

}
## result

{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.19178301,
      "hits": [
         {
            "_index": "whatever",
            "_type": "d",
            "_id": "1",
            "_score": 0.19178301,
            "_source": {
               "name": "Ajay Rathod"
            }
         }
      ]
   },
   "aggregations": {
      "demo1": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "ajay",
               "doc_count": 1
            },
            {
               "key": "rathod",
               "doc_count": 1
            }
         ]
      }
   }
}

it split my name with space
</description><key id="162147794">19060</key><summary>when i use pattern with analyzer then fire agggrigation query then it split data </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingaj</reporter><labels /><created>2016-06-24T13:34:44Z</created><updated>2016-06-24T13:39:52Z</updated><resolved>2016-06-24T13:39:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kingaj" created="2016-06-24T13:35:21Z" id="228346966">please help im stuck with long time
</comment><comment author="dadoonet" created="2016-06-24T13:39:52Z" id="228348127">Please ask questions on discuss.elastic.co instead.
We can help you there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to start the kibana service in Windows 8 64 bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19059</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

Elasticsearch 2.3.3

Installed Java : JDK - 8u91

Windows 8 X64

I have set the environment variables as below  to initialize the installation process of elastic search 

JAVA_HOME -&gt; C:\Program Files\Java\jdk1.8.0_91

Then installed the elastic search using command prompt using admin mode with a command service install -&gt; successfully installed

Then tried to start the service using the command -&gt; service start after that getting issue like 

failed to start failed starting 'elasticsearch-service-x64' service 

Could you please let me know the cause  of the issue and troubble shooting steps. 

Thanks in advance

Regards
Vinoth
</description><key id="162108028">19059</key><summary>Failed to start the kibana service in Windows 8 64 bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vinothn44</reporter><labels /><created>2016-06-24T09:19:55Z</created><updated>2016-06-24T09:43:48Z</updated><resolved>2016-06-24T09:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-24T09:26:32Z" id="228300186">We would need more details to be able to help you find the cause. For example, there are service logs that would be helpful to see here. However, Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions. I suggest requesting help there if looking at those logs does not help you track down the cause.
</comment><comment author="vinothn44" created="2016-06-24T09:43:48Z" id="228303774">please find the attached screen shot error thrown in the event viewer.

![kibana](https://cloud.githubusercontent.com/assets/20125262/16334387/3b87bfce-3a1e-11e6-8bec-76c122e3172a.PNG)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove settings filtering for service_account in GCS repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19058</link><project id="" key="" /><description>Related to #18945 and to this https://github.com/elastic/elasticsearch/commit/35d3bdab84fa05c71e8ae019aaf661759c8b1622#commitcomment-17914150

In GCS Repository plugin we defined a `service_account` setting which is defined as `Property.Filtered`.
It's not needed as it's only a path to a file.

Closes #18946
</description><key id="162104663">19058</key><summary>Remove settings filtering for service_account in GCS repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository GCS</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-24T08:59:47Z</created><updated>2016-06-27T12:29:32Z</updated><resolved>2016-06-24T09:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-24T09:02:31Z" id="228294629">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the `lowercase_expanded_terms` and `locale` options from `(simple_)query_string`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19057</link><project id="" key="" /><description>This pull request uses the `MultiTermAwareComponent` interface in order to
figure out how to deal with queries that match partial strings. This provides a
better out-of-the-box experience and allows to remove the
`lowercase_expanded_terms` and `locale` (which was only used for lowercasing)
options.

Things are expected to work well for custom analyzers. However, built-in
analyzers make it challenging to know which components should be kept for
multi-term analysis. The way it is implemented today is thet there is a default
implementation that returns a lowercasing analyzer, which should be fine for
most language analyzers for european languages. I did not want to go crazy
with configuring the correct multi-term analyzer for those until we have a way
to test that we are sync'ed with what happens in Lucene like we do for testing
which factories need to implement `MultiTermAwareComponent`.

In the future we could consider removing `analyze_wildcards` as well, but the
query parser currently has the ability to tokenize it and generate a term query
for the n-1 first tokens and a wildcard query on the last token. I suspect some
users are relying on this behaviour so I think this should be explored in a
separate change.

Closes #9978
</description><key id="162101530">19057</key><summary>Remove the `lowercase_expanded_terms` and `locale` options from `(simple_)query_string`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>feature</label><label>release highlight</label><label>stalled</label></labels><created>2016-06-24T08:40:09Z</created><updated>2016-08-29T13:15:53Z</updated><resolved>2016-08-29T11:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-30T09:22:06Z" id="229607583">I have been working on https://issues.apache.org/jira/browse/LUCENE-7355 on the Lucene side, that would help simplify this PR considerably. So I am stalling it until LUCENE-7355 is resolved.
</comment><comment author="jpountz" created="2016-08-29T11:06:24Z" id="243095345">Superseded by #20208
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to use elasticsearch 2.3.1 in java.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19056</link><project id="" key="" /><description> I  want to update elasticsearch from 1.4.1 to 2.3.1.but when i use it only find Elasticsearch is unshaded from 2.0 onwards .and need to some third party dependencies like Guava and Joda?
I‘m a newbie,please some one tell me how to use elasticsearch 2.3.1 in java 1.7.and  where I can find the right version dependencies jars.
</description><key id="162095599">19056</key><summary>How to use elasticsearch 2.3.1 in java.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zonglinyue</reporter><labels /><created>2016-06-24T08:00:01Z</created><updated>2016-06-24T08:23:42Z</updated><resolved>2016-06-24T08:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-24T08:01:25Z" id="228282056">Please ask questions like this on discuss.elastic.co.

BTW you could read [this](https://www.elastic.co/blog/to-shade-or-not-to-shade) if needed.
</comment><comment author="zonglinyue" created="2016-06-24T08:23:42Z" id="228286518">I've read this page before，and try to update pom.xml then "mvn package",but failture and still don't understand how to do.
Thank you anyway. @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RestClient improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19055</link><project id="" key="" /><description>This is a meta issue to track improvements that need to be made to the recently added `RestClient` (#18735).
- [x] add async variant of performRequest method (#19301)
- [x] add docs page on how to use it and configuration for common scenarios like ssl, proxy, basic auth etc. although we can refer to apache http client documentation (#19618)
- [ ] higher level client that exposes api specific methods and helps with providing the right parameters. Bonus points if we can keep backwards compatibility with the current java api by ether implementing the `Client` interface or at least reusing the request and response objects that we already have, or something along those lines to ease migration.
</description><key id="162092115">19055</key><summary>RestClient improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java REST Client</label><label>Meta</label></labels><created>2016-06-24T07:34:25Z</created><updated>2017-02-23T16:36:57Z</updated><resolved>2017-02-23T16:36:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-24T13:41:50Z" id="228348621">&gt; Bonus points if we can keep backwards compatibility with the current java api by implementing the Client interface and reusing the RequestBuilder objects that we already have, or something along those lines to ease migration.

I think that'd be super useful but if we're going to do it I'd want to do it in yet another project. So, like `:client:high-level` for a useful high level client and `:client:transport-adapter` or something. That way the high level client doesn't have to pull in all of Elasticsearch's dependencies and we don't have to figure out a way to slice the transport client's interface out of Elasticsearch.
</comment><comment author="mkw" created="2016-08-31T16:28:33Z" id="243820949">I have configured an anonymous user for shield with limited monitoring privileges.  Consequently,  I get back a 403, not a 401, when making anonymous requests.  This prevents the commons http client from responding with the correct credentials.

There should be a way to force pre-emptive authentication, either by setting credentials in the configuration manually (and letting the RestClient manage basic auth by itself), or by exposing the the HttpClientContext object used with the request.  See: https://hc.apache.org/httpcomponents-client-ga/httpclient/examples/org/apache/http/examples/client/ClientPreemptiveBasicAuthentication.java
</comment><comment author="jaymode" created="2016-08-31T17:03:48Z" id="243831385">@mkw you can also set `shield.authc.anonymous.authz_exception: false` in your `elasticsearch.yml` to have the response be a 401 instead of the 403. This should allow you to use the client as is today.
</comment><comment author="mkw" created="2016-08-31T18:20:21Z" id="243854112">@jaymode Thanks!  That is very helpful and vasty superior to manually adding a default "Authentication" header.
</comment><comment author="mkw" created="2016-08-31T18:53:28Z" id="243863936">I may have spoken too soon: While using the challenge/response mechanism is clearly more secure than pre-emptive authentication, it forces a double round-trip for every request because Apache HC will only cache authentication for the duration of an HttpClientContext.  Simple searches are quick enough that the extra request really adds up.
</comment><comment author="brandonkearby" created="2016-09-13T16:53:50Z" id="246748200">Hi Guys, I'm a little late to the party, but started working on a high level rest client. My company has been stuck on 1.4 due to the size of our codebase and number of clients in production. This low level REST client sparked the idea...  [https://github.com/brandonkearby/elasticsearch/tree/rest-client]
What I did was backport the low level rest client into the 1.4 branch and renamed it to InternalRestClient. Then started supporting the various api calls. Everything was going pretty smooth until I started working on Aggregations as they need to provide the type. Anyway I'm interested in working on the high level rest client. 
</comment><comment author="dhay" created="2016-10-31T20:34:09Z" id="257412549">I like the promise of the new 5.0 rest client. However, in it's current form, it isn't terribly useful. We would have to rewrite all of our query and index manipulation code to generate JSON.  Having a Client implementation based on the rest-client would be ideal, but doesn't seem like a straightforward thing to build.
</comment><comment author="javanna" created="2016-11-28T09:10:34Z" id="263218689">Quick update: we are not going to implement the `Client` interface in the upcoming high level REST client. We'd rather to start with a clean design and focus on a better API for the time being. We will reuse as much as possible from the current java API though, meaning requests, responses and builders. We are experimenting on this. We are considering to extract the current java api into a common library, or just make the high level client depend on elasticsearch core, at least in the beginning.</comment><comment author="brandonkearby" created="2016-12-07T10:23:14Z" id="265410540">@javanna Not sure you took a peek, but I have a pretty much complete high level REST client already written for 1.4.1. It works with the existing Client and Admin interfaces

https://github.com/brandonkearby/elasticsearch/commits/rest-client-1.4.1</comment><comment author="jettro" created="2016-12-07T21:27:48Z" id="265578562">@javanna  If at all possible I would not make a dependency on core if that would mean you also have to pull in all the Lucene jars. I personally really like the idea of having as little dependencies as possible.</comment><comment author="javanna" created="2016-12-07T22:00:58Z" id="265587044">&gt; I personally really like the idea of having as little dependencies as possible.

@jettro that makes sense and everybody would like to have no deps of course, but we need to focus first on having something to release rather sooner than later (for which the dependency on es core, hence lucene, is acceptable). After that we can focus on a long term solution, where we'll iterate and hopefully remove the dependency on es core. Other important point is trying to ease the migration from the Java API, for which reusing requests and responses is very important, I think more than removing the dependency on core at this point.</comment><comment author="brandonkearby" created="2016-12-08T06:07:32Z" id="265661099">Yeah @javanna ease of migration was very important for us. It's the primary reason for adding it in 1.4.1 since that's the version we are on. That way we don't have to maintain to ways of doing things while in the transition state. This allows us to continue to use the transport client and once a customer's cluster has been reindex into a 5.0 cluster, we then switch them to the rest client. Same code using transport and rest in the same jvm.</comment><comment author="javanna" created="2016-12-08T09:53:24Z" id="265699361">hey @brandonkearby thanks for the heads up I had a look at your branch. It is similar to what we intend to do, but we will most likely add the parsing code as `fromXContent` methods to all the classes that implement `ToXContent`, one step at a time. Also, ease of migration doesn't mean 1:1 replacement in our plan. We don't plan on implementing the `Client` interface like you did. We have been unhappy with the `Client` interface for a long time, see #9201. We would like to take the chance and clean that up, have a sync method and an async method, rather than a mix of the two. The compatibility will be on the request, builders and response side of things. The call itself will need some slight updating, which should be acceptable.</comment><comment author="brandonkearby" created="2016-12-08T11:32:26Z" id="265719402">@javanna, I started down the path of doing `fromXContent`, but there were places where the object graph didn't match the `JSON`. This made it difficult without a `DOM` approach. As far as the changes in #9201, looks a lot cleaner!</comment><comment author="dhay" created="2016-12-08T14:17:02Z" id="265749823">Has there been any thought to collaborating on improving the existing REST client that is out there, Jest?

https://github.com/searchbox-io/Jest

</comment><comment author="javanna" created="2016-12-14T19:12:00Z" id="267126858">We just published a blogpost on the current state of the Java clients which should answer most of the questions around the REST client and its future: https://www.elastic.co/blog/state-of-the-official-elasticsearch-java-clients .

@dhay as explained in the blog we considered different approaches but we decided to start from our own Java API and take it from there.</comment><comment author="brusic" created="2016-12-14T19:20:31Z" id="267129071">From the blog: "Benchmarks, however,  show that the performance of the HTTP client is close enough to that of the Transport client"

I do not read the results in the same way. :) Would love to see the benchmarks for multi-threaded indexing, not single-threaded. But this issue is regarding the RestClient, so I will rest my defense of the pure Java API for another time.</comment><comment author="danielmitterdorfer" created="2016-12-15T12:17:39Z" id="267314370">&gt; I do not read the results in the same way. :)

@brusic Can you please elaborate? I did the original benchmarks. Maybe something I did not explain something clearly enough in the blog post or I made a mistake (benchmarking is hard and even with great care something can go wrong :)).

&gt; Would love to see the benchmarks for multi-threaded indexing, not single-threaded.

That's one aspect that I thought about originally but I disregarded the idea because we needed to get an initial idea about the performance characteristics of REST vs. transport. Moreover, multi-threaded performance should be dominated by contention in the connection pool and not the client. Nevertheless, we could extend the benchmarks at a later time.</comment><comment author="brusic" created="2016-12-15T18:25:18Z" id="267402631">According to the original blog post "The HTTP client has between 4% and 7% smaller bulk indexing throughput than the transport client." In my world, those numbers are not close enough. :) I want to ask my boss for a 4%-7% raise. Hey, it's close enough!

The blog continues to say that those benchmarks are under lab conditions, but the underlying problem remains. Then again, without multi-threaded benchmarks, I am disregarding the results. :) I use the binary API for indexing and REST for searching since I need all the performance I can get from indexing. The only reason why I use Jest is because I have a dependency on an older version of Lucene in my code and therefore cannot use the elasticsearch jar. </comment><comment author="javanna" created="2016-12-15T21:51:37Z" id="267454847">heya @brusic I think numbers outside of context can be misleading. Using the Java API has so many downsides at this point, like explained in the blogpost, that ~ 5% performance loss is reasonable. We will also work to make the REST client as performant as we can (and do more benchmarks like you are suggesting). That's the right trade-off. The main reason why we had the Java API in the first place wasn't performance, rather that it came for free :) If we started today from scratch we wouldn't expose the transport layer to users anymore, which is why we decided to go down this path.</comment><comment author="godiedelrio" created="2017-02-03T20:10:33Z" id="277349740">If the request / response objects are meant to be reused in the Java Rest Client, care should be taken to not impose java 8 on the client side, IMHO. That way many codebases that can't easly upgrade to Java 8 can still continue to use Elasticsearch 5 through the Java Rest Client.</comment><comment author="javanna" created="2017-02-23T16:28:06Z" id="282043310">hi @godiedelrio , we high level REST client will still depend on Elasticsearch at least in the beginning.  That means it will also require java 8. People that can't upgrade to java 8 or don't want Elasticsearch as a dependency can use the low level REST client, and take care of writing requests and reading back responses in their own application.</comment><comment author="javanna" created="2017-02-23T16:36:57Z" id="282046063">Closing this in favour of #23331 that's more specific to the high level REST client.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseListener.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/main/java/org/elasticsearch/client/SSLSocketFactoryHttpConfigCallback.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/TrackingFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SniffOnFailureListener.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/remote/RemoteScrollableHitSource.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ContextAndHeaderTransportIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsNotSetIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/CorsRegexIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DeprecationHttpIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsDisabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/DetailedErrorsEnabledIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/HttpCompressionIT.java</file><file>qa/smoke-test-http/src/test/java/org/elasticsearch/http/ResponseHeaderPluginIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/RestTestExecutionContext.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestResponse.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/DoSection.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/section/ExecutableSection.java</file></files><comments><comment>Rest client: introduce async performRequest method and use async client under the hood for sync requests too</comment></comments></commit></commits></item><item><title>SearchGraurd plugin not connecting  with Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19054</link><project id="" key="" /><description>I use elasticssearch 2.3.3 
and installed following as plugin in ES
search-guard-ssl - 2.3.3.13
search-guard-2 - 2.3.3.1
1. Created jks file using search-guard-ssl and copied to elasticsearch/config path
2. elasticsearch.yml added entried related to search-guard-ssl 
   searchguard.ssl.transport.keystore_filepath: node-guidanz-keystore.jks
   searchguard.ssl.transport.keystore_password: guidanz123
   searchguard.ssl.transport.truststore_filepath: truststore.jks
   searchguard.ssl.transport.truststore_password: guidanz123
   searchguard.ssl.transport.enforce_hostname_verification: false
   searchguard.ssl.http.enabled: true
   searchguard.ssl.http.keystore_filepath: node-guidanz-keystore.jks
   searchguard.ssl.http.keystore_password: guidanz123
   searchguard.ssl.http.truststore_filepath: truststore.jks
   searchguard.ssl.http.truststore_password: guidanz123
3. elasticsearch.yml added the entry of search-guard-2 plugin
   searchguard.authcz.admin_dn:
   - "DC=com, DC=10.9.14.8, O=Guidanz Com Inc., OU=Guidanz Com Inc. Root CA, CN=Guidanz Com Inc. Root CA"
4. Opened another terminal and ran the following command,
   plugins/search-guard-2/tools/sgadmin.sh -cd plugins/search-guard-2/sgconfig/ -h 10.9.14.8 -p 9300 -ks plugins/search-guard-2/sgconfig/node-guidanz-keystore.jks  -kspass guidanz123 -ts plugins/search-guard-2/sgconfig/truststore.jks -tspass guidanz123  -nrhn
   Got the following error,
   Connect to 10.9.14.8:9300
   Exception in thread "main" NoNodeAvailableException[None of the configured nodes are available: [{#transport#-1}{10.9.14.8}{10.9.14.8:9300}]]
   at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:290)
   at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:207)
   at org.elasticsearch.client.transpotore_filepath: truststore.jks
   searchguard.ssl.transport.truststore_password: guidanz123
   searchguard.ssl.transport.enforce_hostname_verification: false
   searchguard.ssl.http.enabled: true
   searchguard.ssl.http.keystore_filepath: node-guidanz-keystore.jks
   searchguard.ssl.http.keystore_password: guidanz123
   searchguard.ssl.http.truststore_filepath: truststore.jks
   searchguard.ssl.http.truststore_password: guidanz123

searchguard.audit.type: internal_elasticsearch

Kindly suggest to proceed further.rt.support.TransportProxyClient.execute(TransportProxyClient.java:55)
    at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:288)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:348)
    at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:848)
    at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.health(AbstractClient.java:868)
    at com.floragunn.searchguard.tools.SearchGuardAdmin.main(SearchGuardAdmin.java:201)
1. And in elelasticsearch got the following error,

[2016-06-24 11:56:38,625][ERROR][com.floragunn.searchguard.transport.SearchGuardTransportService] No user found in this LivenessRequest for action cluster:monitor/nodes/liveness and transport type netty. If you see this error related when you use sgadmin make sure you connect with a client (and not with a node) certificate.

Below is full configuration of my elasticsearch.yml

cluster.name: my-searchgaurd-application
network.host: 10.9.14.8
http.port: 9500
searchguard.authcz.admin_dn:
- "DC=com, DC=10.9.14.8, O=Guidanz Com Inc., OU=Guidanz Com Inc. Root CA, CN=Guidanz Com Inc. Root CA"

searchguard.ssl.transport.keystore_filepath: node-guidanz-keystore.jks
searchguard.ssl.transport.keystore_password: guidanz123
searchguard.ssl.transport.truststore_filepath: truststore.jks
searchguard.ssl.transport.truststore_password: guidanz123
searchguard.ssl.transport.enforce_hostname_verification: false
searchguard.ssl.http.enabled: true
searchguard.ssl.http.keystore_filepath: node-guidanz-keystore.jks
searchguard.ssl.http.keystore_password: guidanz123
searchguard.ssl.http.truststore_filepath: truststore.jks
searchguard.ssl.http.truststore_password: guidanz123

searchguard.audit.type: internal_elasticsearch

Kindly suggest to proceed further.
</description><key id="162086497">19054</key><summary>SearchGraurd plugin not connecting  with Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kavipreetha</reporter><labels /><created>2016-06-24T06:46:23Z</created><updated>2016-06-24T08:00:06Z</updated><resolved>2016-06-24T08:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-06-24T08:00:05Z" id="228281800">this looks like a searchguard problem. I think it's easier to file an issue in the search guard repository, as this is a plugin not maintained by elasticsearch developers
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simple Query String boolean operators are not simple</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19053</link><project id="" key="" /><description>**Describe the feature**:

The boolean operators `AND`, `OR`, and `NOT` are not simple, as they can act on the term to the left or the right in an often non-obvious manner, [as stated in the documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_boolean_operators).

The `+`, `|`, and `-` operators are much more clear, just as flexible, and all-around simpler.

The `simple_query_string` query language accepts only `+`s, `|`s, and `-`s as boolean operators, and although they look like pluses, pipes, and minuses, [they act like `AND`s, `OR`s, and `NOT`s](https://discuss.elastic.co/t/behavior-of-in-simple-query-strings-vs-query-strings/53823/2)!

This is very unexpected behavior. Pluses, pipes, and minuses should act the same in `query_string` and `simple_query_string` queries.  At the very least, the documentation should clearly state (or better, _emphasize_) that the pluses in query string act as `AND`s, and not pluses.
</description><key id="162031544">19053</key><summary>Simple Query String boolean operators are not simple</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsem</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2016-06-23T21:49:37Z</created><updated>2017-03-31T13:31:27Z</updated><resolved>2017-03-31T13:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-27T12:22:54Z" id="228730461">What part of the syntax documentation do you find unclear?

https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-simple-query-string-query.html#_simple_query_string_syntax
</comment><comment author="dsem" created="2016-06-27T15:53:32Z" id="228788514">When I read, "`+` signifies AND operation," I am not totally convinced that `+` means AND because
- Maybe "AND operation" is a way to describe how the `+` behaves on the `query_string` language too.
- Why would `+` behave differently in the two types of query languages?
- If this is a simple query language, I would be more inclined to believe it uses the simpler boolean operators of `+`, `|`, and `-` (if these can be called boolean operators)
</comment><comment author="clintongormley" created="2016-06-28T16:06:40Z" id="229097704">@dsem feel free to submit a PR to improve the explanation
</comment><comment author="colings86" created="2017-03-31T13:31:27Z" id="290712521">No further feedback</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make parsing of bool queries stricter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19052</link><project id="" key="" /><description>Currently we don't throw an error when there is more than one query clause specified in a must/must_not/should/filter object of the bool query without
using array notation, e.g.: 

```
 {
    "bool": {
        "must": {
            "match" : { ... },
            "match" : { ... }
        }
    }
}
```

In these cases, only the first query will be parsed correctly, possibly leading to silently ignoring the rest of the query.
Instead we should throw a ParsingException if we don't encounter an END_OBJECT token after having parsed the query clause.

Closes #19034
</description><key id="162006940">19052</key><summary>Make parsing of bool queries stricter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-23T19:44:11Z</created><updated>2016-06-27T15:51:21Z</updated><resolved>2016-06-24T09:50:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-23T21:00:47Z" id="228182373">LGTM
</comment><comment author="cbuescher" created="2016-06-24T09:51:36Z" id="228305236">Thanks @jpountz for the review. @clintongormley do you think this should go to the 2.3 and 2.4 branches as well?
</comment><comment author="clintongormley" created="2016-06-27T12:45:05Z" id="228735030">@cbuescher Yeah should probably go to 2.4, thanks
</comment><comment author="cbuescher" created="2016-06-27T15:51:21Z" id="228787833">On 2.4 with adfe5b86
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed typo 'if' -&gt; 'is' in Scripted Metric Aggregation docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19051</link><project id="" key="" /><description>- I believe this should be "is" or some similar wording
</description><key id="162001546">19051</key><summary>Fixed typo 'if' -&gt; 'is' in Scripted Metric Aggregation docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jerryliu55</reporter><labels><label>docs</label></labels><created>2016-06-23T19:16:31Z</created><updated>2016-06-27T12:20:29Z</updated><resolved>2016-06-27T12:20:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-27T12:20:29Z" id="228729962">thanks @jerryliu55 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fixed typo 'if' -&gt; 'is' (#19051)</comment></comments></commit></commits></item><item><title>Remove extra brace from rollover rest api spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19050</link><project id="" key="" /><description>As pointed out by @karmi, there was an extra brace in the rest-api-spec for rollover API.
</description><key id="161968658">19050</key><summary>Remove extra brace from rollover rest api spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:REST</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-23T16:39:56Z</created><updated>2016-06-28T09:32:24Z</updated><resolved>2016-06-23T16:46:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2016-06-23T16:44:06Z" id="228110070">Great, thanks, @areek!
</comment><comment author="jasontedor" created="2016-06-23T16:44:25Z" id="228110154">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19050 from areek/fix/rollover_rest_api_spec</comment></comments></commit></commits></item><item><title>_optimize API results in increased disk usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19049</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.3

**JVM version**:  openjdk version "1.8.0_65"
OpenJDK Runtime Environment (build 1.8.0_65-b17)
OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)

**OS version**: Amazon linux 3.14.35-28.38.amzn1.x86_64 

**Description of the problem including expected versus actual behavior**:

Using _optimize API results in increased disk usage, It used to reduce disk usage in ES 1.3.9.

**Steps to reproduce**:

Before optimize -

```
green open &amp;part-cvreview6         6 2  6949511  3745  64.4gb  21.1gb 
green open &amp;part-prrreview3        6 2 10322051  4917 128.5gb  44.9gb 
green open &amp;part-prrcontributor3   6 2 24509019  2664  48.2gb  16.1gb 
green open &amp;part-cvanswer1         6 2  1493576   425   9.2gb     3gb 
green open &amp;part-prrstory0         6 2   357212     1   3.7gb   1.2gb
```

after doing `curl -XPOST 'http://localhost:9200/_optimize?max_num_segments=1'`

```
green open &amp;part-cvreview6         6 2  6949511 0  91.3gb  30.4gb 
green open &amp;part-prrreview3        6 2 10322051 0 158.4gb  52.8gb 
green open &amp;part-prrcontributor3   6 2 24509019 0  51.5gb  17.1gb
green open &amp;part-cvanswer1         6 2  1493576 0   9.5gb   3.1gb 
green open &amp;part-prrstory0         6 2   357212 0   4.1gb   1.3gb
```

Larger indices are worst affected..
</description><key id="161966922">19049</key><summary>_optimize API results in increased disk usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkelkarbv</reporter><labels /><created>2016-06-23T16:31:31Z</created><updated>2016-06-24T10:19:09Z</updated><resolved>2016-06-23T20:54:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-23T20:54:36Z" id="228180736">This is expected to happen if you have sparse fields in your index. This means that you could probably save a fair amount of disk space - even before optimizing - by modeling your documents a bit differently in order to avoir sparse fields.
</comment><comment author="mkelkarbv" created="2016-06-23T20:57:36Z" id="228181550">@jpountz Can you explain that a bit more? what do you mean by "sparse fields" ?
</comment><comment author="jpountz" created="2016-06-23T21:24:22Z" id="228188489">The current design of Lucene makes it inefficient with fields that only exist in a minority of documents, especially when doc values or norms are enabled on those fields. Typically if only 1% of your documents have a value for field foo, the disk usage for this field will not be the space required for a single value times the number of docs that have a value, but times the total number of documents in the index.

Sometimes this can be addresses by merging two fields together, sometimes by splitting unrelated data into their own indices, etc.
</comment><comment author="mkelkarbv" created="2016-06-23T21:56:16Z" id="228197000">@jpountz thank you for the explanation! 
</comment><comment author="mkelkarbv" created="2016-06-23T22:01:24Z" id="228198290">@jpountz one last thing. Has this functionality changed between lucene 4.9.1 to 4.10.4? 
</comment><comment author="jpountz" created="2016-06-24T10:19:09Z" id="228310118">It does not seem so.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed incorrect indentation in the YAML integration test for the "Rollover" API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19048</link><project id="" key="" /><description>I got failures for the `indices.rollover/10_basic.yaml` test (_expected "logs-1", got nil_) in the Ruby client. The indentation seems wrong, and the test runner cannot properly evaluate the action then.
</description><key id="161964083">19048</key><summary>Fixed incorrect indentation in the YAML integration test for the "Rollover" API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2016-06-23T16:18:08Z</created><updated>2016-06-23T16:57:16Z</updated><resolved>2016-06-23T16:57:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-06-23T16:24:06Z" id="228104631">LGTM! thanks for fixing @karmi 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #19048 from karmi/fix_rollover_integration_test</comment></comments></commit></commits></item><item><title>Fix block checks when no indices are specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19047</link><project id="" key="" /><description>Global cluster blocks were not checked if an empty set of indices were passed as argument to the block checking method.

Closes #8105
</description><key id="161937151">19047</key><summary>Fix block checks when no indices are specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-23T14:30:19Z</created><updated>2016-06-26T10:35:59Z</updated><resolved>2016-06-23T15:52:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-23T15:11:53Z" id="228081932">change LGTM, but I think we should testing on a different level. We should have unit tests on ClusterBlocks
</comment><comment author="ywelsch" created="2016-06-23T15:25:08Z" id="228086464">@bleskes thanks for the review. I've added a unit test.
</comment><comment author="bleskes" created="2016-06-23T15:29:15Z" id="228087871">LGTM. Thx @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IpRangeIT.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file></files><comments><comment>Fix block checks when no indices are specified (#19047)</comment></comments></commit></commits></item><item><title>Add the ability to dynamically update similarity options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19046</link><project id="" key="" /><description>This change allows to dynamically change options for any existing similarities on an open index.
This change also adds the ability to create a new similarity directly on an existing index.
It does not allow to change the type of an existing similarity or to change the similarity of a field.
Options are updatable only if they don't change the way the similarity encodes the norm on disk.
The non-updatable options can't be changed if the index is opened nor closed.
This change adds validation to the similarity settings and throws an exception with a detailed message if a setting is unknown.

Relates #6727
</description><key id="161929603">19046</key><summary>Add the ability to dynamically update similarity options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Similarities</label><label>breaking</label><label>review</label></labels><created>2016-06-23T13:57:47Z</created><updated>2016-09-07T16:11:56Z</updated><resolved>2016-09-06T12:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-23T14:15:46Z" id="228063609">is this breaking?  i think it's just an enhancement, no?
</comment><comment author="jimczi" created="2016-06-23T14:45:55Z" id="228073214">@clintongormley I had to change the SimilarityProvider interface which can be used in a plugin so I marked this as breaking ? It is an enhancement though but it's also a breaking change for users that have a custom similarity plugin.
</comment><comment author="rmuir" created="2016-06-23T14:55:52Z" id="228076491">Yes this logic looks right at a glance: disallow changing `discountOverlaps` as it changes the index-time encoding of the norm value.

NOTE: for all the lucene similarities, the index-time encoding is the same (provided `discountOverlaps` is the same). so changing between them could be allowed, too.

But its definitely important ppl can change stuff like bm25 parameters.
</comment><comment author="jimczi" created="2016-06-23T14:59:23Z" id="228077669">&gt; NOTE: for all the lucene similarities, the index-time encoding is the same (provided discountOverlaps is the same). so changing between them could be allowed, too.

Thanks @rmuir, I want to do this in a follow up but the first step is to be able to change the similarity options. 
</comment><comment author="rmuir" created="2016-06-23T15:28:49Z" id="228087738">&gt; Thanks @rmuir, I want to do this in a follow up but the first step is to be able to change the similarity options.

+1

My only suggestion for this one then, is to improve the exception if you try to change discount_overlaps:
`throw new IllegalArgumentException("");`. 
</comment><comment author="jimczi" created="2016-06-24T10:25:40Z" id="228311222">@s1monw  I have a dynamic setting with a custom validator for the update:

```
AbstractScopedSettings.addSettingsUpdateConsumer(Setting&lt;T&gt; setting, Consumer&lt;T&gt; consumer, Consumer&lt;T&gt; validator)
```

My validator throws RuntimeException if the setting update is not valid. I've added a test that update my setting with invalid values:
https://github.com/jimferenczi/elasticsearch/blob/4829959be53ec19f988532e9d74a7ce6c77dba07/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java#L91
... but it fails. The exception is logged, the settings are applied and the request returns 200 OK.
This looks like a bug but I am not sure that I am using the validator correctly.
Could you please take a quick look at:
https://github.com/jimferenczi/elasticsearch/blob/4829959be53ec19f988532e9d74a7ce6c77dba07/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java#L40
and 
https://github.com/jimferenczi/elasticsearch/blob/4829959be53ec19f988532e9d74a7ce6c77dba07/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java#L119
?
</comment><comment author="s1monw" created="2016-06-25T19:26:15Z" id="228566292">@jimferenczi I just added a unittest to ensure it works but it seems fine, can you point me to the test that fails?
</comment><comment author="jimczi" created="2016-06-26T10:44:46Z" id="228595043">@s1monw thank you for checking. Here is a quick and dirty IT that should fail:
https://github.com/elastic/elasticsearch/compare/master...jimferenczi:dynamic_setting

It does not fail because the setting update is tested on the master with fake index scopped setting:
https://github.com/elastic/elasticsearch/blob/42526ac28e07da0055faafca1de6f8c5ec96cd85/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java#L65

We could retrieve the setting from the index directly but the index may not exist on the master node.
To circumvent this problem the mapping service creates/removes the index:
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L141
... but it seems overkill to do the same for every settings update ?
</comment><comment author="s1monw" created="2016-06-27T11:40:34Z" id="228722531">@jimferenczi I opened  #19088 for the settings problems
</comment><comment author="jimczi" created="2016-06-28T14:52:36Z" id="229073879">Now that https://github.com/elastic/elasticsearch/pull/19088 and https://github.com/elastic/elasticsearch/issues/19122 are merged the tests are ok and this change is ready for review.
</comment><comment author="jimczi" created="2016-07-05T10:18:48Z" id="230441178">Pushed another iteration. I updated the description with the current state. 
@s1monw can you take a look when you have time ? 
</comment><comment author="jpountz" created="2016-07-21T15:25:00Z" id="234288795">This change has a lot to do with settings, which are a bit out of my comfort zone, but the Similarity part looks good to me. I'd suggest to give another day to give Simon a chance to have a last look before merging.
</comment><comment author="jimczi" created="2016-09-06T12:51:33Z" id="244940099">superseded by https://github.com/elastic/elasticsearch/pull/20339 which uses the settings framework and the affix settings to achieve similar behavior.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/indices/settings/UpdateSettingsIT.java</file></files><comments><comment>Validate settings against dynamic updaters on the master (#19088)</comment></comments></commit></commits></item><item><title>Upgrade JNA to 4.2.2 and remove optionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19045</link><project id="" key="" /><description>This commit upgrades JNA from version 4.1.0 to 4.2.2. Additionally, this
dependency is now non-optional as JNA is dual-licensed with Apache
License 2.0 since JNA 4.0.0.

Closes #13245
</description><key id="161919490">19045</key><summary>Upgrade JNA to 4.2.2 and remove optionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha4</label></labels><created>2016-06-23T13:13:06Z</created><updated>2016-06-27T12:16:16Z</updated><resolved>2016-06-23T13:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-23T13:18:42Z" id="228046689">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade JNA to 4.2.2 and remove optionality</comment></comments></commit></commits></item><item><title>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19044</link><project id="" key="" /><description>#18938 has changed the timing in which we send out to nodes to fetch their shard stores. Instead of doing this after the cluster state resulting of the node's join was published, #18938 made it be sent concurrently to the publishing processes. This revealed a couple of points where the shard store fetching is dependent of the current state of affairs of the cluster state, both on the master and the data nodes. The problem discovered were already present without #18938 but required a failure/extreme situations to make them happen.This PR tries to remove as much as possible of these dependencies making shard store fetching simpler and make the way to re-introduce #18938 which was reverted.

These are the notable changes:
1) Allow TransportNodesAction (of which shard store fetching is derived) callers to supply concrete disco nodes, so it won't need the cluster state to resolve them. This was a problem because the cluster state containing the needed nodes was not yet made available through ClusterService. Note that long term we can expect the rest layer to resolve node ids to concrete nodes, making this mode the only one needed.
2) The data node relied on the cluster state to have the relevant index meta data so it can find data when custom paths are used. We now fall back to read the meta data from disk if needed.
3) The data node was relying on it's own IndexService state to indicate whether the data it has corresponds to an existing allocation. This is of course something it can not know until it got (and processed) the new cluster state from the master. This flag in the response is now removed. This is not a problem because we used that flag to protect against double assigning of a  shard to the same node, but we are already protected from it by the allocation deciders.
4) I removed the redundant filterNodeIds method in TransportNodesAction - if people want to filter they can override resolveRequest.
</description><key id="161918163">19044</key><summary>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>resiliency</label><label>v5.0.0-alpha5</label></labels><created>2016-06-23T13:06:34Z</created><updated>2016-06-27T13:05:55Z</updated><resolved>2016-06-27T13:05:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-24T09:48:45Z" id="228304726">Left minor comments. Also there is a word missing in the title ;-)
</comment><comment author="bleskes" created="2016-06-25T16:00:41Z" id="228552328">@ywelsch thx. I pushed another commit
</comment><comment author="ywelsch" created="2016-06-27T09:29:25Z" id="228696512">Left one more comment (to get rid of a line of code). Feel free to push once addressed. LGTM
</comment><comment author="bleskes" created="2016-06-27T13:05:47Z" id="228739667">thanks @ywelsch ! I added the assertion and pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes (#19044)</comment></comments></commit></commits></item><item><title>Fix "key_as_string" for date histogram and epoch_millis/epoch_second format with time zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19043</link><project id="" key="" /><description>When doing a `date_histogram` aggregation with `"format":"epoch_millis"` or `"format" : "epoch_second"` and using a time zone other than UTC, the `key_as_string` ouput in the response does not reflect the UTC timestamp that is used as the key. This happens because when applying the `time_zone` in DocValueFormat.DateTime to an epoch-based formatter, this adds the time zone offset to the value being formated. Instead we should adjust the added display offset to get back the utc instance in EpochTimePrinter.

Closes #19038
</description><key id="161913127">19043</key><summary>Fix "key_as_string" for date histogram and epoch_millis/epoch_second format with time zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>:Dates</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-06-23T12:42:06Z</created><updated>2016-06-29T18:00:39Z</updated><resolved>2016-06-29T17:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-28T10:43:53Z" id="229014734">@jpountz sorry for the confusion about EpochTimePrinter, this is actually part of our codebase and we can correct the time zone adjustment directly there. I'm not completely sure if `epoch_millis` format is supposed to always output the date instance in UTC, maybe @spinscale can conform this since it look like he was the one that introduced the printer class.
</comment><comment author="spinscale" created="2016-06-28T11:08:22Z" id="229019293">yes, epoch time is considered the seconds elapsed since 1970 in UTC timezone.
</comment><comment author="jpountz" created="2016-06-28T14:00:59Z" id="229057824">+1 to always UTC
</comment><comment author="jpountz" created="2016-06-28T14:01:52Z" id="229058104">LGTM
</comment><comment author="cbuescher" created="2016-06-29T18:00:39Z" id="229437630">Merged to 2.4 with 5301ee30
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Increase timeouts for Rest test client ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19042</link><project id="" key="" /><description>... to fix failing tests.
</description><key id="161895181">19042</key><summary>Increase timeouts for Rest test client ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Java REST Client</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-23T10:58:34Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-06-23T12:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/AllocationBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/main/java/org/elasticsearch/test/NamingConventionsCheck.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/DeadHostState.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/test/java/org/elasticsearch/client/CloseableBasicHttpResponse.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/TrackingFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SniffOnFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/Sniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferBuilderTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/MockHostsSniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SnifferBuilderTests.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestCase.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestUtil.java</file><file>core/src/main/java/org/apache/lucene/document/XInetAddressPoint.java</file><file>core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>core/src/main/java/org/apache/lucene/queries/MinDocQuery.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/elasticsearch/Build.java</file><file>core/src/main/java/org/elasticsearch/ResourceNotFoundException.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/CancelTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/TransportCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/ListTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TaskGroup.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TransportListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/Condition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java</file><file>core/src/main/java/org/elasticsearch/action/main/MainResponse.java</file><file>core/src/main/java/org/elasticsearch/action/main/TransportMainAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationTask.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/BaseTasksRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TasksRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/FilterClient.java</file><file>core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/Requests.java</file><file>core/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterName.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/AbstractObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ConstructingObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayModule.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ArabicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/CJKWidthFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/DecimalDigitFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/GermanNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/HindiNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/IndicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MappingCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MultiTermAwareComponent.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PatternReplaceCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PersianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SerbianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SoraniNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/UpperCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/LiveVersionMap.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexGeoPointFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyDateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoPolygonQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxQuery.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/RefreshListeners.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/rest/client/RestTestClient.java</file></files><comments><comment>[TEST] Increase timeouts for Rest test client (#19042)</comment></comments></commit></commits></item><item><title> process creation failed in ES2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19041</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**:1.7

**OS version**: Windows 8

**Description of the problem including expected versus actual behavior**: This was working well in Elasticsearch 2.0.2, I just upgraded to ES 2.3.2, it started throwing the following error : 
I am using nutch internally for crawling file/web, there is a plugin wrapper for it. When we run in Windows, hadoop(internally used by nutch) uses cygwin for running UNIX command "chmod", it used to work fine while the same thing was used in ES2.0, but now it throws the error :

Cannot run program "chmod": CreateProcess error=1816,
Not enough quota is available to process this command

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

java.io.IOException: Cannot run program "chmod": CreateProcess error=1816, Not enough quota is available to process this command
    at java.lang.ProcessBuilder.start(Unknown Source)
    at org.apache.hadoop.util.Shell.runCommand(Shell.java:149)
    at org.apache.hadoop.util.Shell.run(Shell.java:134)
    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)
    at org.apache.hadoop.util.Shell.execCommand(Shell.java:354)
    at org.apache.hadoop.util.Shell.execCommand(Shell.java:337)
    at org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:481)
    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:473)
    at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:280)
    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:266)
    at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:573)
    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:761)
    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)
    at org.apache.nutch.crawl.Injector.inject(Injector.java:323)

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="161893400">19041</key><summary> process creation failed in ES2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chindhuhari</reporter><labels /><created>2016-06-23T10:48:17Z</created><updated>2016-06-24T04:42:43Z</updated><resolved>2016-06-23T11:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-23T11:10:45Z" id="228018511">This is not an Elasticsearch issue (it's not even in your stack trace) but rather your system is low on resources. You need to investigate resource limitations on your machine such as virtual memory, handles, etc. 
</comment><comment author="rmuir" created="2016-06-23T11:16:03Z" id="228019505">No, it is us. But execution is not supported :)
</comment><comment author="jasontedor" created="2016-06-23T11:22:02Z" id="228020585">&gt; No, it is us. But execution is not supported :)

Okay. Thanks for the correction, the error message (not in your control) also appears when a Windows system is running low on resources. 
</comment><comment author="rmuir" created="2016-06-23T11:26:42Z" id="228021485">Good. I am happy that it is confusing.

Users get a completely unambiguous error message from security manager (execution denied) 100% of the time, telling them that execution is not allowed. So it is no real issue.
</comment><comment author="chindhuhari" created="2016-06-24T04:42:43Z" id="228256867">Thank you all for the response.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make bucketsPath syntax for lower and upper standard deviation bounds in the extended_stats aggregation more obvious</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19040</link><project id="" key="" /><description>This issue was raised on the forum here: https://discuss.elastic.co/t/accessing-lower-bound-of-extended-stats-bucket/53763

The output of the extended_stats aggregation looks like this:

```
    "my_stats": {
      "count": 20523877,
      "min": 1,
      "max": 98250000,
      "avg": 165931.3618741722,
      "sum": 3405554861548,
      "sum_of_squares": 2540304986111513000,
      "variance": 96239937025.42874,
      "std_deviation": 310225.6227738591,
      "std_deviation_bounds": {
        "upper": 786382.6074218905,
        "lower": -454519.883673546
      }
    }
```

but the buckets paths to get the upper and lower standard deviation bounds are `my_stats.std_lower` and `my_stats.std_upper` which is not very intuitive given the output above. I think we should change it so the buckets_path (and terms sort path) is `my_stats.std_deviation_bounds.lower` and `my_stats.std_deviation_bounds.upper`
</description><key id="161888990">19040</key><summary>Make bucketsPath syntax for lower and upper standard deviation bounds in the extended_stats aggregation more obvious</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label></labels><created>2016-06-23T10:23:19Z</created><updated>2016-07-01T09:29:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tarunsapra" created="2016-06-23T11:50:07Z" id="228026714">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/nodes api does not return anything for certain nodes </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19039</link><project id="" key="" /><description>_cat/nodes?v&amp;h=n,l,rc,m
n                        l     rc m 
es-data-13-internal   0.26   31gb - 
es-master-02-internal 0.07 10.8gb m 
es-data-05-internal   0.15 31.1gb - 
es-client-02-internal 0.00 17.6gb - 
es-data-11-internal   0.63 30.2gb - 
es-client-01-internal             - 
es-master-01-internal 0.00 10.8gb \* 
es-data-02-internal   0.44 29.8gb - 
es-data-06-internal   0.63 30.8gb -
**Elasticsearch version**:
1.7.5
**JVM version**:
1.8.0._51
**OS version**:
centos 6.6

This is happening for any random node in the cluster.
</description><key id="161879090">19039</key><summary>_cat/nodes api does not return anything for certain nodes </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skshandilya</reporter><labels><label>feedback_needed</label></labels><created>2016-06-23T09:33:33Z</created><updated>2017-03-21T15:01:28Z</updated><resolved>2017-03-21T15:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-06-23T09:46:01Z" id="228001465">How many client nodes do you have in your cluster? Which node do you hit with your REST request?
</comment><comment author="skshandilya" created="2016-06-23T09:53:52Z" id="228003358">we have 2 client nodes which are frontended by a haproxy, so no idea which one it hit
I did the same query on client 1 but then client 2 show no stats
the reverse happened when I ran the query on client 2
</comment><comment author="skshandilya" created="2016-06-23T09:59:02Z" id="228004513">We have also seen the same behavior on some data nodes too and at the same time we have seen some exceptions in log messages we have the marvel plugin running in the cluster
3 masters, 2 clients and 14 data nodes
[2016-06-13 10:37:53,116][ERROR][marvel.agent ] [es-data-07-internal] Background thread had an uncaught exception:
org.elasticsearch.ElasticsearchException: failed to refresh store stats
at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1573)
at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1558)
at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
at org.elasticsearch.index.store.Store.stats(Store.java:290)
at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:638)
at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:139)
at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:55)
at org.elasticsearch.indices.IndicesService.stats(IndicesService.java:231)
at org.elasticsearch.indices.IndicesService.stats(IndicesService.java:188)
at org.elasticsearch.node.service.NodeService.stats(NodeService.java:138)
at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:342)
at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:254)
at java.lang.Thread.run(Unknown Source)
</comment><comment author="javanna" created="2016-06-23T10:03:00Z" id="228005404">that is what I thought. whenever you hit one of the client nodes, you cannot get data from the other client node in the cluster. This is the same problem as #16815, fixed in elasticsearch 5.0.0-alpha1 .
</comment><comment author="javanna" created="2016-06-23T10:09:07Z" id="228006729">@skshandilya I am not sure the exception you find in the logs is related to calling `_cat/nodes`, that smells like another problem to me. Can you post a full recreation of that problem with data nodes?
</comment><comment author="skshandilya" created="2016-06-23T12:03:38Z" id="228029581">Unfortunately that would be difficult we have seen this on a cluster that is processing data, I will update this issue when I see it again.
When this happens the cluster is green and we see no other issues except that our cron job which monitors es using _cat/nodes sees this and alerts. The only way this got fixed is by rebooting the node.
What about the client info not showing up?
</comment><comment author="javanna" created="2016-06-23T12:14:28Z" id="228031672">&gt; What about the client info not showing up?

As I said before, this is the same problem as #16815, fixed in elasticsearch 5.0.0-alpha1 .
</comment><comment author="colings86" created="2017-03-21T15:01:27Z" id="288106424">No further feedback so closing for now</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect 'key_as_string' response in date_histogram when using 'epoch_millis'  and 'time_zone'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19038</link><project id="" key="" /><description>When doing a `date_histogram` aggregation with `"format":"epoch_millis"`, the aggregation response key shows the correct value as UTC time stamp, but the `key_as_string` does not reflect that key if a `time_zone` other than UTC is used. Reproduced on 2.3 and master:

```
PUT /foo

PUT /foo/_mapping/bar
{
  "properties":{
    "timestamp":{
      "type":"date",
      "format":"epoch_millis"
    }
  }
}

POST /foo/bar/1
{
  "timestamp": 1463875200000
}

POST /foo/bar/_search
{
  "aggs": {
    "ts": {
      "date_histogram": {
        "field": "timestamp",
        "interval": "month",
        "time_zone": "+01:00"
      }
    }
  }
}
```

The aggregation in the response:

```
"aggregations": {
    "ts": {
      "buckets": [
        {
          "key_as_string": "1462060800000",
          "key": 1462057200000,
          "doc_count": 1
        }
      ]
    }
  }
```

The key correctly represents the date `2016-05-01T00:00:00.000+01:00`, the expectation would for `epoch_millis` format that the `key_as_string` is the same value.
Same problem happens for `epoch_second` format. With other formats like `date_optional_time` or custom formats `key_as_string` changes correctly according to the keys value.
</description><key id="161876531">19038</key><summary>Incorrect 'key_as_string' response in date_histogram when using 'epoch_millis'  and 'time_zone'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-06-23T09:20:17Z</created><updated>2016-06-30T11:27:35Z</updated><resolved>2016-06-29T17:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>core/src/main/java/org/elasticsearch/search/DocValueFormat.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file></files><comments><comment>Fix key_as_string for date histogram and epoch_millis/epoch_second format</comment></comments></commit></commits></item><item><title>Partial draft for Java Update-by-Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19037</link><project id="" key="" /><description>Slightly cleaned up text from existing Update by Query (REST) doc, needs suitable Java sample code inserted and text cleaned up further after that.
</description><key id="161823599">19037</key><summary>Partial draft for Java Update-by-Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">palecur</reporter><labels><label>docs</label></labels><created>2016-06-23T01:50:33Z</created><updated>2016-06-24T15:14:16Z</updated><resolved>2016-06-24T15:14:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-24T15:14:14Z" id="228373316">Replaced by #19063.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>__es__.tmp left behind from atomic move check prevents node startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19036</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.0

Have an environment where **es**.tmp is left behind from our atomic_move check on the file system.  The directory only has the **es**.tmp file (i.e. no **es**.final).  

total 4
-rw-r--r--. 1 elasticsearch elasticsearch 0 Jun 21 19:36 **es**.tmp
drwxr-xr-x. 39 elasticsearch elasticsearch 4096 Jun 21 00:02 indices
-rw-r--r--. 1 elasticsearch elasticsearch 0 Mar 31 15:12 node.lock
drwxr-xr-x. 2 elasticsearch elasticsearch 33 Jun 6 18:17 _state

```
[2016-06-22 00:00:01,812][INFO ][node                     ] [node_name] version[2.3.0], pid[7400], build[8371be8/2016-03-29T07:54:48Z]
[2016-06-22 00:00:01,812][INFO ][node                     ] [node_name] initializing ...
[2016-06-22 00:00:02,204][INFO ][plugins                  ] [node_name] modules [reindex, lang-expression, lang-groovy], plugins [license, shield, marvel-agent], sites []
[2016-06-22 00:00:02,223][INFO ][env                      ] [node_name] using [1] data paths, mounts [[/var (/dev/mapper/vg00-lv--var)]], net usable_space [580.9gb], net total_space [1tb], spins? [possibly], types [xfs]
[2016-06-22 00:00:02,223][INFO ][env                      ] [node_name] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-06-22 00:00:02,223][WARN ][env                      ] [node_name] max file descriptors [4096] for elasticsearch process likely too low, consider increasing to at least [65536]
[2016-06-22 00:00:02,542][INFO ][http                     ] [node_name] Using [org.elasticsearch.http.netty.NettyHttpServerTransport] as http transport, overridden by [shield]
[2016-06-22 00:00:02,654][INFO ][transport                ] [node_name] Using [org.elasticsearch.shield.transport.ShieldServerTransportService] as transport service, overridden by [shield]
[2016-06-22 00:00:02,654][INFO ][transport                ] [node_name] Using [org.elasticsearch.shield.transport.netty.ShieldNettyTransport] as transport, overridden by [shield]
[2016-06-22 00:00:03,535][ERROR][bootstrap                ] Guice Exception: java.nio.file.FileAlreadyExistsException: /var/lib/elasticsearch-20/cluster_name/nodes/0/__es__.tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.createFile(Files.java:632)
    at org.elasticsearch.env.NodeEnvironment.ensureAtomicMoveSupported(NodeEnvironment.java:794)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:82)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:213)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

It didn't throw an AtomicMoveNotSupportedException so it appears that the check passed, but somehow it has trouble cleaning up the tmp file, so that on the next run, it bombs out when it tries to do a file create  when the tmp file already exists.  Should we handle this more gracefully and still allow the node to start up?  If not, a more intuitive error handling message will be helpful.
</description><key id="161813940">19036</key><summary>__es__.tmp left behind from atomic move check prevents node startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2016-06-23T00:08:13Z</created><updated>2017-01-31T17:45:51Z</updated><resolved>2016-07-07T18:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-23T02:49:20Z" id="227935917">That's odd; we `try/finally` clean up those files. The only way those files are not cleaned up is if the `finally` block did not complete (e.g., someone pulled the plug on the machine after the `__es__.tmp` file was created and we attempted to move it but before the `finally` block executed) or the delete operation failed.

The other interesting item here is how the `node.lock` is really old relative to the `__es__.tmp` file. Is there something else going on here?
</comment><comment author="maxcom" created="2016-09-12T11:14:57Z" id="246317593">This happened after OOM killer killed Elasticsearch. Maybe ES can automaticly remove this lost temp file on startup?
</comment><comment author="s1monw" created="2016-09-12T13:02:25Z" id="246339981">&gt; This happened after OOM killer killed Elasticsearch. Maybe ES can automaticly remove this lost temp file on startup?

I am not sure, if your OOM Killer kicks in at that early stage something is really fishy. It's such an exceptional situation that we should require human interaction? I mean we can maybe throw a better exception but messing with it seems dangerous?
</comment><comment author="benavid" created="2017-01-31T17:45:51Z" id="276437076">Do you know if this issue continue happens with elasticseach 2.4.1?, when I try to start elasticsearch I receive this error: Starting elasticsearch: Exception in thread "main" java.nio.file.FileAlreadyExistsException: /data/elasticsearch/marta/nodes/0/__es__.tmp

</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Translog can delete valid .ckp file when views are closed after the translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19035</link><project id="" key="" /><description>There is simply a coding bug that only happens if translog views are closed
after the translog itself is closed. this can happen for instance if we hit
a disk full exception and try to repeatedly recover the translog. This will
cause a translog-N.ckp file to be deleted since the wrong generation is used
to generate the path to delete. This seems like a copy/past problem.

This bug doesn't affect 5.0

Relates to #16495
</description><key id="161775825">19035</key><summary>Translog can delete valid .ckp file when views are closed after the translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label></labels><created>2016-06-22T20:19:00Z</created><updated>2016-06-23T20:47:32Z</updated><resolved>2016-06-23T20:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-23T12:50:23Z" id="228039760">LGTM. good catch
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file></files><comments><comment>[TEST] Port testcase from #19035 to master</comment></comments></commit></commits></item><item><title>Specific JSON key ordering causes search query failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19034</link><project id="" key="" /><description>**Elasticsearch version**:

5.0.0~alpha3

**JVM version**:

java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:

Linux ip-10-10-155-146 3.13.0-74-generic #118-Ubuntu SMP Thu Dec 17 22:52:10 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
Create an index with two mappings in a parent/child relationship. e.g.

``` bash
$ curl -XPUT 'http://localhost:9200/foo/' -d '{"settings":{"number_of_shards":1},"mappings":{"blog":{"properties":{"name":{"type":"keyword"}}},"tag":{"_parent":{"type":"blog"},"properties":{"name":{"type":"keyword"}}}}}'
{"acknowledged":true}
```

Make this search query, with the `query` key preceding the `has_child` key in the `should` clause.

``` bash
$ curl 'http://localhost:9200/foo/_search' -d '{"query":{"bool":{"should":{"query":{"query_string":{"query":"go"}},"has_child":{"query":{"query_string":{"query":"go"}},"child_type":"tag"}}}}}'
{"error":{"root_cause":[{"type":"parsing_exception","reason":"no [query] registered for [query]","line":1,"col":37}],"type":"parsing_exception","reason":"no [query] registered for [query]","line":1,"col":37},"status":400}
```

But switching the key order makes it work. According to http://json.org/ this serialization represents the exact same object as before.

``` bash
$ curl 'http://localhost:9200/foo/_search' -d '{"query":{"bool":{"should":{"has_child":{"query":{"query_string":{"query":"go"}},"child_type":"tag"},"query":{"query_string":{"query":"go"}}}}}}'
{"took":102,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

**Provide logs (if relevant)**:
No logs appear to be generated for the error.
</description><key id="161774424">19034</key><summary>Specific JSON key ordering causes search query failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nanotone</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-06-22T20:11:42Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-06-24T09:50:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-23T11:49:10Z" id="228026486">Hi @nanotone 

Thanks for trying out the alpha and for reporting this bug. The bug isn't what you think: the `query` query no longer exists, so it is actually the SECOND query which isn't failing correctly.
</comment><comment author="cbuescher" created="2016-06-23T13:02:03Z" id="228042613">To reformulate the problem slightly, when querying something like:

```
GET /foo/_search
{
  "query": {
    "bool": {
      "must": {       
        "unknown_query" : {
          "match": {
            "fuz": "buzz"
          }
        }
      }
    }
  }
}
```

we correctly get the error stated above: `"type": "parsing_exception",  "reason": "no [query] registered for [unknown_query]"`, but if we add a valid query before the invalid one, like the following, we don't get an error:

```
GET /foo/_search
{
  "query": {
    "bool": {
      "must": {
        "match": {
            "fizz": "bizz"
        },
        "unknown_query" : 
        {
          "match": {
            "fuz": "buzz"
          }
        }
      }
    }
  }
}
```
</comment><comment author="cbuescher" created="2016-06-23T14:11:29Z" id="228062317">@clintongormley shouldn't more than one query clauses be specified as an array, or is `"must" : { "query1" : { ... }, "query2" : { ... } }` supported? Because if it is, I think that's whats not working.
</comment><comment author="clintongormley" created="2016-06-23T14:13:25Z" id="228062894">@cbuescher yes absolutely right, so it's just ignoring whatever is left in that object
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Randomize packaging upgrade test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19033</link><project id="" key="" /><description>This commit adds randomization for the packaging upgrade test. In
particular, we extract a list of the released version of Elasticsearch
from Maven Central and randomize the selection of the version to upgrade
from. The randomization is repeatable, and supports the tests.seed
property. Specific versions can be tested by setting the property
tests.packaging.upgrade.from.versions.
</description><key id="161770238">19033</key><summary>Randomize packaging upgrade test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v5.0.0-alpha5</label></labels><created>2016-06-22T19:51:04Z</created><updated>2016-06-29T09:57:59Z</updated><resolved>2016-06-27T16:13:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-23T02:18:11Z" id="227932079">Thanks for the reviews @nik9000 and @rjernst. I added a static resource containing a list of versions to randomly upgrade from. This list of versions will only be validated when an actual packaging test task is executed. I also provided a task to automatically update this list of versions should it go out of date (similar to `gradle updateSHAs`). Let me know what you think?
</comment><comment author="nik9000" created="2016-06-23T20:17:11Z" id="228170573">OK! I had a look. I resolved a few issues with the gradle but wasn't able to fix the IO-at-init-time issue. It turns out that gradle doesn't allow dependencies to be resolved asynchronously. So we'd have to download the dependency from maven central and handle the caching ourselves. Which is icky!

This is where I got: https://github.com/jasontedor/elasticsearch/compare/randomize-upgrade-test...nik9000:pr/19033?expand=1
</comment><comment author="jasontedor" created="2016-06-27T16:10:10Z" id="228793440">@nik9000 I've responded to your feedback.
</comment><comment author="nik9000" created="2016-06-27T16:11:19Z" id="228793772">LGTM. I'm sad to see us having to load the file on script initialization but I don't see a way around it and I want this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Randomize packaging upgrade test</comment></comments></commit></commits></item><item><title>Group client projects under :client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19032</link><project id="" key="" /><description>```
:client ---------&gt; :client:low-level
:client-sniffer -&gt; :client:sniffer
:client-test ----&gt; :client:test
```

This lines the client up with how we do things like modules and plugins.
</description><key id="161750621">19032</key><summary>Group client projects under :client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Java REST Client</label><label>build</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T18:16:03Z</created><updated>2016-07-13T12:24:25Z</updated><resolved>2016-06-22T18:40:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T18:16:36Z" id="227831475">I think it might also be nice to move the package the files are in to line up with their projects but I'd prefer to wait on that because it'll create a noisy diff.
</comment><comment author="nik9000" created="2016-06-22T18:16:50Z" id="227831535">@javanna and @rjernst what do you think?
</comment><comment author="rjernst" created="2016-06-22T18:20:11Z" id="227832488">I think I would call `low-level` instead `rest`? I believe @s1monw has plans to move the transport client into `:client:transport`
</comment><comment author="rjernst" created="2016-06-22T18:22:42Z" id="227833253">LGTM, a couple small suggestions.
</comment><comment author="nik9000" created="2016-06-22T18:25:06Z" id="227833946">&gt; I think I would call low-level instead rest

I'm fine with that. It is the "low-level" REST client but we can rename it later if we want.
</comment><comment author="nik9000" created="2016-06-22T18:40:35Z" id="227838464">Thanks for reviewing @rjernst !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add deprecation logging for `_timestamp` and `_ttl`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19031</link><project id="" key="" /><description>This is a follow-up to #18980.
</description><key id="161744786">19031</key><summary>Add deprecation logging for `_timestamp` and `_ttl`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>deprecation</label><label>v2.4.0</label></labels><created>2016-06-22T17:48:35Z</created><updated>2016-06-23T06:03:39Z</updated><resolved>2016-06-23T06:03:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T17:54:10Z" id="227824652">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Add client-test module and make client tests use randomized runner directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19030</link><project id="" key="" /><description>The lucene-test dependency caused issues with IDEs as they would always load the lucene 5 jar although they shouldn't have, which caused jarhell in es core tests.

If we depend directly on randomized runner we don't have this problem. It is luckily still compatible with java 1.7. This requires though adding a thin module that includes the base test class which can be shared between client and client-sniffer.
</description><key id="161740089">19030</key><summary>[TEST] Add client-test module and make client tests use randomized runner directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T17:25:04Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T17:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-06-22T17:25:23Z" id="227816482">@nik9000 @rjernst can either of you have a look please?
</comment><comment author="nik9000" created="2016-06-22T17:30:02Z" id="227817795">Fine with me.
</comment><comment author="javanna" created="2016-06-22T17:34:02Z" id="227818905">&gt; Fine with me.

is that a LGTM? :)
</comment><comment author="nik9000" created="2016-06-22T17:36:25Z" id="227819605">&gt; is that a LGTM? :)

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client-sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferBuilderTests.java</file><file>client-sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client-sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>client-sniffer/src/test/java/org/elasticsearch/client/sniff/SnifferBuilderTests.java</file><file>client-test/src/main/java/org/elasticsearch/client/RestClientTestCase.java</file><file>client-test/src/main/java/org/elasticsearch/client/RestClientTestUtil.java</file><file>client/src/test/java/org/elasticsearch/client/CloseableBasicHttpResponse.java</file><file>client/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file></files><comments><comment>Merge pull request #19030 from javanna/test/client-test</comment></comments></commit></commits></item><item><title>Move upgrade test to upgrade from version 2.3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19029</link><project id="" key="" /><description>This commit moves the upgrade test to test upgrading from version 2.3.3
instead of from version 2.0.0.

Closes #19026
</description><key id="161738706">19029</key><summary>Move upgrade test to upgrade from version 2.3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T17:18:26Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T17:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-22T17:19:00Z" id="227814676">I tested this locally and this test is still passing.
</comment><comment author="nik9000" created="2016-06-22T17:24:51Z" id="227816321">LGTM. I wonder if we should have multiple upgrade cycles eventually?
</comment><comment author="jasontedor" created="2016-06-22T17:27:03Z" id="227816918">&gt; I wonder if we should have multiple upgrade cycles eventually?

_Eventually_. :smile:
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Move upgrade test to upgrade from version 2.3.3</comment></comments></commit></commits></item><item><title>Add a MultiTermAwareComponent marker interface to analysis factories.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19028</link><project id="" key="" /><description>This is the same as what Lucene does for its analysis factories, and we hawe
tests that make sure that the elasticsearch factories are in sync with
Lucene's. This is a first step to move forward on #9978 and #18064.
</description><key id="161737733">19028</key><summary>Add a MultiTermAwareComponent marker interface to analysis factories.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T17:13:39Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-23T08:19:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T17:23:52Z" id="227816028">Makes sense to me. I left some minor feedback but nothing blocking.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/AllocationBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/main/java/org/elasticsearch/test/NamingConventionsCheck.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/DeadHostState.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/test/java/org/elasticsearch/client/CloseableBasicHttpResponse.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/TrackingFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SniffOnFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/Sniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferBuilderTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/MockHostsSniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SnifferBuilderTests.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestCase.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestUtil.java</file><file>core/src/main/java/org/apache/lucene/document/XInetAddressPoint.java</file><file>core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>core/src/main/java/org/apache/lucene/queries/MinDocQuery.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/elasticsearch/Build.java</file><file>core/src/main/java/org/elasticsearch/ResourceNotFoundException.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/CancelTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/TransportCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/ListTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TaskGroup.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TransportListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/Condition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java</file><file>core/src/main/java/org/elasticsearch/action/main/MainResponse.java</file><file>core/src/main/java/org/elasticsearch/action/main/TransportMainAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationTask.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/BaseTasksRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TasksRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/FilterClient.java</file><file>core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/Requests.java</file><file>core/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterName.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/AbstractObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ConstructingObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayModule.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ArabicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/CJKWidthFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/DecimalDigitFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/GermanNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/HindiNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/IndicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MappingCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MultiTermAwareComponent.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PatternReplaceCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PersianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SerbianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SoraniNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/UpperCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/LiveVersionMap.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexGeoPointFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyDateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoPolygonQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxQuery.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/RefreshListeners.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ArabicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/CJKWidthFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/DecimalDigitFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/GermanNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/HindiNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/IndicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MappingCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MultiTermAwareComponent.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PersianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SerbianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SoraniNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/UpperCaseTokenFilterFactory.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/AnalysisFactoryTests.java</file><file>plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuFoldingTokenFilterFactory.java</file><file>plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerCharFilterFactory.java</file><file>plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java</file><file>plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuTransformTokenFilterFactory.java</file><file>plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/AnalysisICUFactoryTests.java</file><file>plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/KuromojiIterationMarkCharFilterFactory.java</file><file>plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/AnalysisKuromojiFactoryTests.java</file><file>plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/AnalysisPhoneticFactoryTests.java</file><file>plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/AnalysisSmartChineseFactoryTests.java</file><file>plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/AnalysisPolishFactoryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/AnalysisFactoryTestCase.java</file></files><comments><comment>Add a MultiTermAwareComponent marker interface to analysis factories. #19028</comment></comments></commit></commits></item><item><title>Remove the support for `fielddata_fields`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19027</link><project id="" key="" /><description>The parameter `fielddata_fields` is deprecated (https://github.com/elastic/elasticsearch/issues/18943) but we continue to fallback to the fielddata cache when the docvalues are not enabled on the field. 
We should remove the ability to retrieve the fields through the fielddata cache because the output is not equivalent to the docvalue and may greatly differ from the stored field.   
</description><key id="161735905">19027</key><summary>Remove the support for `fielddata_fields`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>adoptme</label><label>breaking</label></labels><created>2016-06-22T17:05:10Z</created><updated>2017-06-28T10:23:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2016-06-30T23:35:10Z" id="229817806">Note that Kibana makes use of fielddata_fields for retrieving the millisecond value of date fields in documents. Is there an alternative for that purpose?
</comment><comment author="rashidkpc" created="2016-06-30T23:36:52Z" id="229818051">I guess `docvalue_fields`? Is that going to break on old indices that don't have docvalues? Is there a way to have a transparent fallback here?
</comment><comment author="clintongormley" created="2016-07-01T11:50:36Z" id="229927522">&gt; I guess docvalue_fields? Is that going to break on old indices that don't have docvalues? Is there a way to have a transparent fallback here?

We need to fallback to fielddata for 5.x (at least for old indices), can be removed in 6.0
</comment><comment author="jimczi" created="2016-07-01T12:17:36Z" id="229932283">discussed in FixItFriday, the fallback to fielddata for 5.x is only for keyword/text fields. It won't work if the field is numeric since the fielddata for numerics has been removed entirely. 

&gt; Note that Kibana makes use of fielddata_fields for retrieving the millisecond value of date fields in documents. Is there an alternative for that purpose?

You'll have to ensure that the doc_values are activated on the field otherwise it won't work. This is a side effect of the fielddata removal for numeric fields. Note that this has nothing to do with this issue, this is how the 5.x works and this is not going to change.
</comment><comment author="clintongormley" created="2016-07-08T09:43:07Z" id="231319008">Fielddata for numeric fields were deprecated in 2.0 and removed in 5.0, so doc values would be enabled for any numeric field in an index created in 2.x, unless the user changed the mappings.

So we're good to remove fielddata_fields, and kibana will need to change their use of fielddata_fields to `docvalue_fields`
</comment><comment author="colings86" created="2017-06-28T10:23:12Z" id="311619819">`fielddata_fields` were [deprecated in 5.0](https://www.elastic.co/guide/en/elasticsearch/reference/5.4/breaking_50_search_changes.html) so can be removed in 6.0</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Upgrading from alpha3 using RPM does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19026</link><project id="" key="" /><description>When trying to upgrade from alpha3 to a more recent (snapshot) version

**Elasticsearch version**: 5.0.0-alpha3

**OS version**: Centos 7

**Steps to reproduce**:

```
rpm -i elasticsearch-5.0.0-alpha3.rpm
systemctl daemon-reload
systemctl start elasticsearch
# use journalctl -f to check that alpha3 has started, msut be aborted
 journalctl -f -n 40

rpm -Uvh elasticsearch-5.0.0-alpha4.rpm
systemctl daemon-reload
systemctl restart elasticsearch
```

Output of journalctl

```
Jun 22 12:44:56 localhost.localdomain elasticsearch[31947]: Exception in thread "main" java.lang.IllegalStateException: Unable to access 'path.scripts' (/etc/elasticsearch/scripts)
```

When installing alpha3 first, then `/etc/elasticsearch/scripts` is not created, neither it is on upgrade to alpha4. However, there are additional problems when creating that directory

```
mkdir /etc/elasticsearch/scripts
systemctl start elasticsearch
journalctl -f -n 40
```

output is

```
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: [2016-06-22 12:49:01,953][INFO ][node                     ] [Hank McCoy] version[5.0.0-alpha4], pid[31996], build[b0da471/2016-06-22T12:33:48.164Z], OS[Linux/3.10.0-327.18.2.el7.x86_64/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_91/25.91-b14]
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: [2016-06-22 12:49:01,954][INFO ][node                     ] [Hank McCoy] initializing ...
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Exception in thread "main" java.lang.IllegalStateException: Unable to initialize modules
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Likely root cause: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/modules/ingest-grok/plugin-descriptor.properties
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newByteChannel(Files.java:361)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newByteChannel(Files.java:407)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newInputStream(Files.java:152)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:74)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginsService.getModuleBundles(PluginsService.java:327)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:131)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:211)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:172)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:175)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:175)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:250)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.Command.main(Command.java:53)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)
Jun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Refer to the log for complete error details.
```

The issue here seems to be that the `ingest-grok` module does not exist after alpha3 anymore (has been moved into `ingest-common`, but it was not cleaned up properly on upgrade.

If we decide that upgrading from alpha3 to newer alpha version does not need to work, than the last part of this can potentially be ignored, however we still need to ensure that upgrading from any older (non alpha) elasticsearch version to a newer one works.
</description><key id="161732912">19026</key><summary>Packaging: Upgrading from alpha3 using RPM does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>bug</label><label>v5.4.4</label></labels><created>2016-06-22T16:51:10Z</created><updated>2017-06-27T10:28:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-22T17:01:22Z" id="227809473">&gt; If we decide that upgrading from alpha3 to newer alpha version does not need to work, than the last part of this can potentially be ignored, however we still need to ensure that upgrading from any older (non alpha) elasticsearch version to a newer one works.

I don't think we need to support upgrading between pre-releses. The packaging tests [do test upgrading from the previous major version](https://github.com/elastic/elasticsearch/blob/6671c0cf09ffe9dbc31936b25524df88d29612ed/qa/vagrant/src/test/resources/packaging/scripts/80_upgrade.bats). Right now they test upgrading from 2.0.0; I will open a PR to move this to 2.3.3.
</comment><comment author="jasontedor" created="2016-06-22T17:19:26Z" id="227814784">I opened #19029.
</comment><comment author="spinscale" created="2016-06-23T06:59:00Z" id="227966158">IMO we should still ensure that uninstalling an RPM cleans up `/usr/share/elasticsearch/modules`

Otherwise this just postpones the problem until an official release uses different modules.
</comment><comment author="clintongormley" created="2016-06-23T11:56:10Z" id="228028049">&gt; IMO we should still ensure that uninstalling an RPM cleans up /usr/share/elasticsearch/modules
&gt; Otherwise this just postpones the problem until an official release uses different modules.

Agreed!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail to start if plugin tries broken onModule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19025</link><project id="" key="" /><description>If a plugin declares `onModule(SomethingThatIsntAModule)` then refuse
to start. Before this commit we just logged a warning that flies by in
the console and is easy to miss. You can't miss refusing to start!
</description><key id="161723179">19025</key><summary>Fail to start if plugin tries broken onModule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>blocker</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T16:06:21Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-22T16:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T16:09:40Z" id="227794197">I marked this as breaking because it'll break some plugins. They were technically already broken by the switch `ScriptPlugin` but instead of just not working this will cause Elasticsearch to fail to start if they are installed. In other words - you'll know that the plugin is broken rather than it being just a WARNING level log on startup.
</comment><comment author="jasontedor" created="2016-06-22T16:13:53Z" id="227795522">I'm fine with the change, I left a question about the additional `ScriptPlugin` message. I think that should just be in the breaking changes doc (I didn't see a note there already).
</comment><comment author="nik9000" created="2016-06-22T16:26:45Z" id="227799212">Thanks for reviewing @jasontedor ! I'll add a breaking changes note and rework the warning a bit and push. I do want to keep the warning because I think that script plugins are fairly common plugins and I figure it can save people who upgrade some time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix JarHell errors when running tests in Eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19024</link><project id="" key="" /><description>Eclipse merges the test and main dependencies of all project because it
is silly. This isn't a problem most of the time but it became a problem
when core's tests started to depend on the client. That caused Eclipse
to add the client's test dependencies to core's test's classpath. That
is a problem because the client's test dependencies use an older version
of Lucene so they can be Java 1.7 compatible. So we end up with two
copies of LuceneTestCase on the classpath when running core's tests.
Which causes JarHell.

This fixes the problem by breaking the client into two projects when
importing it into Eclipse - one for main code and one for tests. Now
client's test dependencies no longer blead into core in Eclipse.
</description><key id="161718128">19024</key><summary>Fix JarHell errors when running tests in Eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label></labels><created>2016-06-22T15:46:33Z</created><updated>2016-06-23T11:44:27Z</updated><resolved>2016-06-22T18:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T15:47:28Z" id="227787092">@rjernst, can you have a look at this? I've just hacked it together to fix the pain of not being able to run the tests properly any more. I expect there is a better way with less code duplication.
</comment><comment author="rjernst" created="2016-06-22T16:29:26Z" id="227799951">I think @javanna is going to remove the need for this by removing the dep on lucene 5.5 in tests for the client.
</comment><comment author="nik9000" created="2016-06-22T16:33:00Z" id="227801049">&gt; I think @javanna is going to remove the need for this by removing the dep on lucene 5.5 in tests for the client.

I'm happy to wait for that.
</comment><comment author="nik9000" created="2016-06-22T18:42:36Z" id="227839071">No longer needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove duplicated read byte array methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19023</link><project id="" key="" /><description>This commit removes duplicated methods for reading byte arrays in
StreamInput. One method would read a byte array by repeatedly calling
StreamInput#readByte in a loop, and the other would just call
StreamInput#readBytes. In this commit, we remove the former.
</description><key id="161711905">19023</key><summary>Remove duplicated read byte array methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T15:21:39Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T15:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-22T15:30:25Z" id="227781658">LGTM
</comment><comment author="jpountz" created="2016-06-22T15:33:53Z" id="227782784">LGTM, thanks for tackling this!
</comment><comment author="jasontedor" created="2016-06-22T15:56:13Z" id="227789915">Thanks for the reviews @nik9000 and @jpountz.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file></files><comments><comment>Remove duplicated read byte array methods</comment></comments></commit></commits></item><item><title>Fix disabled loop counter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19022</link><project id="" key="" /><description>You can disable this feature with a parameter, but we don't test it. Currently it will always hit NPE.
</description><key id="161671347">19022</key><summary>Fix disabled loop counter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label></labels><created>2016-06-22T12:41:27Z</created><updated>2016-06-23T11:45:06Z</updated><resolved>2016-06-23T11:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-22T17:45:12Z" id="227822019">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDo.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SWhile.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file></files><comments><comment>Merge pull request #19022 from rmuir/loopCounter</comment></comments></commit></commits></item><item><title>Refresh interval -1 still makes docs searchable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19021</link><project id="" key="" /><description>Elastics 2.3.3

`refresh_interval: -1` makes docs searchable after some period (possibly cluster default value)

also `refresh_interval: -1s` not helping

.

docs: https://www.elastic.co/guide/en/elasticsearch/guide/2.x/near-real-time.html#refresh-api and in few other places in docs
</description><key id="161665659">19021</key><summary>Refresh interval -1 still makes docs searchable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcinkubica</reporter><labels /><created>2016-06-22T12:12:46Z</created><updated>2016-06-22T12:28:10Z</updated><resolved>2016-06-22T12:28:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-22T12:28:09Z" id="227727705">@marcinkubica segments will be written when the indexing buffer needs to be cleared.  There is a PR which is being discussed about whether we can change this behaviour.

Closing in favour of https://github.com/elastic/elasticsearch/pull/16028
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make sure TimeIntervalRounding is monotonic for increasing dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19020</link><project id="" key="" /><description>Currently there are cases when using TimeIntervalRounding#round() that if `date1 &lt; date2` we get `round(date2) &lt; round(date1)`. These errors can happen when using a non-fixed time zone and the values to be rounded are slightly after a time zone offset change (e.g. DST transition).

Here is an example for the "CET" time zone with a 45 minute rounding interval. The dates to be rounded are on the left (with utc time stamp), the rounded values on the right. The error case is marked:

| orig. date | rounded |
| --- | --- |
| 2011-10-30T01:40:00.000+02:00 1319931600000 |  2011-10-30T01:30:00.000+02:00 1319931000000 |
| 2011-10-30T02:02:30.000+02:00 1319932950000 |  2011-10-30T01:30:00.000+02:00 1319931000000 |
| 2011-10-30T02:25:00.000+02:00 1319934300000 |  2011-10-30T02:15:00.000+02:00 1319933700000 |
| 2011-10-30T02:47:30.000+02:00 1319935650000 |  2011-10-30T02:15:00.000+02:00 1319933700000 |
| 2011-10-30T02:10:00.000+01:00 1319937000000 |  2011-10-30T01:30:00.000+02:00 1319931000000 &lt; previous rounded value |
| 2011-10-30T02:32:30.000+01:00 1319938350000 |  2011-10-30T02:15:00.000+01:00 1319937300000 |
| 2011-10-30T02:55:00.000+01:00 1319939700000 |  2011-10-30T02:15:00.000+01:00 1319937300000 |
| 2011-10-30T03:17:30.000+01:00 1319941050000 |  2011-10-30T03:00:00.000+01:00 1319940000000 |

We should correct this by detecting that we are crossing a transition when rounding, and in that case pick the largest valid rounded value before the transition.

This change adds this correction logic to the rounding function and adds this invariant to the randomized TimeIntervalRounding tests. Also adding the example test case from above (with corrected behaviour) for illustrative purposes.
</description><key id="161657530">19020</key><summary>Make sure TimeIntervalRounding is monotonic for increasing dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-06-22T11:29:41Z</created><updated>2016-06-30T15:25:16Z</updated><resolved>2016-06-30T15:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-22T11:31:00Z" id="227716739">@jpountz @jimferenczi this is a small spin-off of a larger change I have locally, maybe one of you can take a look.
</comment><comment author="jpountz" created="2016-06-30T14:58:46Z" id="229684817">LGTM
</comment><comment author="cbuescher" created="2016-06-30T15:05:05Z" id="229687032">@jpountz thanks for the review
</comment><comment author="cbuescher" created="2016-06-30T15:25:16Z" id="229694046">Merged to 2.4 with 31ade93.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file></files><comments><comment>Make sure TimeIntervalRounding is monotonic for increasing dates (#19020)</comment></comments></commit></commits></item><item><title>corrected property name for "scripts" folder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19019</link><project id="" key="" /><description>according to https://github.com/elastic/elasticsearch/pull/12668/commits/d7d25fe6b595422c38426ed1b39ab8f93f5b5919
the property name should be path.scripts, note the extra 's'
</description><key id="161629418">19019</key><summary>corrected property name for "scripts" folder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shou1dwe</reporter><labels><label>docs</label></labels><created>2016-06-22T09:06:30Z</created><updated>2016-06-22T10:38:03Z</updated><resolved>2016-06-22T10:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-22T10:37:59Z" id="227706669">thanks @shou1dwe - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove guice from Mapper plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19018</link><project id="" key="" /><description>This changes adds a MapperPlugin interface which allows pull style
retrieval of mappers and metadata mappers added by plugins. For now, I
have kept the MapperRegistry, but this should be removed in the future
as it is just a silly container for 2 maps which could themselves be
passed around.
</description><key id="161600480">19018</key><summary>Remove guice from Mapper plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>v5.0.0-alpha5</label></labels><created>2016-06-22T06:02:14Z</created><updated>2016-07-29T11:43:42Z</updated><resolved>2016-06-27T18:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-22T06:34:31Z" id="227655943">left two comments - LGTM otherwise
</comment><comment author="jpountz" created="2016-06-22T06:39:41Z" id="227656693">LGTM
</comment><comment author="clintongormley" created="2016-06-22T10:31:58Z" id="227705469">@rjernst should this be "enhancement" or "breaking"?
</comment><comment author="rjernst" created="2016-06-27T18:22:49Z" id="228831218">@clintongormley I added the `breaking` label, and added a note to the migration guide.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/MapperPlugin.java</file><file>core/src/test/java/org/elasticsearch/index/IndexModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapperPlugin.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorPlugin.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/MapperAttachmentsPlugin.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java</file><file>plugins/mapper-murmur3/src/main/java/org/elasticsearch/plugin/mapper/MapperMurmur3Plugin.java</file><file>plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizePlugin.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Merge pull request #19018 from rjernst/mapper_plugin_api</comment></comments></commit></commits></item><item><title>need to save same field in different type and field have different data type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19017</link><project id="" key="" /><description>in elasticsearch 1.7.3 have no issue when i created one index have two type and have same filed in in type with different data type 

but this is not work in 2.3.1 it fire number format exception.

like example 

PUT indexname1/type1/1
{
    "id":1
}

then i created second type in same index

PUT indexname1/type2/1
{
    "id":"v1"
}

then it fire error number format but it work 1.7.3 version 
</description><key id="161596789">19017</key><summary>need to save same field in different type and field have different data type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingaj</reporter><labels /><created>2016-06-22T05:26:59Z</created><updated>2016-06-22T09:08:06Z</updated><resolved>2016-06-22T07:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kingaj" created="2016-06-22T05:27:24Z" id="227647106">please help 
</comment><comment author="cbuescher" created="2016-06-22T07:44:48Z" id="227667858">Please read [this article](https://www.elastic.co/blog/great-mapping-refactoring) about the reasons why this behaviour changed. Also, please use the forums at https://discuss.elastic.co/ for questions like this in the future since elasticsearch reserves Github for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Hot methods redux</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19016</link><project id="" key="" /><description>This pull request addresses some additional hot methods that could not
be inlined due to exceeding `MaxFreqInlineSize`. In particular, this
pull request addresses all hot methods in the indexing path of the Rally
default benchmarking run:

```
org.elasticsearch.action.search.AbstractSearchAsyncAction::&lt;init&gt; (355 bytes)   hot method too big
org.elasticsearch.action.search.TransportSearchAction::doExecute (380 bytes)   hot method too big
org.elasticsearch.action.support.replication.ReplicationOperation::execute (353 bytes)   hot method too big
org.elasticsearch.common.io.stream.StreamInput::readGenericValue (497 bytes)   hot method too big
org.elasticsearch.common.io.stream.StreamOutput::writeGenericValue (799 bytes)   hot method too big
org.elasticsearch.index.engine.InternalEngine::innerIndex (465 bytes)   hot method too big
org.elasticsearch.tasks.TaskManager::register (336 bytes)   hot method too big
```

Relates #16725
</description><key id="161572956">19016</key><summary>Hot methods redux</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-22T01:03:58Z</created><updated>2016-06-22T10:41:38Z</updated><resolved>2016-06-22T10:41:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-22T06:47:40Z" id="227657880">Looks fine to me, I think it even makes things slightly more readable.
</comment><comment author="jasontedor" created="2016-06-22T10:40:45Z" id="227707198">&gt; I think it even makes things slightly more readable

Agree! Thanks for reviewing!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/LiveVersionMap.java</file><file>core/src/main/java/org/elasticsearch/tasks/TaskManager.java</file></files><comments><comment>Merge pull request #19016 from jasontedor/hot-methods-redux</comment></comments></commit></commits></item><item><title>X-Pack Alpha 3 - Security Web UI </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19015</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha3

**OS version**: Debian 8.4

**Description of the problem including expected versus actual behavior**:
I submitted a bug earlier today about the Web UI for Role Management and I have since found more bugs with the same menu. When creating new roles if you type something into the query field and then delete it and save the role. The role get created with an empty query field rather than no query field. This can cause problems when the role is assigned. I also noticed the "field" field has similar problems even if you don't select it.
</description><key id="161545600">19015</key><summary>X-Pack Alpha 3 - Security Web UI </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CCoffie</reporter><labels /><created>2016-06-21T21:36:44Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-06-21T22:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukasolson" created="2016-06-21T22:07:36Z" id="227587237">This will be fixed in alpha4. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: migration notes for _timestamp and _ttl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19014</link><project id="" key="" /><description>We aren't able to actually create an index with _timestamp enabled to test the migration, or, at least, we won't be able to after #18980 is re-merged. But the docs are still ok.

Closes #19007
</description><key id="161533851">19014</key><summary>Docs: migration notes for _timestamp and _ttl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T20:38:53Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T18:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-22T12:11:34Z" id="227724340">This looks great!
</comment><comment author="nik9000" created="2016-06-22T14:29:18Z" id="227761985">@clintongormley I pushed an update, could you review it?
</comment><comment author="clintongormley" created="2016-06-22T14:56:27Z" id="227770630">LGTM
</comment><comment author="nik9000" created="2016-06-22T18:51:52Z" id="227841676">Thanks for reviewing @clintongormley and @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Non-blocking primary relocation hand-off</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19013</link><project id="" key="" /><description>Primary relocation and indexing concurrently can currently lead to a deadlock situation as indexing operations are blocked on a (bounded) thread pool during the hand-off phase between old and new primary. This PR replaces blocking of indexing operations by putting operations that cannot be executed during relocation hand-off in a queue to be executed once relocation completes.

Relates to #18553, #15900.
</description><key id="161526991">19013</key><summary>Non-blocking primary relocation hand-off</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-21T20:05:48Z</created><updated>2016-07-02T07:35:55Z</updated><resolved>2016-07-02T07:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-27T15:31:29Z" id="228781807">Thx @ywelsch . I left some comments that I think will simplify things. My main concern here is the extra IndexShardOperationsLock wrapper around `SuspendableRefContainer`. I'm not sure we need an extra abstraction instead of making SuspendableRefContainer implement the API we need (or just rename it). It makes things more complex, for example, IndexShardOperationsLock assumes that the only reason why tryAcquire can fail is that a block operation is going on. This is true now, but only because we use Integer.MAX_VALUE as the total amount of operations. Some one can change that and not realize the implications it has. 
</comment><comment author="ywelsch" created="2016-06-29T06:10:49Z" id="229265525">@bleskes I've updated the PR with the following main changes:
- introduced `AsyncPrimaryAction` and use the original flow of the method to distinguish `isRelocated` on the `PrimaryShardReference`.
- inlined `SuspendableRefContainer` into `IndexShardOperationsLock`.
- simplified lock acquisition retry (eliminating the loop) to remove some of the non-blocking fanciness.
  Please have another look.
</comment><comment author="bleskes" created="2016-06-30T13:22:57Z" id="229656223">@ywelsch I like it! left a bunch of comments.
</comment><comment author="ywelsch" created="2016-07-01T11:13:14Z" id="229921606">@bleskes I've pushed a new set of changes addressing your comments. I've also added a `close` method to IndexShardOperationLock. This is not strictly necessary but makes operations fail faster when the shard is closed.
</comment><comment author="bleskes" created="2016-07-01T16:33:55Z" id="229991503">LGTM. Thx @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>core/src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/SuspendableRefContainer.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationsLock.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/SuspendableRefContainerTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardOperationsLockTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file></files><comments><comment>Make primary relocation handoff non-blocking (#19013)</comment></comments></commit></commits></item><item><title>Painless Initializers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19012</link><project id="" key="" /><description>Adds initializers for arrays, lists, and maps.
</description><key id="161525600">19012</key><summary>Painless Initializers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T19:59:01Z</created><updated>2016-06-22T10:20:40Z</updated><resolved>2016-06-21T20:08:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-21T20:05:48Z" id="227555508">+1, this looks great! this will make things much easier on the user.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EListInit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EMapInit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/package-info.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/InitializerTests.java</file></files><comments><comment>Merge pull request #19012 from jdconrad/init2</comment></comments></commit></commits></item><item><title>Also do not serialize `_index` key in search response for parent/child inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19011</link><project id="" key="" /><description>The `_index` key is always the same as the parent search hit.
</description><key id="161506143">19011</key><summary>Also do not serialize `_index` key in search response for parent/child inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T18:21:52Z</created><updated>2016-06-21T19:24:57Z</updated><resolved>2016-06-21T19:24:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-21T18:27:06Z" id="227528855">LGTM
</comment><comment author="martijnvg" created="2016-06-21T19:24:57Z" id="227544947">thx @abeyad!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java</file></files><comments><comment>Merge pull request #19011 from martijnvg/inner_hits/index_type_id_serialization1</comment></comments></commit></commits></item><item><title>Remove guice from ScriptService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19010</link><project id="" key="" /><description>Makes ScriptModule just a plain class that manages building the ScriptSettings and ScriptService from plugins. When we _need_ to bind ScriptService with guice we bind it in a lambda.
</description><key id="161491634">19010</key><summary>Remove guice from ScriptService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T17:11:56Z</created><updated>2016-06-28T09:51:24Z</updated><resolved>2016-06-21T21:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T17:19:26Z" id="227509519">@s1monw I think this is something you'd appreciate. It isn't perfect but it is another `@Inject` gone. Actually remove Guice entirely from one test.
</comment><comment author="s1monw" created="2016-06-21T19:10:10Z" id="227540986">awesome thanks for working on this! LGTM
</comment><comment author="nik9000" created="2016-06-21T20:48:13Z" id="227567000">Thanks for reviewing @rjernst and @s1monw ! I'll make the changes you recommended and merge.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>date_histogram w/ extended_bounds fails on alias/index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19009</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.7.0_67

**OS version**: OSX 10.11.4

**Description of the problem including expected versus actual behavior**:

Start with two indices and an alias for both, the second with a new field introduced:

```
curl -XPUT 'http://localhost:9200/test_index_1/dates/1?pretty' -d '{"when_received": "2016-04-25T13:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/2?pretty' -d '{"when_received": "2016-05-28T14:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/3?pretty' -d '{"when_received": "2016-06-28T17:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/4?pretty' -d '{"when_received": "2016-06-29T17:21:24.000Z"}'

curl -XPUT 'http://localhost:9200/test_index_2/dates/1?pretty' -d '{"when_recorded": "2016-04-25T13:21:24.000Z", "when_received": "2015-04-25T13:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_2/dates/2?pretty' -d '{"when_recorded": "2016-05-28T14:21:24.000Z", "when_received": "2015-05-28T14:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_2/dates/3?pretty' -d '{"when_recorded": "2016-06-28T17:21:24.000Z", "when_received": "2015-06-28T17:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_2/dates/4?pretty' -d '{"when_recorded": "2016-06-29T17:21:24.000Z", "when_received": "2015-06-29T17:21:24.000Z"}'

curl -XPOST 'http://localhost:9200/test_index_1/_refresh'
curl -XPOST 'http://localhost:9200/test_index_2/_refresh'

curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "index" : "test_index_1", "alias" : "all_indices" } },
        { "add" : { "index" : "test_index_2", "alias" : "all_indices" } }
    ]
}'
```

I want to do a `date_histogram` aggregation over the alias with `extended_bounds`. The results for each index individually are what I would expect: 

```
curl -XGET 'http://localhost:9200/test_index_1/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ ]
    }
  }
}
```

```
curl -XGET 'http://localhost:9200/test_index_2/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ {
        "key_as_string" : "2016-01-01T00:00:00.000Z",
        "key" : 1451606400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-02-01T00:00:00.000Z",
        "key" : 1454284800000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-03-01T00:00:00.000Z",
        "key" : 1456790400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-04-01T00:00:00.000Z",
        "key" : 1459468800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-05-01T00:00:00.000Z",
        "key" : 1462060800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-06-01T00:00:00.000Z",
        "key" : 1464739200000,
        "doc_count" : 2
      } ]
    }
  }
}
```

However, when using the alias, the `extended_bounds` fail: 

```
curl -XGET 'http://localhost:9200/all_indices/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "hits" : {
    "total" : 8,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ {
        "key_as_string" : "2016-04-01T00:00:00.000Z",
        "key" : 1459468800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-05-01T00:00:00.000Z",
        "key" : 1462060800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-06-01T00:00:00.000Z",
        "key" : 1464739200000,
        "doc_count" : 2
      } ]
    }
  }
}
```

Here's the tricky part: this behavior depends on the actual index name. Same steps, but a different name for the second index: 

```
curl -XPUT 'http://localhost:9200/test_index_1/dates/1?pretty' -d '{"when_received": "2016-04-25T13:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/2?pretty' -d '{"when_received": "2016-05-28T14:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/3?pretty' -d '{"when_received": "2016-06-28T17:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/test_index_1/dates/4?pretty' -d '{"when_received": "2016-06-29T17:21:24.000Z"}'

curl -XPUT 'http://localhost:9200/foobar/dates/1?pretty' -d '{"when_recorded": "2016-04-25T13:21:24.000Z", "when_received": "2015-04-25T13:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/foobar/dates/2?pretty' -d '{"when_recorded": "2016-05-28T14:21:24.000Z", "when_received": "2015-05-28T14:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/foobar/dates/3?pretty' -d '{"when_recorded": "2016-06-28T17:21:24.000Z", "when_received": "2015-06-28T17:21:24.000Z"}'
curl -XPUT 'http://localhost:9200/foobar/dates/4?pretty' -d '{"when_recorded": "2016-06-29T17:21:24.000Z", "when_received": "2015-06-29T17:21:24.000Z"}'

curl -XPOST 'http://localhost:9200/test_index_1/_refresh'
curl -XPOST 'http://localhost:9200/foobar/_refresh'

curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "index" : "test_index_1", "alias" : "all_indices" } },
        { "add" : { "index" : "foobar", "alias" : "all_indices" } }
    ]
}'
```

The results for each index: 

```
curl -XGET 'http://localhost:9200/test_index_1/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ ]
    }
  }
}
```

```
curl -XGET 'http://localhost:9200/foobar/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ {
        "key_as_string" : "2016-01-01T00:00:00.000Z",
        "key" : 1451606400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-02-01T00:00:00.000Z",
        "key" : 1454284800000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-03-01T00:00:00.000Z",
        "key" : 1456790400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-04-01T00:00:00.000Z",
        "key" : 1459468800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-05-01T00:00:00.000Z",
        "key" : 1462060800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-06-01T00:00:00.000Z",
        "key" : 1464739200000,
        "doc_count" : 2
      } ]
    }
  }
}
```

Except this time, the aggregation over the alias works as expected: 

```
curl -XGET 'http://localhost:9200/all_indices/_search?pretty' -d '{
    "size": 0,
    "aggs": 
    {"monthly_date_histogram": 
      {"date_histogram": {"field": "when_recorded", 
                          "interval": "month",
                          "min_doc_count": 0,
                          "extended_bounds": {"max": "now", "min": "now-5M"}}}}
}
'

{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "hits" : {
    "total" : 8,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "monthly_date_histogram" : {
      "buckets" : [ {
        "key_as_string" : "2016-01-01T00:00:00.000Z",
        "key" : 1451606400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-02-01T00:00:00.000Z",
        "key" : 1454284800000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-03-01T00:00:00.000Z",
        "key" : 1456790400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2016-04-01T00:00:00.000Z",
        "key" : 1459468800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-05-01T00:00:00.000Z",
        "key" : 1462060800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2016-06-01T00:00:00.000Z",
        "key" : 1464739200000,
        "doc_count" : 2
      } ]
    }
  }
}
```

The steps to reproduce are above, and I would expect the query against the alias to respect the `extended_bounds` parameter no matter what the index names are. 
</description><key id="161484873">19009</key><summary>date_histogram w/ extended_bounds fails on alias/index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">wrobstory</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-21T16:39:42Z</created><updated>2016-06-28T15:22:50Z</updated><resolved>2016-06-27T13:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-22T09:41:27Z" id="227694134">Thanks for the clear recreation.  Actually, I'd disagree with the output from `test_index_1` being correct. You've asked for extended bounds and yet you get no buckets back at all? I think all buckets should be returned instead.

@colings86 could you take a look please?
</comment><comment author="colings86" created="2016-06-22T09:57:50Z" id="227697982">Hmm, I haven't yet run your recreation @wrobstory (thanks for such a complete explanation/recreation btw) but it looks like the problem here is that the extended bounds information is not sent back to the coordinating node in the shard response if the shard had no matching documents. We arbitrarily pick one of the responses to use as the guide for the reduce phase (this is actually the first in the list and I wouldn't be surprised if the list is in fact sorted by index name first) so if we choose one which matched no documents it won't do the last step of completing the extended bounds (leading to the weird behaviour where the name of the index makes a difference to the result). In theory it should be an easy fix, to send back the extended bounds information as part of the empty aggregation response. I'll look into making this change soon.
</comment><comment author="colings86" created="2016-06-27T09:52:56Z" id="228701863">@wrobstory I have raise #19085 to address this issue
</comment><comment author="wrobstory" created="2016-06-28T15:22:50Z" id="229083544">Thanks for the great communication and quick turnaround! Much appreciated!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java</file></files><comments><comment>Pass resolved extended bounds to unmapped histogram aggregator</comment></comments></commit></commits></item><item><title>_source filtering input validation &amp; behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19008</link><project id="" key="" /><description>[_source filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html) returns the full contents of the `_source` when supplied with a number (as opposed to a string). It would be better to return nothing and/or not be valid + throw an error.

Add a document:

```
PUT myindex/mytype/1
{
  "field1": "Hello World!"
}
```

Pass in a number:

```
GET myindex/_search
{
  "_source": 1
}
```

The result:

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "myindex",
        "_type": "mytype",
        "_id": "1",
        "_score": 1,
        "_source": {
          "field1": "Hello World!"
        }
      }
    ]
  }
}
```
</description><key id="161479917">19008</key><summary>_source filtering input validation &amp; behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">n0othing</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label></labels><created>2016-06-21T16:17:29Z</created><updated>2016-07-01T09:41:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T16:57:47Z" id="227503314">&gt; "_source": 1

Nice!
</comment><comment author="clintongormley" created="2016-06-22T09:43:38Z" id="227694702">This is the equivalent of _source: true.  _source: 0 is the equivalent of _source:false
</comment><comment author="n0othing" created="2016-06-22T15:19:40Z" id="227778116">Binary switches like 0 and 1 make perfect sense for false/true, but anything &gt; 1 will return all fields in the source (e.g.):

```
{
  "_source": 54684
}
```
</comment><comment author="s1monw" created="2016-07-01T09:37:43Z" id="229903684">we have to deprecate the perl client first! I am +1 right here
</comment><comment author="s1monw" created="2016-07-01T09:41:04Z" id="229904449">I think we should remove this ability slowly (deprecate in 5.x and remove in 6.x?) By this I mean the support for anything that `true|false` on boolean fields. I think our API should b NOT lenient her at all. Clients should fix that n their end
</comment><comment author="s1monw" created="2016-07-01T09:41:24Z" id="229904513">@clintongormley can you pick this up as a task for deprecation
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs about timestamp/ttl migration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19007</link><project id="" key="" /><description>This is a follow-up to #18980. We should add documentation to explain how to move away from `_timestamp` and `_ttl` with bonus points for showing how to use the reindex API to migrate a 2.x index using `_timestamp` to 5.x by extracting the `_timestamp` into a regular field.
</description><key id="161477779">19007</key><summary>Docs about timestamp/ttl migration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>blocker</label><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T16:08:08Z</created><updated>2016-06-23T14:12:17Z</updated><resolved>2016-06-22T18:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: migration notes for _timestamp and _ttl</comment></comments></commit></commits></item><item><title>Yaml parser does not support full set of boolean options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19006</link><project id="" key="" /><description>As it stands today, the boolean parser [only accepts the following](https://github.com/elastic/elasticsearch/blob/v2.3.3/core/src/main/java/org/elasticsearch/common/Booleans.java#L123-L134):
- true: `"true"`, `"on"`, `"yes"`, `1`
- false: `"false"`, `"off"`, `"no"`, `0`

But the [yaml spec says these are all valid](http://yaml.org/type/bool.html):

&gt;  y|Y|yes|Yes|YES|n|N|no|No|NO
&gt; |true|True|TRUE|false|False|FALSE
&gt; |on|On|ON|off|Off|OFF

I'm not sure how this relates to the effort to [make settings less lenient](https://github.com/elastic/elasticsearch/issues/11503), but it feels problematic right now that not all valid yaml booleans are recognized (e.g. "True").  Maybe I'm just missing something?

Arose from user question: https://discuss.elastic.co/t/value-cannot-be-parsed-to-boolean-is-there-some-to-get-es-to-say-which-property-this-is
</description><key id="161473784">19006</key><summary>Yaml parser does not support full set of boolean options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels /><created>2016-06-21T15:51:39Z</created><updated>2016-06-21T17:08:58Z</updated><resolved>2016-06-21T16:52:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-21T16:52:47Z" id="227501814">There is no requirement to support language-independent types, these are optional. The only schema that is required is the [failsafe schema](http://www.yaml.org/spec/1.2/spec.html#id2802346). Note that the (optional) [JSON schema](http://www.yaml.org/spec/1.2/spec.html#id2803629) only requires support for `(true|false)`.
</comment><comment author="jasontedor" created="2016-06-21T16:54:26Z" id="227502306">My preference is that we go the other direction and _only_ support `(true|false)` and even get rid of `on`, `yes` and `1`, and `off`, `no` and `0`. I've always found `on` and `no` to be particularly offensive because the risk of an undetectable typo is very high.
</comment><comment author="polyfractal" created="2016-06-21T17:02:00Z" id="227504603">I'd definitely be in favor of _just_ supporting `true|false` as well.  The current situation is very confusing, since you aren't really sure what parts of the yaml spec are actually supported.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>jsr166e library comes inbuilt in Java 1.8. Changes need to make use of it.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19005</link><project id="" key="" /><description>As Java 1.8 comes inbulit with jsr166e library (java.util.concurrent.atomic.LongAdder), woudn't it be nice idea to make use of it than an external jsr166e library(com.twitter.jsr166e.LongAdder).

I know java 1.7 doesn't have it hence jsr166e library is shipped along with ES. Till when ES would be supporting java versions less than 1.8?
</description><key id="161472377">19005</key><summary>jsr166e library comes inbuilt in Java 1.8. Changes need to make use of it.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SKumarMN</reporter><labels /><created>2016-06-21T15:46:32Z</created><updated>2016-06-21T15:49:12Z</updated><resolved>2016-06-21T15:49:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-21T15:49:12Z" id="227483502">&gt; As Java 1.8 comes inbulit with jsr166e library (java.util.concurrent.atomic.LongAdder), woudn't it be nice idea to make use of it than an external jsr166e library(com.twitter.jsr166e.LongAdder).

We already do in master.

&gt; Till when ES would be supporting java versions less than 1.8?

The master branch already requires Java 8.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade `string` fields to `text`/`keyword` even if `include_in_all` is set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19004</link><project id="" key="" /><description>Closes #18974
</description><key id="161468960">19004</key><summary>Upgrade `string` fields to `text`/`keyword` even if `include_in_all` is set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T15:32:56Z</created><updated>2016-06-21T16:00:24Z</updated><resolved>2016-06-21T16:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T15:34:02Z" id="227478758">LGTM
</comment><comment author="jpountz" created="2016-06-21T16:00:16Z" id="227486887">Thanks for the quick review @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/StringMappingUpgradeTests.java</file></files><comments><comment>Upgrade `string` fields to `text`/`keyword` even if `include_in_all` is set. #19004</comment></comments></commit></commits></item><item><title>Add augmentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19003</link><project id="" key="" /><description>This adds the ability to have additional methods to whitelisted classes. These are just static methods taking the receiver as the first parameter. There isn't a performance hit or anything.

I added a few of groovy's enhancements to the collections api (e.g. maps n lists) to make those easier to work with. These are things such as `each`, `collect`, and so on that can be less verbose than java streams apis. Closure parameters were just swapped out for the appropriate java functional interface.

We may want to do more of them, candidates IMO would be around CharSequence/String/Pattern, to make manipulation easier for update scripts/ingest/etc.

This replaces the previous `aliases` functionality (which didn't work quite right in some cases) and ensures that these methods work in all situations (e.g. target of a method reference and so on). The aliases are just accomplished this way instead.
</description><key id="161468839">19003</key><summary>Add augmentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T15:32:27Z</created><updated>2016-06-22T09:48:25Z</updated><resolved>2016-06-21T17:15:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T15:38:08Z" id="227479981">&gt; CharSequence/String/Pattern

Indeed! I can have a look at some of those once this is in. Lots of them aren't really good, but a few will be helpful.
</comment><comment author="rmuir" created="2016-06-21T15:55:11Z" id="227485322">also for the map apis, we should make a choice whether they should be BiFunction/BiPredicate like i have them here, or just Function/Predicate but taking the Map.Entry. 

Its the difference between:

```
map.collect((key,value) -&gt; key + value)
```

and

```
map.collect(entry -&gt; entry.key + entry.value)
```

I really have no strong opinions, but in the scripting api its nearly impossible for me to tell what is a map and what is a list. By requiring lambdas on maps to take two parameters (key, value), if things are wrong it will be very clear. This is also consistent with the apis java has on Map. On the other hand it means parentheses...
</comment><comment author="jdconrad" created="2016-06-21T16:22:46Z" id="227493511">LGTM!  @rmuir Thanks for doing this!
</comment><comment author="nik9000" created="2016-06-21T16:56:07Z" id="227502821">I prefer the BiFunction version because it has you name the key and value something interesting and you rarely ever **need** a Map.Entry from one of those.
</comment><comment author="rmuir" created="2016-06-21T17:08:37Z" id="227506443">ok lets stick with it. we can always change it. The BiFunction has the advantage of providing the clearest error messages and least confusion, because arity gives us that (even if types are murky).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Augmentation.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LListShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LMapShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AugmentationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/LambdaTests.java</file></files><comments><comment>Merge pull request #19003 from rmuir/augmentation</comment></comments></commit></commits></item><item><title>Adding repository index generational files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19002</link><project id="" key="" /><description>Before, a repository would maintain an index file (named 'index') per
repository, that contained the current snapshots in the repository.
This file was not atomically written, so repositories had to depend on
listing the blobs in the repository to determine what the current
snapshots are, and only rely on the index file if the repository does
not support the listBlobs operation.  This could cause an incorrect view
of the current snapshots in the repository if any prior snapshot delete
operations failed to delete snapshot metadata files.

This commit introduces the atomic writing of the index file, and because
atomic writes are not guaranteed if the file already exists, we write to
a generational index file (index-N, where N is the current generation).
We also maintain an index-latest file that contains the current
generation, for those repositories that cannot list blobs.

Relates #18156 
</description><key id="161467333">19002</key><summary>Adding repository index generational files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-21T15:26:35Z</created><updated>2016-07-01T21:53:57Z</updated><resolved>2016-07-01T21:53:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-21T15:31:31Z" id="227477984">@imotov FYI

Follow up question: In this PR, we are removing listing the snap-\* files to determine the snapshots in favor of the index generational files.  Do you think we need to keep the snap-\* way of listing snapshots as a last resort in case we have an old repository with only the "index" file and that index file happens to be corrupted or missing?  Or is this unnecessary?
</comment><comment author="imotov" created="2016-07-01T14:34:06Z" id="229962894">@abeyad it's probably unnecessary. 

LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java</file><file>core/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file></files><comments><comment>Adding repository index generational files</comment></comments></commit></commits></item><item><title>Docs: Convert aggs/misc to CONSOLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19001</link><project id="" key="" /><description>They should be more readable and tested during the build.
</description><key id="161466999">19001</key><summary>Docs: Convert aggs/misc to CONSOLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T15:25:14Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T18:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T15:25:31Z" id="227476045">@MaineC this is another one of those curl docs.
</comment><comment author="MaineC" created="2016-06-22T07:49:31Z" id="227668769">LGTM
</comment><comment author="nik9000" created="2016-06-22T19:01:40Z" id="227844543">Thanks for reviewing @MaineC !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>X-Pack Alpha 3 - Role Management - Web UI not correctly auto-completing for field-level security field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19000</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha3

**OS version**: Debian 8.4

**Description of the problem including expected versus actual behavior**:
Field security field in Kibana's new Security tab is autocompleting with document types rather than document fields.

**Steps to reproduce**:
1. Open Kibana and navigate to the Security tab
2. Then go to the Role menu and attempt to create a new role
3. You will first have to fill in the "Add an index" field with your index
4. Then proceed to click on the "Add a field" field and you'll notice it recommend recommend some things. Those recommendations are suppose to be Document fields but you will be given Document types.
</description><key id="161465202">19000</key><summary>X-Pack Alpha 3 - Role Management - Web UI not correctly auto-completing for field-level security field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CCoffie</reporter><labels /><created>2016-06-21T15:18:25Z</created><updated>2017-05-09T08:15:26Z</updated><resolved>2016-06-21T16:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-21T16:43:05Z" id="227499175">Hi @CCoffie 

Thanks for reporting.  This fix will be in alpha4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Read Elasticsearch manifest via URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18999</link><project id="" key="" /><description>This commit modifies reading the Elasticsearch jar manifest via the URL
instead of converting the URL to an NIO path for increased portability.

Closes #18996
</description><key id="161462086">18999</key><summary>Read Elasticsearch manifest via URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T15:06:38Z</created><updated>2016-06-21T15:14:48Z</updated><resolved>2016-06-21T15:14:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T15:11:30Z" id="227471421">LGTM. It'd be nice to have a test for this because it looks finicky but I can see that that same finicky nature makes it hard to test.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Build.java</file><file>core/src/test/java/org/elasticsearch/BuildTests.java</file></files><comments><comment>Read Elasticsearch manifest via URL</comment></comments></commit></commits></item><item><title>Add a how-to section to the docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18998</link><project id="" key="" /><description>This moves the "Performance Considerations for Elasticsearch Indexing" blog post
to the reference guide and adds similar recommendations for tuning disk usage
and search speed.
</description><key id="161454637">18998</key><summary>Add a how-to section to the docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>review</label></labels><created>2016-06-21T14:39:05Z</created><updated>2016-06-24T08:59:28Z</updated><resolved>2016-06-24T08:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-21T17:04:16Z" id="227505211">I pushed a commit to address the comments.
</comment><comment author="nik9000" created="2016-06-21T17:18:17Z" id="227509197">I'm happy with it! I think @clintongormley has an open point about IOPS and replacing "recent" with a date is all that is left I think.
</comment><comment author="clintongormley" created="2016-06-22T10:15:40Z" id="227702116">Love it!  Have a look at how it renders with `--chunk=1` (which is how the reference docs are built).  You probably want each chapter (`==`)  on a single page, in which case you should add `[float]` before the `===` and `====` headers
</comment><comment author="jpountz" created="2016-06-23T06:36:17Z" id="227962577">I pushed one more commit to address feedback.
</comment><comment author="nik9000" created="2016-06-23T13:00:56Z" id="228042353">LGTM
</comment><comment author="clintongormley" created="2016-06-23T13:01:58Z" id="228042594">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add a how-to section to the docs. #18998</comment></comments></commit></commits></item><item><title>Beef up Translog testing with random channel exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18997</link><project id="" key="" /><description>Today we only throw random exceptions on the translog writer. This commit
extends it to also throw exceptions during checkpoint writing etc to test
if the correct flags are provided to open method etc.
</description><key id="161438705">18997</key><summary>Beef up Translog testing with random channel exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T13:36:45Z</created><updated>2016-06-21T19:25:01Z</updated><resolved>2016-06-21T19:25:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T13:41:45Z" id="227443024">Makes sense to me.
</comment><comment author="bleskes" created="2016-06-21T13:57:19Z" id="227447653">LGTM. Left a minor suggestion. Happy no failure came out of this.
</comment><comment author="jasontedor" created="2016-06-21T14:53:20Z" id="227465284">I like it too, and agree with @bleskes suggestion.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/translog/ChannelFactory.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file></files><comments><comment>Beef up Translog testing with random channel exceptions (#18997)</comment></comments></commit></commits></item><item><title>TransportClient's ClusterStatsResponse deserialization failing within JBoss 6.1 (new issue in 5.0.0-alpha3)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18996</link><project id="" key="" /><description>Within JBoss 6.1, calling TransportClient::admin().cluster().prepareClusterStats().get() breaks in 5.0.0-alpha3, used to work fine in 5.0.0-alpha2

**Elasticsearch version**: 5.0.0-alpha3

**JVM version**: 1.8.0_92

**OS version**: Windows 2012 R2

**Description of the problem including expected versus actual behavior**:

We are using TransportClient within JBoss 6.1.

In 5.0.0-alpha2 we were able to make the following call:

```
ClusterStatsResponse clusterStatsResponse = client.admin().cluster().prepareClusterStats().get();
```

Now when we do the same call in 5.0.0-alpha3 we are getting the following exception:

```
java.nio.file.FileSystemNotFoundException: Provider "vfs" not installed
```

**Steps to reproduce**:
1. Have the call to TransportClient::admin().cluster().prepareClusterStats().get() within JBoss 6.1

**Provide logs (if relevant)**: Full exception from logs (only client side, not errors on elasticsearch side):

TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse]]; nested: ExceptionInInitializerError; nested: FileSystemNotFoundException[Provider "vfs" not installed];
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:172)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:143)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ExceptionInInitializerError
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:199)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readNodeInfo(NodeInfo.java:191)
    at org.elasticsearch.action.admin.cluster.stats.ClusterStatsNodeResponse.readFrom(ClusterStatsNodeResponse.java:85)
    at org.elasticsearch.action.admin.cluster.stats.ClusterStatsNodeResponse.readNodeResponse(ClusterStatsNodeResponse.java:74)
    at org.elasticsearch.common.io.stream.StreamInput.readList(StreamInput.java:769)
    at org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse.readNodesFrom(ClusterStatsResponse.java:102)
    at org.elasticsearch.action.support.nodes.BaseNodesResponse.readFrom(BaseNodesResponse.java:110)
    at org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse.readFrom(ClusterStatsResponse.java:85)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:169)
    ... 23 more
Caused by: java.nio.file.FileSystemNotFoundException: Provider "vfs" not installed
    at java.nio.file.Paths.get(Paths.java:147)
    at org.elasticsearch.common.io.PathUtils.get(PathUtils.java:75)
    at org.elasticsearch.Build.getElasticsearchCodebase(Build.java:87)
    at org.elasticsearch.Build.&lt;clinit&gt;(Build.java:50)
    ... 32 more

**Describe the feature**:

Looks like some changes were made into alpha3 in the way the ClusterStatsResponse object is deserialized, this is impacting our client usage within JBoss 6.1.

Details of cause of issue:

+) Making the call to getting ClusterStatsResponse, elasticsearch server returns stream that needs to be deserialized into ClusterStatsResponse.
+) ClusterStatsResponse inherits from BaseNodesResponse.
+) readFrom method in BaseNodesResponse now has a call to 'readNodesFrom' (line 110 of BaseNodesResponse, alpha3 tag).
+) 'readNodesFrom' in ClusterStatsResponse calls readNodeResponse (line 102 of ClusterStatsResponse, alpha3 tag).
+) Eventually call makes it to ClusterStatsNodeResponse::readFrom, taht class NodeInfo.readNodeInfo (line 85 of ClusterStatsNodeResponse, alpha3 tag).
+) NodeInfo::readNodeInfo ends up calling NodeInfo::readFrom which calls Build.readBuild (line 199 of NodeInfo, alpha 3 tag).
+) Calling Build.readBuild forces the Build class static block to be executed to populate 'CURRENT'.
+) The code in getElasticsearchCodebase uses java nio (see lines 85-87 of Build, alpha3 tag). The URL returned within JBoss 6.1 is 'vfs://'.
*) There is no file system provider for 'vfs://' so exception is thrown.

This use is only for client, which probably does not care much for Build.CURRENT anyway? 
Could this code be changed to either lazily calculate 'CURRRENT' or to set it to 'Unknown' if an exception is thrown attempting
to call 'getLeasticsearchCodebase' (probably better as we cannot really calculate the current build with current code)? 
As it stands, this code cannot be used within JBoss 6.1.
</description><key id="161425098">18996</key><summary>TransportClient's ClusterStatsResponse deserialization failing within JBoss 6.1 (new issue in 5.0.0-alpha3)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">fassisrosa</reporter><labels /><created>2016-06-21T12:34:45Z</created><updated>2017-05-09T08:15:25Z</updated><resolved>2016-06-21T15:14:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-21T13:28:15Z" id="227439140">This has nothing to do with the serialization of the `ClusterStatsNodeResponse`, `vfs` is a JBoss thing and this silliness with JBoss and jar paths is a known issue. That said, I think we can address this. Can you try building Elasticsearch with this patch and let me know if it resolves the issue? I'll follow-up with a PR if it does.

``` diff
diff --git a/core/src/main/java/org/elasticsearch/Build.java b/core/src/main/java/org/elasticsearch/Build.java
index f844e3b..7b2b829 100644
--- a/core/src/main/java/org/elasticsearch/Build.java
+++ b/core/src/main/java/org/elasticsearch/Build.java
@@ -19,16 +19,11 @@

 package org.elasticsearch;

-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;

 import java.io.IOException;
-import java.net.URISyntaxException;
 import java.net.URL;
-import java.nio.file.Files;
-import java.nio.file.Path;
 import java.util.jar.JarInputStream;
 import java.util.jar.Manifest;

@@ -47,9 +42,9 @@ public class Build {
         final String date;
         final boolean isSnapshot;

-        Path path = getElasticsearchCodebase();
-        if (path.toString().endsWith(".jar")) {
-            try (JarInputStream jar = new JarInputStream(Files.newInputStream(path))) {
+        final URL url = getElasticsearchCodebase();
+        if (url.toString().endsWith(".jar")) {
+            try (JarInputStream jar = new JarInputStream(url.openStream())) {
                 Manifest manifest = jar.getManifest();
                 shortHash = manifest.getMainAttributes().getValue("Change");
                 date = manifest.getMainAttributes().getValue("Build-Date");
@@ -80,14 +75,8 @@ public class Build {
     /**
      * Returns path to elasticsearch codebase path
      */
-    @SuppressForbidden(reason = "looks up path of elasticsearch.jar directly")
-    static Path getElasticsearchCodebase() {
-        URL url = Build.class.getProtectionDomain().getCodeSource().getLocation();
-        try {
-            return PathUtils.get(url.toURI());
-        } catch (URISyntaxException bogus) {
-            throw new RuntimeException(bogus);
-        }
+    static URL getElasticsearchCodebase() {
+        return Build.class.getProtectionDomain().getCodeSource().getLocation();
     }

     private String shortHash;
diff --git a/core/src/test/java/org/elasticsearch/BuildTests.java b/core/src/test/java/org/elasticsearch/BuildTests.java
index f02d2f2..7bff832 100644
--- a/core/src/test/java/org/elasticsearch/BuildTests.java
+++ b/core/src/test/java/org/elasticsearch/BuildTests.java
@@ -22,16 +22,16 @@ package org.elasticsearch;
 import org.elasticsearch.test.ESTestCase;

 import java.io.IOException;
-import java.nio.file.AccessMode;
-import java.nio.file.Path;
+import java.io.InputStream;
+import java.net.URL;

 public class BuildTests extends ESTestCase {

     /** Asking for the jar metadata should not throw exception in tests, no matter how configured */
     public void testJarMetadata() throws IOException {
-        Path path = Build.getElasticsearchCodebase();
+        URL url = Build.getElasticsearchCodebase();
         // throws exception if does not exist, or we cannot access it
-        path.getFileSystem().provider().checkAccess(path, AccessMode.READ);
+        try (InputStream ignored = url.openStream()) {}
         // these should never be null
         assertNotNull(Build.CURRENT.date());
         assertNotNull(Build.CURRENT.shortHash());
```
</comment><comment author="fassisrosa" created="2016-06-21T14:00:43Z" id="227448756">I completely agree this is a JBoss silliness, it just reflects on the deserialization of this instance... Thanks for the super quick response... I'll give it a try and post my results here. 
</comment><comment author="fassisrosa" created="2016-06-21T14:48:23Z" id="227463717">Works like a charm for me (as url for JBoss ends in 'jar/' not 'jar' this ends up in 'Unknown' version).
If you can make this go into product (alpha4 onwards) that would be absolutely fantastic. 

Thanks so much for the incredibly quick turnaround!
</comment><comment author="jasontedor" created="2016-06-21T15:05:15Z" id="227469400">&gt; Works like a charm for me (as url for JBoss ends in 'jar/' not 'jar' this ends up in 'Unknown' version.

More silliness; this appears to be [JBCL-177](https://issues.jboss.org/browse/JBCL-177) and I'm inclined to _not_ work around it here but at least we avoid the `vfs` provider not found now.

&gt; If you can make this go into product (alpha4 onwards) that would be absolutely fantastic.

I'll try, no promises on alpha4.

&gt; Thanks so much for the incredibly quick turnaround!

Thanks for testing!
</comment><comment author="fassisrosa" created="2016-06-21T17:00:24Z" id="227504116">No worries on JBCL-177. The trailing '/' does not impact me at all... it does generate the 'Unknown' but I can live with it for now. Avoiding the 'vfs' provider issue does it.

Whatever you can do to make it to product is really appreciated. :)
</comment><comment author="jasontedor" created="2016-06-21T17:03:35Z" id="227505044">&gt; Whatever you can do to make it to product is really appreciated. :)

The fix is integrated and will be in alpha4. :smile:
</comment><comment author="jasontedor" created="2016-06-21T17:05:44Z" id="227505615">I've marked you as eligible for the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program); thanks for reporting!
</comment><comment author="fassisrosa" created="2016-06-21T17:52:56Z" id="227519020">Thanks! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't include `_id`, `_type` and `_index` keys in search response for inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18995</link><project id="" key="" /><description>PR for #18091
</description><key id="161418604">18995</key><summary>Don't include `_id`, `_type` and `_index` keys in search response for inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T12:02:36Z</created><updated>2016-06-21T12:14:57Z</updated><resolved>2016-06-21T12:14:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-21T12:06:04Z" id="227419990">LGTM
</comment><comment author="martijnvg" created="2016-06-21T12:14:57Z" id="227421705">thx @jimferenczi!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>file is -&gt; file name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18994</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="161405029">18994</key><summary>file is -&gt; file name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefourtheye</reporter><labels><label>docs</label></labels><created>2016-06-21T10:48:00Z</created><updated>2016-06-21T11:20:30Z</updated><resolved>2016-06-21T11:20:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-21T11:20:30Z" id="227411188">Thanks @thefourtheye - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>file is -&gt; file name (#18994)</comment></comments></commit></commits></item><item><title>Remove disableCoord check on boolean query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18993</link><project id="" key="" /><description>The SynonymQuery (added in Lucene 6) now handles the case where multiple terms appear at the same position (synonyms for instance) and thus should not be part of the minimum should match computation.
</description><key id="161397103">18993</key><summary>Remove disableCoord check on boolean query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2016-06-21T10:07:06Z</created><updated>2016-09-14T17:29:12Z</updated><resolved>2016-09-13T08:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-21T10:48:39Z" id="227405089">Yeah, if its coming from QueryBuilder it wont even be a BooleanQuery anymore, but a SynonymQuery. So if we want to alter the minimum should match computation when there are synonyms, we now should look for SynonymQuery instead.
</comment><comment author="jimczi" created="2016-06-21T11:26:38Z" id="227412339">@rmuir we want to apply minimum should match on queries that have at least 2 positions, if the QueryBuilder returns a SynonymQuery we consider that we have only one clause and in such case minShouldMatch is not relevant.
</comment><comment author="rmuir" created="2016-06-21T11:35:21Z" id="227413965">OK, if the logic was looking to apply this to any booleanquery before, then its fixed. This is good.
</comment><comment author="jpountz" created="2016-06-21T15:11:38Z" id="227471458">LGTM
</comment><comment author="dakrone" created="2016-09-12T22:05:04Z" id="246509940">@jimferenczi I think this can be merged right?
</comment><comment author="jimczi" created="2016-09-13T08:03:52Z" id="246606123">@dakrone this cannot be merged since it would break some corner cases where we still rely on coords (simple_query_string with multiple fields for instance) to apply the min should match. I'll close for now and I'll fix where it is possible.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename `fields` to `stored_fields` and add `docvalue_fields`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18992</link><project id="" key="" /><description>`stored_fields` parameter will no longer try to retrieve fields from the _source but will only return stored fields.
`fields` will throw an exception if the user uses it.
Add `docvalue_fields` as an adjunct to `fielddata_fields` which is deprecated. `docvalue_fields` will try to load the value from the docvalue and fallback to fielddata cache if docvalues are not enabled on that field.

Closes #18943
</description><key id="161388557">18992</key><summary>Rename `fields` to `stored_fields` and add `docvalue_fields`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>breaking</label><label>v5.0.0-alpha5</label></labels><created>2016-06-21T09:28:37Z</created><updated>2016-07-04T14:53:48Z</updated><resolved>2016-06-22T15:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T13:07:24Z" id="227433607">I wonder if we're ok removing the methods that you've deprecated rather than just deprecating them? If you do keep them as deprecated can you add a note about when we expect to remove them? Notes like that are nice because they make it obvious to the caller and to us when we can remove them.
</comment><comment author="nik9000" created="2016-06-21T13:27:17Z" id="227438875">LGTM. I left only minor stuff. If you want I can make the documentation modifications that I proposed after you merge.
</comment><comment author="jimczi" created="2016-06-22T15:51:53Z" id="227788509">Thanks @nik9000 !

&gt; It'd be awesome to have a complete example here that indexes a document, does this search, and then shows what the output looks like. I don't think most users understand what they are getting with this option.

I think we should first remove the support for the fielddata cache. It should not be recommended and IMO it would be great if we can remove it completely. I'll open another issue to discuss ... 
</comment><comment author="clintongormley" created="2016-07-04T14:53:47Z" id="230307823">Re-applied in https://github.com/elastic/elasticsearch/commit/afe99fcdcde62af9184658b60646d3af0aec9233
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/ExceptionRetryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractGeoTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/source/SourceFetchingIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/GeoDistanceTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequest.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java</file></files><comments><comment>Merge pull request #18992 from jimferenczi/fields_rename</comment></comments></commit></commits></item><item><title>[README.textfile] From source build </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18991</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master branch

**JVM version**: 1.8

**OS version**: windows 8

**Description of the problem including expected versus actual behavior**:
h3. Building from Source
Elasticsearch uses "Gradle":http://gradle.org for its build system. You'll need to have a modern version of Gradle installed - 2.8 should do.
In order to create a distribution, simply run the @gradle build@ command in the cloned directory.

**Steps to reproduce**:
1. clone elasticsearch
2. cd source home
3. gradle build

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
how about change discription?
gradle :distribution:rpm:assemble
gradle :distribution:zip:assemble
gradle :distribution:tar:assemble

more simple and fast 
</description><key id="161381682">18991</key><summary>[README.textfile] From source build </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sjyun</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2016-06-21T08:55:15Z</created><updated>2016-06-21T11:41:53Z</updated><resolved>2016-06-21T11:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-21T10:01:32Z" id="227395599">What do you mean?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Clarify building from sources docs in README</comment></comments></commit></commits></item><item><title>[feature request] close inactive index and when it is visited first time, open it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18990</link><project id="" key="" /><description>**Describe the feature**:
close inactive index(without read or write) for configable time.
when it is visited first time, open it.
</description><key id="161342563">18990</key><summary>[feature request] close inactive index and when it is visited first time, open it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-06-21T03:32:46Z</created><updated>2016-06-21T09:55:55Z</updated><resolved>2016-06-21T09:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-21T06:25:50Z" id="227351320">What is the reason that makes you want this feature?
</comment><comment author="makeyang" created="2016-06-21T06:32:09Z" id="227352225">some users take ES as the main storage system and keep like years data, which is structed as daily rolling data, in ES as the archieve data.
some old data are seldom used but they will be used one day. 
in order to reduce the cluster's memory presure, I'd like this feature.
</comment><comment author="clintongormley" created="2016-06-21T09:55:55Z" id="227394378">Duplicate of https://github.com/elastic/elasticsearch/issues/10869
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature request] provide other shard allocation strategies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18989</link><project id="" key="" /><description>[situation description]
I have a big cluster which stores daily rolling index and it keeps many days index in cluster. one day, the write traffic will grow for some reason and that days index will grow much larger than usual. So I have to add more nodes to the cluster to share the write traffic. meantime, I don't want to rebalance the data between existing nodes and newly added nodes. 

so I need below shard allocation strategy

**Describe the feature**:
allocation shards based on index balance not overall shard balance.
</description><key id="161342306">18989</key><summary>[feature request] provide other shard allocation strategies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2016-06-21T03:30:00Z</created><updated>2016-06-21T10:20:39Z</updated><resolved>2016-06-21T10:16:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-21T03:32:30Z" id="227331865">I think you can use allocation filtering to get this.
On Jun 20, 2016 11:30 PM, "makeyang" notifications@github.com wrote:

&gt; [situation description]
&gt; I have a big cluster which stores daily rolling index and it keeps many
&gt; days index in cluster. one day, the write traffic will grow for some reason
&gt; and that days index will grow much larger than usual. So I have to add more
&gt; nodes to the cluster to share the write traffic. meantime, I don't want to
&gt; rebalance the data between existing nodes and newly added nodes.
&gt; 
&gt; so I need below shard allocation strategy
&gt; 
&gt; _Describe the feature_:
&gt; allocation shards based on index balance not overall shard balance.
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18989, or mute the
&gt; thread
&gt; https://github.com/notifications/unsubscribe/AANLoqKhNfl3vAMtEfSTqnUcu_QXhkNCks5qN1q8gaJpZM4I6VGN
&gt; .
</comment><comment author="makeyang" created="2016-06-21T03:35:29Z" id="227332190">@nik9000  I'm talking about a user friendly way to achieve so. u are talking about there is a way to achieve so. so basiclly we are talking about different thing. 
</comment><comment author="clintongormley" created="2016-06-21T09:53:03Z" id="227393736">@makeyang I have no idea what this means:

&gt; allocation shards based on index balance not overall shard balance.

Please can you provide some detail on your issues otherwise I'm just going to close them.
</comment><comment author="makeyang" created="2016-06-21T10:08:36Z" id="227397091">@clintongormley
let me give a example. Let's say I have a cluster with A and B, 2 nodes. And I have a daily rolling index named index_20160101, index_20160102 etc. at the beginning, all the shards are allocated one A and B.  one day, I added 2 nodes, C and D into cluster with rebalance set to false. (because I added nodes to cover write spikes) but when I add these 2 nodes, newly created index are all alocated on C and D because they carry little shards. but this is not I added nodes for. I added nodes to balance write traffic between A B C and D.
 so without consider overall shards allocation but only one index allocation, which means to see if shard per index is balanced acoss the nodes, I can achieve so.
I actually do it by manually allocate shards with reroute API. but remember, I have to do it time to time. so this API is a bounds rather than necessarity.
but the use case is common.
</comment><comment author="ywelsch" created="2016-06-21T10:12:06Z" id="227397836">If you want a balance purely based on spreading shards of each index you can set `cluster.routing.allocation.balance.shard` to 0 and `cluster.routing.allocation.balance.index` to some value &gt; 0 (see https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html#_shard_balancing_heuristics ).
</comment><comment author="clintongormley" created="2016-06-21T10:16:13Z" id="227398714">And if @ywelsch 's suggestion isn't enough, you can use https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html
</comment><comment author="makeyang" created="2016-06-21T10:20:39Z" id="227399653">@clintongormley @ywelsch 
let me see. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature request]more memory consumption metrix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18988</link><project id="" key="" /><description>**Describe the feature**:
provide more memory consumption metrix like 
index requests memory useage
index response memory usage
search requests memory usage
search workflow(lucene doc etc) memory usage
search response memory usage
cluster status memory usage
etc
</description><key id="161340789">18988</key><summary>[feature request]more memory consumption metrix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Stats</label><label>feedback_needed</label></labels><created>2016-06-21T03:13:11Z</created><updated>2016-06-21T11:40:51Z</updated><resolved>2016-06-21T10:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-21T09:32:01Z" id="227388905">@makeyang as usual you have provided a few keywords without explaining what you would like this feature to look like, how you want to use it, etc.  Please provide a reasonable explanation.
</comment><comment author="makeyang" created="2016-06-21T09:53:11Z" id="227393770">@clintongormley  sorry, I thought u guys would thouth the same thing ~~~
better provide below APIs for monitor nodes/cluster memory consuption:
request API: GET _cat/memory
response:
node index_request_memory, index_response_memory, etc, segment_memory, fielddata, filtercache
XXXXX

this is basiclly what I thought how I would use it. 
the basic idea is:
when I give each node some memory for heap space, I can use JVM tools to detected how much heap memory is used, but I don't know the details. 
the details are consisted of what the part I mention plus segment memory, fielddata etc.
we have to know if memory is upder presure or not and which part consump majority part of memory. 
with and only with this, we can estimate the capisity of memory very precisily. 
</comment><comment author="clintongormley" created="2016-06-21T10:09:14Z" id="227397234">@makeyang we provide memory stats (with a lot of detail) for segments, fielddata and doc values, caches,  indexing buffers, and a host of other values.  Look at `GET _nodes/stats`. Index request/response and search request/response is all mixed in together as they use a pool of BigArrays, and the memory use is usually transient so it doesn't make sense to report on that.

The one number we don't report is cluster state size, but there is an open issue for it: https://github.com/elastic/elasticsearch/issues/3415

What else is missing?
</comment><comment author="makeyang" created="2016-06-21T10:25:03Z" id="227400611">@clintongormley  I don't agree with this one: the memory use is usually transient so it doesn't make sense to report on that. 
consider this situation: if the memory presure is caused by heavy indexing or searching, that makes a lot of sense to report these memory consumption.
remember, we get these values when and only when memory is under presure, then these transient value u mentioned is "persisited". 
</comment><comment author="clintongormley" created="2016-06-21T10:35:20Z" id="227402678">For this we have the [`inflight_requests` circuit breaker](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#in-flight-circuit-breaker), which will report its current memory usage.

What else is missing?
</comment><comment author="makeyang" created="2016-06-21T10:40:08Z" id="227403556">circuit breaker can't report the memory consumption. 
</comment><comment author="clintongormley" created="2016-06-21T10:42:18Z" id="227403960">Yes it can.  That's what the `estimated_size_in_bytes` is:

```
    "in_flight_requests": {
      "limit_size_in_bytes": 2112618496,
      "limit_size": "1.9gb",
      "estimated_size_in_bytes": 0,
      "estimated_size": "0b",
      "overhead": 1,
      "tripped": 0
    },
```
</comment><comment author="bleskes" created="2016-06-21T11:40:51Z" id="227415070">@makeyang I'm not sure what you mean exactly with 

&gt; we get these values when and only when memory is under presure, then these transient value u mentioned is "persisited".

But maybe you are looking for something like Marvel - this is a monitoring tool that collects stats data and store it so you can see the historical behavior . See https://www.elastic.co/products/marvel
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix ignore_failure behavior in _simulate?verbose and more cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18987</link><project id="" key="" /><description>- fix it so that processors with the `ignore_failure` option do not
  record their exception in the response
- add more tests to make empty `on_failure` block behavior more explicit

🎩  tip to @BigFunger for reporting these issues!
</description><key id="161327135">18987</key><summary>Fix ignore_failure behavior in _simulate?verbose and more cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-21T00:47:57Z</created><updated>2016-06-21T22:48:41Z</updated><resolved>2016-06-21T20:29:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-06-21T00:51:56Z" id="227311973">@martijnvg Looks like we missed this bug regarding `ignore_failure`.

This PR contains a fix for ignoring exceptions caused by `actualProcessor`s whose parent CompoundProcessor has `ignoreFailure = true`.

While diving into this issue, I also realized there is an inconsistency regarding the differences between an empty `on_failure` block and `ignore_failure: true`. Looks like they do not behave the same way.

should we reject configurations with empty `on_failure` blocks?
behavior could lead to confusion. I've added tests to reflect the current behavior, and I would like to know if this should be changed.
</comment><comment author="martijnvg" created="2016-06-21T07:19:05Z" id="227360047">&gt; Looks like we missed this bug regarding ignore_failure.

I completely forgot to check how this setting affects the simulate api, thanks for fixing! 

In my mind I thought I had this covered, but because of how the tracking processors get decorated
the tracking processor doesn't know about the ignore flag in the CompoundProcessor.

I think your fix looks good. Not sure how to do this differently with how currently the processor get decorated.

&gt; should we reject configurations with empty on_failure blocks?

+1 I think this makes sense. 
</comment><comment author="talevy" created="2016-06-21T19:27:47Z" id="227545703">@martijnvg updated to reject empty `on_failure` from pipelines and processors.
</comment><comment author="martijnvg" created="2016-06-21T20:05:10Z" id="227555349">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/main/java/org/elasticsearch/ingest/ConfigurationUtils.java</file><file>core/src/main/java/org/elasticsearch/ingest/Pipeline.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/ingest/TrackingResultProcessorTests.java</file><file>core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java</file></files><comments><comment>Fix ignore_failure behavior in _simulate?verbose (#18987)</comment></comments></commit></commits></item><item><title>Quiet the logging of the docs tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18986</link><project id="" key="" /><description>Significantly quiets the logging of the docs tests by:
1. Switching two log statements to debug level.
2. Only calling ESTestCase#afterIfFailed if the test failure wasn't
just assumptions being violated.
</description><key id="161306796">18986</key><summary>Quiet the logging of the docs tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T22:03:14Z</created><updated>2016-06-21T12:53:45Z</updated><resolved>2016-06-21T12:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-20T22:03:53Z" id="227283917">Technically this quiets the logging of all the REST tests, but the doc tests are the most loud.
</comment><comment author="jpountz" created="2016-06-21T07:45:03Z" id="227365053">LGTM
</comment><comment author="nik9000" created="2016-06-21T12:53:43Z" id="227430239">Thanks for reviewing @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index creation waits for write consistency shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18985</link><project id="" key="" /><description>Before returning, index creation now waits for the write consistency
number of shards to be available. An index can not take any indexing or
other replication operations without the write consistency level of
shards being available anyway, so waiting on the index creation response
in order for this condition to be met makes sense, and allows API users
to not depend on cluster health checks before attempting indexing
operations on the newly created index.
</description><key id="161289270">18985</key><summary>Index creation waits for write consistency shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha5</label></labels><created>2016-06-20T20:41:11Z</created><updated>2016-07-15T15:19:28Z</updated><resolved>2016-07-15T15:19:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-20T21:37:53Z" id="227277552">Wonderful news! I don't know a ton about the cluster state stuff but it seems sane and after applying it this gist passes:
https://gist.github.com/nik9000/3d65d5d883c7fd918a0835947c6549eb
</comment><comment author="abeyad" created="2016-07-07T01:02:43Z" id="230951463">@ywelsch I added your `MetaDataCreateIndexService` refactoring commit and added this commit to address other code review comments: https://github.com/elastic/elasticsearch/pull/18985/commits/237dc92c6dc3e1a4b25917c912e3e373b4cfbdf6
</comment><comment author="abeyad" created="2016-07-07T19:42:47Z" id="231185750">@ywelsch https://github.com/elastic/elasticsearch/pull/18985/commits/5d4663e834412301e154e6b2d802a58632698506 addresses the latest comments
</comment><comment author="ywelsch" created="2016-07-08T16:35:56Z" id="231408400">Left a few more comments (it's getting close...)
</comment><comment author="abeyad" created="2016-07-09T03:26:12Z" id="231512165">@ywelsch https://github.com/elastic/elasticsearch/pull/18985/commits/b0d4f2865cd64f789e5539c869a78286f680d0b5 addresses the latest comments
</comment><comment author="abeyad" created="2016-07-11T20:34:56Z" id="231856220">@bleskes @ywelsch https://github.com/elastic/elasticsearch/pull/18985/commits/ad4554645e66453b68dc1e377e772414597f05fa uses Boaz's patch

https://github.com/elastic/elasticsearch/pull/18985/commits/bc6b0456d3a5b07641f13ed12f1df5e0065bc56f addresses the other code review comments from Boaz
</comment><comment author="bleskes" created="2016-07-13T13:20:53Z" id="232352485">This looks great. I left some very minor comments. I do think we miss the change to TransportIndexAction and TransportBulkAction where we set the waitForActive shards to 0. 

Of course, @ywelsch should also LGTM this as he is the main reviewer 
</comment><comment author="abeyad" created="2016-07-13T20:12:45Z" id="232472511">@bleskes Thank you for the review.  https://github.com/elastic/elasticsearch/pull/18985/commits/6c2e48a78369318330e5357d5c84d885c0a9423b addresses your code review comments.
</comment><comment author="abeyad" created="2016-07-14T15:44:13Z" id="232704549">@bleskes https://github.com/elastic/elasticsearch/pull/18985/commits/fcf60eeff9d69365c1c0740573d4bd5bf6734a9c addresses the latest comments
</comment><comment author="ywelsch" created="2016-07-15T14:58:22Z" id="232975033">I left minor comments. Once addressed, feel free to push. LGTM! Thanks @abeyad.
</comment><comment author="abeyad" created="2016-07-15T15:17:50Z" id="232980976">@ywelsch @bleskes Thank you very much for all the valuable feedback!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionListener.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardCount.java</file><file>core/src/main/java/org/elasticsearch/action/support/ActiveShardsObserver.java</file><file>core/src/main/java/org/elasticsearch/cluster/ack/CreateIndexClusterStateUpdateResponse.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestRolloverIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestShrinkIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/allocation/ClusterAllocationExplainIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardCountTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ActiveShardsObserverIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java</file><file>core/src/test/java/org/elasticsearch/indices/state/SimpleIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Index creation waits for active shard copies before returning (#18985)</comment></comments></commit></commits></item><item><title>Failed to build store metadata causes OutOfMemory exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18984</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.7.0_91

**OS version**: CentOS 6.7

We received the following error message in the logs

``` log
[2016-06-20 19:33:24,465][WARN ][index.store              ] [search1_node1] [v4_messages_12_2013][3]failed to build store metadata. checking segment info integrity (with commit [no])
java.nio.file.NoSuchFileException: /var/lib/elasticsearch/stocktwits/nodes/0/indices/v4_messages_12_2013/3/index/segments_27
```

which immediately caused the node to reach an OutOfMemory exception, even though it was only using 70% of the heap. After doing some digging it appears that index it is referring to could be corrupted. Is this a cause of running an older version of java?
</description><key id="161287554">18984</key><summary>Failed to build store metadata causes OutOfMemory exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edalford11</reporter><labels><label>:Store</label><label>discuss</label></labels><created>2016-06-20T20:33:49Z</created><updated>2016-06-21T18:52:58Z</updated><resolved>2016-06-21T18:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-21T09:25:43Z" id="227387426">@edalford11 can you post the rest of the error message? I'm curious to see the stack trace.

&gt;  After doing some digging it appears that index it is referring to could be corrupted

Can you share what you found?

&gt; Is this a cause of running an older version of java?

This should not be the case, but if you can upgrade (which is recommended and will be required as of 5.0) it will tell us more.
</comment><comment author="edalford11" created="2016-06-21T18:10:22Z" id="227524130">Turns out that the original log message posted came after the OutOfMemory exception happened so that makes a little more sense.

I seem to be missing log files for when the OutOfMemory exception occurred. I see the logs on all of the other nodes when the event occurred but for some reason the node that actually went out of memory is missing logs, which makes this problem hard to solve.

All of the nodes were 70% and under on the heap usage which makes me confused as to why this happened but without logs I suppose it makes this hard to diagnose. I'll keep digging but this can probably be closed until I get more information.
</comment><comment author="bleskes" created="2016-06-21T18:52:58Z" id="227536305">yeah, OOMs are nasty. I'm closing this for now. Please reopen (or open a new ticket) if you find something interesting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Infer lambda arguments/return type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18983</link><project id="" key="" /><description>We setup lambdas with optional typing and return value of `def`. But if we know the functional interface, we should use the type info we have, and replace all `def` with the real type.

Today things work, but for example all Predicate's return values are boxed for no good reason.
Of course the worst case is primitive specializations of interfaces:

```
.mapToDouble(Double::valueOf).map(x -&gt; x + 1)
```

Instead of:

```
  private static synthetic lambda$0(Ljava/lang/Object;)Ljava/lang/Object;
    ALOAD 0
    ICONST_1
    INVOKEDYNAMIC add(Ljava/lang/Object;I)Ljava/lang/Object; [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
      // arguments:
      8, 
      0
    ]
    ARETURN
```

We do:

```
  private static synthetic lambda$0(D)D
    DLOAD 0
    DCONST_1
    DADD
    DRETURN
```

In order for things to be consistent, I added the ability to call methods on primitive types so that we can always replace `def` with its corresponding primitive if needed and it behaves the same. 
</description><key id="161269471">18983</key><summary>Infer lambda arguments/return type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T19:02:49Z</created><updated>2016-06-21T09:21:53Z</updated><resolved>2016-06-20T21:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-20T19:40:38Z" id="227247072">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Locals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicAPITests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOptimizationTests.java</file></files><comments><comment>Merge pull request #18983 from rmuir/lambda_types</comment></comments></commit></commits></item><item><title>exception in thread "main" java.lang.RuntimeException bootstrap checks failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18982</link><project id="" key="" /><description>hello there,

thanks in advance to create such a wonderfull tools like elasticsearch.
i wanna to implement it in my application to boost-up the search feature that i have.

the problems is, when i ran **bin\elasticsearch.bat** , i always getting this kinda error.

&gt; exception in thread "main" java.lang.RuntimeException bootstrap checks failed
&gt; 
&gt; initial heap size [268435456] not equal to maximum heap size
&gt; [2147483648] this can cause resize pause and prevents mlockall
&gt; from locking the entire heap
&gt; 
&gt; please set discovery.zen.minimum_mater_nodes to a majority
&gt; of the number of master eligible nodes in your cluster

**this is my environment information** 

**Elasticsearch version:**
Version: 5.0.0-alpha1, Build: 7d4ed5b/2016-04-04T10:39:25.841Z, JVM: 1.8.0_91

**JVM version:**
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS Version**
Win 10 64 bit.

i read this article : 
https://www.elastic.co/guide/en/elasticsearch/guide/1.x/_important_configuration_changes.html

and then i decide to set : 
**discovery.zen.minimum_master_nodes: 2**

the same error cames up.
what should i do in order to fix that?

i hope you guys can help me to fix this kinda of error.
</description><key id="161257938">18982</key><summary>exception in thread "main" java.lang.RuntimeException bootstrap checks failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gutasaputra</reporter><labels /><created>2016-06-20T18:08:48Z</created><updated>2016-06-20T22:45:37Z</updated><resolved>2016-06-20T18:34:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-20T18:34:32Z" id="227229462">The bootstrap checks are [documented](https://www.elastic.co/guide/en/elasticsearch/reference/master/bootstrap-checks.html). You're running into the [heap size check](https://www.elastic.co/guide/en/elasticsearch/reference/master/_heap_size_check.html) and the [minimum master nodes check](https://www.elastic.co/guide/en/elasticsearch/reference/master/_minimum_master_nodes_check.html). It looks like you already figured out to set `discovery.zen.minimum_master_nodes` but I do not see any indication that you also tuned the heap size? You need configure the minimum heap size to be equal to the maximum heap size if Elasticsearch is reachable via an external network interface.

&gt; and then i decide to set : 
&gt; **discovery.zen.minimum_master_nodes: 2**
&gt; 
&gt; the same error cames up.

Are you sure that that is what happened? If so, can you please provide a reproduction? I would have expected that the minimum master nodes bootstrap check would be passing and that you would only see an error on the heap size bootstrap check failing.

Also, note that you should only set it to two if you have three master-eligible nodes (and you should never have exactly two master-eligible nodes).

Please note that Elastic reserves GitHub for verified bug reports and feature requests. If you have general questions, the [Elastic Discourse forum](https://discuss.elastic.co) is a great place to get help.
</comment><comment author="gutasaputra" created="2016-06-20T20:29:21Z" id="227259605">ohya, i forget to change the xms and xmx value.
i updated to this : 

&gt; -Xms2g
&gt; -Xmx2g

thanks alot

![elastic](https://cloud.githubusercontent.com/assets/1556126/16206888/c2d74e08-3754-11e6-95bf-c7b9f7db0946.png)
</comment><comment author="jasontedor" created="2016-06-20T22:45:37Z" id="227292571">&gt; thanks alot

You're welcome.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wrong name for values field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18981</link><project id="" key="" /><description>We wrote that the document is:

``` json
{
  "value" : ["foo", "bar", "baz"]
}
```

But the processor is using a `values` field:

``` json
{
  "foreach" : {
    "field" : "values",
    "processors" : [
      // ...
    ]
  }
}
```

It should be `values`.
</description><key id="161243979">18981</key><summary>Wrong name for values field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label></labels><created>2016-06-20T16:59:05Z</created><updated>2016-06-22T21:14:02Z</updated><resolved>2016-06-22T21:14:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-20T16:59:29Z" id="227203149">@martijnvg could you look please?
</comment><comment author="javanna" created="2016-06-22T14:05:42Z" id="227754072">LGTM thanks @dadoonet 
</comment><comment author="martijnvg" created="2016-06-22T19:46:57Z" id="227855837">@dadoonet Sorry for the late reply. LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #18981 from elastic/doc/ingest-foreach</comment></comments></commit></commits></item><item><title>Remove `_timestamp` and `_ttl` on 5.x indices.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18980</link><project id="" key="" /><description>This removes the ability to use `_timestamp` and `_ttl` on indices created on
or after 5.0.

Closes #18280
</description><key id="161233704">18980</key><summary>Remove `_timestamp` and `_ttl` on 5.x indices.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T16:11:05Z</created><updated>2016-06-22T09:32:06Z</updated><resolved>2016-06-21T16:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-20T16:47:23Z" id="227199636">I wonder if we should still keep some of those REST tests? I thought we could set the created version. While we still support the behavior we should try.

Should we yank the reindex module's support for timestamp and ttl? There is some basic scripting support in there. We could do it as another PR.

All of the test looks good to me though.
</comment><comment author="jpountz" created="2016-06-21T08:45:26Z" id="227378063">Apparently we cannot set the version in the REST tests. I am not sure what exactly to do for reindex, would it be easy today already to move the value of the _timestamp to a regular field? It looks so using a script given that we put the _timestamp into the context of the script?
</comment><comment author="nik9000" created="2016-06-21T12:37:25Z" id="227426477">&gt; It looks so using a script given that we put the _timestamp into the context of the script?

Yeah, I think it'd be easy to do.

Maybe we don't do anything to reindex now and wait for 6.0 or something. That way reindex can still deal with timestamp, including moving it to a field.
</comment><comment author="jpountz" created="2016-06-21T15:19:32Z" id="227474023">This plan sounds good to me.
</comment><comment author="nik9000" created="2016-06-21T15:21:24Z" id="227474646">LGTM.

I can add a use case example to the docs for reindex that shows moving _timestamp to a field.
</comment><comment author="jpountz" created="2016-06-21T15:28:33Z" id="227477005">I think it would be great to have it in the migration notes!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: migration notes for _timestamp and _ttl</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Remove `_timestamp` and `_ttl` on 5.x indices. #18980</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Revert "Remove `_timestamp` and `_ttl` on 5.x indices. #18980"</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/FieldSortIT.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Remove `_timestamp` and `_ttl` on 5.x indices. #18980</comment></comments></commit></commits></item><item><title>Fail doc tests when any shard fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18979</link><project id="" key="" /><description>ES only sends a non-200 response all shards fail but we should
fail the tests generated by docs if any of them fail.

Depending on the outcome of #18978 this might be a temporary
workaround.
</description><key id="161231195">18979</key><summary>Fail doc tests when any shard fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>docs</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T16:00:13Z</created><updated>2016-06-20T16:50:01Z</updated><resolved>2016-06-20T16:50:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-20T16:03:21Z" id="227187409">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Should Elasticsearch return a non-200 response if there are shard failures?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18978</link><project id="" key="" /><description>Lots of Elasticsearch tasks are forked onto a bunch of shards. When those shards fail, Elasticsearch returns the failures in the json at `_shards.failures` including the HTTP response code that that failure deserves _but_ it will return an HTTP 200 response code so long as a single shard succeeds. I think it should return a non-200 HTTP status if any shard fails.

Do we want to:
1. Continue as we are and return a 200 code if any shard succeeds.
2. Return a non-200 code if any shards fail. We'd return the highest numbered failure because that is what we do now if _all_ shards fail. You can get each response code in the `failures` array in the response. We could also talk about [RFC 4918](https://tools.ietf.org/html/rfc4918)'s `207 Multi-Status` but at first glance that specifies some XML response we aren't going to implement.
3. Add a boolean to the request to toggle between the two behaviors. We'd have to pick a default but we could default to the old behavior for 5.0 if we didn't want the change to be breaking.
</description><key id="161215769">18978</key><summary>Should Elasticsearch return a non-200 response if there are shard failures?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>discuss</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T14:58:27Z</created><updated>2016-11-07T15:39:16Z</updated><resolved>2016-06-20T16:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-20T16:00:29Z" id="227186594">&gt; Add a boolean to the request to toggle between the two behaviors.

Please no.

&gt; Return a non-200 code if any shards fail. 

This would cause all of the clients to report an exception instead of returning partial results.  This is not ideal, eg I'd rather see some results than none at all.

Given that there isn't an HTTP status that indicates partial success, I vote to keep it as 200
</comment><comment author="jpountz" created="2016-06-20T16:00:38Z" id="227186639">This is something that bugged me in the past as well, but I do not feel too strongly about it since we give users all the information they need to do whathever they want to do on application side (raise an error, show a big red warning that there is a partial failure, etc.). I am a bit torn as to whether it would be worth maitaining a parameter or setting for this. So I think I am leaning towards 1.
</comment><comment author="bleskes" created="2016-06-20T16:02:31Z" id="227187150">+1 to 1 . So sad we don’t have a partial response status code.

&gt; On 20 Jun 2016, at 18:00, Adrien Grand notifications@github.com wrote:
&gt; 
&gt; This is something that bugged me in the past as well, but I do not feel too strongly about it since we give users all the information they need to do whathever they want to do on application side (raise an error, show a big red warning that there is a partial failure, etc.). I am a bit torn as to whether it would be worth maitaining a parameter or setting for this. So I think I am leaning towards 1.
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment><comment author="nik9000" created="2016-06-20T16:07:03Z" id="227188469">207 is a partial response code but it is all mixed up with WebDAV....

I still think it'd be much more intuitive to return a non-200 and have the client check if the request looks like a partial failure rather than a complete one....

I know we don't check it all the right places in our tests.
</comment><comment author="TheGreatYamcha" created="2016-11-07T15:36:06Z" id="258868610">This is IMHO, a poor design decision and goes against a [fail-fast](https://en.m.wikipedia.org/wiki/Fail-fast) system.
Imagine what happens when people only sometimes see the complete search results. They'll be spending a long time to figure out what is wrong.
</comment><comment author="clintongormley" created="2016-11-07T15:39:15Z" id="258869552">@TheGreatYamcha this is intentional, eg we'd rather show you some of your friends' updates instead of none.  The response includes the `_shards` parameter which tells you how many shards failed or succeeded which you can use if you prefer to only show complete results.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fail doc tests when any shard fails</comment></comments></commit></commits></item><item><title>Duplicate documents retrieved from _search using sorting on string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18977</link><project id="" key="" /><description>When searching trough a few documents (1206 in that case) in an index (updated with deletes, inserts, updates from time to time), I got some duplicates or not depending on the sorting I supply.

**Elasticsearch version**:
2.1.0  

**JVM version**:
openjdk version "1.8.0_66-internal"
OpenJDK Runtime Environment (build 1.8.0_66-internal-b17)
OpenJDK 64-Bit Server VM (build 25.66-b17, mixed mode)

**OS version**:
Linux CBISES02 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1+deb8u6 (2015-11-09) x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
I have an index with the following mapping:

```
{  
   "state":"open",
   "settings":{  
      "index":{  
         "creation_date":"1466088694234",
         "number_of_shards":"10",
         "number_of_replicas":"1",
         "uuid":"4tiKdB_6RWe-x2LoxDGOFg",
         "version":{  
            "created":"2010099"
         }
      }
   },
   "mappings":{  
      "product":{  
         "_routing":{  
            "required":true
         },
         "dynamic":"false",
         "_all":{  
            "enabled":false
         },
         "properties":{  
            "id":{  
               "index":"not_analyzed",
               "type":"string"
            },
            "entityId":{  
               "index":"not_analyzed",
               "type":"string"
            },
            "categories":{  
               "type":"integer"
            },
            "name":{  
               "type":"string",
               "fields":{  
                  "raw":{  
                     "index":"not_analyzed",
                     "type":"string"
                  }
               }
            },
            "routingKey":{  
               "index":"not_analyzed",
               "type":"string"
            }
            (...)
         }
      }
   },
   "aliases":[  
      (.. some aliases)
   }
```

Retrieving the documents using the following query returns 1206 documents, **with** duplicates and therefor some documents are missing. Note that re-indexing changes the number of duplicates.

```
{
  "from": page * 3, // page from 0 to no more documents 
  "size": 3,
  "sort": [
    {
      "name.raw": {
        "order": "asc"
      }
    }
  ]
}
```

Retrieving the documents using the following query returns 1206 documents, **without** duplicates.

```
{
  "from": page * 3, //page from 0 to no more documents 
  "size": 3,
  "sort": [
    {
      "entityId": {
        "order": "asc"
      }
    }
  ]
}
```

As there is a routing key, only one shard is hit at any time for both searches so the sorting should be consistent.

**Expected result**:
No duplicates, regardless of the sorting supplied.
</description><key id="161179523">18977</key><summary>Duplicate documents retrieved from _search using sorting on string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bqk-</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2016-06-20T12:02:58Z</created><updated>2016-06-21T07:40:14Z</updated><resolved>2016-06-21T07:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-20T14:58:40Z" id="227167783">Can you show the result of the following request:

```
{
  "from": page * 3, // page from 0 to no more documents 
  "size": 3,
  "fields" : ["_routing"],
  "sort": [
    {
      "name.raw": {
        "order": "asc"
      }
    }
  ]
}
```

I suspect that some documents have been put into different shards because they had different routing keys.
</comment><comment author="bqk-" created="2016-06-20T15:11:33Z" id="227171760">```
{  
   "took":8,
   "timed_out":false,
   "_shards":{  
      "total":1,
      "successful":1,
      "failed":0
   },
   "hits":{  
      "total":1448,
      "max_score":null,
      "hits":[  
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:632589",
            "_score":null,
            "_routing":"ApiKey-v3",
            "sort":[  
               "cbis:632589"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:632594",
            "_score":null,
            "_routing":"ApiKey-v3",
            "sort":[  
               "cbis:632594"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:632642",
            "_score":null,
            "_routing":"ApiKey-v3",
            "sort":[  
               "cbis:632642"
            ]
         }
      ]
   }
}
```

As mentioned, I am querying and indexing using a the routing key (ApiKey-v3). There is also `"_routing":{  
            "required":true
         },` in the mapping of the index.
</comment><comment author="jpountz" created="2016-06-20T15:25:29Z" id="227175867">The above response does not contain duplicates as all documents do NOT have the same (`_index`, `_type`, `_id`) tuple (which uniquely identify a document within a cluster)?
</comment><comment author="bqk-" created="2016-06-21T07:00:08Z" id="227356653">Query:

```
{
  "from": 258,
  "size": 3,
  "sort": [
    {
      "name.raw": {
        "order": "asc"
      }
    }
  ]
}
```

Result:

```
{  
   "took":66,
   "timed_out":false,
   "_shards":{  
      "total":1,
      "successful":1,
      "failed":0
   },
   "hits":{  
      "total":1207,
      "max_score":null,
      "hits":[  
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:423457",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drängstugan",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:423457",
               "entityId":"cbis:423457",
               "updated":"2016-06-20T13:12:10.8807738+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drängstugan"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"**ApiKey-v3-cbis:1222242**",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drömstuga 2+2",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:1222242",
               "entityId":"cbis:1222242",
               "updated":"2016-06-20T13:12:32.2718554+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drömstuga 2+2"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:1222259",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drömstuga 2+2",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:1222259",
               "entityId":"cbis:1222259",
               "updated":"2016-06-20T13:12:32.8031266+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drömstuga 2+2"
            ]
         }
      ]
   }
}
```

Query:

```
{
  "from": 261,
  "size": 3,
  "sort": [
    {
      "name.raw": {
        "order": "asc"
      }
    }
  ]
}
```

Result:

```
{  
   "took":15,
   "timed_out":false,
   "_shards":{  
      "total":1,
      "successful":1,
      "failed":0
   },
   "hits":{  
      "total":1207,
      "max_score":null,
      "hits":[  
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:1222261",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drömstuga 2+2",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:1222261",
               "entityId":"cbis:1222261",
               "updated":"2016-06-20T13:12:32.8031266+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drömstuga 2+2"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"**ApiKey-v3-cbis:1222242**",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drömstuga 2+2",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:1222242",
               "entityId":"cbis:1222242",
               "updated":"2016-06-20T13:12:32.2718554+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drömstuga 2+2"
            ]
         },
         {  
            "_index":"product-sv-se",
            "_type":"product",
            "_id":"ApiKey-v3-cbis:1222262",
            "_score":null,
            "_routing":"ApiKey-v3",
            "_source":{  
               "name":"Drömstuga 4+2",
               "version":3,
               "apiKey":"ApiKey",
               "id":"ApiKey-v3-cbis:1222262",
               "entityId":"cbis:1222262",
               "updated":"2016-06-20T13:12:32.8031266+02:00",
               "routingKey":"ApiKey-v3"
            },
            "sort":[  
               "Drömstuga 4+2"
            ]
         }
      ]
   }
}
```

One thing I noticed is that the duplicates are always on subsequent pages.
</comment><comment author="jpountz" created="2016-06-21T07:40:13Z" id="227364047">Ah ok, this is a known issue. When sorting, elasticsearch uses the Lucene doc id as a tie-break. However, these doc ids can be different across shards. What is happening in your case is that `**ApiKey-v3-cbis:1222242**` should be on page 86 according to one shard and on page 87 according to another shard (due to the fact that you have several documents that have `Drömstuga 2+2` as a value for `name.raw`).

You can work-around the issue either by giving elasticsearch another field to use as a tie breaker, eg.

```
"sort": [
    {
      "name.raw": {
        "order": "asc"
      }
    },
    {
      "updated": {
        "order": "asc"
      }
    }
  ]
```

or by creating a random [preference](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-preference.html) on the first page, and then making sure to always reuse it for all pages so that all pages will be requested on the same shard.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: OpenCloseIndexIT (suite) breaks with certain seed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18976</link><project id="" key="" /><description>From CI: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=centos/620/console

Also fails on osx locally. Test leaks file handles.

To reproduce: 

```
gradle clean :core:integTest -Dtests.seed=8E8A4389665B4EFB -Dtests.class=org.elasticsearch.indices.state.OpenCloseIndexIT -Dtests.security.manager=true -Dtests.locale=en-US -Dtests.timezone=UTC
```
</description><key id="161179036">18976</key><summary>Tests: OpenCloseIndexIT (suite) breaks with certain seed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2016-06-20T11:59:51Z</created><updated>2016-06-20T12:14:55Z</updated><resolved>2016-06-20T12:14:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-20T12:13:25Z" id="227125444">I think this is fixed by https://github.com/elastic/elasticsearch/commit/459665914b3aa3b107a3e3c9069ab812ebbf36b7 but I will verify
</comment><comment author="s1monw" created="2016-06-20T12:14:55Z" id="227125716">yeah doensn't fail anymore it's the `@Inject` annotation on NodeEnviroment that messes things up here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Compile each Groovy script in its own classloader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18975</link><project id="" key="" /><description>Backport of #18918 for 2.x
</description><key id="161170651">18975</key><summary>Compile each Groovy script in its own classloader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v2.4.0</label></labels><created>2016-06-20T11:07:34Z</created><updated>2016-06-27T13:59:21Z</updated><resolved>2016-06-20T15:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-20T14:34:28Z" id="227160264">LGTM
</comment><comment author="tlrx" created="2016-06-20T15:34:44Z" id="227178658">Thanks @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Possible Regression on 5.0A3 for string to text conversion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18974</link><project id="" key="" /><description>**Elasticsearch version**: 
5.0 A3

**JVM version**: 
1.8.0_91

**OS version**:
CentOS 7

**Description of the problem including expected versus actual behavior**:
On 5.0 A2 the automatic upgrade https://github.com/elastic/elasticsearch/pull/17861 was working fine for a mapped string using an analyzer (Path Hierarchy Tokenizer), after upgrade to 5.0 A3 it didn't work, so I think we have a regression at that point.

**Steps to reproduce**:
1. Define an analyser for a path_hierarchy

"analyzer": {
        "paths": {
          "tokenizer": "path_hierarchy"
        }
1. Set it for an analyzed string on mapping 

**Provide logs (if relevant)**:
</description><key id="161163322">18974</key><summary>Possible Regression on 5.0A3 for string to text conversion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">isaias</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-06-20T10:23:02Z</created><updated>2017-05-09T08:15:25Z</updated><resolved>2016-06-21T16:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-20T11:00:16Z" id="227112264">I tried the following locally on a fresh download of 5.0 alpha3

```
PUT test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "path_analyzer": {
          "type": "custom",
          "tokenizer": "path_tokenizer"
        }
      },
      "tokenizer": {
        "path_tokenizer": {
          "type": "path_hierarchy"
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string",
          "analyzer": "path_analyzer"
        }
      }
    }
  }
}

GET test/_mapping
```

which returned

```
{
  "test": {
    "mappings": {
      "test": {
        "properties": {
          "foo": {
            "type": "text",
            "analyzer": "path_analyzer"
          }
        }
      }
    }
  }
}
```

This looks correct to me. Could you provide a recreation for the issue that you are seeing?
</comment><comment author="isaias" created="2016-06-20T13:15:03Z" id="227138442">@jpountz I've tried to reproduce on my environment and it didn't happen anymore. The only difference was that I updated from Alpha 2 to Aplha3 using RPM. I've tried to do it again but the error didn't happen. I'll close. Thanks.
</comment><comment author="isaias" created="2016-06-20T22:05:35Z" id="227284293">@jpountz, I reproduced the error with a fresh Alpha 3, see the "bar" definition.

```
{
  "settings": {
    "analysis": {
      "analyzer": {
        "path_analyzer": {
          "type": "custom",
          "tokenizer": "path_tokenizer"
        }
      },
      "tokenizer": {
        "path_tokenizer": {
          "type": "path_hierarchy"
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string",
          "analyzer": "path_analyzer"
        },
        "bar": {
            "type": "string",
            "index": "not_analyzed",
            "fields": {
              "tree": {
                "type": "string",
                "analyzer": "path_analyzer"
              }
            },
            "include_in_all": false,
            "fielddata": false
          }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-06-21T09:28:28Z" id="227388091">Actually, this  is just to do with `include_in_all`:

```
PUT t
{
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string",
          "include_in_all": false
        }
      }
    }
  }
}
```
</comment><comment author="jpountz" created="2016-06-21T12:51:40Z" id="227429750">OK, so we need to add `include_in_all` to the whitelist of parameters we automatically upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/StringMappingUpgradeTests.java</file></files><comments><comment>Upgrade `string` fields to `text`/`keyword` even if `include_in_all` is set. #19004</comment></comments></commit></commits></item><item><title>Detach BigArrays from Guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18973</link><project id="" key="" /><description>BigArrays can be fully constructed without Guice, this change cleans up
it's creation and the mocking in MockNode.
</description><key id="161162837">18973</key><summary>Detach BigArrays from Guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T10:20:09Z</created><updated>2016-06-20T11:18:19Z</updated><resolved>2016-06-20T11:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-20T10:26:09Z" id="227105650">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/NodeModule.java</file><file>test/framework/src/main/java/org/elasticsearch/common/util/MockBigArrays.java</file><file>test/framework/src/main/java/org/elasticsearch/node/MockNode.java</file><file>test/framework/src/main/java/org/elasticsearch/node/NodeMocksPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/NodeConfigurationSource.java</file></files><comments><comment>Detach BigArrays from Guice (#18973)</comment></comments></commit></commits></item><item><title>Corrupt index after disk full, unclean shutdown / Possible to avoid translog recovery at all?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18972</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1 then after trying the steps below we have decided to update to the newest version, so probably the bugs will have been fixed

**JVM version**: openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**: CentOS 7

**Description of the problem including expected versus actual behavior**:

The disk space on our Elasticsearch server got full and we had to remove some logs in order to reduce the disk space usage. Moreover, we restarted elasticsearch and once we have done so immediately we have seen that some of our indices were broken (please see attached below the logs).

We have moved the entire elasticsearch 'data' directory out of the way as it was an urgent to have logs and restarted elasticsearch which started with a fresh 'data' directory.

In order to restore the indices we have copied some of them to another Elasticsearch Server because we did not wanted to do this on our Live Elasticsearch server and we did as follows:
1. Stop the ES server
2. Copy + change permission of the index
3. Restart ES server

also:
- removed all the translog files

Error message:

[2016-06-20 09:01:56,764][WARN ][cluster.action.shard     ] [Famine] [logstash-2016.05.11][3] received shard failed for target shard [[logstash-2016.05.11][3], node[d-z_xFoqTrGd-kip-Tly_w], [P], v[3], s[INITIALIZING], a[id=yh9ngGZNQ5CIrq87hxZRTg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-06-20T09:01:56.636Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]]], indexUUID [xN9jy5ZiRqaALYvkDxdFgA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]

Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: Caused by: [logstash-2016.05.11][[logstash-2016.05.11][0]] EngineCreationFailureException[fa
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFac
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:
Jun 20 09:01:49 log-01.novalocal elasticsearch[9894]: at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)

[2016-06-20 07:12:24,538][WARN ][cluster.action.shard     ] [Brain-Child] [logstash-2016.05.11][3] received shard failed for [logstash-2016.05.11][3], node[FcS6VgojTuijenAeemDE6Q], [P], v[3], s[INITIALIZING], a[id=OdCh5n8GSDeZpiCLW-CKZg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-06-20T07:12:24.364Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]], indexUUID [xN9jy5ZiRqaALYvkDxdFgA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException; ]
[logstash-2016.05.11][[logstash-2016.05.11][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;
- removed the translog.ckp

Caused by: [logstash-2016.05.11][[logstash-2016.05.11][4]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/logstash-2016.05.11/4/translog/translog.ckp];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
- removed everything besides translog.ckp in the translog folder
- we have also used java -cp lucene-core5.4.1.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex to check our indexes and it said that everything is ok, however still we kept on getting errors.

Could someone please inform me what I have done wrong and what could I do to restore my indices ?

Thanks,
Sergiu
</description><key id="161162691">18972</key><summary>Corrupt index after disk full, unclean shutdown / Possible to avoid translog recovery at all?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SergiuCip</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2016-06-20T10:19:20Z</created><updated>2016-08-23T13:17:55Z</updated><resolved>2016-08-12T11:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-20T15:27:35Z" id="227176480">@SergiuCip When you say:

&gt; and we had to remove some logs in order to reduce the disk space usage.

do you mean transaction logs or files in the `logs/` directory?
</comment><comment author="uplexPaul" created="2016-06-20T15:34:40Z" id="227178640">Hi. I am working with @SergiuCip on that issue

@clintongormley:
He doesn't mean that we removed translog files. Just forget the sentence ;)
But it seems like we somehow lost translog files by moving indexes around or whatever.

But anyway, the main INDEX itself is intact (checked with lucene). So even if we don't have translog files anymore for these indices, they SHOULD be able to recover even without these translog when the index is not corrupted. That is, wat we would like to achieve.
</comment><comment author="SergiuCip" created="2016-06-20T16:01:25Z" id="227186837">@clintongormley: I am sorry for being ambigue :) what I meant was that we have deleted some logs from /var/log/elasticsearch because they were growing at fast rate because of the recovery process. as @uplexPaul mentioned we do have the translog.ckp but not the translog files itself. 

When you try to start elasticsearch with those indices it will complain that cannot start due to reasons that I have posted previously in my first post.

Any help it is appreciated.

Thanks
</comment><comment author="clintongormley" created="2016-06-20T16:06:51Z" id="227188405">Sorry to ask but are you sure you're on 2.3.3?  On all nodes? We had a bug in early versions of 2.x which ended up with corrupt transaction logs after disk full, but I haven't heard reports of this since 2.3
</comment><comment author="bleskes" created="2016-06-20T16:11:12Z" id="227189708">and one more clarification - you say you do have the translog.ckp file, but the exception  indicates differently:

```
NoSuchFileException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/logstash-2016.05.11/4/translog/translog.ckp];
```
</comment><comment author="SergiuCip" created="2016-06-20T16:14:11Z" id="227190562">@clintongormley Hi and thanks for the update, yes I beg your pardon we were running ES version 2.3.1 when the issue has occurred and then we updated to 2.3.3 

@bleskes: So, we have tried everything ....keeping the .ckp file and tried to start ES but no luck...also I have tried to delete the .ckp file...and try to start ES but no luck either
</comment><comment author="s1monw" created="2016-06-20T18:12:51Z" id="227223335">@SergiuCip I can see some EOF exceptions but I am missing the part where it says which file goes EOF is it possible to get the full exceptions? Do you also have the log from when this thing went disk full? Would be great to get all of the logs and exceptions traces
</comment><comment author="SergiuCip" created="2016-06-20T18:50:43Z" id="227233974">@s1monw I will try to see if I can find it in the log where it says EOF, however, for the log when the disk started to become full, I am afraid that I don't have that anymore as that was the first log we started truncating because of fast size increase.

I will do my best to provide you with as much logging as possible ;)
</comment><comment author="SergiuCip" created="2016-06-21T10:03:12Z" id="227395943">Good morning everyone!

Thank you for all the support along the way! As promised I have run the test again in order to have a detailed log and here it is: https://gist.github.com/SergiuCip/86dc26c3f67a526d7e4abc060c327eb4

Also please find attached the .ckp file for this log
[translog.ckp.zip](https://github.com/elastic/elasticsearch/files/325266/translog.ckp.zip)
</comment><comment author="SergiuCip" created="2016-06-21T10:20:08Z" id="227399543">The EOF error occurs when I manually touch the translog-21.tlog file:

https://gist.github.com/SergiuCip/984f26e1ac6cae562b35a48be7a27164
</comment><comment author="s1monw" created="2016-06-21T12:45:40Z" id="227428334">@SergiuCip thanks for all the files and infos, lemme ask some more questinons:
- Do you still see a translog-20.ckp file sitting around and is there a translog-20.tlog file as well?
- when you hit the disk full outage which exact ES version did you run sicne this looks pretty much like this https://github.com/elastic/elasticsearch/pull/15788
- when you say the `The EOF error occurs when I manually touch the translog-21.tlog file:` does this mean the `translog-21.tlog` file didn't exists and you added it?
- the original exception was a `FileNotFound` is this correct?

thanks!
</comment><comment author="SergiuCip" created="2016-06-21T12:51:11Z" id="227429635">@s1monw You're welcome and thank you for helping me debug it.
1. No I cannot see the translog-20.ckp file anywhere...I had to create it manually but again it complained.
2. When the issue happen I was running version 3.2.1
3. Yes it is correct. When you add the broken index these are the first lines of the log:

[logstash-2016.05.12][[logstash-2016.05.12][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nes
ted: NoSuchFileException[/home/scorn/test_es/data/elasticsearch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2016.05.12][[logstash-2016.05.12][3]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/home/scorn/test_es/data/elasticse
arch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
    ... 5 more
Caused by: java.nio.file.NoSuchFileException: /home/scorn/test_es/data/elasticsearch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:335)
    at org.elasticsearch.index.translog.Translog.openReader(Translog.java:374)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:334)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:179)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
    ... 11 more
[2016-06-21 09:41:30,949][WARN ][cluster.action.shard     ] [Advisor] [logstash-2016.05.12][3] received shard failed for target shard [[logstash-2016.05.12][3], node[Tgnuvv4DSTut
gGCRQpR0ig], [P], v[3], s[INITIALIZING], a[id=ekDqqz2BQEi0ur8Fi7Ow1w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-21T09:41:30.116Z]]], indexUUID [HLREui0SRAO1LZt8ybnT
WA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: No
SuchFileException[/home/scorn/test_es/data/elasticsearch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog]; ]
[logstash-2016.05.12][[logstash-2016.05.12][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nes
ted: NoSuchFileException[/home/scorn/test_es/data/elasticsearch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2016.05.12][[logstash-2016.05.12][3]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/home/scorn/test_es/data/elasticse
arch/nodes/0/indices/logstash-2016.05.12/3/translog/translog-21.tlog];

Thank you :)
</comment><comment author="s1monw" created="2016-06-21T13:17:39Z" id="227436254">&gt; When the issue happen I was running version 3.2.1

I assume you mean 2.3.1? 

&gt; No I cannot see the translog-20.ckp file anywhere...I had to create it manually but again it complained.

can I get a directory listing of `home/scorn/test_es/data/elasticsearch/nodes/0/indices/logstash-2016.05.12/3/translog/` or can you maybe even zip it?
</comment><comment author="SergiuCip" created="2016-06-21T13:23:48Z" id="227437914">@s1monw Thank you,

Yes I meant 2.3.1 :) Sorry about that.

And I will be sending you the compressed directory through email. 

Please note that the translog-21.tlog are just touched files by me. The translog directory only contained translog.ckp
</comment><comment author="SergiuCip" created="2016-06-21T13:35:44Z" id="227441304">@s1monw  And this is what I get when I do not have the translog-21.tlog files touched:

[2016-06-21 13:15:11,257][WARN ][indices.cluster          ] [Timeslip] [[logstash-2016.05.11][2]] marking and sending shard failed due to [failed recovery]
[logstash-2016.05.11][[logstash-2016.05.11][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nes
ted: EOFException;
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2016.05.11][[logstash-2016.05.11][2]] EngineCreationFailureException[failed to create engine]; nested: EOFException;
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
    ... 5 more
Caused by: java.io.EOFException
    at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)
    at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)
    at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)
    at org.elasticsearch.index.translog.Checkpoint.&lt;init&gt;(Checkpoint.java:54)
    at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:83)
    at org.elasticsearch.index.translog.Translog.readCheckpoint(Translog.java:1930)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:163)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
    ... 11 more
[2016-06-21 13:15:11,263][WARN ][cluster.action.shard     ] [Timeslip] [logstash-2016.05.11][2] received shard failed for target shard [[logstash-2016.05.11][2], node[Ja7_6NXIR8i
EzqxDnLi2Yg], [P], v[3], s[INITIALIZING], a[id=KCrRrfxmRA-O_bJcR3rp_w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-21T13:15:10.560Z]]], indexUUID [HLREui0SRAO1LZt8ybn
TWA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: E
OFException; ]
[logstash-2016.05.11][[logstash-2016.05.11][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nes
ted: EOFException;
</comment><comment author="s1monw" created="2016-06-21T13:41:28Z" id="227442947">&gt; @s1monw And this is what I get when I do not have the translog-21.tlog files touched:

what do you mean by _touched_?
</comment><comment author="SergiuCip" created="2016-06-21T13:43:44Z" id="227443558">@s1monw touch translog-21.tlog
</comment><comment author="s1monw" created="2016-06-21T14:02:17Z" id="227449194">ok there are several things that don't make sense:

 there is no way you get past this:

```
Caused by: java.io.EOFException
at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)
at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)
at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)
at org.elasticsearch.index.translog.Checkpoint.(Checkpoint.java:54)
at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:83)
at org.elasticsearch.index.translog.Translog.readCheckpoint(Translog.java:1930)
at org.elasticsearch.index.translog.Translog.(Translog.java:163)
at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
at org.elasticsearch.index.engine.InternalEngine.(InternalEngine.java:151)
```

if you have the`translog-21.tlog` file or not given the file is really truncated so I wonder what your magic `touch` does. Also the files I got are via email have several `translog-21.tlog` files but they are not empty so there must be more to it than a touch? is it possible to get the original files not the ones you messed around with?
</comment><comment author="uplexPaul" created="2016-06-21T14:06:56Z" id="227450589">@s1monw:
The original .tlog-Files are all lost. As we already said, we only have the ORIGINAL translog.ckp. @SergiuCip just tried to touch the file after elasticsearch was complaining about it couldn't find it. They were not there before. Possibly we lost them - we don't have them - we can't send them to you, sorry.
The files you have in your archive are really only the result of "touch translog-21.tlog"
</comment><comment author="SergiuCip" created="2016-06-21T14:27:16Z" id="227456839">@s1monw Yes @uplexPaul is right, the index translog directory contained only translog.ckp, the rest of the files inside the translog directory were gone. Therefore this is the issue of not being able to recover.

So one of my attempts to 'try' and recover it was to 'touch translog-21.tlog' and see what happens.
</comment><comment author="s1monw" created="2016-06-21T14:28:01Z" id="227457063">&gt; @s1monw Yes @uplexPaul is right, the index translog directory contained only translog.ckp, the rest of the files inside the translog directory were gone. Therefore this is the issue of not being able to recover.

I understand but the translog.ckp files you send me are all fine - they are not broken or anything so I wonder if I got the wrong one?
</comment><comment author="s1monw" created="2016-06-21T14:35:30Z" id="227459453">you posted an exception that runs into `EOFException` and this is on a `translog.ckp` file. I read all of them and they are all good. Also the `translog-21.tlog` contain the translog headers so they must have been created by elasticsearch hence my confusion
</comment><comment author="SergiuCip" created="2016-06-21T14:36:37Z" id="227459820">I have run now the space without the translog-21.tlog files touched and I get this:

Jun 21 14:34:43 log-01.novalocal elasticsearch[20591]: [2016-06-21 14:34:43,085][INFO ][gateway                  ] [Morph] recovered [1] indices into cluster_state
Jun 21 14:34:43 log-01.novalocal elasticsearch[20591]: [2016-06-21 14:34:43,087][INFO ][gateway                  ] [Morph] auto importing dangled indices [logstash-2016.05.12/OPE
[scorn@log-01 0]$ sudo journalctl -eu elasticsearch_scorn
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: ... 5 more
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: Caused by: java.io.EOFException
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Checkpoint.&lt;init&gt;(Checkpoint.java:54)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:83)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Translog.readCheckpoint(Translog.java:1930)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:163)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: ... 11 more
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: [2016-06-21 14:34:44,811][WARN ][cluster.action.shard     ] [Morph] [logstash-2016.05.12][1] received shard failed for targ
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: [logstash-2016.05.12][[logstash-2016.05.12][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: Engin
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at java.lang.Thread.run(Thread.java:745)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: Caused by: [logstash-2016.05.12][[logstash-2016.05.12][1]] EngineCreationFailureException[failed to create engine]; nested:
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: ... 5 more
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: Caused by: java.io.EOFException
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.DataInput.readInt(DataInput.java:101)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.apache.lucene.store.DataInput.readLong(DataInput.java:157)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Checkpoint.&lt;init&gt;(Checkpoint.java:54)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:83)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Translog.readCheckpoint(Translog.java:1930)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:163)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: ... 11 more
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: [2016-06-21 14:34:44,811][WARN ][indices.cluster          ] [Morph] [[logstash-2016.05.12][3]] marking and sending shard fa
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: [logstash-2016.05.12][[logstash-2016.05.12][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: Engin
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
Jun 21 14:34:44 log-01.novalocal elasticsearch[20591]: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
</comment><comment author="SergiuCip" created="2016-06-21T14:37:08Z" id="227459978">@s1monw And the header I have added in order to test it :) was not there at the beginning, I have copied from another index translog file which is working
</comment><comment author="uplexPaul" created="2016-06-21T14:59:49Z" id="227467552">@s1monw Just to exclude human mistakes i re-did the same than @SergiuCip already did. Took one of these indexes which don't want to recover on startup and tar-ed everything as i just tried to start it ~5mins ago. Also included the complete logfile. Sent to you via mail.
</comment><comment author="s1monw" created="2016-06-21T15:13:42Z" id="227472098">thanks guys I am loooking again
</comment><comment author="s1monw" created="2016-06-21T16:58:01Z" id="227503384">@uplexPaul @SergiuCip what filesystem are you guys running on? is this a shared FS by any chance?
</comment><comment author="uplexPaul" created="2016-06-22T09:35:25Z" id="227692479">@s1monw We are not running a shared volume (NFS mounted or smth. like that).

Our elasticsearch is on a VM running on a VMWare ESXi-Host in a vSphere-Environment with storage attached (actually don't know how, but shouldn't matter).

The VM runs under CentOS Linux release 7.1.1503 (Core) with the following kernel version: 3.10.0-229.20.1.el7.x86_64
</comment><comment author="s1monw" created="2016-06-22T12:07:30Z" id="227723491">@uplexPaul @SergiuCip I looked into your files and the `translog.ckp` files are all fine. Now the question is how you got into this state and I don't have an answer to this. 
Yet, I am still puzzled about the content of the `translog-21.tlog` files you send me since you said they where just _touched_. When I touch a file it has 0 bytes but the files I got from you have ~33 bytes which are actually ~33 valid bytes (the header). Do you have a good answer how they showed up half written?
</comment><comment author="uplexPaul" created="2016-06-22T12:34:25Z" id="227729150">@s1monw Yes i have an answer. @SergiuCip just catted some other translog files and copied the stuff there. Just forget these files, please - we created them and they were not created by elasticsearch.

Its clear for me that you are searching for how this issue could have happened. But shoudln't you also address a fix for the behaviour of the current startup? Shouldn't it be able to correctly startup? And - when you say the translog.ckp-Files are fine. Why is elasticsearch trying to read from "translog-21.tlog" which is apparently not there? Why it's not just skipping this translog recovery. We would really love to access the data anyway and that should be the general goal, isn't it?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>One similarity per index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18971</link><project id="" key="" /><description>Currently you can mix BM25 and the classic similarity in the same index:

```
{
  "book" : {
    "properties" : {
      "title" : { "type" : "text", "similarity" : "BM25" },
      "text" : { "type" : "text", "similarity" : "classic" }
    }
}
```

... this can be buggy since the base similarity cannot be picked properly. The base similarity is used to compute a factor for the coordinates and the query norm (cf Similarity.coord and Similarity.queryNorm).
It is a per query setting that can change the score completely. For `BM25` the coord and the queryNorm are disabled (returns 1.0f) whereas they are important factors in the `classic` similarity. To resolve this we have two settings for the index similarity `index.similarity.default.type` and `index.similarity.base.type`.
In order to make the things easier we could have a single setting:
`index.similarity.type`
... that is dynamically updatable if the similarities are compatible: 
https://github.com/elastic/elasticsearch/issues/6727
This would not make the "are compatible?" easier but this would prevent the user to shoot themselves in the foot.
</description><key id="161160395">18971</key><summary>One similarity per index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Settings</label><label>breaking</label></labels><created>2016-06-20T10:07:03Z</created><updated>2017-06-07T19:51:37Z</updated><resolved>2017-06-07T19:51:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-20T15:21:15Z" id="227174674">I agree that we should combine those two settings into one, but am unsure about the name.  Don't forget that there are other configuration options which can appear under [`index.similarity`](https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules-similarity.html#configuration).  Perhaps we should just call it `index.similarity.default.type`.

Btw in master, if I do:

```
PUT t/t/1
{"text": "foo"}

GET t/_mapping/field/text?include_defaults
```

I get:

```
          "similarity": "classic",
```

so something even more is wrong...  Is it the get-field-mapping API or the default similarity?
</comment><comment author="clintongormley" created="2016-06-20T15:29:31Z" id="227177073">Ah I see you're on it already: https://github.com/elastic/elasticsearch/pull/18948
</comment><comment author="clintongormley" created="2016-06-20T15:54:56Z" id="227184868">@jimferenczi re uniting those two settings i'm having second thoughts...  The only reason for having `.base.type` is to support TF/IDF for bwc.  But query norm and coord are going to be removed in Lucene (see https://issues.apache.org/jira/browse/LUCENE-7347)

We shouldn't encourage people to use TF/IDF by default for new indices, just for bwc for queries on old indices.  The  `.base.type` setting can be removed in ES 6.0, so perhaps we should just keep it as it is today, ie two separate settings, but set to bm25 by default.

Note: similarities can be updated on closed indices, so people can move entirely to BM25 even with existing indices.
</comment><comment author="jpountz" created="2016-06-20T15:58:40Z" id="227186021">This is only under discussion for now and it might be a bit controversial, but indeed it would have the benefit of removing the need for a `base` similarity.
</comment><comment author="rmuir" created="2016-06-21T15:43:54Z" id="227481806">+1 to move away from queryNorm and coord, even if they have to hang around lucene for some reason.

Really: this is 1980s type stuff :)
</comment><comment author="jimczi" created="2016-07-01T12:21:03Z" id="229932903">Discussed in FixItFriday. One option could be to remove the support for the `classic` similarity in ES. BM25 should be considered as a full replacement for TFIDF similarity. Since the classic similarity is the only user of the queryNorm, coord API we could completely remove the `base` similarity if `classic` is removed.
</comment><comment author="rmuir" created="2016-07-01T12:29:34Z" id="229934541">&gt; Discussed in FixItFriday. One option could be to remove the support for the classic similarity in ES. BM25 should be considered as a full replacement for TFIDF similarity.

I'd be +1 to this. Here is why. users of `classic` will be unhappy with Lucene 6 regardless of what we do: see https://issues.apache.org/jira/browse/LUCENE-6711

This is a great improvement to all similarities, including `classic`. But it means, there is literally zero "value" backwards-compatibility-wise, in providing `classic` to old users. If they have _any sparsity at all_, not just scores, but also relative ranking of documents will change (for the better). 

So why keep `classic` around at all?
</comment><comment author="jimczi" created="2017-06-07T19:51:37Z" id="306905791">Duplicate of #23208 </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch always true option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18970</link><project id="" key="" /><description>Due to a major [shortcoming ](http://stackoverflow.com/questions/6114435/in-mustache-templating-is-there-an-elegant-way-of-expressing-a-comma-seperated-l)with the mustache templating language I have found that it very difficult to query based on an array passed as a parameter as there is a known issue with a trailing comma.

To cut a long story short, a more elegant solution that awkwardly manipulating the input data would be possible if an always true option existed within elastic

```
{
"bool" : {
         "must" : [
             {
                 "term" : "myInputtedTerm1" 
             },
             {
                  "term" : "myInputtedTerm2"
             },            
                 alwaysTrue         
        ]
    }
}
```

Digging around I can't see anything like this that exists so I do believe it is worth implementing. Or if somebody has a workaround, please let me know.

Cheers
</description><key id="161157967">18970</key><summary>ElasticSearch always true option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jack-Collins</reporter><labels><label>:Search Templates</label><label>discuss</label></labels><created>2016-06-20T09:54:00Z</created><updated>2017-02-28T16:29:48Z</updated><resolved>2016-06-29T07:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-20T14:48:27Z" id="227164489">@tlrx would your toJSON change help here?
</comment><comment author="tlrx" created="2016-06-20T15:33:54Z" id="227178418">The trailing comma is a well known issue and we'll provide a workaround for that. It should help in this case, yes.

@Jack-Collins How your parameters look like for the sample you gave?
</comment><comment author="Jack-Collins" created="2016-06-20T15:37:55Z" id="227179629">the param is just an array: ["myInputtedTerm1", "myInputtedTerm2"]
</comment><comment author="tlrx" created="2016-06-22T15:08:09Z" id="227774461">@Jack-Collins Thanks.

The pull request #18856 might help you. It allows something like:

``` json
{
    "inline": "{\"query\":{\"bool\":{\"must\": {{#toJson}}clauses{{/toJson}} }}}",
    "params": {
        "clauses": [
            { "term": "foo" },
            { "term": "bar" }
        ]
   }
}
```

to be rendered as:

``` json
{
    "query" : {
      "bool" : {
        "must" : [
          {
            "term" : "foo"
          },
          {
            "term" : "bar"
          }
        ]
      }
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/CustomMustacheFactory.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/NoneEscapingMustacheFactory.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java</file></files><comments><comment>Mustache: Add util functions to render JSON and join array values</comment></comments></commit></commits></item><item><title>Remove a bunch of unneeded Modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18969</link><project id="" key="" /><description>This PR removes a bunch of unneeded modules that just add unnecessary classes to the system. They are not extension points nor do they serve any structural purpose. This change also removes the VersionModule and all ability to specify a node version on startup which pretended to be able to act like
a different version node which is simply not true.
</description><key id="161155492">18969</key><summary>Remove a bunch of unneeded Modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-20T09:41:18Z</created><updated>2016-06-20T10:15:29Z</updated><resolved>2016-06-20T10:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-20T09:56:31Z" id="227099492">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/main/TransportMainAction.java</file><file>core/src/main/java/org/elasticsearch/client/node/NodeClientModule.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/env/EnvironmentModule.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironmentModule.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayModule.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsModule.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeClientNode.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeModule.java</file><file>core/src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>core/src/main/java/org/elasticsearch/watcher/ResourceWatcherModule.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TaskManagerTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java</file><file>core/src/test/java/org/elasticsearch/action/main/MainActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeServiceTests.java</file><file>core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingDisabledTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>core/src/test/java/org/elasticsearch/transport/NettyTransportServiceHandshakeTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TransportModuleTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryTests.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java</file><file>plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureRepositoryF.java</file><file>test/framework/src/main/java/org/elasticsearch/node/MockNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Merge pull request #18969 from s1monw/remove_version_module</comment></comments></commit></commits></item><item><title>Update configuration.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18968</link><project id="" key="" /><description>It seems that there are some lines has been placed the wrong place.
</description><key id="161147758">18968</key><summary>Update configuration.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ScsUndefined</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-06-20T09:00:33Z</created><updated>2016-06-21T09:58:38Z</updated><resolved>2016-06-21T09:58:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ScsUndefined" created="2016-06-20T09:01:19Z" id="227087096">It seems that there are some lines has been placed the wrong place.
</comment><comment author="ScsUndefined" created="2016-06-20T09:01:49Z" id="227087223">It seems that there are some lines has been placed the wrong place.
</comment><comment author="clintongormley" created="2016-06-20T14:46:50Z" id="227163958">Hi @ScsUndefined 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/

Note: the email address you use to sign the CLA must also be present on your github account
</comment><comment author="ScsUndefined" created="2016-06-21T08:15:05Z" id="227371300">all right . wait a moment.
</comment><comment author="ScsUndefined" created="2016-06-21T08:22:09Z" id="227372778">I have signed.
</comment><comment author="clintongormley" created="2016-06-21T09:58:38Z" id="227394986">thanks @ScsUndefined - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update configuration.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18967</link><project id="" key="" /><description>It seems that there are some lines has been placed the wrong place.
</description><key id="161144556">18967</key><summary>Update configuration.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ScsUndefined</reporter><labels /><created>2016-06-20T08:44:48Z</created><updated>2016-06-20T08:58:35Z</updated><resolved>2016-06-20T08:58:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>save base64 file ------mapper-attachments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18966</link><project id="" key="" /><description>Hi:
## some problems as follows:

**Elasticsearch version**: 2.2.1

**JVM version**: jdk1.8

**OS version**:  windows 7

**Description of the problem including expected versus actual behavior**:
1.  I  install  plugin:  mapper-attachments

after it,  reboot   Elasticsearch
1.  create a mapping
   mapping = XContentFactory.jsonBuilder().startObject()
   
   ```
               // 索引库名（类似数据库中的表）
               .startObject(indexType).startObject("properties")
                       // ID
               .startObject("id").field("type", "string").endObject()
   
               .startObject("logtime").field("type", "string").endObject()
   
                       // 标题  中文分词, field("analyzer", "ik")
               .startObject("title").field("type", "string").field("analyzer", "ik").field("include_in_all", false)
               .endObject()
   
                       // 描述
               .startObject("atta").## field("type", "attachment")/*.field("analyzer", "ik")*/
               .endObject()
   
               .endObject().endObject().endObject();
   ```
2.  then  I test it.   using  firefox  RestClient

http://localhost:9200/op/atta/1

{
"id":"at001",
"logtime":'2016-06-20 10:30:00",
"title":"上传附件测试",
"attach":"6IOM5pmv6Imy5piv57qi6Imy55qELOiuoeeul+acuuaKgOacr+eahOWPkeWxlSzlhZrnmoQxM+Wkp+W3sue7j+WPrOW8gC5ramxpbmvpnIDopoHmi5vogZjmlrDlkZjlt6XkuoblkJc/Cg=="

}

and the result is ,

{"error":{"root_cause":[{"type":"mapper_parsing_exception","reason":"failed to parse"}],"type":"mapper_parsing_exception","reason":"failed to parse","caused_by":{"type":"json_parse_exception","reason":"Unexpected character (''' (code 39)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n at [Source: org.elasticsearch.common.io.stream.InputStreamStreamInput@37def43e; line: 3, column: 12]"}},"status":400}

so  what's wrong ???
</description><key id="161136115">18966</key><summary>save base64 file ------mapper-attachments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzhou2020</reporter><labels /><created>2016-06-20T07:54:44Z</created><updated>2016-06-20T10:29:54Z</updated><resolved>2016-06-20T08:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-20T08:36:58Z" id="227081819">Please ask your questions on discuss.elastic.co where we can definitely help you better.

Also the code you provided is incorrect and you probably modify it for this ticket. See `##` for example.
Would be better to GET the actual mapping you have with your RestClient and copy in discuss its JSON form.
</comment><comment author="xuzhou2020" created="2016-06-20T08:45:21Z" id="227083592">1. discuss.elastic.co    register???  git  account cannot log in.

## 2. here is the java code..

package demo;

import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;
import tool.ESTool;

import java.io.IOException;

/**
- Created by Administrator on 2016/6/8.
  */
  public class BaiduClient {
  
  public static void main(String[] args) throws Exception {
  
  /\*  //数据库名字 db
      String index = "op";
  
  ```
  //表的名字 table
  String type = "atta";
  
  //生成的create table 语句
  XContentBuilder mapping = createMapping(type);
  
  ESTool esTool = new ESTool();
  esTool.setIp("127.0.0.1").setIndexName(index).setIndexType(type);
  
  //建立表(create table )
  esTool.createIndex(index, type, mapping);
  
  //添加数据
  //        URL resource = Object.class.getResource("/config/data");
  //        System.out.println(resource);
  //        Files.lines(Paths.get(resource.toURI()))
  //                .forEach(txt -&gt; {
  //
  //
  //                    System.out.println(txt);
  //                    esTool.save( BaseUtil.uuid(),txt);
  //                });
  
  String id = "10001";
  
  //      String jsonData=  {"id":"1004","logtime":"2014-12-24 09:20:30","title":"浅析理论资源与文学理论体系建构的危机","desc":"经多年积累，文艺学学科建设进入到一个新的发展阶段，其显在的表征在于部分文学理论家在体系建构方面更趋自觉。有学者就认为，在反本质主义的视域中，新世纪有三种理论体系尤..."}
  String jsonData = "上面的json字符串";
  
  //新增数据
  esTool.save(id, jsonData);
  
  //修改
  esTool.update(id, jsonData);
  
  //删除
  esTool.deleteById(id);
  
  //查询
  String keyword = "文学";
  //第一页
  int page = 1;
  
  //每页显示15行
  int rows = 15;
  esTool.query(keyword, page, rows);
  ```
  
  */
  
  ```
  //数据库名字 db
  String index = "op";
  
  //表的名字 table
  String type = "atta";
  
  //生成的create table 语句
  XContentBuilder mapping = createAttaMapping(type);
  
  
  ESTool esTool = new ESTool();
  esTool.setIp("127.0.0.1").setIndexName(index).setIndexType(type);
  
  //建立表(create table )
  esTool.createIndex(index, type, mapping);
  
  //添加数据
  //        URL resource = Object.class.getResource("/config/data");
  //        System.out.println(resource);
  //        Files.lines(Paths.get(resource.toURI()))
  //                .forEach(txt -&gt; {
  //
  //
  //                    System.out.println(txt);
  //                    esTool.save( BaseUtil.uuid(),txt);
  //                });
  ```
  
  //
      String id = "10001";
  //
  //      //      String jsonData=  {"id":"1004","logtime":"2014-12-24 09:20:30","title":"浅析理论资源与文学理论体系建构的危机","desc":"经多年积累，文艺学学科建设进入到一个新的发展阶段，其显在的表征在于部分文学理论家在体系建构方面更趋自觉。有学者就认为，在反本质主义的视域中，新世纪有三种理论体系尤..."}
  
  ```
  String jsonData ="{\"id\":\"at001\", \"logtime\":'2016-06-20 10:30:00\", \"title\":\"上传附件测试\", \"attach\":\"6IOM5pmv6Imy5piv57qi6Imy55qELOiuoeeul+acuuaKgOacr+eahOWPkeWxlSzlhZrnmoQxM+Wkp+W3sue7j+WPrOW8gC5ramxpbmvpnIDopoHmi5vogZjmlrDlkZjlt6XkuoblkJc/Cg==\"}";
  ```
  
  //      String jsonData = "上面的json字符串";
  
  ```
  jsonData="{\"id\":\"at001\", \"logtime\":'2016-06-20 10:30:00\", \"title\":\"上传附件测试\", \"attach\":\"6IOM5pmv6Imy5piv57qi6Imy55qELOiuoeeul+acuuaKgOacr+eahOWPkeWxlSzlhZrnmoQxM+Wkp+W3sue7j+WPrOW8gC5ramxpbmvpnIDopoHmi5vogZjmlrDlkZjlt6XkuoblkJc/Cg==\"}";
  ```
  
  //
  //      //新增数据
      esTool.save(id, jsonData);
  //
  //      //修改
  //      esTool.update(id, jsonData);
  //
  //      //删除
  //      esTool.deleteById(id);
  //
  //      //查询
  //      String keyword = "文学";
  //      //第一页
  //      int page = 1;
  //
  //      //每页显示15行
  //      int rows = 15;
  //      esTool.query(keyword, page, rows);
  
  }
  
  /**
  - 创建 建表的语句  create table ()...
    *
  - @param indexType 表名
  - @return
    */
    public static XContentBuilder createMapping(String indexType) {
    XContentBuilder mapping = null;
    try {
        mapping = XContentFactory.jsonBuilder().startObject()
    
    ```
            // 索引库名（类似数据库中的表）
            .startObject(indexType).startObject("properties")
            // ID
            .startObject("id").field("type", "string").endObject()
    
            .startObject("logtime").field("type", "string").endObject()
    
            // 标题  中文分词, field("analyzer", "ik")
            .startObject("title").field("type", "string").field("analyzer", "ik").field("include_in_all", false)
            .endObject()
    
            // 描述
            .startObject("desc").field("type", "string").field("analyzer", "ik").field("include_in_all", false)
            .endObject()
    
    
    
            .endObject().endObject().endObject();
    ```
    
    } catch (IOException e) {
        e.printStackTrace();
    }
    return mapping;
    }
  
  /**
  - 测试附件
  - @param indexType
  - @return
    */
    public static XContentBuilder createAttaMapping(String indexType) {
    XContentBuilder mapping = null;
    try {
        mapping = XContentFactory.jsonBuilder().startObject()
    
    ```
            // 索引库名（类似数据库中的表）
            .startObject(indexType).startObject("properties")
                    // ID
            .startObject("id").field("type", "string").endObject()
    
            .startObject("logtime").field("type", "string").endObject()
    
                    // 标题  中文分词, field("analyzer", "ik")
            .startObject("title").field("type", "string").field("analyzer", "ik").field("include_in_all", false)
            .endObject()
    
                    // 描述
            .startObject("atta").field("type", "attachment")/*.field("analyzer", "ik")*/
            .endObject()
    
            .endObject().endObject().endObject();
    ```
    
    } catch (IOException e) {
        e.printStackTrace();
    }
    return mapping;
    }

}

package tool;

import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
import org.elasticsearch.action.index.IndexRequestBuilder;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.search.SearchType;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.Requests;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.text.Text;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.index.query.MultiMatchQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.SearchHits;
import org.elasticsearch.search.highlight.HighlightField;
import org.elasticsearch.search.sort.SortOrder;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.Map;

/**
- Created by Administrator on 2016/6/8.
  */
  public class ESTool {
  
  //elastic服务器ip
  private String ip="127.0.0.1";
  
  //数据库db
  private String indexName;
  
  //表 table
  private String indexType;
  
  public String getIp() {
      return ip;
  }
  
  public ESTool setIp(String ip) {
      this.ip = ip;
      return this;
  }
  
  public String getIndexName() {
      return indexName;
  }
  
  public ESTool setIndexName(String indexName) {
      this.indexName = indexName;
      return this;
  }
  
  public String getIndexType() {
      return indexType;
  }
  
  public ESTool setIndexType(String indexType) {
      this.indexType = indexType;
      return this;
  }
  
  /**
  - 创建一个 客户端连接
  - @return
    */
    public static Client esClient(){
    
    Client esClient = null;
    try {

//            esClient = new TransportClient().addTransportAddress(new InetSocketTransportAddress("127.0.0.1", 9300));
//

//            esClient = TransportClient.builder().build()
//                    .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByAddress(new byte[]{(byte)127,(byte)0,(byte)0,(byte)1}), 9300));

```
        esClient = TransportClient.builder().build()
                .addTransportAddress(new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 9300)));
    } catch (Exception e) {
        e.printStackTrace();
    }

    return esClient;
}



// 创建索引 database/table
// 第一次 加载的时候,创建

/**
 *
 * @param indexName   db
 * @param indexType   table
 * @throws java.io.IOException
 */
public static void createIndex(String indexName, String indexType,XContentBuilder mapping) throws IOException {
    Client esClient = ESTool.esClient();
```

//        System.out.println("mapping:" + mapping.string());

```
    // 创建一个空索引  database
    try {
        esClient.admin().indices().prepareCreate(indexName).execute().actionGet();
    }catch (Exception e){
        e.printStackTrace();
    }

    //表结构  执行
    try {
        PutMappingRequest putMapping = Requests.putMappingRequest(indexName).type(indexType).source(mapping);
        PutMappingResponse response = esClient.admin().indices().putMapping(putMapping).actionGet();

        if (!response.isAcknowledged()) {
            System.out.println("Could not define mapping for type [" + indexName + "]/[" + indexType + "].");
        } else {
            System.out.println("Mapping definition for [" + indexName + "]/[" + indexType + "] succesfully created.");
        }
    }catch (Exception e){
        e.printStackTrace();
    }

    esClient.close();
}



/**
 * 添加一行数据
 * @param id
 * @param jsonData
 */
public  void save(String id,String jsonData){

    Client client=ESTool.esClient();

    IndexRequestBuilder requestBuilder = client.prepareIndex(indexName, indexType,id).setRefresh(true);

    /*IndexResponse indexResponse = */requestBuilder.setSource(jsonData).execute().actionGet();

    client.close();
}


/**
 * 更新一行数据
 * @param id
 * @param jsonData
 */
public  void update(String id,String jsonData){

    Client client=ESTool.esClient();

    //更新数据
    client.prepareUpdate().setIndex(indexName).setType(indexType).setId(id).setDoc(jsonData).get();

    client.close();
}



/**
 * 删除一行数据 byId
 * @param id
 */
public  void deleteById(String id){

    Client client=ESTool.esClient();

    //delete数据
    client.prepareDelete().setIndex(indexName).setType(indexType).setId(id).get();
    client.close();

}


/**
 * 根据关键字查询
 * @param keyword  关键字
 * @param page     第几页, 1,2,3
 * @param rows     每页显示多少行
 */
public  void query(String keyword,int page, int rows){



    Client esClient = ESTool.esClient();

    MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(
            keyword,     // Text you are looking for
            //kimchy elasticsearch是要查询的字符串
            "title"/*, "desc"*/           // Fields you query on
            //user 和 message都是field
    );

    SearchResponse searchResponse = esClient
            .prepareSearch(indexName)
            .setTypes(indexType)

            //设置高亮显示字段
            .addHighlightedField("title")

            //高亮显示标识
            .setHighlighterPreTags("&lt;em&gt;")
            .setHighlighterPostTags("&lt;/em&gt;")

            .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
            .setQuery(multiMatchQueryBuilder)

            //排序字段
            .addSort("logtime", SortOrder.DESC)
            .setFrom(page-1)
            .setSize(rows)
            .execute().actionGet();

    SearchHits hits = searchResponse.getHits();

    long total = hits.totalHits();
    System.out.println("total="+total);

    if(total&gt;0){

        SearchHit[] searchHists = hits.getHits();

        for (SearchHit hit : searchHists) {

            Map&lt;String, Object&gt; source = hit.getSource();
            System.out.println(source);

            // 获取对应的高亮域
            Map&lt;String, HighlightField&gt; result = hit.highlightFields();

            // 从设定的高亮域中取得指定域
            HighlightField titleField = result.get("title");

            if(titleField!=null){

                // 取得定义的高亮标签
                Text[] fragments = titleField.fragments();

                // 为title串值增加自定义的高亮标签
                String title = "";
                for (Text text : fragments) {
                    title += text;
                }
                // 将追加了高亮标签的串值重新填充到对应的对象
                // product.setTitle(title);
                // 打印高亮标签追加完成后的实体对象
                System.out.println(title);
            }


        }

    }



}
```

}
</comment><comment author="dadoonet" created="2016-06-20T08:50:07Z" id="227084577">Please join https://discuss.elastic.co/ 
</comment><comment author="xuzhou2020" created="2016-06-20T10:02:42Z" id="227100872">Hi:
    I ask a question, but  it seemed that nobody pays attention to it .
    https://discuss.elastic.co/
</comment><comment author="dadoonet" created="2016-06-20T10:29:54Z" id="227106375">You need to be patient on discuss. 

Read:
- https://discuss.elastic.co/t/notes-on-using-these-forums/118
- https://www.elastic.co/fr/community/codeofconduct

If you need to have "immediate" answers with a SLA we can connect you to the sales team.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't ignore custom sharding settings in create index request for `.scripts` index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18965</link><project id="" key="" /><description>PR for #18912
</description><key id="161132407">18965</key><summary>Don't ignore custom sharding settings in create index request for `.scripts` index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>review</label><label>v2.4.0</label></labels><created>2016-06-20T07:29:18Z</created><updated>2016-08-26T13:24:23Z</updated><resolved>2016-06-21T06:31:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-20T13:35:04Z" id="227143349">Found a typo but otherwise LGTM
</comment><comment author="martijnvg" created="2016-06-21T06:30:13Z" id="227351932">thx @nik9000!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ReplicaAfterPrimaryActiveAllocationDecider prevent shard promotion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18964</link><project id="" key="" /><description>I have a three node ES 2.3.3 cluster with an index `myindex` consisting of one shard an 2 replicas and a few document in it. If i should down all nodes and then start only one of them the  `myindex`status is red with all shards unassigned. If i now change the replica count of  `myindex` to 1 or 0 the index becomes green. 

Steps to reproduce:
- Start one ES node (Version 2.3.3)
- Create a new empty index `myindex` with one shard and two (or more) replicas
- Shutdown ES
- Start ES, cluster is red and you will see logs like that:

&lt;pre&gt;
[2016-06-19 22:30:34,519][TRACE][gateway                  ] [Ent] found state file: [id:7, legacy:false, file:/Users/temp/alloc-test/elasticsearch-2.3.3-2/data/altest_12771/nodes/0/indices/myindex/_state/state-7.st]
[2016-06-19 22:30:34,545][TRACE][gateway                  ] [Ent] found state file: [id:7, legacy:false, file:/Users/temp/alloc-test/elasticsearch-2.3.3-2/data/altest_12771/nodes/0/indices/myindex/_state/state-7.st]
[2016-06-19 22:30:37,674][TRACE][gateway                  ] [Ent] found state file: [id:7, legacy:false, file:/Users/temp/alloc-test/elasticsearch-2.3.3-2/data/altest_12771/nodes/0/indices/myindex/_state/state-7.st]
[2016-06-19 22:30:37,713][TRACE][gateway                  ] [Ent] [myindex][0] fetching [shard_started] from [uQ85ko_PTaudfHrjdj00vw]
[2016-06-19 22:30:37,713][TRACE][gateway                  ] [Ent] [myindex][0] loading local shard state info
[2016-06-19 22:30:37,713][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]: ignoring allocation, still fetching shard started state
[2016-06-19 22:30:37,714][TRACE][gateway                  ] [Ent] found state file: [id:1, legacy:false, file:/Users/temp/alloc-test/elasticsearch-2.3.3-2/data/altest_12771/nodes/0/indices/myindex/0/_state/state-1.st]
[2016-06-19 22:30:37,715][DEBUG][gateway                  ] [Ent] [myindex][0] shard state info found: [version [4], primary [false]]
[2016-06-19 22:30:37,715][TRACE][gateway                  ] [Ent] [myindex][0] processing fetched [shard_started] results
[2016-06-19 22:30:37,716][TRACE][gateway                  ] [Ent] [myindex][0] marking uQ85ko_PTaudfHrjdj00vw as done for [shard_started]
[2016-06-19 22:30:37,716][TRACE][gateway                  ] [Ent] [myindex][0] scheduling reroute for post_response
[2016-06-19 22:30:37,719][TRACE][cluster.routing.allocation.decider] [Ent] Can not allocate [[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]] on node [uQ85ko_PTaudfHrjdj00vw] due to [ReplicaAfterPrimaryActiveAllocationDecider]
[2016-06-19 22:30:37,719][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]: ignoring allocation, can't be allocated on any node
[2016-06-19 22:30:37,719][TRACE][cluster.routing.allocation.decider] [Ent] Can not allocate [[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]] on node [uQ85ko_PTaudfHrjdj00vw] due to [ReplicaAfterPrimaryActiveAllocationDecider]
[2016-06-19 22:30:37,719][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]: ignoring allocation, can't be allocated on any node
-- index [myindex]
----shard_id [myindex][0]
--------[myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
--------[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
--------[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
--------[myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
--------[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
--------[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]
[2016-06-19 22:30:37,734][TRACE][gateway                  ] [Ent] [myindex] writing state, reason [freshly created]
[2016-06-19 22:30:37,754][TRACE][gateway                  ] [Ent] [[myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]] on node [{Ent}{uQ85ko_PTaudfHrjdj00vw}{127.0.0.1}{127.0.0.1:9300}] has version [4] of shard
[2016-06-19 22:30:37,754][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]] candidates for allocation: [[Ent] -&gt; 4, ]
[2016-06-19 22:30:37,754][DEBUG][gateway                  ] [Ent] [myindex][0] found 1 allocations of [myindex][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]], highest version: [4]
[2016-06-19 22:30:37,754][DEBUG][gateway                  ] [Ent] [myindex][0]: not allocating, number_of_allocated_shards_found [1]
[2016-06-19 22:30:37,757][TRACE][cluster.routing.allocation.decider] [Ent] Can not allocate [[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]] on node [uQ85ko_PTaudfHrjdj00vw] due to [ReplicaAfterPrimaryActiveAllocationDecider]
[2016-06-19 22:30:37,757][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]: ignoring allocation, can't be allocated on any node
[2016-06-19 22:30:37,757][TRACE][cluster.routing.allocation.decider] [Ent] Can not allocate [[myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]] on node [uQ85ko_PTaudfHrjdj00vw] due to [ReplicaAfterPrimaryActiveAllocationDecider]
[2016-06-19 22:30:37,757][TRACE][gateway                  ] [Ent] [myindex][0], node[null], [R], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-06-19T20:30:37.687Z]]: ignoring allocation, can't be allocated on any node
&lt;/pre&gt;


Environment:
- Elasticsearch 2.3.3
- OS: Mac
- JVM: 1.8.0_45 (Oracle)
</description><key id="161091753">18964</key><summary>ReplicaAfterPrimaryActiveAllocationDecider prevent shard promotion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">floragunncom</reporter><labels /><created>2016-06-19T20:51:57Z</created><updated>2016-06-20T07:08:30Z</updated><resolved>2016-06-20T07:08:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-20T07:08:30Z" id="227065514">The setting that is preventing your cluster from allocating the index is `index.recovery.initial_shards` (see https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index-modules.html#index.recovery.initial_shards ). With 3 shard copies it requires 2 of them to be present before allocating the copy with the highest version as primary (see also https://discuss.elastic.co/t/index-recovery-initial-shards/5999/2 ). It is essentially a safeguard that tries to ensure that a stale shard copy does not become primary, resulting in data loss. It is not perfectly safe (see #14671), and we have taken steps in v5.0.0 to make this setting obsolete (The system now tracks non-stale copies, see #14739).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw if the local node is not set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18963</link><project id="" key="" /><description>This is a possible fix for #18962 

I didn't find a good way to add test code as all the TestCluster infrastructure is taking care of really starting the Node.
</description><key id="161087738">18963</key><summary>Throw if the local node is not set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-19T19:26:39Z</created><updated>2016-06-19T22:25:33Z</updated><resolved>2016-06-19T21:25:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-19T20:12:34Z" id="227017814">Thx @fhopf - how about throwing an `IllegalStateException`  in ClusterService if someone asks for localNode and it's still null?  message can be something like "local node is not yet set, did you start the node?"
</comment><comment author="fhopf" created="2016-06-19T20:21:23Z" id="227018190">@bleskes Sounds good for me as well. But wouldn't this change the behavior too much, some users might rely on returning null? Might break some code during runtime if an Exception is thrown instead.
</comment><comment author="bleskes" created="2016-06-19T20:24:10Z" id="227018330">I don't think anyone reckons on local node being null. Throwing and exception there seems like the right thing regardless of your issue. If the tests run and pass, I think we're good.

On 19 jun. 2016 10:21 PM +0200, Florian Hopfnotifications@github.com, wrote:

&gt; @bleskes(https://github.com/bleskes)Sounds good for me as well. But wouldn't this change the behavior too much, some users might rely on returning null? Might break some code during runtime if an Exception is thrown instead.
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly,view it on GitHub(https://github.com/elastic/elasticsearch/pull/18963#issuecomment-227018190), ormute the thread(https://github.com/notifications/unsubscribe/AA9bJ05Ig6debpnmp6OnzNbc1IIAZN3Yks5qNaTQgaJpZM4I5O0_).
</comment><comment author="fhopf" created="2016-06-19T20:59:44Z" id="227019971">Updated to check for localNode in ClusterService instead
</comment><comment author="jasontedor" created="2016-06-19T21:25:35Z" id="227021207">Thanks @fhopf; I ran the tests and they passed so we are good here.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file></files><comments><comment>Throw if the local node is not set</comment></comments></commit></commits></item><item><title>NullPointerException when indexing in embedded node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18962</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3/5.0.0-alpha3

**JVM version**: 1.8.0_91

**OS version**: Linux

**Description of the problem including expected versus actual behavior**: When creating a new embedded Node (in my case for Testing) I constantly forget to start the Node. This will lead to a NullPointerException in TransportMasterNodeAction because no DiscoveryNode can be found.
It would be nice if there is a more descriptive error message hinting at starting the node.

**Steps to reproduce**:

```
    public void testNullPointerException() {
        String tmpDirPath = System.getProperty("java.io.tmpdir");
        File elasticsearchHome = new File(tmpDirPath, "foo");
        elasticsearchHome.mkdir();
        Settings.Builder settingsBuilder = Settings.builder().put("path.home", elasticsearchHome.getAbsolutePath()).put("cluster.name", "clustername");
        Node elasticsearchNode = new Node(settingsBuilder.build());
        Client elasticsearchClient = elasticsearchNode.client();
        IndexResponse response = elasticsearchClient.prepareIndex("foo", "doc").setSource("foo", "bar").execute().actionGet();

    }
```

**Provide logs (if relevant)**:

```
java.lang.NullPointerException
    at __randomizedtesting.SeedInfo.seed([8BE59430520DB61D:7DFC028A35C0343E]:0)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.&lt;init&gt;(TransportMasterNodeAction.java:124)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:102)
    at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:52)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:178)
    at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:80)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:176)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:149)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:97)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:63)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:178)
    at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:66)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:176)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:149)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:87)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:64)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:80)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:54)
```
</description><key id="161086254">18962</key><summary>NullPointerException when indexing in embedded node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels /><created>2016-06-19T18:50:06Z</created><updated>2016-06-19T22:39:57Z</updated><resolved>2016-06-19T22:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>painless doc examples could be tested better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18961</link><project id="" key="" /><description>I want to revamp this doc, i really do. But right now the examples in there aren't really being tested. I can put garbage methods in scripts and nothing fails. Perhaps its only compiling them and not actually running them against any documents, I don't know...
</description><key id="161085975">18961</key><summary>painless doc examples could be tested better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>docs</label><label>test</label></labels><created>2016-06-19T18:43:19Z</created><updated>2016-06-21T13:40:35Z</updated><resolved>2016-06-21T13:34:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-19T21:27:12Z" id="227021288">It depends....  I can have a look at this on Monday probably.
On Jun 19, 2016 2:43 PM, "Robert Muir" notifications@github.com wrote:

&gt; I want to revamp this doc, i really do. But right now the examples in
&gt; there aren't really being tested. I can put garbage methods in scripts and
&gt; nothing fails. Perhaps its only compiling them and not actually running
&gt; them against any documents, I don't know...
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18961, or mute the
&gt; thread
&gt; https://github.com/notifications/unsubscribe/AANLooD47dTKWuHY_KDuEshfsNFNLEk7ks5qNY3MgaJpZM4I5OUr
&gt; .
</comment><comment author="rmuir" created="2016-06-19T22:03:12Z" id="227023089">Thanks... It may be a simple bug in the examples themselves. Generated yaml seemed ok but no expert... I just have the feeling no actual docs are being hit
</comment><comment author="rmuir" created="2016-06-20T10:55:13Z" id="227111263">here is an example of what i'm trying that isn't provoking any fail. Of course this will compile, but if it were actually to run on any document it would crap out:

```
--- a/docs/reference/modules/scripting/painless.asciidoc
+++ b/docs/reference/modules/scripting/painless.asciidoc
@@ -109,7 +109,7 @@ GET hockey/_search
     "total_goals": {
       "script": {
         "lang": "painless",
-        "inline": "int total = 0; for (int i = 0; i &lt; doc['goals'].length; ++i) { total += doc['goals'][i]; } return total;"
+        "inline": "doc['goals'].bogusMethod()"
       }
     }
   }
```

Since we have a lot of dynamic functionality i want it to catch me if i screw up. Part of the problem I think is that our examples are too lenient? I imagine having a lot of these to explain various syntax and stuff like that... Is there another way to encode checks other than `//TESTRESPONSE` ? Or another way to put it, a way to do `//TESTRESPONSE` but without it being user-visible in the docs?
</comment><comment author="clintongormley" created="2016-06-20T15:40:25Z" id="227180400">&gt; Or another way to put it, a way to do //TESTRESPONSE but without it being user-visible in the docs?

Yes, use comment blocks:

```
////////////////////
None of this is visible:

[source,js]
---------------
PUT foo/bar/1
{ "foo": 5 }
---------------
// TEST
////////////////////

This example is visible:

[source,js]
---------------
GET foo/_search
{
  "script_fields": {
    "my_field": {
      "script": {
        "lang": "painless",
        "inline": "doc['foo'].value+1"
      }
    }
  }
}
---------------
// CONSOLE
// TEST[continued]

////////////////////
The response can be hidden as well

[source,js]
---------------
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "foo",
        "_type": "bar",
        "_id": "1",
        "_score": 1,
        "fields": {
          "my_field": [
            6
          ]
        }
      }
    ]
  }
}
---------------
// TESTRESPONSE[s/"took".*/"took": "$body.took",/]
////////////////////
```
</comment><comment author="rmuir" created="2016-06-20T15:46:15Z" id="227182141">thanks @clintongormley 
</comment><comment author="nik9000" created="2016-06-21T13:34:29Z" id="227440953">I merged https://github.com/elastic/elasticsearch/pull/18979 which might make the comments not a required thing for painless. You bogusMethod example fails now. I'm going to close this because I think it is enough. If not, open it again.
</comment><comment author="rmuir" created="2016-06-21T13:40:35Z" id="227442727">@nik9000 thanks a lot for looking into this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update forbiddenapis to 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18960</link><project id="" key="" /><description>Forbidden-Apis 2.2 was released an hour ago.

This version supports/fixes the following important stuff:
- allows to forbid signature polymorphic methods (e.g., slow `MethodHandle.invoke(...)` when used instead of `MethodHandle.invokeExact(...)`
- Works around a bug in Gradle's Spyware Daemon
- commons-io signatures for 2.5; also update/review older commons-io signatures

I only updated the version numbers in the build. I did not reenable the Gradle Daemon, this is something separate. Forbiddenapis now detects automatically if the Gradle Daemon is used to build and switches to slow, non-caching mode :-)

I also added `MethodHandle.invoke(...)` to the forbidden signatures. I had to fix 2 tests that used the slow method.
</description><key id="161085873">18960</key><summary>Update forbiddenapis to 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>build</label><label>v5.0.0-alpha4</label></labels><created>2016-06-19T18:41:03Z</created><updated>2016-06-20T14:29:42Z</updated><resolved>2016-06-19T21:17:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-19T18:46:56Z" id="227013492">@rjernst You can look into the spyware daemon, if you like! There should be no changes required, because it detect automatically.

For me it passed several times:
- `gradle --daemon --rerun-tasks forbiddenApis`
- `gradle --daemon --rerun-tasks clean forbiddenApis`
- `gradle --daemon --rerun-tasks forbiddenApis`

...and so on. No `FileNotFoundException` anymore!
</comment><comment author="rmuir" created="2016-06-19T21:18:40Z" id="227020892">Thanks @uschindler ! I am glad invoke is banned... Bad naming on that one
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ArrayTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file></files><comments><comment>Merge pull request #18960 from uschindler/forbiddenapis-2.2</comment></comments></commit></commits></item><item><title>Cleanup some things after removal of joda-time hack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18959</link><project id="" key="" /><description>After removing this hack, we can cleanup a bit. Particularly important is to be able to treat codebases as a unique thing (since ES jar is no longer duplicated on the classpath to be "in front of" joda-time). This gives a lot of internal checks in case something goes wrong.

I know the other day i accidentally installed groovy as a plugin (its already a module), and the error was strange. So using Set and checking the return values is an easy improvement.
</description><key id="161060827">18959</key><summary>Cleanup some things after removal of joda-time hack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>v5.4.0</label></labels><created>2016-06-19T07:29:08Z</created><updated>2017-03-21T19:12:17Z</updated><resolved>2017-03-21T19:12:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-20T08:10:33Z" id="227076403">LGTM left some minors
</comment><comment author="elasticmachine" created="2017-02-23T18:14:44Z" id="282074263">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="elasticmachine" created="2017-02-23T18:14:49Z" id="282074284">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/TikaImpl.java</file><file>test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Packaging: Remove classpath ordering hack (#23596)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/TikaImpl.java</file><file>test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Packaging: Remove classpath ordering hack (#23596)</comment></comments></commit></commits></item><item><title>Fix explicit casts and improve tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18958</link><project id="" key="" /><description>math operators for dynamic types adopt the return value, but we don't handle explicit user casts.

for the cached case, we can use explicitCastArguments (but with checks to prevent the insane stuff it will do with boolean types). For the fallback case we use a slower method.

I beefed up testing around this area and found a few bugs here and there. 
</description><key id="161060685">18958</key><summary>Fix explicit casts and improve tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-19T07:24:03Z</created><updated>2016-06-20T17:40:12Z</updated><resolved>2016-06-20T17:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-19T09:16:56Z" id="226987450">Cool! I like the idea also to randomize the caching, although I am not so happy with the initialCallSiteDepth parameter passed into every invokeDynamic call. I agree this is the best solution, but I have to think about it.

Do you know if this has an impact on speed of invokedynamic?
</comment><comment author="rmuir" created="2016-06-19T12:49:08Z" id="226995643">&gt; Cool! I like the idea also to randomize the caching, although I am not so happy with the initialCallSiteDepth parameter passed into every invokeDynamic call. I agree this is the best solution, but I have to think about it.
&gt; 
&gt; Do you know if this has an impact on speed of invokedynamic?

No, as this phase's job is just to return a CallSite which is permanent? We already pass args here, and they hint that the call is going to happen via invokeWithArguments if you have any at all. Also i ran benchmarks and they are happy.

For now at least, we need the test coverage, for the math operators. We can test it directly some other way and then remove it. But these slave labor tests are exhausting man.
</comment><comment author="rmuir" created="2016-06-19T13:02:46Z" id="226996218">If we arent going to pass this param, I'll open a separate PR to remove dynamic math operator support and just let math be slower (boxing and instanceof like before) but correct. Otherwise, if we want the performance improvement that comes with that math operator support, we need to ensure its tested, and that is what this PR is about.
</comment><comment author="uschindler" created="2016-06-19T13:15:06Z" id="226996737">I am fine. I just tried to think about some better way to do it. But I did not find one.
</comment><comment author="rmuir" created="2016-06-19T15:24:06Z" id="227003034">I saw the benefit when i saw minor inconsistencies like WrongMethodType vs ClassCastException for operators depending on how many types we had seen. 

We can of course replace this with explicit tests, but currently we need all the tests we can get because it is not straightforward to see the logic. In testing all this, i found a bug in EChain with compound assignment that only impacts shift operators, for example. 
</comment><comment author="jdconrad" created="2016-06-20T16:12:03Z" id="227189952">+1 from me.
</comment><comment author="uschindler" created="2016-06-20T16:33:46Z" id="227195972">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/CompilerSettings.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefMath.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/PainlessScriptEngineService.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/CastTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOptimizationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScriptTestCase.java</file></files><comments><comment>Merge pull request #18958 from rmuir/explicit_casts</comment></comments></commit></commits></item><item><title>Fix NPEs due to disabled source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18957</link><project id="" key="" /><description>This commit fixes some NPEs caused by implicitly performing a get request for a document that exists with its _source disabled and then trying to access the source. Instead of causing an NPE the following queries will throw an exception with a "source disabled" message (similar behavior as if the document does not exist).:
- GeoShape query for pre-indexed shape (throws IllegalArgumentException)
- Percolate query for an existing document (throws IllegalArgumentException)

A Terms query with a lookup will ignore the document if the source does not exist (same as if the document does not exist).

The GET and HEAD requests to [fetch the _source directly](https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html#_source) will return a 404 if the source is disabled (even if the document exists).

Updated tests (all passing) in  :core and :modules:percolator.
Added REST test `rest-api-spec/test/get_source/85_source_missing`.

Note: this bug still exists in the old Percolate and Multi Percolate APIs, but IMO not work fixing/testing since they are deprecated.
</description><key id="161056126">18957</key><summary>Fix NPEs due to disabled source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">qwerty4030</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-06-19T04:36:17Z</created><updated>2017-05-09T08:15:25Z</updated><resolved>2016-06-27T07:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="qwerty4030" created="2016-06-21T03:27:27Z" id="227331263">To reproduce index a doc with _source disabled:

```
PUT test-index
{
  "mappings": {
    "test-type": {
      "_source": {
        "enabled": false
      }
    }
  }
}

PUT test-index/test-type/1
{}
```

Then try any of the following requests that implicitly run a get request and try to access the source: 

```
POST test-index/_search
{
  "query": {
    "terms": {
      "field": {
        "index": "test-index",
        "type": "test-type",
        "id": "1",
        "path": "test-field"
      }
    }
  }
}

POST test-index/_search
{
  "query": {
    "geo_shape": {
      "location": {
        "indexed_shape": {
          "index": "test-index",
          "type": "test-type",
          "id": "1",
          "path": "test-field"
        }
      }
    }
  }
}

POST test-index/_search
{
  "query": {
    "percolate": {
      "field": "query",
      "document_type": "test-type",
      "index": "test-index",
      "type": "test-type",
      "id": "1"
    }
  }
}

GET test-index/test-type/1/_source
```

Reponse is an NPE:

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test-index",
            "node": "GImAaTDTTTW5h0lRacp-ng",
            "reason": {
               "type": "null_pointer_exception",
               "reason": null
            }
         }
      ],
      "caused_by": {
         "type": "null_pointer_exception",
         "reason": null
      }
   },
   "status": 500
}
```

Also the following request returns a 200, when it should be a 404:

```
HEAD test-index/test-type/1/_source
```

This PR updates the code to throw an exception with a more useful message (percolate and geo):

```
{
   "error": {
      "root_cause": [
         {
            "type": "illegal_argument_exception",
            "reason": "Shape with ID [1] in type [test-type] source disabled"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test-index",
            "node": "HKM5UtbeQlyE2lULZvn4Mg",
            "reason": {
               "type": "illegal_argument_exception",
               "reason": "Shape with ID [1] in type [test-type] source disabled"
            }
         }
      ],
      "caused_by": {
         "type": "illegal_argument_exception",
         "reason": "Shape with ID [1] in type [test-type] source disabled"
      }
   },
   "status": 400
}
```

In case someone was wondering how I found this: I was working on #18563 and used an old version of the [create index docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/indices-create-index.html)(since fixed) that had an example with `_source` disabled in the mapping. I ran into this NPE when trying to use the old percolate API on an indexed doc. I found and fixed all places in the code that tried to access the _source from a get request without checking if it was null/empty.
</comment><comment author="clintongormley" created="2016-06-21T09:51:26Z" id="227393347">@martijnvg could you review this please?
</comment><comment author="martijnvg" created="2016-06-21T10:50:20Z" id="227405419">Thanks @qwerty4030, this looks good! I left a couple of comments.
</comment><comment author="martijnvg" created="2016-06-24T09:46:17Z" id="228304236">@qwerty4030 Your update looks good! Can you also resolve the merge conflicts? I'll will then happily merge it .
</comment><comment author="qwerty4030" created="2016-06-24T17:40:38Z" id="228411544">@martijnvg awesome! Thanks for the review. Do you suggest merging master into my branch or rebase my branch on master?
</comment><comment author="martijnvg" created="2016-06-24T19:23:08Z" id="228438278">Since the change is small, lets rebase your branch to master?
</comment><comment author="qwerty4030" created="2016-06-25T05:23:11Z" id="228513056">@martijnvg rebased and squashed the commit. Can you take a look a this REST test I added: `rest-api-spec/test/get_source/85_source_missing` Thanks!
All the following tests pass for me:
- `gradle :distribution:integ-test-zip:integTest`
- `gradle :core:test`
- `gradle :core:integTest`
- `gradle :modules:perolcator:test`
- `gradle :modules:perolcator:integTest`
</comment><comment author="qwerty4030" created="2016-06-25T06:45:38Z" id="228516504">Oh and thanks for the tip about registering the handlers in `NetworkModule`. Totally missed that in the initial commit! What exactly is the purpose of that module? When I forgot to register the new handler class everything seemed to work OK? I poked around the code and noticed `RestIndexAction.CreateHandler` is not registered in `NetworkModule` (howerver `RestIndexAction` is).
</comment><comment author="martijnvg" created="2016-06-27T06:49:26Z" id="228666416">&gt; Can you take a look a this REST test I added: rest-api-spec/test/get_source/85_source_missing

This test looks good. Thank you!

&gt; What exactly is the purpose of that module? 

It instantiates the rest actions, netty/local transport and their infrastructure.

&gt; When I forgot to register the new handler class everything seemed to work OK?

I think that because in the previous version of this PR the RestHeadAction itself was registered in the network module and in the constructor (like RestIndexAction) additional handles were registered.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/get/RestHeadAction.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolateQueryBuilder.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorQuerySearchIT.java</file></files><comments><comment>Merges PR #18957</comment></comments></commit></commits></item><item><title>0.13</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18956</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="161055923">18956</key><summary>0.13</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sygool</reporter><labels /><created>2016-06-19T04:26:55Z</created><updated>2016-06-19T04:34:26Z</updated><resolved>2016-06-19T04:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Build: Require exactly gradle 2.13</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18955</link><project id="" key="" /><description>see #18935
</description><key id="161034828">18955</key><summary>Build: Require exactly gradle 2.13</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha4</label></labels><created>2016-06-18T17:29:10Z</created><updated>2016-06-21T19:24:20Z</updated><resolved>2016-06-21T19:24:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-06-20T05:55:43Z" id="227055968">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #18955 from rjernst/gradle_req</comment></comments></commit></commits></item><item><title>Add lambda captures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18954</link><project id="" key="" /><description>Painless has working lambdas now but they are incomplete, can't capture any variables from the current scope.

This adds capture support. To keep it simple nodes get a new `extractVariables(Set&lt;String&gt;)` to push their variable names. We call this on the lambda body before analysis to give us the capture info we need.

Captures work slightly differently than in java. In java they must be effectively final variables. Instead in painless they are marked read-only inside the lambda body (so you cannot assign them to a different value inside the lambda, as that would not do anything).
</description><key id="161027092">18954</key><summary>Add lambda captures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-18T14:29:45Z</created><updated>2016-06-26T07:11:30Z</updated><resolved>2016-06-20T17:05:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-19T09:18:22Z" id="226987504">LGTM
</comment><comment author="jdconrad" created="2016-06-20T16:05:01Z" id="227187930">@rmuir LGTM too.  Thanks for getting lambdas working!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/LambdaLocals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Locals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ANode.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBool.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBoolean.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConditional.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConstant.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EDecimal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EExplicit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENull.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENumeric.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LArrayLength.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallLocal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LListShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LMapShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewObj.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LRegex.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LStatic.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LString.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LVariable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBreak.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SCatch.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SContinue.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclaration.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDo.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIf.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIfElse.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SReturn.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SThrow.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/STry.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SWhile.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/LambdaTests.java</file></files><comments><comment>Merge pull request #18954 from rmuir/lambda_captures</comment></comments></commit></commits></item><item><title>Remove forked joda time BaseDateTime class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18953</link><project id="" key="" /><description>This class was forked in 0.20 to remove a volatile keyword. While there
is no issue attached to the commit, no evidence of the criticality of the
change nor does it seem to be correct since we set this value internally as well
I think this class should be used as is from joda time even if we have to pay
the price of volatile reads. We can't do 3rd party optimization in our codebase that
way it just not maintainable.

This was added in 2280915d3cc3e9fe0fe5fab57ae57774e9044af7

@rmuir did we add any special casing for this class in security manager or something like this?
</description><key id="161020759">18953</key><summary>Remove forked joda time BaseDateTime class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-18T11:59:17Z</created><updated>2016-06-28T09:31:58Z</updated><resolved>2016-06-18T15:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-18T12:02:54Z" id="226937891">There is plenty of special casing, including startup scripts and similar. I can clean it up as a followup.
</comment><comment author="rmuir" created="2016-06-18T12:11:15Z" id="226938232">+1

I have looked into what changes in the assembler, it just causes a lock prefix to be added to an instruction. similar to atomicint vs int. @danielmitterdorfer ran some benchmarks and in the worst case (e.g. just setting the value to something, getting a month) its a 10% difference. Add the time it takes to fetch the value in the first place (e.g. docvalues read) and the scripting api and other stuff, and it will be drowned out even more.

If we want to optimize the performance around scripts, there is much lower hanging fruit...
</comment><comment author="danielmitterdorfer" created="2016-06-18T12:16:15Z" id="226938422">Here are the detailed results that @rmuir  has mentioned: https://gist.github.com/danielmitterdorfer/04dfb0c82345f677284f8dd2d198d4cc

I ran the `DateBenchmark#mutableDateTimeSetMillisGetDayOfMonth()` once with our patch and once with the original class from Joda time.

I am also +1 on the change.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/joda/time/base/BaseDateTime.java</file></files><comments><comment>Remove forked joda time BaseDateTime class (#18953)</comment></comments></commit></commits></item><item><title>Allow to run elasticsearch queries as "raw json" string like in curl requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18952</link><project id="" key="" /><description>Allow to run elasticsearch queries as "raw json" strings .

```
{
    ...,

    "aggs" : {
        "grades_count" : {
            "value_count" : {
                "script" : {
                    "file": "my_script",
                    "params" : {
                        "field" : "grade"
                    }
                }
            }
        }
    }
}
```

 Currently to run them through spring data es, i have to rebuild them with a builder pattern. But i just want to load some external ready to use query in form as json that used on the low level api as you would do with curl. The jest client ( https://github.com/spring-projects/spring-data-elasticsearch/pull/147 ) does not work on local embeeded elasticsearch. Thats the reason the ElasticSearchTemplate does not provide the needed API to run that kind of raw query (like for exmaple the SearchSourceBuilder in Jest) . In https://github.com/elastic/elasticsearch/pull/18735 it was created a new pull request, but does it support this requirement?
The following restrictions have to get solved:
- JestElasticsearchTemplate() does not implement the getClient() method.

```
public class JestElasticsearchTemplate implements ElasticsearchOperations, ApplicationContextAware {...
@Override
    public Client getClient() {
        throw new UnsupportedOperationException();
    }
```
- Spring data "ElasticsearchTemplate" does not implement raw string query feature like the example query (with aggs!!!) from above. 
- The base class "ElasticsearchOperations" does not provide the raw string query feature from JestElasticsearchTemplate()

Her i tried to make use of a snapshot of https://github.com/spring-projects/spring-data-elasticsearch/pull/147

```
@Bean
    public ElasticsearchOperations elasticsearchTemplate() {

        ElasticsearchOperations elasticsearchOperations;
        if (connection.isEmbedded()) {
            Client client = NodeBuilder.nodeBuilder().local(true).node().client();
            elasticsearchOperations = new ElasticsearchTemplate(client);
        } else {
            JestClientFactory factory = new JestClientFactory();
            factory.setHttpClientConfig(new HttpClientConfig.Builder(connection.getUrl())
                    .multiThreaded(connection.isMultiThreaded()).build());
            JestClient client = factory.getObject();
            elasticsearchOperations = new JestElasticsearchTemplate(client);
        }
        return elasticsearchOperations;
    }
```
</description><key id="161013462">18952</key><summary>Allow to run elasticsearch queries as "raw json" string like in curl requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cforce</reporter><labels /><created>2016-06-18T08:16:12Z</created><updated>2016-06-20T07:04:31Z</updated><resolved>2016-06-18T15:31:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-18T08:29:41Z" id="226929421">It will be possible with Elasticsearch 5.0 REST client.
Spring data team will just have to use it.

In the meantime may be JEST could provide such a raw method? May be you should ask there?
</comment><comment author="cforce" created="2016-06-18T08:42:24Z" id="226929894">As i use ElasticsearchOperations as wrapper for index creation and searching, the problem is i can't access Jest api without braking the abstraction layer. I then must work with two "approaches dependent of the use case index/search"  - as i would to go along which spring as far as possible until thiy feature is available.
I have (as you see above) an switch to configure if i use the embedded es (for dev env) or the external es (for cloud stage). I would additional need to manage an "loop" connection for jest queries on the embedded  es if that is even possible . I have no spring (auto) configuration support here.
</comment><comment author="cforce" created="2016-06-18T08:43:57Z" id="226929953">What is the release plan for Elasticsearch 5.0 REST client?
</comment><comment author="dadoonet" created="2016-06-18T10:40:37Z" id="226934889">But I don't understand what we can add in elasticsearch to help here?
</comment><comment author="cforce" created="2016-06-18T15:31:24Z" id="226948458">Well you are right.. i have to post that at spring data es project.
</comment><comment author="cforce" created="2016-06-20T07:04:31Z" id="227064948">https://jira.spring.io/browse/DATAES-266
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor painless variables handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18951</link><project id="" key="" /><description>This is currently very limiting, there is a manually managed stack (with slower access), various nodes saving away bytecode variable slot numbers, nodes changing program state in analyze(), etc.

I refactored it enough to move lambda logic out of Walker into ELambda node. For captures we will have to do a little bit more (our choices are to analyze twice, or do a more complicated variable renumbering).

I think its a good stopping point as the changes impact a lot of code.
</description><key id="160980894">18951</key><summary>Refactor painless variables handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T21:46:22Z</created><updated>2016-06-20T14:18:02Z</updated><resolved>2016-06-18T12:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-17T22:06:47Z" id="226892912">LGTM!  @rmuir Thanks for making the scoping for variables much more clear!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Compiler.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Constant.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Globals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/LambdaLocals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Locals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ALink.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AStatement.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBool.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBoolean.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConditional.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConstant.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EDecimal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EExplicit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENull.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENumeric.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ILambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LArrayLength.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallLocal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LListShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LMapShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewObj.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LRegex.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LStatic.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LString.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LVariable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBreak.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SCatch.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SContinue.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclaration.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDo.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIf.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIfElse.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SReturn.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SThrow.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/STry.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SWhile.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ReservedWordTests.java</file></files><comments><comment>Merge pull request #18951 from rmuir/refactor_variables</comment></comments></commit></commits></item><item><title>Make request cache's key generation pluggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18950</link><project id="" key="" /><description>Also adds ThreadPoolPlugin for plugins that need access to the node's ThreadPool. Together, these allow plugins that hijack the search process to avoid poisoning the cache because they have control over the key. Now they can alter the key to keep the cache healthy.
</description><key id="160956198">18950</key><summary>Make request cache's key generation pluggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cache</label><label>enhancement</label><label>review</label></labels><created>2016-06-17T19:13:31Z</created><updated>2016-06-20T12:11:47Z</updated><resolved>2016-06-17T20:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-17T19:14:16Z" id="226856983">@s1monw: I tried to conform to what I think you are starting to do with plugins with ThreadPoolPlugin.
</comment><comment author="s1monw" created="2016-06-17T20:09:53Z" id="226869542">@nik9000 I think you got ThreadPoolPlugin backwards, I added `ScriptPlugin` to pull extension points from the plugin. I think we should never pass instances to Plugins that way. I think we will in the near future have a set of base services and instances we can provide for instance as a ctor argument to Plugins like `BigArrays`, `ThreadPool`, `NodeEnvironemnt` etc. but we should only pass for instance `TransportService` if we pull some instance from the plugin that explicitly depends on that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Persist sequence number checkpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18949</link><project id="" key="" /><description>This commit adds persistence for local and global sequence number
checkpoints. We also recover the max sequence number in a shard,
although there will be loss here today from delete operations. This will
be addressed in a follow-up.

Relates #10708
</description><key id="160955833">18949</key><summary>Persist sequence number checkpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Sequence IDs</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T19:11:33Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T15:05:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-20T08:45:26Z" id="227083605">looks good! left some minor comments
</comment><comment author="jasontedor" created="2016-06-21T14:28:03Z" id="227457077">&gt; looks good! left some minor comments

@bleskes I've responded to all of your feedback.
</comment><comment author="bleskes" created="2016-06-22T14:21:27Z" id="227759437">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SeqNoFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/GlobalCheckpointService.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointService.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SequenceNumbersService.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/GlobalCheckpointTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/LocalCheckpointServiceTests.java</file></files><comments><comment>Persist sequence number checkpoints</comment></comments></commit></commits></item><item><title>Change default similarity to BM25</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18948</link><project id="" key="" /><description>The default similarity was set to `classic` which refers to TFIDF and has not been moved after the upgrade to Lucene 6.

Though moving to BM25 could have some downside for queries that relies on coordination factor (match_query, multi_match_query) ?
relates #18944
</description><key id="160934011">18948</key><summary>Change default similarity to BM25</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T17:11:10Z</created><updated>2016-06-21T09:32:38Z</updated><resolved>2016-06-21T09:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-21T07:01:05Z" id="227356802">@jpountz I rephrased the documentation. Can you take a look please ?
</comment><comment author="jpountz" created="2016-06-21T07:29:48Z" id="227362031">LGTM
</comment><comment author="jimczi" created="2016-06-21T09:32:38Z" id="227389062">Thanks @jpountz ! 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/test/java/org/elasticsearch/index/similarity/SimilarityServiceTests.java</file></files><comments><comment>Merge pull request #18948 from jimferenczi/bm25</comment></comments></commit></commits></item><item><title>marvel: changing marvel.agent.interval to '60s' leads to 0/s rates for "Last hour"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18947</link><project id="" key="" /><description>Hey there,

I'm observing an issue with marvel after increasing marvel.agent.interval to '60s'.

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: Java(TM) SE Runtime Environment (build 1.8.0_72-b15)

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**: After increasing the marvel.agent.interval to '60s', the marvel overview in Kibana shows search and index rates of all zeroes when the "Last 1 hour" interval is selected. When selecting the "Last 4 hours" interval, the index and search rate show sensible values. Expected would be that "Last 1 hour" shows proper values as well.

The API returns all zeroes for the 1 hour case in the `metrics.cluster_index_latency` for the `/indices` endpoint, so it's not a display issue. `curl'ing` by hand with a 2 hour interval shows that `metrics.cluster_index_latency` is populated with sensible values.

**Steps to reproduce**:
1. Change marvel.agent.interval to '60s' in elasticsearch.yml
2. Delete all marvel indices
3. Restart elasticsearch
4. Visit marvel page after a few minutes with "Last hour" selected
5. Search and Index rate show only 0/s
6. Switch to "Last 4 hours" -&gt; Search and Index rate look sensible

Let me know if you need more info!

Thanks,
   Arne
</description><key id="160931419">18947</key><summary>marvel: changing marvel.agent.interval to '60s' leads to 0/s rates for "Last hour"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">awelzel</reporter><labels /><created>2016-06-17T16:56:41Z</created><updated>2017-03-21T17:51:08Z</updated><resolved>2017-03-21T17:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-04T10:08:29Z" id="230255237">Thanks for reporting @awelzel 

I can reproduce this issue and I just pinged some people in charge of Marvel user interface /cc @tsullivan @pickypg because they might have an opinion on what's going on here.
</comment><comment author="awelzel" created="2016-10-24T23:11:33Z" id="255891907">Hi @tlrx,

just a quick update: I upgraded ES to 2.4.1, Kibana to 4.6.2 and I still see the "all zeroes" behavior for the 1hour interval.

Thanks,
   Arne
</comment><comment author="pickypg" created="2016-10-24T23:21:20Z" id="255893492">@awelzel 

The Marvel UI that came is associated with Kibana 4.6.2 did not get any fixes associated with the "unusual" shift in data collection. The reason is that there's no extra variable that goes into our calculation _yet_ to adjust based on collection size. That won't change in 5.0, but I do expect it to change in a 5.x release.

In the mean time, I think the best course is to just do exactly what you're doing. You can change the default time picker in the Advanced -&gt; Settings screen to be `now-4h` to get the expected view without having to click anything.
</comment><comment author="colings86" created="2017-03-21T17:51:08Z" id="288163167">Since this is an issue relating to x-pack I have raised an issue in our internal bug tracker for X-Pack. I will close this issue here as the issue does not directly relate to Elasticsearch</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>service_account is not filtered in GCS repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18946</link><project id="" key="" /><description>Related to #18945 and to this https://github.com/elastic/elasticsearch/commit/35d3bdab84fa05c71e8ae019aaf661759c8b1622#commitcomment-17914150

In GCS Repository plugin we defined a `service_account` setting which is defined as `Property.Filtered`.

``` java
public static final Setting&lt;String&gt; SERVICE_ACCOUNT = simpleString("service_account", Property.NodeScope, Property.Dynamic, Property.Filtered);
```

But as this setting is not registered when the plugin starts, it's not shaded when you ask for `GET _snapshot/gcsrepo`.

We should either remove `Filtered` if it does not make any sense to filter it or register the setting or wait for a fix for #18945.

If we want to keep it, we can add a REST test like this one: https://github.com/elastic/elasticsearch/blob/master/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/20_repository.yaml

``` yaml
# Integration tests for Repository GCS component
#
"GCS repository can be registered":
    - do:
        snapshot.create_repository:
          repository: test_repo_gcs_1
          verify: false
          body:
            type: gcs
            settings:
              service_account: "whatever"

    # Get repository
    - do:
        snapshot.get_repository:
          repository: test_repo_gcs_1

    - is_true: test_repo_gcs_1
    - is_false: test_repo_gcs_1.settings.service_account
```
</description><key id="160916681">18946</key><summary>service_account is not filtered in GCS repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository GCS</label><label>bug</label></labels><created>2016-06-17T15:39:10Z</created><updated>2016-06-24T09:12:27Z</updated><resolved>2016-06-24T09:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-20T07:31:34Z" id="227069262">Note: `service_account` point to the service account file which contains the necessary credentials to authenticate against Google Cloud Storage. The unfiltered value might show something like `service_account: /path/to/iam-googleaccount-whatever@google.com` but it is not sensitive data like the credentials.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepository.java</file></files><comments><comment>Remove settings filtering for service_account in GCS repository</comment></comments></commit></commits></item><item><title>Don't register repository settings in S3 plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18945</link><project id="" key="" /><description>Follow up for https://github.com/elastic/elasticsearch/pull/17784#discussion_r64575845

Today we are registering repository settings when `S3RepositoryPlugin` starts:

``` java
        settingsModule.registerSetting(S3Repository.Repository.KEY_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.SECRET_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BUCKET_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.ENDPOINT_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.PROTOCOL_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.REGION_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.SERVER_SIDE_ENCRYPTION_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BUFFER_SIZE_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.MAX_RETRIES_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.CHUNK_SIZE_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.COMPRESS_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.STORAGE_CLASS_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.CANNED_ACL_SETTING);
        settingsModule.registerSetting(S3Repository.Repository.BASE_PATH_SETTING);
```

We don't need to register those settings as they are repository level settings and not node level settings.
We can remove all that code but `KEY_SETTING`  and `SECRET_SETTING` can not be removed here otherwise when we ask for `GET _snapshot/s3repo` those `@Filtered` settings won't be filtered.

And this test will fail: https://github.com/elastic/elasticsearch/blob/master/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/20_repository.yaml
</description><key id="160912380">18945</key><summary>Don't register repository settings in S3 plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label></labels><created>2016-06-17T15:19:04Z</created><updated>2016-07-20T10:11:43Z</updated><resolved>2016-07-20T10:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java</file></files><comments><comment>Don't register repository settings in S3 plugin</comment></comments></commit><commit><files><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepository.java</file></files><comments><comment>Remove settings filtering for service_account in GCS repository</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/test/java/org/elasticsearch/common/blobstore/FsBlobStoreContainerTests.java</file><file>core/src/test/java/org/elasticsearch/common/blobstore/FsBlobStoreTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/FsBlobStoreRepositoryIT.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobContainer.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobStore.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStorageModule.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStoragePlugin.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepository.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageService.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobStoreContainerTests.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobStoreTests.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/common/blobstore/gcs/MockHttpTransport.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageBlobStoreRepositoryTests.java</file><file>plugins/repository-gcs/src/test/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRepositoryRestIT.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreContainerTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreRepositoryIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreTestCase.java</file></files><comments><comment>Add Google Cloud Storage repository plugin</comment></comments></commit></commits></item><item><title>Multi_match should not enable coordination in bool query with BM25</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18944</link><project id="" key="" /><description>In 5.0 we use BM25, which means that query coordination should always be disabled.  This works correctly with the `bool` query but the `multi_match` query enables coordination incorrectly:

```
PUT t/t/1
{
  "foo": "one",
  "bar": "two"
}

GET t/_search
{
  "query": {
    "multi_match": {
      "query": "one two",
      "fields": ["foo", "bar"]
    }
  },
  "explain": true
}
```

Returns:

```
            {
              "value": 0.5,
              "description": "coord(1/2)",
              "details": []
            }
```
</description><key id="160904628">18944</key><summary>Multi_match should not enable coordination in bool query with BM25</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-06-17T14:44:40Z</created><updated>2016-06-21T12:13:32Z</updated><resolved>2016-06-21T11:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-17T17:32:36Z" id="226831901">&gt; In 5.0 we use BM25, which means that query coordination should always be disabled.

The default similarity is still TFIDF which is referred as `classic`. I opened https://github.com/elastic/elasticsearch/pull/18948 to change the default similarity to BM25. 

&gt; This works correctly with the bool query but the multi_match query enables coordination incorrectly

This is how the `match_query` works. It's the same on 2.x, I didn't test 1.7 but it should do the same. 
Coords are disabled only when multiple terms are at the same position in the query otherwise the coords are always enabled and we rely on this functionality for the relevancy (documents matching a lot of terms are scored first). Regarding BM25, things will change since the coords are not taken into account in this similarity but this should not be considered as a bug ? To be honest I don't know what's the impact on the relevancy for queries produced by a `match_query` or a `multi_match_query`. @jpountz @rmuir WDYT ? 
</comment><comment author="rmuir" created="2016-06-17T19:37:53Z" id="226862813">&gt; This is how the match_query works.

This is why a SynonymQuery was added when defaulting to BM25 that handles this case in a more generic way for any scoring system (including classic TF/IDF): 

&gt; One issue was the generation of synonym queries (posinc=0) by QueryBuilder (used by parsers). This is kind of a corner case (query-time synonyms), but we should make it nicer. The current code in trunk disables coord, which makes no sense for anything but the vector space impl. Instead, this patch adds a SynonymQuery which treats occurrences of any term as a single pseudoterm. With english wordnet as a query-time synonym dict, this query gives 12% improvement in MAP for title queries on BM25, and 2% with Classic (not significant). So its a better generic approach for synonyms that works with all scoring models.
&gt; 
&gt; I wanted to use BlendedTermQuery, but it seems to have problems at a glance, it tries to "take on the world", it has problems like not working with distributed scoring (doesn't consult indexsearcher for stats). Anyway this one is a different, simpler approach, which only works for a single field, and which calls tf(sum) a single time. 

https://issues.apache.org/jira/browse/LUCENE-6789

Please use it :)
</comment><comment author="rpedela" created="2016-06-20T03:05:43Z" id="227041541">I am currently using 2.3.3 and planning to start experimenting with BM25 since Lucene 6.0 makes that the default. I assumed it was ready to be used in ES as well. Is that not the case? Should I wait until 5.0 especially since I use `multi_match` heavily?
</comment><comment author="jimczi" created="2016-06-20T07:01:25Z" id="227064483">Thanks for the clarification @rmuir. 
@rpedela please use it as well ;) Concerning the `multi_match` you may want to experiment different boosts as the scoring for BM25 is different and the range of possible values differ. 
</comment><comment author="jimczi" created="2016-06-21T11:38:56Z" id="227414666">@clintongormley I think we can close this issue (please reopen if you disagree). The coords are a TF/IDF thing that was added as a countermeasure for terms with very high term frequency where the score constantly increases and never reaches a saturation point like in `BM25`. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/test/java/org/elasticsearch/index/similarity/SimilarityServiceTests.java</file></files><comments><comment>Change default similarity to BM25</comment></comments></commit></commits></item><item><title>Rename `fields` to `stored_fields` and add `docvalue_fields`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18943</link><project id="" key="" /><description>As of https://github.com/elastic/elasticsearch/pull/15017 the `fields` parameter will no longer try to retrieve fields from the `_source` but will only return stored fields.  This is a breaking change and there is nothing to warn the user that their response may well be different.

Instead, let's rename `fields` to `stored_fields` and throw an exception if the user uses `fields`.

Also, add `docvalue_fields` as an adjunct to `fielddata_fields`
</description><key id="160887842">18943</key><summary>Rename `fields` to `stored_fields` and add `docvalue_fields`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>blocker</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha5</label></labels><created>2016-06-17T13:27:36Z</created><updated>2016-07-04T08:40:27Z</updated><resolved>2016-07-04T08:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2016-06-17T13:32:36Z" id="226769530">we could also maybe throw a nice exception, if the field is stored, we can suggest stored_fields, and if not, we can suggest source filtering?
</comment><comment author="jpountz" created="2016-06-24T10:27:08Z" id="228311451">Should we make `fields` a deprecated alias for `stored_fields`? Sure this makes things trappy if users used to retrieve `_source` using `fields`, but on the other hand we documented since 1.0 that `fields` should only be used for stored fields only: we are breaking all users of the `fields` option rather than only those who used it to retrieve fields from source?
</comment><comment author="pickypg" created="2016-06-24T17:01:07Z" id="228401573">I think both should be deprecated aliases for the other side. It's trappy, but it at least allows people that are using those fields _properly_ to get the right thing without an issue. Then remove them in 6.0.

We should still throw an exception for bad usage, but presumably that check is in there anyway just because it has to be to ensure we do the right thing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>client/client-benchmark-noop-api-plugin/src/main/java/org/elasticsearch/plugin/noop/action/bulk/RestNoopBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/ExplainRequest.java</file><file>core/src/main/java/org/elasticsearch/action/explain/ExplainRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>core/src/main/java/org/elasticsearch/action/get/GetRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/get/GetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/get/GetResult.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/TermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestGetAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestHeadAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/document/RestUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestExplainAction.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/subphase/FetchSourceContext.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkWithUpdatesIT.java</file><file>core/src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/update/UpdateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java</file><file>core/src/test/java/org/elasticsearch/explain/ExplainActionIT.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolateRequest.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java</file></files><comments><comment>Fixed naming inconsistency for fields/stored_fields in the APIs (#20166)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/docvalues/DocValueFieldsContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/docvalues/DocValueFieldsFetchSubPhase.java</file></files><comments><comment>Rename FieldDataFieldsContext and FieldDataFieldsFetchSubPhase in DocValueFieldsContext and DocValueFieldsFetchSubPhase</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/ExceptionRetryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractGeoTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/source/SourceFetchingIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/GeoDistanceTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequest.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java</file></files><comments><comment>Restore reverted change now that alpha4 is out:</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/ExceptionRetryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractGeoTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/source/SourceFetchingIT.java</file><file>core/src/test/java/org/elasticsearch/update/TimestampTTLBWIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/GeoDistanceTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBulkByScrollRequest.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java</file></files><comments><comment>Rename `fields` to `stored_fields` and add `docvalue_fields`</comment></comments></commit></commits></item><item><title>Add did-you-mean for plugin cli</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18942</link><project id="" key="" /><description>This commit adds error messages like: `Unknown plugin xpack, did you mean [x-pack]?`

Closes #18896
</description><key id="160857423">18942</key><summary>Add did-you-mean for plugin cli</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T10:22:05Z</created><updated>2016-06-17T12:23:08Z</updated><resolved>2016-06-17T12:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-17T10:35:00Z" id="226737095">This is pretty cool!
LGTM
</comment><comment author="jimczi" created="2016-06-17T12:22:59Z" id="226755307">Thanks @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java</file></files><comments><comment>Merge pull request #18942 from jimferenczi/did_you_mean_plugin</comment></comments></commit></commits></item><item><title>Cleanup ClusterService dependencies and detached from Guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18941</link><project id="" key="" /><description>This change removes some unnecessary dependencies from ClusterService
and cleans up ClusterName creation. ClusterService is now not created
by guice anymore.
</description><key id="160843674">18941</key><summary>Cleanup ClusterService dependencies and detached from Guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T09:06:17Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-17T15:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-17T10:35:27Z" id="226737168">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/AllocationBenchmark.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/TransportCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TransportListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/main/MainResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterName.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterNameModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/CancellableTasksTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TaskManagerTestCase.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shrink/TransportShrinkActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/TransportBulkActionTookTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/AutoCreateIndexTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/master/TransportMasterNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTestCase.java</file><file>core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/NodeConnectionsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/OperationRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/PrimaryTermsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ActiveAllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/MaxRetryAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayServiceTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/http/HttpServerTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingDisabledTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/ClusterStateChanges.java</file><file>core/src/test/java/org/elasticsearch/indices/cluster/IndicesClusterStateServiceRandomUpdatesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTestCase.java</file><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java</file><file>core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>core/src/test/java/org/elasticsearch/transport/NettyTransportServiceHandshakeTests.java</file><file>core/src/test/java/org/elasticsearch/transport/TransportModuleTests.java</file><file>core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/TransportRethrottleAction.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryTests.java</file><file>plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ClusterServiceUtils.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Cleanup ClusterService dependencies and detached from Guice (#18941)</comment></comments></commit></commits></item><item><title>Remove support for sorting terms aggregation by ascending count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18940</link><project id="" key="" /><description>closes #17614
</description><key id="160839954">18940</key><summary>Remove support for sorting terms aggregation by ascending count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aggregations</label><label>breaking</label><label>review</label></labels><created>2016-06-17T08:45:21Z</created><updated>2016-06-17T17:38:44Z</updated><resolved>2016-06-17T13:07:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-17T11:48:54Z" id="226749537">LGTM, can you put a comment on https://github.com/elastic/kibana/issues/6833 saying we are moving forward on this?
</comment><comment author="jimczi" created="2016-06-17T13:08:12Z" id="226764280">Sure, thanks @jpountz !
</comment><comment author="jpountz" created="2016-06-17T13:25:56Z" id="226768062">@jimferenczi Let's add deprecation logging on 2.4/2.x?
</comment><comment author="jimczi" created="2016-06-17T17:38:44Z" id="226833387">The functionality is deprecated in 2.4/2.x but this PR has been reverted since it requires changes in Kibana. I'll merge it after the alpha release.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java</file></files><comments><comment>Merge pull request #18940 from jimferenczi/terms_count_asc</comment></comments></commit></commits></item><item><title>Update to jackson 2.8.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18939</link><project id="" key="" /><description>Jackson 2.8.1 has been released. This PR updates the version to the latest version of Jackson which fix #18076 and is also more strict when it build objects.
</description><key id="160834695">18939</key><summary>Update to jackson 2.8.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Core</label><label>upgrade</label><label>v5.0.0-beta1</label></labels><created>2016-06-17T08:13:11Z</created><updated>2016-08-26T02:14:17Z</updated><resolved>2016-08-05T10:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-07-08T07:25:31Z" id="231293347">2.8.0 has been released: https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.8
</comment><comment author="tlrx" created="2016-08-02T09:46:58Z" id="236856540">2.8.1 has been released: https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.8.1
</comment><comment author="tlrx" created="2016-08-03T08:33:25Z" id="237176817">@s1monw Do you want to have another look?
</comment><comment author="s1monw" created="2016-08-03T09:27:53Z" id="237188888">left some comments @tlrx thanks for doing this
</comment><comment author="tlrx" created="2016-08-03T09:44:37Z" id="237192560">@s1monw Thanks for your review! I removed all pretty printings.
</comment><comment author="tlrx" created="2016-08-05T07:40:26Z" id="237778553">Rally benchmarks shows no performance regression. I'd like to merge it soon - can I go @s1monw ?
</comment><comment author="s1monw" created="2016-08-05T09:26:04Z" id="237804931">LGTM
</comment><comment author="tlrx" created="2016-08-05T10:28:56Z" id="237817585">Thanks @s1monw !
</comment><comment author="wgorder" created="2016-08-26T02:14:17Z" id="242606145">Great need this one too +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/ListTasksResponse.java</file></files><comments><comment>Fix toString method</comment></comments></commit></commits></item><item><title>Inline reroute with process of node join/master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18938</link><project id="" key="" /><description>In the past, we had the semantics where the very first cluster state a node processed after joining could not contain shard assignment to it. This was to make sure the node cleans up local / stale shard copies before receiving new ones that might confuse it.  Since then a lot of work in this area, most notably the introduction of allocation ids and #17270 . This means we don't have to be careful and just reroute in the same cluster state change where we process the join, keeping things simple and following the same pattern we have in other places.
</description><key id="160832622">18938</key><summary>Inline reroute with process of node join/master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>:Discovery</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-17T07:59:51Z</created><updated>2016-07-04T11:50:44Z</updated><resolved>2016-06-17T15:32:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-17T08:01:06Z" id="226705991">@ywelsch can you take a look? 
</comment><comment author="ywelsch" created="2016-06-17T08:21:30Z" id="226709989">Left 2 comments. I really like this change!
</comment><comment author="bleskes" created="2016-06-17T11:01:43Z" id="226741714">@ywelsch I pushed another commit addressing your comments
</comment><comment author="ywelsch" created="2016-06-17T11:16:43Z" id="226744156">Left one minor, no need for another iteration. LGTM. Thanks @bleskes!
</comment><comment author="bleskes" created="2016-06-23T06:52:43Z" id="227965065">I reverted this one due to secondary problems with async shard fetch. I'm working on fixing those before re-committing this. 

From the revert commit message:

&gt; There are secondary issues with async shard fetch going out to nodes before they have a cluster state published to them that need to be solved first. For example:
&gt; - async fetch uses transport node action that resolves nodes based on the cluster state (but it's not yet exposed by ClusterService since we inline the reroute)
&gt; - after disruption nodes will respond with an allocated shard (they didn't clean up their shards yet) which throws of decisions master side.
&gt; - nodes deed the index meta data in question but they may not have if they didn't recieve the latest CS
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file></files><comments><comment>Make static Store access shard lock aware (#19416)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/test/NoopDiscovery.java</file></files><comments><comment>re-introduce: Inline reroute with process of node join/master election (#18938)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes (#19044)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes (#19044)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes (#19044)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInputReader.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/action/support/nodes/TransportNodesActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodesTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/StreamTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/StoreTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Make shard store fetch less dependent on the current cluster state, both on master and non data nodes (#19044)</comment></comments></commit><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/AllocationBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file><file>buildSrc/src/main/java/org/elasticsearch/test/NamingConventionsCheck.java</file><file>buildSrc/src/test/java/org/elasticsearch/test/NamingConventionsCheckBadClasses.java</file><file>client/rest/src/main/java/org/elasticsearch/client/DeadHostState.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpDeleteWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/HttpGetWithEntity.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RequestLogger.java</file><file>client/rest/src/main/java/org/elasticsearch/client/Response.java</file><file>client/rest/src/main/java/org/elasticsearch/client/ResponseException.java</file><file>client/rest/src/main/java/org/elasticsearch/client/RestClient.java</file><file>client/rest/src/test/java/org/elasticsearch/client/CloseableBasicHttpResponse.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RequestLoggerTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientBuilderTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientIntegTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientMultipleHostsTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/RestClientSingleHostTests.java</file><file>client/rest/src/test/java/org/elasticsearch/client/TrackingFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/HostsSniffer.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/SniffOnFailureListener.java</file><file>client/sniffer/src/main/java/org/elasticsearch/client/sniff/Sniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferBuilderTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/HostsSnifferTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/MockHostsSniffer.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SniffOnFailureListenerTests.java</file><file>client/sniffer/src/test/java/org/elasticsearch/client/sniff/SnifferBuilderTests.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestCase.java</file><file>client/test/src/main/java/org/elasticsearch/client/RestClientTestUtil.java</file><file>core/src/main/java/org/apache/lucene/document/XInetAddressPoint.java</file><file>core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>core/src/main/java/org/apache/lucene/queries/MinDocQuery.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/elasticsearch/Build.java</file><file>core/src/main/java/org/elasticsearch/ResourceNotFoundException.java</file><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/ActionRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/CancelTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/cancel/TransportCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/GetTaskResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/get/TransportGetTaskAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/ListTasksResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TaskGroup.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/tasks/list/TransportListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/Condition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxAgeCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/MaxDocsCondition.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/RolloverResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/rollover/TransportRolloverAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/TrackingResultProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java</file><file>core/src/main/java/org/elasticsearch/action/main/MainResponse.java</file><file>core/src/main/java/org/elasticsearch/action/main/TransportMainAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationTask.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/BaseTasksRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TasksRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/tasks/TransportTasksAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/FilterClient.java</file><file>core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/Requests.java</file><file>core/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterName.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Streamable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/Writeable.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/common/network/NetworkModule.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java</file><file>core/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>core/src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/AbstractObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ConstructingObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPing.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayModule.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ArabicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/CJKWidthFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/DecimalDigitFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/GermanNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/HindiNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/IndicNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MappingCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/MultiTermAwareComponent.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PatternReplaceCharFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/PersianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SerbianNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SoraniNormalizationFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/UpperCaseTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/LiveVersionMap.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexGeoPointFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyDateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoPolygonQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxQuery.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/RefreshListeners.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file></files><comments><comment>Merge branch 'master' into feature/seq_no</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/test/NoopDiscovery.java</file></files><comments><comment>revert - Inline reroute with process of node join/master election (#18938)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/test/NoopDiscovery.java</file></files><comments><comment>Inline reroute with process of node join/master election (#18938)</comment></comments></commit></commits></item><item><title>Same mapping over the index in elasticsearch 2.3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18937</link><project id="" key="" /><description>&lt;!--
Request for different mapping over same index..
--&gt;

**Elasticsearch version**:2.3.3

**JVM version**:1.7

**OS version**:14.04

I am working on a project which is currently using elasticsearch 1.4, Now we are going to upgrade elasticsearch,We have so many field which have different mapping on different record type .

We have some common field on different record type and have different implementation as per our requirement so they have different mapping as well. Is there any way to have different mapping over the same index?

Is there any meaning to make field mapping nested type which just require type string?

**Just add feature that user can have different mapping over the same index for same field through index setting**:
</description><key id="160832299">18937</key><summary>Same mapping over the index in elasticsearch 2.3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgc445</reporter><labels /><created>2016-06-17T07:57:49Z</created><updated>2016-06-17T15:12:59Z</updated><resolved>2016-06-17T15:12:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-17T15:12:58Z" id="226796475">Hi @sgc445 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>grammar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18936</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="160809918">18936</key><summary>grammar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlenRSmith</reporter><labels><label>docs</label><label>v2.3.4</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-17T04:27:46Z</created><updated>2016-06-17T08:09:51Z</updated><resolved>2016-06-17T08:08:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Gradle 2.14 compatibility?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18935</link><project id="" key="" /><description>**Elasticsearch version**: master

**JVM version**:

```
java version "1.8.0_77"
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)
```

**OS version**: OS X 10.11.5

**Description of the problem including expected versus actual behavior**:
Trying to run `gradle build` but getting an error instead of build output, console output below. It looks like the `org.gradle.logging.progress` package was [added in 2.14](https://github.com/gradle/gradle/commit/a8be591089bbf9df86fcc58fc155b8e1329df524) and moved the `org.gradle.logging.ProgressLogger` class in the process.

**Steps to reproduce**:
1. Install gradle 2.14
2. Checkout `master`
3. Run `gradle build`

**Provide logs (if relevant)**:

``` sh
elasticsearch [master] $ gradle build
:buildSrc:clean
:buildSrc:compileJava
:buildSrc:compileGroovy
startup failed:
/Users/spalger/dev/es/elasticsearch/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestProgressLogger.groovy: 28: unable to resolve class org.gradle.logging.ProgressLogger
 @ line 28, column 1.
   import org.gradle.logging.ProgressLogger
   ^

/Users/spalger/dev/es/elasticsearch/buildSrc/src/main/groovy/org/elasticsearch/gradle/vagrant/TapLoggerOutputStream.groovy: 25: unable to resolve class org.gradle.logging.ProgressLogger
 @ line 25, column 1.
   import org.gradle.logging.ProgressLogger
   ^

/Users/spalger/dev/es/elasticsearch/buildSrc/src/main/groovy/org/elasticsearch/gradle/vagrant/VagrantLoggerOutputStream.groovy: 23: unable to resolve class org.gradle.logging.ProgressLogger
 @ line 23, column 1.
   import org.gradle.logging.ProgressLogger
   ^

3 errors

:buildSrc:compileGroovy FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':compileGroovy'.
&gt; Compilation failed; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 5.056 secs
```
</description><key id="160804922">18935</key><summary>Gradle 2.14 compatibility?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels><label>bug</label><label>build</label></labels><created>2016-06-17T03:24:51Z</created><updated>2017-01-24T13:04:17Z</updated><resolved>2017-01-24T13:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spalger" created="2016-06-17T03:39:31Z" id="226673207">Just confirmed this is not broken in gradle 2.13
</comment><comment author="xuzhou2020" created="2016-06-17T06:01:21Z" id="226687292">I have the save question as you when building with gradle
</comment><comment author="javanna" created="2016-06-17T07:31:14Z" id="226700446">Related: https://discuss.gradle.org/t/gradle-2-14-breaks-plugins-using-consolerenderer/18045 .
</comment><comment author="rjernst" created="2016-06-18T17:19:08Z" id="226953965">This is actually worse than just the class being moved. Apparently they considered the package org.gradle.logging to be "internal", and in 2.14 internal classes are finally not available to plugins (and this class move makes it truly internal). So until they add back ProgressLogger as part of the plugin API, all our nice logging would disappear...

I'm going to add a check for now in BuildPlugin init that the gradle version is equal exactly to 2.13...
</comment><comment author="rjernst" created="2016-06-18T17:21:20Z" id="226954067">Ah and of course this is hard to check for 2.13 because the failure happens inside buildSrc before we even get to check the gradle version...
</comment><comment author="rjernst" created="2016-06-18T17:29:37Z" id="226954486">I opened #18955 as a stopgap so at least the error message is clear when trying to use 2.14
</comment><comment author="jprante" created="2016-06-24T13:28:15Z" id="228345225">Due to gradle core developer Adrian Kelly

https://discuss.gradle.org/t/bug-in-gradle-2-14-rc1-no-service-of-type-styledtextoutputfactory/17638/3

there is no big chance that ProgressLogger will be available (again). So my suggestion is to adapt to Gradle 2.14 (including upcoming Gradle 3) as soon as possible by aligning the Elasticsearch build scripts/plugins to the reduced capabilities in https://docs.gradle.org/current/userguide/logging.html
</comment><comment author="mfussenegger" created="2016-07-13T21:48:32Z" id="232497092">Any chance to reconsider https://github.com/elastic/elasticsearch/pull/13744 due to this issue here?
I'm not sure if keeping a 50kb blob out of the repo is worth forcing potential contributors to either downgrade system gradle or start keeping around a bunch of gradle versions that happen to work with ES.
</comment><comment author="rjernst" created="2016-07-13T22:19:54Z" id="232503962">@jprante 

&gt; there is no big chance that ProgressLogger will be available (again)

That is simply not true. I spoke with developers at gradle during Gradle Summit and they understand that progress logger is important. I expect it to come back, in some form, in the future:
https://discuss.gradle.org/t/can-gradle-team-add-progresslogger-and-progressloggerfactory-to-public-api/18176/6

@mfussenegger 

&gt; Any chance to reconsider #13744 due to this issue here?

The size is not the issue there. It is that we do not want _binary_ blobs in our repo. I would be ok with a custom equivalent of the gradle wrapper that depended on java 8 and jjs to download the gradle binary, but I have not investigated the real feasibility of such a solution. In the meantime, you don't need to manage "a bunch" of versions, just two, 2.13 and whatever other version you are on. You can add your own gradle wrapper file then that just runs gradle 2.13 wherever it is on your system. I would even be ok with adding this to the gitignore so that you can update the repo without it looking like some outlier file.
</comment><comment author="mfussenegger" created="2016-07-13T22:39:38Z" id="232507837">&gt; It is that we do not want binary blobs in our repo.

Aren't the zip files for bwc testing also binary files?

&gt;  In the meantime, you don't need to manage "a bunch" of versions, just two, 2.13 

I'm probably being a bit too pessimistic here and exaggerating.
Anyway, it's not much of a problem for me personally. Just wanted to bring it up because it definetly _is_ a stepping stone.
</comment><comment author="manterfield" created="2016-10-03T17:40:22Z" id="251172909">I think it would be helpful to add the requirement for Gradle 2.13 to the docs for contribution and to make it more explicit that it is required in the main readme. Currently the readme says: 

&gt; You’ll need to have a modern version of Gradle installed – 2.13 should do.

Which makes it sound like 2.13 or upwards is fine. 

There's no mention of version on the contribution doc.

It's only a small issue and the error message makes it very clear what has gone wrong, but it could save the time of people like me, as I just downloaded the latest version of Gradle purely for the sake of contributing to the project.

I'd be happy to make the change myself since I was after something simple first anyway. Is it precisely version 2.13 that works, or can slightly older Gradle versions work too? 
</comment><comment author="rjernst" created="2016-10-03T17:41:57Z" id="251173332">@manterfield Please do make a PR! I agree we should update the readme/contrib doc wording given our current limitation.
</comment><comment author="rjernst" created="2016-10-03T17:42:29Z" id="251173469">And it must be 2.13 at this time.
</comment><comment author="manterfield" created="2016-10-06T10:20:58Z" id="251922832">Thanks @rjernst, made a PR (#20776) with doc updates in. 
</comment><comment author="ywelsch" created="2017-01-19T11:20:27Z" id="273750165">Closed by #22669. The docs will be updated once we have moved our builds to use Gradle 3.x and feel comfortable removing support for 2.13.</comment><comment author="jasontedor" created="2017-01-20T23:18:36Z" id="274205684">Sorry, this has to be reopened, IntelliJ is unhappy with the change.</comment><comment author="ywelsch" created="2017-01-24T13:04:17Z" id="274797407">Pushed a fix for IntelliJ.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Build: Require exactly gradle 2.13</comment></comments></commit></commits></item><item><title>Fix typo in 1.5 documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18934</link><project id="" key="" /><description>Just a small typo I noticed in using the documentation.

My company and I have each signed the contributor's agreement.
</description><key id="160776396">18934</key><summary>Fix typo in 1.5 documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brentax</reporter><labels /><created>2016-06-16T22:35:09Z</created><updated>2016-06-17T14:58:42Z</updated><resolved>2016-06-17T14:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-17T14:58:42Z" id="226792510">thanks @brentax - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix compound assignment with string concats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18933</link><project id="" key="" /><description>in Java 9 there is no stringbuilder on stack! This closes #18929
</description><key id="160775961">18933</key><summary>Fix compound assignment with string concats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T22:32:03Z</created><updated>2016-06-16T22:39:29Z</updated><resolved>2016-06-16T22:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-16T22:39:29Z" id="226634351">Thanks @uschindler !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file></files><comments><comment>Merge pull request #18933 from uschindler/painless-issue-18929</comment></comments></commit></commits></item><item><title>improve Debugger to print code even if it hits exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18932</link><project id="" key="" /><description>The whole purpose of this thing is to debug something. Problem is, if you hit an exception inside asm (e.g. frame computing), you wont see anything, because we never created a `byte[]` of code to print.

Instead we pass a stream. If an exception strikes, we can show what happened so far. 

Example:

```
   &gt; Throwable #1: java.lang.ArrayIndexOutOfBoundsException: -1
   &gt;    at __randomizedtesting.SeedInfo.seed([5773748C555FE6E1:6E1A2B596D455EAA]:0)
   &gt;    at org.objectweb.asm.Frame.merge(Frame.java:1397)
   &gt;    at org.objectweb.asm.Frame.merge(Frame.java:1374)
   &gt;    at org.objectweb.asm.MethodWriter.visitMaxs(MethodWriter.java:1475)
   &gt;    at org.objectweb.asm.MethodVisitor.visitMaxs(MethodVisitor.java:867)
   &gt;    at org.objectweb.asm.util.TraceMethodVisitor.visitMaxs(TraceMethodVisitor.java:284)
   &gt;    at org.objectweb.asm.commons.GeneratorAdapter.endMethod(GeneratorAdapter.java:1603)
   &gt;    at org.elasticsearch.painless.MethodWriter.endMethod(MethodWriter.java:382)
   &gt;    at org.elasticsearch.painless.node.SSource.write(SSource.java:162)
   &gt;    at org.elasticsearch.painless.Compiler.compile(Compiler.java:134)
   &gt;    at org.elasticsearch.painless.Debugger.toString(Debugger.java:42)
   &gt;    at org.elasticsearch.painless.Debugger.toString(Debugger.java:33)
   &gt;    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@9-ea/Native Method)
   &gt;    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@9-ea/NativeMethodAccessorImpl.java:62)
   &gt;    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@9-ea/DelegatingMethodAccessorImpl.java:43)
   &gt;    at java.lang.Thread.run(java.base@9-ea/Thread.java:843)
   &gt;    Suppressed: java.lang.Exception: current bytecode: 
   &gt; // class version 52.0 (52)
   &gt; // access flags 0x31
   &gt; public final class org/elasticsearch/painless/Executable$Script extends org/elasticsearch/painless/Executable  {
   &gt;   // compiled from: &lt;debugging&gt;
   &gt;   // access flags 0x1
   &gt;   public &lt;init&gt;(Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
   &gt;     ALOAD 0
   &gt;     ALOAD 1
   &gt;     ALOAD 2
   &gt;     ALOAD 3
   &gt;     INVOKESPECIAL org/elasticsearch/painless/Executable.&lt;init&gt; (Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
   &gt;     RETURN
   &gt;     MAXSTACK = 0
   &gt;     MAXLOCALS = 0
   &gt;   // access flags 0x1
   &gt;   public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   &gt;    L0
   &gt;     LINENUMBER 9 L0
   &gt;    L1
   &gt;     LINENUMBER 9 L1
   &gt;     NEW java/util/HashMap
   &gt;     DUP
   &gt;     INVOKESPECIAL java/util/HashMap.&lt;init&gt; ()V
   &gt;    L2
   &gt;     LINENUMBER 9 L2
   &gt;     ASTORE 5
   &gt;    L3
   &gt;     LINENUMBER 24 L3
   &gt;     ALOAD 5
   &gt;     DUP_X1
   &gt;    L4
   &gt;     LINENUMBER 25 L4
   &gt;     INVOKEDYNAMIC cat(Ljava/lang/Object;)Ljava/lang/Object; [
   &gt;       // handle kind 0x6 : INVOKESTATIC
   &gt;       org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
   &gt;       // arguments:
   &gt;       1
   &gt;     ]
   &gt;     LDC "cat"
   &gt;     INVOKEDYNAMIC concat(Ljava/lang/Object;Ljava/lang/String;)Ljava/lang/String; [
   &gt;       // handle kind 0x6 : INVOKESTATIC
   &gt;       java/lang/invoke/StringConcatFactory.makeConcat(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
   &gt;       // arguments: none
   &gt;     ]
   &gt;     DUP_X1
   &gt;    L5
   &gt;     LINENUMBER 25 L5
   &gt;     INVOKEDYNAMIC cat(Ljava/lang/Object;Ljava/lang/Object;)V [
   &gt;       // handle kind 0x6 : INVOKESTATIC
   &gt;       org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
   &gt;       // arguments:
   &gt;       2
   &gt;     ]
   &gt;    L6
   &gt;     LINENUMBER 24 L6
   &gt;     ARETURN
   &gt;     MAXSTACK = 0
   &gt;     MAXLOCALS = 0
   &gt;        at org.elasticsearch.painless.Debugger.toString(Debugger.java:45)
   &gt;        ... 39 more
```
</description><key id="160767124">18932</key><summary>improve Debugger to print code even if it hits exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T21:37:58Z</created><updated>2016-06-17T14:58:12Z</updated><resolved>2016-06-16T21:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-16T21:42:00Z" id="226622376">LGTM.  Thanks @rmuir ... this looks good and seems minimally invasive.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Compiler.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/Debugger.java</file></files><comments><comment>Merge pull request #18932 from rmuir/painless_debug_exception</comment></comments></commit></commits></item><item><title>Move semicolon hack into lexer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18931</link><project id="" key="" /><description>Perviously we used token level lookbehind in the parser. That worked,
but only if the parser didn't have any ambiguity _at all_. Since the
parser has ambiguity it didn't work everywhere. In particular it failed
when parsing blocks in lambdas like `a -&gt; {int b = a + 2; b * b}`.

This moves the hack from the parser into the lexer. There we can use
token lookbehind (same trick) to _insert_ semicolons into the token
stream. This works much better for antlr because antlr's prediction
code can work with real tokens.

Also, the lexer is simpler than the parser, so if there is a place
to introduce a hack, that is a better place.
</description><key id="160766782">18931</key><summary>Move semicolon hack into lexer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T21:36:13Z</created><updated>2016-06-17T20:21:25Z</updated><resolved>2016-06-17T20:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-16T21:37:17Z" id="226621357">@rmuir this fixes `;`s inside of lambdas.

@jdconrad, this is for you to review. Antlr likes this way better because it doesn't have anything tricky in the parser grammar.
</comment><comment author="jdconrad" created="2016-06-17T18:02:18Z" id="226839193">Couple of minor comments, otherwise +1.
</comment><comment author="nik9000" created="2016-06-17T20:21:25Z" id="226872009">Thanks for reviewing @jdconrad ! I renamed the class, fixed the style and merged. We can rename the class again if anyone comes up with a better name.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce Replication unit tests using real shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18930</link><project id="" key="" /><description>This commit introduce unit testing infrastructure to test replication operations using real index shards. This is infra is complementary to the full integration tests and unit testing of ReplicationOperation  we already have. The new ESIndexLevelReplicationTestCase base makes it easier to test and simulate failure mode that require real shards and but do not need the full blow stack of a complete node.

The commit also add a simple "nothing is wrong" test plus a test that checks we don't drop docs during the various stages of recovery.

 For now, only single doc indexing is supported but this can be easily extended in the future.

This is still rough around the edges but I think it's in good enough shape to get feedback. @s1monw @ywelsch @jasontedor your input is welcome
</description><key id="160731698">18930</key><summary>Introduce Replication unit tests using real shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T18:43:36Z</created><updated>2016-06-18T16:53:47Z</updated><resolved>2016-06-18T16:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-17T07:48:07Z" id="226703505">I left a bunch of comments, but hey lets push this it's really such an improvement over what we have, we should not wait! LGTM 
</comment><comment author="bleskes" created="2016-06-17T14:34:33Z" id="226785520">@s1monw I pushed another commit
</comment><comment author="s1monw" created="2016-06-18T15:11:12Z" id="226947412">looks good - get it in!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Introduce Replication unit tests using real shards (#18930)</comment></comments></commit></commits></item><item><title>Painless: String += fails with map in java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18929</link><project id="" key="" /><description>https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+java9-periodic/256/consoleText

This looks like a fairly minimal reproduction:

```
    public void testAppendStringIntoMap() {
        assertEquals("nullcat", exec("def a = new HashMap(); a.cat += 'cat'"));
    }
```

I'm not super familiar with this code. Does someone else want it or should I grab it?
</description><key id="160729178">18929</key><summary>Painless: String += fails with map in java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T18:31:13Z</created><updated>2016-06-16T23:15:00Z</updated><resolved>2016-06-16T22:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-16T19:03:45Z" id="226582177">I'm going to disable the java-9 indy string concat stuff until this is fixed because it is causing build failures on java-9. Without the indy thing no ArrayIndexOutOfBoundsException....
</comment><comment author="nik9000" created="2016-06-16T19:10:53Z" id="226584076">I pushed 13d16fbf41e0cf90c9051e015681cc2198bb0334 which disables java-9's indy string work and adds a unit test that fails with it enabled (basically just the snippet in the description).
</comment><comment author="uschindler" created="2016-06-16T19:54:29Z" id="226595072">This is a bug in above code, it should not even trigger string concats! It is just a side effect that it works.
</comment><comment author="uschindler" created="2016-06-16T19:56:21Z" id="226595572">Whats the exception you get on this?
</comment><comment author="uschindler" created="2016-06-16T20:01:39Z" id="226596928">Can somebody please update this outdated java version?
</comment><comment author="uschindler" created="2016-06-16T20:08:00Z" id="226598585">We need to first rint the bytecode. AIIOBE always happens when you fck up the stack frames.
</comment><comment author="nik9000" created="2016-06-16T20:16:49Z" id="226600926">&gt; Can somebody please update this outdated java version?

I'll poke the right folks for that.
</comment><comment author="nik9000" created="2016-06-16T20:18:27Z" id="226601341">&gt; I'll poke the right folks for that.

Done. It still happens for 122 as well.
</comment><comment author="nik9000" created="2016-06-16T20:18:45Z" id="226601412">&gt; Done. It still happens for 122 as well.

Or to be clear - I've poked the right people. They haven't upgraded yet.
</comment><comment author="uschindler" created="2016-06-16T20:22:59Z" id="226602491">&gt; Done. It still happens for 122 as well.

Yeah, that was just a side note. See above, the problem is a bug in compound statements. We are just lucky that Java 8 does not fail (because stack keeps valid).
</comment><comment author="uschindler" created="2016-06-16T20:26:14Z" id="226603370">I think the problem here is that the code in EChain does not correctly set cat=true, because the first arg is not actually a string. I think we only have tests for "string" + blabla, but never anyothertype + "xxx"
</comment><comment author="rmuir" created="2016-06-16T21:13:48Z" id="226615672">I see this if i print bytecode up to this point. Yes I am hooking this in so it will be easier!

```
 1&gt; hit exception during compile, bytecode so far: 
  1&gt; // class version 52.0 (52)
  1&gt; // access flags 0x31
  1&gt; public final class org/elasticsearch/painless/Executable$Script extends org/elasticsearch/painless/Executable  {
  1&gt;   // compiled from: &lt;debugging&gt;
  1&gt;   // access flags 0x1
  1&gt;   public &lt;init&gt;(Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
  1&gt;     ALOAD 0
  1&gt;     ALOAD 1
  1&gt;     ALOAD 2
  1&gt;     ALOAD 3
  1&gt;     INVOKESPECIAL org/elasticsearch/painless/Executable.&lt;init&gt; (Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
  1&gt;     RETURN
  1&gt;     MAXSTACK = 0
  1&gt;     MAXLOCALS = 0
  1&gt;   // access flags 0x1
  1&gt;   public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
  1&gt;    L0
  1&gt;     LINENUMBER 9 L0
  1&gt;    L1
  1&gt;     LINENUMBER 9 L1
  1&gt;     NEW java/util/HashMap
  1&gt;     DUP
  1&gt;     INVOKESPECIAL java/util/HashMap.&lt;init&gt; ()V
  1&gt;    L2
  1&gt;     LINENUMBER 9 L2
  1&gt;     ASTORE 5
  1&gt;    L3
  1&gt;     LINENUMBER 24 L3
  1&gt;     ALOAD 5
  1&gt;     DUP_X1
  1&gt;    L4
  1&gt;     LINENUMBER 25 L4
  1&gt;     INVOKEDYNAMIC cat(Ljava/lang/Object;)Ljava/lang/Object; [
  1&gt;       // handle kind 0x6 : INVOKESTATIC
  1&gt;       org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
  1&gt;       // arguments:
  1&gt;       1
  1&gt;     ]
  1&gt;     LDC "cat"
  1&gt;     INVOKEDYNAMIC concat(Ljava/lang/Object;Ljava/lang/String;)Ljava/lang/String; [
  1&gt;       // handle kind 0x6 : INVOKESTATIC
  1&gt;       java/lang/invoke/StringConcatFactory.makeConcat(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
  1&gt;       // arguments: none
  1&gt;     ]
  1&gt;     DUP_X1
  1&gt;    L5
  1&gt;     LINENUMBER 25 L5
  1&gt;     INVOKEDYNAMIC cat(Ljava/lang/Object;Ljava/lang/Object;)V [
  1&gt;       // handle kind 0x6 : INVOKESTATIC
  1&gt;       org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
  1&gt;       // arguments:
  1&gt;       2
  1&gt;     ]
  1&gt;    L6
  1&gt;     LINENUMBER 24 L6
  1&gt;     ARETURN
  1&gt;     MAXSTACK = 0
  1&gt;     MAXLOCALS = 0
```
</comment><comment author="uschindler" created="2016-06-16T21:16:33Z" id="226616373">How does Java 8 look like?
</comment><comment author="rmuir" created="2016-06-16T21:20:33Z" id="226617335">```
// class version 52.0 (52)
// access flags 0x31
public final class org/elasticsearch/painless/Executable$Script extends org/elasticsearch/painless/Executable  {

  // compiled from: &lt;debugging&gt;

  // access flags 0x1
  public &lt;init&gt;(Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
    ALOAD 0
    ALOAD 1
    ALOAD 2
    ALOAD 3
    INVOKESPECIAL org/elasticsearch/painless/Executable.&lt;init&gt; (Ljava/lang/String;Ljava/lang/String;Ljava/util/BitSet;)V
    RETURN
    MAXSTACK = 4
    MAXLOCALS = 4

  // access flags 0x1
  public execute(Ljava/util/Map;Lorg/apache/lucene/search/Scorer;Lorg/elasticsearch/search/lookup/LeafDocLookup;Ljava/lang/Object;)Ljava/lang/Object;
   L0
    LINENUMBER 9 L0
    LINENUMBER 9 L0
    NEW java/util/HashMap
    DUP
    INVOKESPECIAL java/util/HashMap.&lt;init&gt; ()V
   L1
    LINENUMBER 9 L1
    ASTORE 5
   L2
    LINENUMBER 24 L2
    NEW java/lang/StringBuilder
    DUP
    INVOKESPECIAL java/lang/StringBuilder.&lt;init&gt; ()V
    ALOAD 5
    DUP_X1
   L3
    LINENUMBER 25 L3
    INVOKEDYNAMIC cat(Ljava/lang/Object;)Ljava/lang/Object; [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
      // arguments:
      1
    ]
    INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/Object;)Ljava/lang/StringBuilder;
    LDC "cat"
    INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder;
    INVOKEVIRTUAL java/lang/StringBuilder.toString ()Ljava/lang/String;
    DUP_X1
   L4
    LINENUMBER 25 L4
    INVOKEDYNAMIC cat(Ljava/lang/Object;Ljava/lang/Object;)V [
      // handle kind 0x6 : INVOKESTATIC
      org/elasticsearch/painless/DefBootstrap.bootstrap(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;I[Ljava/lang/Object;)Ljava/lang/invoke/CallSite;
      // arguments:
      2
    ]
   L5
    LINENUMBER 24 L5
    ARETURN
    MAXSTACK = 3
    MAXLOCALS = 6
}
```
</comment><comment author="uschindler" created="2016-06-16T21:33:59Z" id="226620614">Hi,
I think I know the problem:
The Java 8 old code assumes while duplicating stack arguments that there is a stringbuilder on stack. But for Java 9, it is not there. So the "dup_x1" after the "aload 5" duplicates the wrong stack item.

I have to look at the generating code ton find a way how to handle this correctly. In fact we duplicate the wronmg item. For indy string concats a plain "dup" would be correct.
</comment><comment author="rmuir" created="2016-06-16T21:38:21Z" id="226621602">I opened #18932 with the Debugger improvement.
</comment><comment author="uschindler" created="2016-06-16T21:38:28Z" id="226621627">The bug is in EChain#write: 

``` java
writer.writeDup(link.size, 1);         // dup the StringBuilder
```

See the comment. The second arguments must be 0 for indy string concats. Not sure how to fix this.
</comment><comment author="rmuir" created="2016-06-16T21:45:39Z" id="226623249">Yes, thats the fix. all tests pass with that on java 9. we just have to conditionally do it.
</comment><comment author="uschindler" created="2016-06-16T21:51:01Z" id="226624456">I have to verify the other parts doing concats, too. The compound code is hairy (and very complicated), and we had stack bugs before, too.

The second problem with indy string concats is: You pop a lot of stuff onto stack, while with Java 8 you pop one item and call append. In Java 9 you have no chance to look back in stack unless you record sizes - but then bytecode does not allow it.
</comment><comment author="uschindler" created="2016-06-16T21:55:44Z" id="226625481">EBinary looks fine, no crazy stack duping. So its only EChain that hits the issue. I think we can easily fix this:
- I let writeNewStrings return an int (the actual size of element it pushed), for java 8 it returns 1, with indy concats just 0. This is the size of the element on stack used for the concats
- we just use return value in the dup instruction
</comment><comment author="uschindler" created="2016-06-16T22:32:24Z" id="226633052">I added a PR (see above).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file></files><comments><comment>Fix compound assignment with string concats. in Java 9 there is no stringbuilder on stack! This closes #18929</comment></comments></commit><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/StringTests.java</file></files><comments><comment>Painless: Disable java-9 indy string thing</comment></comments></commit></commits></item><item><title>Upgrading cluster from 1.7 to 2.3 mapper attachment plugin copied data but not mapping settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18928</link><project id="" key="" /><description>**Elasticsearch version**:
Upgrading from 1.7 to 2.3.3

**Mapper Attachment version**
Upgrading from 2.7.1(version targeted for es1.7) to 2.3.3

**JVM version**:
1.8.0_91

**OS version**:
Windows Server 2012 R2 Standard

**Description of the problem including expected versus actual behavior**:
I'm upgrading our 1.7 cluster to the latest 2.3 cluster. We have 2 nodes but the topology of our cluster isn't relevant for this issue.

After successfully upgrading our cluster I noticed the mapping for our attachment fields did not carry over. Starting with mapper attachment version 3.0.3 the meta field containing the indexed attachment content changed from the name of the field to "content". The upgrade successfully copied my data to the new "content" meta field but the field settings (analyzer, term vectors) were not copied over. This is important because the data was indexed using English analyzer and that's how it's stored in the field. I verified this by returning script field document("field.content") to see what data is actually in the field.

This concrete example will explain it much better:

This is my attachment field mapping in ES1.7

``` javascript
"contentPdf": {
        "path": "full",
        "type": "attachment",
        "fields": {
          "date": {
            "format": "dateOptionalTime",
            "type": "date"
          },
          "keywords": {
            "type": "string"
          },
          "content_type": {
            "type": "string"
          },
          "author": {
            "type": "string"
          },
          "name": {
            "type": "string"
          },
          "language": {
            "type": "string"
          },
          "title": {
            "type": "string"
          },
          "contentPdf": {
            "analyzer": "english",
            "term_vector": "with_positions_offsets",
            "store": true,
            "type": "string",
            "fields": {
              "standard": {
                "analyzer": "standard",
                "type": "string"
              }
            }
          },
          "content_length": {
            "store": true,
            "type": "string"
          }
        }
```

There you can see the meta field with the actual indexed content is called `contentPdf` - has to be the same name as the property field name. There I set an `english` analyzer, `term_vector`, `store`, and a sub field with a `standard` analyzer.

After upgrading both ES and the mapper plugin to 2.3.3 this is my mapping

``` javascript
"contentPdf": {
        "path": "full",
        "type": "attachment",
        "fields": {
          "date": {
            "format": "epoch_millis||dateOptionalTime",
            "type": "date"
          },
          "keywords": {
            "type": "string"
          },
          "content_type": {
            "type": "string"
          },
          "contentPdf.content": {
            "type": "string",
            "index_name": "contentPdf"
          },
          "author": {
            "type": "string"
          },
          "name": {
            "type": "string"
          },
          "language": {
            "type": "string"
          },
          "title": {
            "type": "string"
          },
          "content_length": {
            "store": true,
            "type": "string"
          }
        }
```

As you can see, the field properties were not copied over from `contentPdf` meta field to `contentPdf.content`. The plugin also added `index_name` for backwards compatibility I believe.

I verified the data was copied by running a query and specifying script fields so I could see what was actually in the index:

``` javascript
"script_fields": {
    "pdf content": {
      "script": "doc['contentPdf']"
    }
  },
```

**Problems this causes**
1. Attachment field can't be queried properly because the analyzer info is gone. Since I was using an `english` analyzer all words were indexed with their stemmed form but stemming is not applied when queried. Future documents will be indexed using default analyzer (`standard`) for the content field.
2. Hit highlighting will not work without `term_vectors: with_positions_offsets` and `store: true`.

**Solution if this is not fixed**
After I realized the mapping was not copied over, I reverted to ES1.7 and attempted to add a `content` meta field with correct settings to my `contentPdf` property using PUT mapping API, hoping that during the upgrade the data will be copied to a field that already has the correct mapping. Unfortunately attachment fields do not let you add any other meta fields. I verified this by creating another attachment field with test meta fields and my test meta fields were not taken. 

The only alternative I see is to reindex into a 2.3 cluster then switch over to the new cluster. This is what we are preparing to do right now.

**Steps to reproduce**:
1. Use PUT mapping API to create a type with an `attachment` property and set `"term_vector": "with_positions_offsets",
           "store": true` on the meta field with the same name as the property. E.g.

``` javascript
{
  "TestDoc": {
     "properties" :{
        "myAttachment": {
        "type": "attachment",
        "fields": {
          "myAttachment": {
            "term_vector": "with_positions_offsets",
            "store": true,
            "type": "string"
          }
     }
 }
}
```
1. Index a document with a base64 encoded binary file as the content of the `myAttachment` field
2. Shut down all nodes in cluster, upgrade ES and mapper plugin to 2.3.3, restart cluster
   1.  Data will now be in `myAttachment.content` field but the mappings for that field will be missing.

Thanks!
</description><key id="160719451">18928</key><summary>Upgrading cluster from 1.7 to 2.3 mapper attachment plugin copied data but not mapping settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pholly</reporter><labels /><created>2016-06-16T17:43:16Z</created><updated>2016-06-21T08:40:37Z</updated><resolved>2016-06-21T07:53:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-21T07:53:29Z" id="227366730">Hi @pholly 

I'm afraid this is an upgrade bug we've just recently discovered and you are correct: your only solution is to reindex these documents.  To compound matters, `contentPdf.content` can't have sub-fields, and doesn't support `copy_to` either.  I realise that this is a real problem, but I can't offer you much of a solution in 2.x.

In 5.x, we have deprecated the mapper-attachements plugin in favour of the ingest-attachment plugin.  This will extract your document during ingestion so that fields like content, author, etc will actually become part of the `_source` and so will work like any other non-magical field.

Sorry I can't give you better news.
</comment><comment author="clintongormley" created="2016-06-21T08:40:37Z" id="227376942">@pholly one correction, the `.content` field _can_  have sub-fields after all
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add flag support to regexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18927</link><project id="" key="" /><description>- Add support for //m
- Add support for //s
- Add support for //i
- Add support for //u
- Add support for //U
- Add support for //l

This means "literal" and is exposed for completeness sake with the java api.
- Add support for //c

c enables Java's CANON_EQ (canonical equivalence) flag which makes unicode characters that are canonically equal match. Java's javadoc gives "a\u030A" being equal to "\u00E5". That is that the "a" code point followed by the "combining ring above" code point is equal to the "a with combining ring above" code point.
</description><key id="160714029">18927</key><summary>Add flag support to regexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T17:16:20Z</created><updated>2016-06-20T12:00:21Z</updated><resolved>2016-06-16T19:01:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-16T18:42:20Z" id="226575996">@rmuir I rewrote the tests and made most of pattern accessible. 
</comment><comment author="rmuir" created="2016-06-16T18:52:10Z" id="226578833">+1

thanks again for tackling all this regex stuff
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 6.1.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18926</link><project id="" key="" /><description>The release is not announced yet but the jars are on Maven Central already.
</description><key id="160698013">18926</key><summary>Upgrade to Lucene 6.1.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T15:59:19Z</created><updated>2016-06-28T10:04:39Z</updated><resolved>2016-06-17T07:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-16T16:07:38Z" id="226533293">LGTM
</comment><comment author="litong01" created="2016-06-16T16:09:06Z" id="226533735">is there any way at all to make a dependency update a bit easier? not necessarily this patch, just in general. too many files need to be touched.
</comment><comment author="jpountz" created="2016-06-17T07:02:11Z" id="226695826">If you are talking about the sha1 files, this is automated in the gradle build (`gradle updateShas`). For the security files we have been careful about changing the version automatically in there since policy files are not something you want to mess up with, but that could be automated too I guess.

And for the record, there are a lot of things that are already transparent like adding a snapshot mirror when we depend on a lucene snapshot, etc.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java</file></files><comments><comment>Upgrade to Lucene 6.1.0. #18926</comment></comments></commit></commits></item><item><title>Multi_match search query ignoring fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18925</link><project id="" key="" /><description>I have simple tests to test elasticsaerch queries, but sometimes queries behave very strange - like multi_match sometimes ignoring fields I'd like to query.

Eg. I have following address data

| Margaretenstraße | 1            | Krailling | Deutschland |
| Margaretenstraße | 2A          | Krailling | Deutschland |
| Margaretenstraße | 3            | Krailling | Deutschland |
| Margaretenstraße | 4            | Krailling | Deutschland |

I created index - only simple without analysing.

I called _search query

```
{
  "explain": true, 
  "query": {
    "multi_match": {
      "type": "most_fields",
      "query": "margaretenstraße krailling 4",
      "tie_breaker": 1,
      "fields": [
        "house_number",
        "street",
        "city"
      ]
    }
  }
}
```

And it returns correct object "margaretenstraße krailling 4". But when I add some boosting to city - city^2 it looks like it ignores house number field and returns "margaretenstraße krailling 4". When I analyzed scoring in explain, there is **no score for matching house_number field at all**.

What can cause this problem? Is it common behavior? I tried it more times and sometimes when I create a new, the same index it works but sometimes not.
</description><key id="160689530">18925</key><summary>Multi_match search query ignoring fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cechovsky</reporter><labels /><created>2016-06-16T15:24:22Z</created><updated>2016-06-27T13:41:38Z</updated><resolved>2016-06-17T14:49:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cechovsky" created="2016-06-16T15:48:06Z" id="226527300">I analyzed scoring explanation and
- the same data
- the same query

and scoring explanation is different
![image](https://cloud.githubusercontent.com/assets/2878125/16123206/781ffb2a-33ea-11e6-9de0-3350f00aa7ba.png)
</comment><comment author="clintongormley" created="2016-06-17T14:49:07Z" id="226789659">This is just a side effect of query rewriting.  You're probably using 5 shards with a small amount of data, so you have one document on each shard.  The term `4` only appears on one shard, and so will only appear in the explain output for that doc.  The other docs won't have it.

Try creating the index with just one shard (or adding lots more data), then it will work correctly:

```
PUT t 
{
  "settings": {
    "number_of_shards": 1
  }
}


POST t/t/_bulk
{"index": {}}
{"house_number": "1", "street": "Margaretenstraße", "city": "Krailling"}
{"index": {}}
{"house_number": "2A", "street": "Margaretenstraße", "city": "Krailling"}
{"index": {}}
{"house_number": "3", "street": "Margaretenstraße", "city": "Krailling"}
{"index": {}}
{"house_number": "4", "street": "Margaretenstraße", "city": "Krailling"}

GET t/_search
{
  "explain": true,
  "query": {
    "multi_match": {
      "type": "most_fields",
      "query": "margaretenstraße krailling 4",
      "tie_breaker": 1,
      "fields": [
        "house_number",
        "street",
        "city^2"
      ]
    }
  }
}
```
</comment><comment author="cechovsky" created="2016-06-26T13:09:16Z" id="228600662">Thank you. You're right, number_of_shards fixed it.

But I'm still wondering why it has such behavior that when I search for "margaretenstraße krailling 4" and I'm sure that I have this address in my index why it returns "margaretenstraße krailling 1". 
</comment><comment author="clintongormley" created="2016-06-27T13:41:38Z" id="228748571">See https://www.elastic.co/guide/en/elasticsearch/guide/2.x/relevance-is-broken.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>improve lambda syntax (allow single expression)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18924</link><project id="" key="" /><description>Currently lambdas need both braces and semicolons, which is onerous for many situations. Java allows a single expression too, instead of:

```
.mapToInt(x -&gt; { x + 1; })
```

```
.mapToInt(x -&gt; x + 1)
```

I really have no idea if it creates ambiguity or problems, but it seems to work. Someone that actually knows grammars needs to help me with this change :) 
</description><key id="160685386">18924</key><summary>improve lambda syntax (allow single expression)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T15:07:27Z</created><updated>2016-06-17T14:54:44Z</updated><resolved>2016-06-16T17:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-16T15:37:07Z" id="226523874">Neat. It LGTM but I think @jdconrad will want to look too.

As for ambiguity - it might be worth testing lambdas with and without in a for loop and comparing the compilation time. I still think it should be ok though.
</comment><comment author="uschindler" created="2016-06-16T16:13:07Z" id="226534867">...I have no idea about antlr, but the syntax improevemnts look good.

The main difference in Java is: with `{}` braces you need a `return` statement, without braces, its not needed. And you can only give one expression.
</comment><comment author="rmuir" created="2016-06-16T16:17:06Z" id="226536024">Yes its the same here. `return` is a statement (SReturn), so it cannot work (same as java). This syntax only allows an expression.  Java works hard to avoid ambiguities, so thats why i think it has the possibility to be safe.
</comment><comment author="jdconrad" created="2016-06-16T16:32:24Z" id="226540216">+1 here.  If ambiguities come up during testing, we'll resolve them then.  Mimicking Java exactly to have expressions return nothing would be a bit more difficult since we would have to turn off auto-return, and it would be really difficult if we didn't make the expression into a statement node with the way things are currently setup.
</comment><comment author="rmuir" created="2016-06-16T16:41:36Z" id="226542657">I added a few more simple tests: lambda inside loop, nested lambda, method taking two lambda parameters.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/LambdaTests.java</file></files><comments><comment>Merge pull request #18924 from rmuir/improve_lambda_syntax</comment></comments></commit></commits></item><item><title>Update snapshots.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18923</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="160681771">18923</key><summary>Update snapshots.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">easherma</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-06-16T14:52:56Z</created><updated>2016-06-20T14:00:15Z</updated><resolved>2016-06-20T13:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-17T14:25:49Z" id="226783131">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="easherma" created="2016-06-17T20:21:33Z" id="226872040">Sure thing, just signed. Enjoy your comma! ;-p

On Fri, Jun 17, 2016 at 9:26 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Thanks for the PR. Please could I ask you to sign the CLA so that I can
&gt; merge it in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/18923#issuecomment-226783131,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AKDTT7SJB2M1RyP2q4o1Yi_KNwjrnSgNks5qMq6tgaJpZM4I3eHU
&gt; .

## 

Eric Sherman
224-402-0004
</comment><comment author="clintongormley" created="2016-06-20T14:00:15Z" id="227150203">that comma was awesome! 💃 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update snapshots.asciidoc (#18923)</comment></comments></commit></commits></item><item><title>Template field in templates field name vagueness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18922</link><project id="" key="" /><description>**Description of the problem including expected versus actual behavior**:
The short explaination:
The field template In templates is vague because it is pattern matching the indices which will be working on, I think that the name should be somthing like Index pattern or so.

The Long one:
First to mention, I have started to use elasticsearch today,
I have used a twitter template which I couldn't understand why is not working.
I have searched the template file maybe It contains the index name and thats the reason It didn't work , I have noticed that when I changed the index name to the template name everything was just fine so I understood that I need to write about Index templates and I have understood that there is an index pattern matching with the template name.
</description><key id="160677491">18922</key><summary>Template field in templates field name vagueness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davebarda</reporter><labels><label>:Index Templates</label><label>enhancement</label></labels><created>2016-06-16T14:35:45Z</created><updated>2016-11-03T13:35:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-17T07:14:25Z" id="226697665">Do you have any recommendation?
</comment><comment author="davebarda" created="2016-06-17T08:42:31Z" id="226714130">Well as I have wrote,
I think that index pattern should be suggestive, 
It will make people are new to templates to check what is all about.
</comment><comment author="jpountz" created="2016-06-17T08:50:56Z" id="226715827">OK, so something like:

```
PUT /_template/template_1
{
  "index_pattern": "te*",
  "settings": {
    "number_of_shards": 1
  }
}
```

rather than

```
PUT /_template/template_1
{
  "template": "te*",
  "settings": {
    "number_of_shards": 1
  }
}
```
</comment><comment author="clintongormley" created="2016-06-17T15:14:15Z" id="226796844">I agree - I've never liked that template parameter.  
</comment><comment author="clintongormley" created="2016-06-17T15:15:24Z" id="226797165">Depending on which gets in first, also see https://github.com/elastic/elasticsearch/pull/17300
</comment><comment author="nik9000" created="2016-11-03T13:35:47Z" id="258143569">Removing `adoptme` because we have an open PR for this: #21009
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update function-score-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18921</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="160675896">18921</key><summary>Update function-score-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">boguth</reporter><labels /><created>2016-06-16T14:29:29Z</created><updated>2016-06-16T14:39:16Z</updated><resolved>2016-06-16T14:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve error message if a setting is not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18920</link><project id="" key="" /><description>Today we only emit that the setting wasn't found unless we have
some DYM suggestions. Yet, if a setting is not found at all and there
are no suggestions due to typos it's likely a removed setting or the plugin
that is supposed to be configured is not installed.
This commit adds some info text to the exception to help the user debugging
the problem before opening bug-reports.

Instead of emitting:

`unknown setting [foo.bar]`

we now emit:

`unknown setting [foo.bar] please check the migration guide for removed settings and ensure that the plugin you are configuring is installed`

Relates to #18663

@clintongormley can you take a look
</description><key id="160660963">18920</key><summary>Improve error message if a setting is not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T13:25:18Z</created><updated>2016-06-20T15:41:01Z</updated><resolved>2016-06-20T11:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="litong01" created="2016-06-16T13:43:25Z" id="226488682">Is it all possible to indicate where the migration guide might be? I understand that the change wants to be helpful when there is a problem, but this change feels like the error message "Contact your administrator". If have a link to the migration guide is not possible, then this change at least provides some hints, so still a good effort, just my 2 cents. Thanks.
</comment><comment author="s1monw" created="2016-06-16T13:46:34Z" id="226489546">&gt; Is it all possible to indicate where the migration guide might be? I understand that the change wants to be helpful when there is a problem, but this change feels like the error message "Contact your administrator". If have a link to the migration guide is not possible, then this change at least provides some hints, so still a good effort, just my 2 cents. Thanks.

I try to not have version dependent links in the code. I think that will only lead to stale links?
</comment><comment author="clintongormley" created="2016-06-16T13:53:37Z" id="226491541">I agree with not having the doc links, but we don't really have something called the migration guide. perhaps:

```
please check that any required plugins are installed, or check the breaking changes documentation for removed settings
```
</comment><comment author="litong01" created="2016-06-16T14:12:56Z" id="226497227">@clintongormley good suggestion. Can we even improve the message a bit more? I think in many cases probably it is just a misspell.

"please check that any required plugins are installed, removed settings are not used and settings are correctly spelled"
</comment><comment author="s1monw" created="2016-06-16T14:16:47Z" id="226498368">&gt; @clintongormley good suggestion. Can we even improve the message a bit more? I think in many cases probably it is just a misspell.

@litong01 if its a spelling you get a did you mean message with the right setting instead of this message
</comment><comment author="litong01" created="2016-06-16T14:43:13Z" id="226506511">@s1monw thanks for your response, I just think in many cases, the misspell of a setting is to blame. Not really sure you can guess what the right setting is though.
</comment><comment author="s1monw" created="2016-06-16T14:52:18Z" id="226509264">&gt; @s1monw thanks for your response, I just think in many cases, the misspell of a setting is to blame. Not really sure you can guess what the right setting is though.

we can! try it out it's pretty good though. We use the same technology as used for the spellchecker.
</comment><comment author="s1monw" created="2016-06-20T08:38:31Z" id="227082128">@clintongormley I updated the msg...
</comment><comment author="jpountz" created="2016-06-20T10:48:03Z" id="227109944">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/state/SimpleIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file></files><comments><comment>Improve error message if a setting is not found (#18920)</comment></comments></commit></commits></item><item><title>Mark shard copies only as stale when other active copies receive acknowledged writes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18919</link><project id="" key="" /><description>We're currently tracking non-stale shard copies using their allocation ids in the cluster state. When a node leaves the cluster, shard copies of that node are currently marked as stale by removing their allocation ids from the active set in the cluster. For full cluster restarts, this can have the unwanted effect that only the last node holding a copy of the shard will be seen as non-stale. The other shard copies are not really stale though as long as no writes have happened on this shard copy. Shard copies should thus only be marked as stale (by the master in the cluster state) if other active shards have received acknowledged writes.
</description><key id="160653347">18919</key><summary>Mark shard copies only as stale when other active copies receive acknowledged writes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-beta1</label></labels><created>2016-06-16T12:48:45Z</created><updated>2016-09-14T14:44:26Z</updated><resolved>2016-08-30T09:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-08-30T09:14:05Z" id="243380496">Closed by #20023
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Compile each Groovy script in its own classloader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18918</link><project id="" key="" /><description>This commit changes the `GroovyScriptEngineService` so that each Groovy script is compiled in its own classloader. This way `GroovyClassLoader` and `GroovyClassLoader$InnerLoader` are correctly garbaged collected when scripts are removed from cache.

It also removes the forced cache clearing call on script's cache eviction that is now useless (groovy script class are cached in its own classloader that will be garbage collected).  

Closes #18572
</description><key id="160648340">18918</key><summary> Compile each Groovy script in its own classloader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T12:22:10Z</created><updated>2017-01-05T20:15:03Z</updated><resolved>2016-06-20T08:32:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-16T12:23:55Z" id="226469727">@rmuir Sorry to bother you, but it would be great if you find some time to review this, thanks!
</comment><comment author="rmuir" created="2016-06-16T14:05:40Z" id="226495115">hi @tlrx, thanks for looking into this! this looks good, like what I had in mind. 
</comment><comment author="s1monw" created="2016-06-20T07:56:53Z" id="227073839">LGTM 2 
</comment><comment author="s1monw" created="2016-06-20T07:57:22Z" id="227073935">@tlrx I think if it's simple lets backport to 2.x
</comment><comment author="tlrx" created="2016-06-20T08:34:13Z" id="227081231">@rmuir @s1monw Thanks for the reviews!

It should be OK to backport it, I'll do that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MapperParsingException and field mapping conflict all out of the sudden</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18917</link><project id="" key="" /><description>**Elasticsearch version**:

&gt; {
&gt;   "name" : "Alpha Ray",
&gt;   "cluster_name" : "elasticsearch",
&gt;   "version" : {
&gt;     "number" : "2.3.3",
&gt;     "build_hash" : "218bdf10790eef486ff2c41a3df5cfa32dadcfde",
&gt;     "build_timestamp" : "2016-05-17T15:40:04Z",
&gt;     "build_snapshot" : false,
&gt;     "lucene_version" : "5.5.0"
&gt;   },
&gt;   "tagline" : "You Know, for Search"
&gt; }

**JVM version**:

&gt; openjdk version "1.8.0_91"
&gt; OpenJDK Runtime Environment (build 1.8.0_91-b14)
&gt; OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:

&gt; CentOS Linux release 7.2.1511 (Core)

**Description of the problem including expected versus actual behavior**:
One of my syslog inputs from logstash stopped working this morning exactly at 2:00 am
Elasticsearch didn't add any new shards for this very type past this point but started throwing a MapperParsingException in elasticsearch.log for every new shard like:

```
MapperParsingException[failed to parse [timestamp]]; nested: IllegalArgumentException[Invalid format: "Jun 16 02:00:28"];
        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:329)
        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309)
        at org.elasticsearch.index.mapper.DocumentParser.parseAndMergeUpdate(DocumentParser.java:738)
        at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:625)
        at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:442)
        at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262)
        at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:529)
        at org.elasticsearch.index.shard.IndexShard.prepareCreateOnPrimary(IndexShard.java:506)
        at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:215)
        at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:119)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid format: "Jun 16 02:00:28"
        at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187)
        at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
        at org.elasticsearch.index.mapper.core.DateFieldMapper$DateFieldType.parseStringValue(DateFieldMapper.java:362)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:528)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:241)
        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:321)
        ... 24 more
```

Note that this is the timestamp field from syslog, not the @timestamp added by elasticsearch.
Furthermore the field in question now shows a mapping conflict in kibana, it was previously mapped as a string and the last shard created was at 1:59:36 am

Now as i see it elasticsearch is suddenly trying to parse the said timestamp field as an actual timestamp instead of a string (which cannot be parsed as such since it is missing the year part i suppose) which in return is creating the mapping conflict.

The setup has been running for a while by now and nothing on the configuration has been changed recently, hence i am curious what has caused elasticsearch to thread "Jun 16 01:59:36" as a string and "Jun 16 02:00:28" as a timestamp.
</description><key id="160644346">18917</key><summary>MapperParsingException and field mapping conflict all out of the sudden</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">defkev</reporter><labels /><created>2016-06-16T11:59:14Z</created><updated>2016-06-16T14:52:13Z</updated><resolved>2016-06-16T13:40:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="defkev" created="2016-06-16T13:40:16Z" id="226487782">Never mind.
After some further digging it looks this is to blame on one of my log sources which is sending its json formatted message with a timestamp (instead of a @timestamp) field populated in ISO8601 

&gt; 2016-06-16T14:59:41.828214+0200

If such a message is the first to hit logstash after elasticsearch creates a new index at 2:00 am the timestamp field will of course get mapped as a date instead of a string.
This in return is creating the MapperParsingException and field mapping conflict once one of my other log sources, which are all either not sending a timestamp field in their json formated messages (or are sending a @timestamp in ISO8601) causing elastic to use the syslog time for the timestamp field which is not in ISO8601 but syslogs rfc3164 default format

&gt; Jun 16 01:59:36

tl;dr: Mapping a date as a string is not a problem, mapping a string as a date is.

Guess i was just lucky in the past this never happened before during the daily index rotate :hankey:
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>build it with gradle error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18916</link><project id="" key="" /><description>os : windows 7
gradle: 2.14

----------please  add  pom.xml  , thanks
## error:
- Where:
  Build file 'E:\elasticsearch-mapper-attachments-master\build.gradle' line: 20
- What went wrong:
  Could not compile build file 'E:\elasticsearch-mapper-attachments-master\build.g
  radle'.
  
  &gt; startup failed:
  &gt; build file 'E:\elasticsearch-mapper-attachments-master\build.gradle': 20: unab
  &gt; le to resolve class org.elasticsearch.gradle.ElasticsearchProperties
  &gt; @ line 20, column 1.
  &gt;    import org.elasticsearch.gradle.ElasticsearchProperties
  &gt;    ^
  
  1 error
</description><key id="160633536">18916</key><summary>build it with gradle error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzhou2020</reporter><labels /><created>2016-06-16T10:54:56Z</created><updated>2016-07-11T09:57:12Z</updated><resolved>2016-06-16T11:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-16T11:31:29Z" id="226459586">We have no idea about what you are doing.
We are building the project every day and we don't really hit issues like this.

You could ask for help on discuss.elastic.co but provide more info please.

Closing. Feel free to reopen if you think it's a bug.
</comment><comment author="jasontedor" created="2016-06-16T11:32:19Z" id="226459764">This appears to be from the [elastic/elasticsearch-mapper-attachments](https://github.com/elastic/elasticsearch-mapper-attachments) repository, so if you're trying to build for 2.x, you should open an issue in that repository.
</comment><comment author="dadoonet" created="2016-06-16T11:47:24Z" id="226462520">lol. He did: https://github.com/elastic/elasticsearch-mapper-attachments/issues/217 :)

But from 2.2 IIRC, mapper-attachments is now living here. (with maven)
</comment><comment author="xuzhou2020" created="2016-06-17T00:57:04Z" id="226655113">Hi:
     just add pom.xml,  so I can build it with maven .
     thank you
</comment><comment author="dadoonet" created="2016-06-17T04:55:13Z" id="226680402">No. We won't do that.
You have to use gradle to build the latest version.
If you are looking at the 2.x version, it uses Maven. Just switch to 2.x branch.
</comment><comment author="javanna" created="2016-06-17T07:29:36Z" id="226700170">This seems the same problem as #18935 after all.
</comment><comment author="xuzhou2020" created="2016-06-17T09:01:31Z" id="226718027">## now test again:
1. Windows 7
2. jdk1.8 64bit
3. gradle 2.13
   ------------------gradle build 
   wait for a long time.... ok

file list  in dir
--E:\elasticsearch-master\plugins\mapper-attachments\build\distributions
mapper-attachments-5.0.0-alpha4-SNAPSHOT.jar
mapper-attachments-5.0.0-alpha4-SNAPSHOT.pom
mapper-attachments-5.0.0-alpha4-SNAPSHOT.zip
mapper-attachments-5.0.0-alpha4-SNAPSHOT-javadoc.jar
mapper-attachments-5.0.0-alpha4-SNAPSHOT-sources.jar

--- elasticsearch  version
{
  "name" : "Lodestone",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.2.1",
    "build_hash" : "d045fc29d1932bce18b2e65ab8b297fbf6cd41a1",
    "build_timestamp" : "2016-03-09T09:38:54Z",
    "build_snapshot" : false,
    "lucene_version" : "5.4.1"
  },
  "tagline" : "You Know, for Search"
}

=========now  install mapper-attachments=========
E:\elasticsearch-rtf-master\bin&gt;plugin install file:///e:/mapper-attachments-5.0
.0-alpha4-SNAPSHOT.zip
-&gt; Installing from file:/e:/mapper-attachments-5.0.0-alpha4-SNAPSHOT.zip...
Trying file:/e:/mapper-attachments-5.0.0-alpha4-SNAPSHOT.zip ...
Downloading ....................................................................
................................................................................
.............................................................DONE
Verifying file:/e:/mapper-attachments-5.0.0-alpha4-SNAPSHOT.zip checksums if ava
ilable ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .
md5 file to verify)
ERROR: Plugin [mapper-attachments] must be at least a jvm or site plugin

so, what's wrong with me ???

please help
</comment><comment author="dadoonet" created="2016-06-17T09:04:04Z" id="226718612">You built a plugin for 5.0 but you are trying to install it on another version.

Not sure what is exactly your need but please ask on discuss.elastic.co.
</comment><comment author="xuzhou2020" created="2016-06-17T09:34:41Z" id="226725135">Hi:

```
maybe I didn't say it clear,  
my elasticsearch verion is  2.2.1 
so I choose "github"
and when I build it with  gradle.   
then    result :   mapper-attachments-5.0.0-alpha4-SNAPSHOT.zip

so now  I am  puzzled...
```

---

https://github.com/elastic/elasticsearch-mapper-attachments/

```
 bin/plugin install elasticsearch/elasticsearch-mapper-attachments/3.1.2
```

You need to install a version matching your Elasticsearch version:

Elasticsearch   Attachments Plugin  Docs

&gt; 2.1   elasticsearch repository    github
&gt; es-2.1  Build from source   es-2.1
&gt; 2.1.2   3.1.2   3.1.2
&gt; 2.1.1   3.1.1   3.1.1
&gt; 2.1.0   3.1.0   3.1.0
&gt; es-2.0  Build from source   es-2.0
&gt; 2.0.2   3.0.4   3.0.4
&gt; 2.0.1   3.0.3   3.0.3
&gt; 2.0.0   3.0.2   3.0.2
&gt; 2.0.0-beta2 3.0.1   3.0.1
&gt; 2.0.0-beta1 3.0.0   3.0.0
&gt; es-1.7  2.7.1   2.7.1
&gt; es-1.6  2.6.0   2.6.0
&gt; es-1.5  2.5.0   2.5.0
&gt; es-1.4  2.4.3   2.4.3
&gt; es-1.3  2.3.2   2.3.2
&gt; es-1.2  2.2.1   2.2.1
&gt; es-1.1  2.0.0   2.0.0
&gt; es-1.0  2.0.0   2.0.0
&gt; es-0.90 1.9.0   1.9.0
&gt; To build a SNAPSHOT version, you need to build it with Maven:

gradle clean assemble
plugin --install mapper-attachments \
       --url file:target/releases/elasticsearch-mapper-attachments-X.X.X-SNAPSHOT.zip
</comment><comment author="dadoonet" created="2016-06-17T10:25:58Z" id="226735493">I see. You did not notice the very first lines of the README, right?

So read the doc: https://www.elastic.co/guide/en/elasticsearch/plugins/2.2/mapper-attachments.html

You don't need to build the plugin...
</comment><comment author="rjernst" created="2016-06-18T17:08:15Z" id="226953440">@dadoonet can we delete the master branch in that repo to remove some confusion? The gradle code there is horribly out of date (still points to a 3.0 snapshot of build-tools), and will never be used.
</comment><comment author="dadoonet" created="2016-06-18T17:23:25Z" id="226954174">I asked the same myself for all plugins some months ago.
I think it makes sense. I'd like to remove all files but the README or remove the master branch and default to es-1.7 branch.
</comment><comment author="rjernst" created="2016-06-18T17:26:58Z" id="226954349">&gt; I'd like to remove all files but the README

That sounds the best to me.
</comment><comment author="dadoonet" created="2016-07-11T09:57:12Z" id="231691579">@rjernst FYI I cleaned most of our repos ^^^ :) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added ItemsAPI in integrations page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18915</link><project id="" key="" /><description /><key id="160622297">18915</key><summary>Added ItemsAPI in integrations page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cigolpl</reporter><labels><label>docs</label></labels><created>2016-06-16T09:53:24Z</created><updated>2016-06-17T14:19:10Z</updated><resolved>2016-06-17T14:19:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-17T14:19:10Z" id="226781303">thanks @cigolpl - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update integrations.asciidoc (#18915)</comment></comments></commit></commits></item><item><title>Add total_indexing_buffer/_in_bytes to nodes info API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18914</link><project id="" key="" /><description>@bleskes suggested this in #18651.

When you set the indexing buffer (default: 10% of JVM's heap) it's somewhat non-trivial, since it has a buffer size and also min/max, so it would be nice to include what actual indexing buffer the node got
in nodes info.

I just added the indexing buffer size to `NodeInfo`, and expose it in the nodes info response as `total_indexing_buffer` (human readable) and `total_indexing_buffer_in_bytes`.

I also tried to improve the docs for the "core settings" returned by nodes info API.
</description><key id="160608754">18914</key><summary>Add total_indexing_buffer/_in_bytes to nodes info API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T08:47:05Z</created><updated>2016-06-23T14:11:46Z</updated><resolved>2016-06-22T21:13:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="litong01" created="2016-06-16T14:03:40Z" id="226494539">@mikemccand what happens when the setting is missing? is there a default value? Can not seem to figure out where the setting will be enforced. Thanks.
</comment><comment author="mikemccand" created="2016-06-16T16:50:00Z" id="226544908">@litong01 the setting defaults to 10% of the JVM's heap.
</comment><comment author="jpountz" created="2016-06-17T07:12:02Z" id="226697264">LGTM
</comment><comment author="bleskes" created="2016-06-17T07:41:28Z" id="226702264">Thx @mikemccand for picking this up - I left some minor comments
</comment><comment author="mikemccand" created="2016-06-20T18:24:21Z" id="227226619">Thank you @bleskes and @jpountz: I folded in the feedback.  I think it's ready.
</comment><comment author="mikemccand" created="2016-06-22T13:57:03Z" id="227751488">@bleskes I pushed another commit with your feedback, thanks!
</comment><comment author="bleskes" created="2016-06-22T13:59:56Z" id="227752343">LGTM. Left one minor comment - no need for another review imo. Thanks again @mikemccand 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/NodeInfoStreamingTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoIT.java</file></files><comments><comment>Merge pull request #18914 from mikemccand/node_info_indexing_buffer</comment></comments></commit></commits></item><item><title>Remove useless dropArguments in megamorphic cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18913</link><project id="" key="" /><description>The documentation of MethodHandles.foldArguments  document, that the combiner method does not need to have equal arguments, it must just have the first ones matching the target signature. Because of that we don't need dropArguments. This should improve performance, as we don't push args on stack that we dont use!

I also added some tests for the megamorphic case, including stack inspection (so we see that it really works megamorphic).
</description><key id="160607699">18913</key><summary>Remove useless dropArguments in megamorphic cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T08:41:20Z</created><updated>2016-06-17T14:21:44Z</updated><resolved>2016-06-16T12:07:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-16T09:47:17Z" id="226438971">I also added a fix for a comment in the monomorphic case
</comment><comment author="rmuir" created="2016-06-16T12:09:25Z" id="226466748">Thanks @uschindler ! I need to check if we are doing this anywhere else, too.
</comment><comment author="uschindler" created="2016-06-16T12:32:41Z" id="226471586">Your code is fine, because you only drop arguments at the target. The combiner has more arguments in your case. This is why you are not affected.

It only affects you if you drop arguments from combiner - you don't need to.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file></files><comments><comment>Merge pull request #18913 from uschindler/painless_megamorphic_opto</comment></comments></commit></commits></item><item><title>Putting .scripts index takes `index.number_of_shards` into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18912</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 1.8.0_60

**OS version**: osx

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Set `index.number_of_shards: 3` in `config/elasticsearch.yml`
2. Create the scripts index with a single shard

```
DELETE /.scripts/

PUT /.scripts/
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}
```
1. Call `GET /.scripts/` and see there are three shards for this index.

This does not happen with index templates it seems.
Also regular indexes are created correctly with one shard.
</description><key id="160600399">18912</key><summary>Putting .scripts index takes `index.number_of_shards` into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v2.4.0</label></labels><created>2016-06-16T08:02:19Z</created><updated>2016-06-21T06:51:03Z</updated><resolved>2016-06-21T06:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-16T09:15:00Z" id="226431561">&gt; This does not happen with index templates it seems.

Using templates is the way to go IMO.
If I'm not mistaken, in 5.0, you can't set anymore index level settings in `elasticsearch.yml`.
</comment><comment author="martijnvg" created="2016-06-16T09:25:01Z" id="226433944">The problem is here:
https://github.com/elastic/elasticsearch/blob/b2c4c323e1014563dd02e9cc2f3a9175fd831e78/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L279

When creating `.scripts` index, it always overrides the number of shards settings irregardless if it has been specified in the create index request.
</comment><comment author="martijnvg" created="2016-06-21T06:51:03Z" id="227355075">Fixed via #18965
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>non-capturing lambda support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18911</link><project id="" key="" /><description>Syntax for this was added in #18824 as a stub.

This adds the non-capturing case. Walker just handles it the same array method references. I think we should do this first and iterate (i found some screwy unrelated bugs in casts already just playing around). The capturing case can be a followup.

Semantics are simple: 
- return type is always `def`
- argument types default to `def` unless the user specifies.
</description><key id="160580775">18911</key><summary>non-capturing lambda support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T05:31:36Z</created><updated>2016-06-16T14:31:55Z</updated><resolved>2016-06-16T14:31:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-16T06:11:46Z" id="226396726">LGTM!
</comment><comment author="uschindler" created="2016-06-16T10:07:52Z" id="226443484">Is `ELambda` class still used?
</comment><comment author="rmuir" created="2016-06-16T12:11:34Z" id="226467176">It was never used. See the TODO in the code. It will be used for captures, because we have to push logic into a node to support that. We have to do analysis first to even know which case it is (or its too crazy).
</comment><comment author="uschindler" created="2016-06-16T12:37:45Z" id="226472581">Yeah, that what I expected.
</comment><comment author="uschindler" created="2016-06-16T12:38:09Z" id="226472667">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/LambdaTests.java</file></files><comments><comment>Merge pull request #18911 from rmuir/noncapturing_lambdas</comment></comments></commit></commits></item><item><title>Update aws sdk to 1.11.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18910</link><project id="" key="" /><description>From 1.10.69 (see #17784), here are the most important updates:
## Minor 1.10 releases:
- Amazon S3 Added support for a new configuration named BucketAccelerateConfiguration which supports faster uploads/downloads to S3 buckets.
- Adding several missing throttling error codes for API Gateway and S3.
- Amazon S3 Introducing a new version of the ListObjects (ListObjectsV2) API that allows listing objects with a large number of delete markers.
## 1.11:

AWS SDK for Java:
- Improved URL encoding for REST clients.
- Dropped usage of Json.org library in favor of Jackson.
- Updated retry policies to include jitter during backoffs.
- Generate output POJOs for all operations.
- Renamed the aws-java-sdk-flow-build-tools-{sdkversion}.jar to aws-swf-build-tools-1.0.jar. The jar is also available in Maven.

AWS SDK for Java - Amazon S3:
- Added support to return the part count of an object in object metadata. You can also download a part by setting part number in GetObjectRequest.
- TransferManager supports parallel downloads for multipart objects.
- Default to Signature Version 4 signing process in all regions.

Apache HttpClient upgraded to 4.5.2
</description><key id="160579168">18910</key><summary>Update aws sdk to 1.11.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>adoptme</label></labels><created>2016-06-16T05:14:57Z</created><updated>2016-10-21T14:48:26Z</updated><resolved>2016-10-21T14:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-10-21T14:48:26Z" id="255397499">Closing as we closed also the related PR #19594

Note that this is something we will probably have to do at some point.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TransportClient instead of Client in the code snippet at Transport Client documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18909</link><project id="" key="" /><description>Code snippet in Transport Client section is still using Client class. It should be TransportClient class.
`addTransportAddress` is not available in Client class, it's only available in TransportClient.
</description><key id="160571653">18909</key><summary>TransportClient instead of Client in the code snippet at Transport Client documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">philipskokoh</reporter><labels><label>docs</label></labels><created>2016-06-16T03:46:02Z</created><updated>2016-06-16T04:02:31Z</updated><resolved>2016-06-16T03:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-16T03:52:54Z" id="226381968">Thanks @philipskokoh!
</comment><comment author="dadoonet" created="2016-06-16T04:02:31Z" id="226382960">Also backported in 2.x branch with b70cbc374c25e1cba61fbafad3f0ddfa25635e0f
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #18909 from philipskokoh/master</comment></comments></commit></commits></item><item><title>Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.lucene.Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18908</link><project id="" key="" /><description>While writing on ES through hadoop job ,it freezes with logs as :Caused by: **java.lang.NoClassDefFoundError**: Could not initialize class **org.elasticsearch.common.lucene.Lucene**,What might be the reason?

**Elasticsearch version**:2.3.3

**JVM version**:1.7

**OS version**:14.04

**Logs**:
RemoteTransportException[[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: ExceptionInInitializerError; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]];
Caused by: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: ExceptionInInitializerError; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]];
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:180)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:138)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ExceptionInInitializerError
    at org.elasticsearch.Version.fromId(Version.java:564)
    at org.elasticsearch.Version.readVersion(Version.java:308)
    at org.elasticsearch.cluster.node.DiscoveryNode.readFrom(DiscoveryNode.java:339)
    at org.elasticsearch.cluster.node.DiscoveryNode.readNode(DiscoveryNode.java:322)
    at org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse.readFrom(LivenessResponse.java:52)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:178)
    ... 23 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]
    at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:114)
    at org.apache.lucene.codecs.PostingsFormat.forName(PostingsFormat.java:112)
    at org.elasticsearch.common.lucene.Lucene.&lt;clinit&gt;(Lucene.java:65):
</description><key id="160571463">18908</key><summary>Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.lucene.Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgc445</reporter><labels /><created>2016-06-16T03:44:06Z</created><updated>2016-08-30T07:44:18Z</updated><resolved>2016-06-16T04:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-16T04:00:06Z" id="226382720">Probably a duplicate of #15071.
You are probably missing `META-INF/services` somewhere.
Are you shading?

Read this: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/_embedding_jar_with_dependencies.html

I'm closing at I don't feel it's an issue but feel free to reopen or open a discussion on discuss.elastic.co.
</comment><comment author="kdubezerra" created="2016-08-30T07:44:18Z" id="243359229">Can the original poster please confirm if this was in fact a duplicate? I'm facing the exact same issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix horrible capture</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18907</link><project id="" key="" /><description>This addresses @uschindler TODO in #18899

He hacked around it with a "capture", but we dont want that, very groovy-like and may have bad side effects.

We just need to pass the correct information as BSM args instead of the stack (the names of deferred lambdas/method references). For now we still emit a `null` on the stack as a placeholder.
</description><key id="160553884">18907</key><summary>Fix horrible capture</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T00:33:01Z</created><updated>2016-06-16T06:54:32Z</updated><resolved>2016-06-16T00:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-16T00:40:45Z" id="226359601">LGTM!
</comment><comment author="uschindler" created="2016-06-16T06:54:32Z" id="226402940">Cool thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file></files><comments><comment>Merge pull request #18907 from rmuir/fix_horrible_capture</comment></comments></commit></commits></item><item><title>Plugins: Remove name() and description() from api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18906</link><project id="" key="" /><description>In 2.0 we added plugin descriptors which require defining a name and
description for the plugin. However, we still have name() and
description() which must be overriden from the Plugin class. This still
exists for classpath plugins. But classpath plugins are mainly for
tests, and even then, referring to classpath plugins with their class is
a better idea. This change removes name() and description(), replacing
the name for classpath plugins with the full class name.
</description><key id="160551831">18906</key><summary>Plugins: Remove name() and description() from api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-16T00:13:21Z</created><updated>2016-06-16T21:35:26Z</updated><resolved>2016-06-16T21:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-16T04:19:25Z" id="226384667">LGTM
</comment><comment author="s1monw" created="2016-06-16T06:59:16Z" id="226403705">LGTM - nice cleanup these plugin API might actually be nice and simple at some point
</comment><comment author="s1monw" created="2016-06-16T07:30:40Z" id="226409236">I just ran into some javadocs that reference these methods as an example in `AnalysisModule` you might wanna remove that too
</comment><comment author="s1monw" created="2016-06-16T19:59:25Z" id="226596349">LGTM thx ryan
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/analysis/AnalysisModule.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TestTaskPlugin.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/SettingsFilteringIT.java</file><file>core/src/test/java/org/elasticsearch/index/SettingsListenerIT.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapperPlugin.java</file><file>core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/analysis/DummyAnalysisPlugin.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/indices/template/IndexTemplateFilteringIT.java</file><file>core/src/test/java/org/elasticsearch/ingest/IngestCloseIT.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/plugin/dummy1/TestPlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/responseheader/TestResponseHeaderPlugin.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java</file><file>core/src/test/java/org/elasticsearch/search/SearchServiceTests.java</file><file>core/src/test/java/org/elasticsearch/search/SearchTimeoutIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateScriptMocks.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IpRangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptPlugin.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScorePluginIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterPlugin.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterPlugin.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/aggs-matrix-stats/src/main/java/org/elasticsearch/search/aggregations/matrix/MatrixAggregationPlugin.java</file><file>modules/ingest-common/src/main/java/org/elasticsearch/ingest/IngestCommonPlugin.java</file><file>modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java</file><file>modules/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyPlugin.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/PainlessPlugin.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorPlugin.java</file><file>modules/reindex/src/main/java/org/elasticsearch/index/reindex/ReindexPlugin.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/CancelTests.java</file><file>plugins/analysis-icu/src/main/java/org/elasticsearch/plugin/analysis/icu/AnalysisICUPlugin.java</file><file>plugins/analysis-kuromoji/src/main/java/org/elasticsearch/plugin/analysis/kuromoji/AnalysisKuromojiPlugin.java</file><file>plugins/analysis-phonetic/src/main/java/org/elasticsearch/plugin/analysis/AnalysisPhoneticPlugin.java</file><file>plugins/analysis-smartcn/src/main/java/org/elasticsearch/plugin/analysis/smartcn/AnalysisSmartChinesePlugin.java</file><file>plugins/analysis-stempel/src/main/java/org/elasticsearch/plugin/analysis/stempel/AnalysisStempelPlugin.java</file><file>plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java</file><file>plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java</file><file>plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java</file><file>plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureDiscoveryClusterFormationTests.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java</file><file>plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryClusterFormationTests.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java</file><file>plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverTests.java</file><file>plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/IngestAttachmentPlugin.java</file><file>plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java</file><file>plugins/jvm-example/src/main/java/org/elasticsearch/plugin/example/JvmExamplePlugin.java</file><file>plugins/lang-javascript/src/main/java/org/elasticsearch/plugin/javascript/JavaScriptPlugin.java</file><file>plugins/lang-python/src/main/java/org/elasticsearch/plugin/python/PythonPlugin.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/MapperAttachmentsPlugin.java</file><file>plugins/mapper-murmur3/src/main/java/org/elasticsearch/plugin/mapper/MapperMurmur3Plugin.java</file><file>plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizePlugin.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceIntegTestCase.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/plugin/repository/gcs/GoogleCloudStoragePlugin.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsPlugin.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java</file><file>plugins/store-smb/src/main/java/org/elasticsearch/plugin/store/smb/SMBStorePlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java</file><file>test/framework/src/main/java/org/elasticsearch/index/MockEngineFactoryPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/ingest/IngestTestPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/node/NodeMocksPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java</file><file>test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalSettingsPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/test/MockIndexEventListener.java</file><file>test/framework/src/main/java/org/elasticsearch/test/store/MockFSIndexStore.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Merge pull request #18906 from rjernst/plugin_name_api</comment></comments></commit></commits></item><item><title>Fetch result when wait_for_completion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18905</link><project id="" key="" /><description>This makes this sequence:

```
curl -XPOST 'localhost:9200/_reindex?pretty&amp;wait_for_completion=false' -d'{
  "source": {
    "index": "source"
  },
  "dest": {
    "index": "dest"
  }
}'

curl 'localhost:9200/_tasks/Jsyd6d9wSRW-O-NiiKbPcQ:237?wait_for_completion&amp;pretty'
```

Return task _AND_ the response to the user.

This also renames "result" to "response" in the persisted task info to line it up with how we name the objects in Elasticsearch.
</description><key id="160540169">18905</key><summary>Fetch result when wait_for_completion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T22:38:54Z</created><updated>2016-06-21T18:33:10Z</updated><resolved>2016-06-21T18:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-06-21T14:31:09Z" id="227458056">Left a couple of comments. Otherwise, LGTM.
</comment><comment author="nik9000" created="2016-06-21T18:33:10Z" id="227530567">Thanks for reviewing @imotov ! I refactored like you asked and merged.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation: update build instructions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18904</link><project id="" key="" /><description>I followed the instructions but `gradle build` repeatedly failed on an integration test (`icuInstall` or something like that). When I realized that this was an integration test, I tried running `gradle assemble`, and once that completed successfully, I was able to `gradle build` without a problem. So this just updates the instructions so that newcomers `assemble` before they `build`.

I was tempted to also fix some weird formatting at the top of `TESTING.asciidoc`, but I have no idea what to do with the text there (it looks out of place).
</description><key id="160524686">18904</key><summary>Documentation: update build instructions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jonaf</reporter><labels /><created>2016-06-15T21:10:29Z</created><updated>2016-06-16T17:32:23Z</updated><resolved>2016-06-16T17:32:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-15T21:14:48Z" id="226322306">&gt; I followed the instructions but gradle build repeatedly failed on an integration test (icuInstall or something like that).

It might be that you hit it at a bad time. We had a bug that caused that to fail earlier today. Could you pull a fresh copy of master, clean, and try again?
</comment><comment author="jasontedor" created="2016-06-15T21:25:17Z" id="226324826">I don't think this is necessary:

``` bash
17:22:42 ⌂65% [jason:~/src/elastic/elasticsearch] master ± gradle -m build | grep "\(assemble\|build\) \(SKIPPED\|UP-TO-DATE\)"
:buildSrc:assemble UP-TO-DATE
:buildSrc:build UP-TO-DATE
:benchmarks:assemble SKIPPED
:benchmarks:build SKIPPED
:build-tools:assemble SKIPPED
:build-tools:build SKIPPED
:core:assemble SKIPPED
:core:build SKIPPED
:docs:assemble SKIPPED
:distribution:integ-test-zip:assemble SKIPPED
:distribution:zip:assemble SKIPPED
:docs:build SKIPPED
:rest-api-spec:assemble SKIPPED
:rest-api-spec:build SKIPPED
:distribution:deb:assemble SKIPPED
:distribution:deb:build SKIPPED
:distribution:integ-test-zip:build SKIPPED
:distribution:rpm:assemble SKIPPED
:distribution:rpm:build SKIPPED
:distribution:tar:assemble SKIPPED
:distribution:tar:build SKIPPED
:distribution:zip:build SKIPPED
:modules:aggs-matrix-stats:assemble SKIPPED
:modules:aggs-matrix-stats:build SKIPPED
:modules:ingest-common:assemble SKIPPED
:modules:ingest-common:build SKIPPED
:modules:lang-expression:assemble SKIPPED
:modules:lang-expression:build SKIPPED
:modules:lang-groovy:assemble SKIPPED
:modules:lang-groovy:build SKIPPED
:modules:lang-mustache:assemble SKIPPED
:modules:lang-mustache:build SKIPPED
:modules:lang-painless:assemble SKIPPED
:modules:lang-painless:build SKIPPED
:modules:percolator:assemble SKIPPED
:modules:percolator:build SKIPPED
:modules:reindex:assemble SKIPPED
:modules:reindex:build SKIPPED
:plugins:analysis-icu:assemble SKIPPED
:plugins:analysis-icu:build SKIPPED
:plugins:analysis-kuromoji:assemble SKIPPED
:plugins:analysis-kuromoji:build SKIPPED
:plugins:analysis-phonetic:assemble SKIPPED
:plugins:analysis-phonetic:build SKIPPED
:plugins:analysis-smartcn:assemble SKIPPED
:plugins:analysis-smartcn:build SKIPPED
:plugins:analysis-stempel:assemble SKIPPED
:plugins:analysis-stempel:build SKIPPED
:plugins:discovery-azure:assemble SKIPPED
:plugins:discovery-azure:build SKIPPED
:plugins:discovery-ec2:assemble SKIPPED
:plugins:discovery-ec2:build SKIPPED
:plugins:discovery-gce:assemble SKIPPED
:plugins:discovery-gce:build SKIPPED
:plugins:ingest-attachment:assemble SKIPPED
:plugins:ingest-attachment:build SKIPPED
:plugins:ingest-geoip:assemble SKIPPED
:plugins:ingest-geoip:build SKIPPED
:plugins:jvm-example:assemble SKIPPED
:plugins:jvm-example:build SKIPPED
:plugins:lang-javascript:assemble SKIPPED
:plugins:lang-javascript:build SKIPPED
:plugins:lang-python:assemble SKIPPED
:plugins:lang-python:build SKIPPED
:plugins:mapper-attachments:assemble SKIPPED
:plugins:mapper-attachments:build SKIPPED
:plugins:mapper-murmur3:assemble SKIPPED
:plugins:mapper-murmur3:build SKIPPED
:plugins:mapper-size:assemble SKIPPED
:plugins:mapper-size:build SKIPPED
:plugins:repository-azure:assemble SKIPPED
:plugins:repository-azure:build SKIPPED
:plugins:repository-gcs:assemble SKIPPED
:plugins:repository-gcs:build SKIPPED
:plugins:repository-hdfs:assemble SKIPPED
:plugins:repository-hdfs:build SKIPPED
:plugins:repository-s3:assemble SKIPPED
:plugins:repository-s3:build SKIPPED
:plugins:store-smb:assemble SKIPPED
:plugins:store-smb:build SKIPPED
:qa:backwards-5.0:assemble SKIPPED
:qa:backwards-5.0:build SKIPPED
:qa:evil-tests:assemble SKIPPED
:qa:evil-tests:build SKIPPED
:qa:smoke-test-client:assemble SKIPPED
:qa:smoke-test-client:build SKIPPED
:qa:smoke-test-ingest-disabled:assemble SKIPPED
:qa:smoke-test-ingest-disabled:build SKIPPED
:qa:smoke-test-ingest-with-all-dependencies:assemble SKIPPED
:qa:smoke-test-ingest-with-all-dependencies:build SKIPPED
:qa:smoke-test-multinode:assemble SKIPPED
:qa:smoke-test-multinode:build SKIPPED
:qa:smoke-test-plugins:assemble SKIPPED
:qa:smoke-test-plugins:build SKIPPED
:qa:smoke-test-reindex-with-painless:assemble SKIPPED
:qa:smoke-test-reindex-with-painless:build SKIPPED
:test:framework:assemble SKIPPED
:test:framework:build SKIPPED
:test:logger-usage:assemble SKIPPED
:test:logger-usage:build SKIPPED
:test:fixtures:example-fixture:assemble SKIPPED
:test:fixtures:example-fixture:build SKIPPED
:test:fixtures:hdfs-fixture:assemble SKIPPED
:test:fixtures:hdfs-fixture:build SKIPPED
17:24:49 [jason:~/src/elastic/elasticsearch] master ± 
```

showing that `assemble` is a dependent task for `build`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify ScriptModule and script registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18903</link><project id="" key="" /><description>Registering a script engine or native scripts still uses Guice today
and is much more complicated than needed. This change moves to a pull
based model where script plugins have to implement a dedicated interface
`ScriptPlugin` and defines simple getter returning instances rather than
classes.
</description><key id="160511743">18903</key><summary>Simplify ScriptModule and script registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>:Scripting</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T20:07:25Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-16T07:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-15T20:09:08Z" id="226304803">@rjernst can you take a look at this too? @jasontedor you might be interested too though
</comment><comment author="rjernst" created="2016-06-15T21:05:56Z" id="226320017">LGTM!
</comment><comment author="s1monw" created="2016-06-15T21:08:22Z" id="226320668">thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/plugins/ScriptPlugin.java</file><file>core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/NativeScriptFactory.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptEngineRegistry.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/test/java/org/elasticsearch/index/IndexModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/FileScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptModesTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/search/SearchTimeoutIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateScriptMocks.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IpRangeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptPlugin.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java</file><file>modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>modules/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyPlugin.java</file><file>modules/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java</file><file>modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/PainlessPlugin.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/PainlessScriptEngineService.java</file><file>plugins/lang-javascript/src/main/java/org/elasticsearch/plugin/javascript/JavaScriptPlugin.java</file><file>plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java</file><file>plugins/lang-python/src/main/java/org/elasticsearch/plugin/python/PythonPlugin.java</file><file>plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java</file><file>qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/AbstractScriptTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java</file><file>test/framework/src/main/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptNoParams.java</file><file>test/framework/src/main/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptWithParams.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Simplify ScriptModule and script registration (#18903)</comment></comments></commit></commits></item><item><title>Test: wait for task to start before waiting for it to finish</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18902</link><project id="" key="" /><description /><key id="160506402">18902</key><summary>Test: wait for task to start before waiting for it to finish</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T19:40:55Z</created><updated>2016-06-16T09:02:45Z</updated><resolved>2016-06-15T22:42:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-15T19:41:21Z" id="226297475">@imotov I found it!
</comment><comment author="nik9000" created="2016-06-15T22:42:43Z" id="226341655">I got an out of band approval from @imotov so I'll just merge this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file></files><comments><comment>Test: wait for task to start before waiting for it to finish (#18902)</comment></comments></commit></commits></item><item><title>Update TESTING.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18901</link><project id="" key="" /><description /><key id="160499171">18901</key><summary>Update TESTING.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T19:05:20Z</created><updated>2016-06-15T19:06:56Z</updated><resolved>2016-06-15T19:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-15T19:06:56Z" id="226288539">Thanks for fixing it!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow FieldStatsRequest to disable cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18900</link><project id="" key="" /><description>This is useful in more paranoid environments.
</description><key id="160497945">18900</key><summary>Allow FieldStatsRequest to disable cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T18:59:37Z</created><updated>2016-06-15T19:11:25Z</updated><resolved>2016-06-15T19:11:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-06-15T19:09:13Z" id="226289128">LGTM
</comment><comment author="nik9000" created="2016-06-15T19:11:25Z" id="226289710">Thanks for reviewing @jaymode !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix bugs in operators and more improvements for the dynamic case</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18899</link><project id="" key="" /><description>Fix a few bugs:
- static compound assignment/shift had bugs (would allow bogus stuff)
- dynamic compound assignments are consistent with static case. This never worked ever, it would always promote. Now it works.

Improve some performance:
- dynamic xor was done in a slow way
- remove null guard from + operator when we have enough type info to know its invalid
- add megamorphic cache (from @uschindler  !) so we have a legit performant fallback

Add lots of tests.

I tested the benchmark script by disabling some caches manually:
- inline cache: 0.45s
- megamorphic cache: 0.82s
- no caches: 3.65s

So Uwe's cache really helps the worst case scenario, where tons of types are seen. Its no longer trappy now :)
</description><key id="160496030">18899</key><summary>fix bugs in operators and more improvements for the dynamic case</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T18:50:44Z</created><updated>2016-06-15T21:07:09Z</updated><resolved>2016-06-15T21:06:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-15T19:15:10Z" id="226290735">This looks awesome!  +1 Thanks @rmuir and @uschindler 
</comment><comment author="rmuir" created="2016-06-15T21:07:09Z" id="226320339">Thanks for tackling that cache @uschindler !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/AnalyzerCaster.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefMath.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AdditionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AndTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/CompoundAssignmentTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOptimizationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DivisionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/IncrementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/MultiplicationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/OrTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/RemainderTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScriptTestCase.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ShiftTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/StringTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/SubtractionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/XorTests.java</file></files><comments><comment>Merge pull request #18899 from rmuir/more_def_cleanup</comment></comments></commit></commits></item><item><title>Add a {{#toJson}} function  (that accepts map as an argument)  to the the elasticsearch mustache integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18898</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**JVM version**:  openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**: centOS 7

**Describe the feature**:

I'm having the exact same issue of this [thread](https://discuss.elastic.co/t/webhook-action-hits-collection-not-json/24568): Basically watcher ctx is giving me a java.util.HashMap object and I need a json object. Today, is only possible to do a tedious manual construction of it.

I opened a closed issue here: 

https://github.com/elastic/elasticsearch/issues/18807

After recommendation, I moved the discussion to the official elasticsearch discussion [forum](https://discuss.elastic.co/t/geting-json-from-ctx-payload-hits-in-a-webhook-action-that-ruby-consumes/52478):

There, I received feedback from Alexander Reelsen as I quote:

&gt; indeed I dont see a clean way that this is possible at the moment, without tedious manual construction inside of a string. Maybe a mustache function like {{#toJson}} (that accepts map as an argument) could help a lot here - but I am not sure if this is how mustache works, as I would expect it to only be able to handle strings - need to check it out.

And

&gt; Taking a look at the elasticsearch mustache integration and the possibility to add a JSON function might make sense here. This could be reused in watcher then.

So, in my client parsing this result, I created some regex in ruby where I'm able to parse from the topbeat java.util.HashMap sent in payload.hits to json. But it would be nice if we can add this feature into the elasticsearch mustache integration as Alexander says.

I'm willing to contribute to this as a developer, if needed. Just not quite sure how to start.
</description><key id="160478556">18898</key><summary>Add a {{#toJson}} function  (that accepts map as an argument)  to the the elasticsearch mustache integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rvalenciano</reporter><labels><label>discuss</label></labels><created>2016-06-15T17:25:37Z</created><updated>2016-06-17T06:52:18Z</updated><resolved>2016-06-17T06:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-06-16T09:02:57Z" id="226428784">@tlrx has come up with a PR, that renders a map as JSON, see https://github.com/elastic/elasticsearch/pull/18856

would that work for you?
</comment><comment author="rvalenciano" created="2016-06-16T16:58:12Z" id="226547207">@spinscale pure gold, thanks!
</comment><comment author="spinscale" created="2016-06-17T06:52:18Z" id="226694299">all right, closing this one then!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>http output plugin fails on CONNECT request </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18897</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_77

**OS version**: CentOs 7

**Description of the problem including expected versus actual behavior**:

I'm trying to use http output plugin to send data via proxy.
It works till original url is not HTTPS.
Once original url is HTTPS proxy authentication stops to work.
When I looked into tcp dump I found out that logstash does not send headers in HTTP request and hence proxy authentication failed.
It also generates an exception:

NameError: no method 'setHeader' for arguments (org.jruby.RubySymbol,org.jruby.RubySymbol) on Java::OrgApacheHttpClientMethods::HttpPost
  available overloads:
    (java.lang.String,java.lang.String)
    (java.lang.String,java.lang.String)
                   []= at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/java_extensions.rb:18
  request_from_options at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/client.rb:490
                  each at org/jruby/RubyHash.java:1342
  request_from_options at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/client.rb:490
               request at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/client.rb:418
                  post at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/client.rb:265
                  post at /opt/logstash/vendor/bundle/jruby/1.9/gems/manticore-0.5.5-java/lib/manticore/client/proxies.rb:31
               receive at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-http-2.1.3/lib/logstash/outputs/http.rb:120
         multi_receive at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-http-2.1.3/lib/logstash/outputs/http.rb:93
                  each at org/jruby/RubyArray.java:1613
         multi_receive at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-http-2.1.3/lib/logstash/outputs/http.rb:93
  worker_multi_receive at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/output_delegator.rb:130
         multi_receive at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/output_delegator.rb:114
          output_batch at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/pipeline.rb:301
                  each at org/jruby/RubyHash.java:1342
          output_batch at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/pipeline.rb:301
           worker_loop at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/pipeline.rb:232
         start_workers at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.3.1-java/lib/logstash/pipeline.rb:201

My configuration is:
output {
                http {
                        http_method =&gt; "post"
                        url =&gt; "https://my-domain.com/"
                        cacert =&gt; "/etc/ssl/certs/GeoTrust_SSL_CA_G2.cer"
                        codec =&gt; "json"
                        headers =&gt; {
                                "Proxy-Authorization" =&gt; "Basic c2XXXXXXXXXcnQ="
                                "Pragma" =&gt; "no-cache"
                        }
                       proxy =&gt; "http://10.10.0.254:3128"
                }
}

It works (and sends headers) if I try to send data to http://my-domain.com  instead of httpS://my-domain.com
</description><key id="160472136">18897</key><summary>http output plugin fails on CONNECT request </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arkadyi</reporter><labels /><created>2016-06-15T16:56:41Z</created><updated>2016-06-15T17:04:12Z</updated><resolved>2016-06-15T17:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-15T17:04:11Z" id="226252626">There is a [repository](https://github.com/elastic/logstash) that is dedicated to Logstash, I think that you'll have better luck seeking an answer there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add did-you-mean for plugin cli</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18896</link><project id="" key="" /><description>From #18876: 

&gt; If we want to be extra fancy, we could apply a similar logic which we use when parsing setting names, using levenshtein to check for similar plugins in order to help on typos

This should be easy to do, just adding a static init of an automata from the official plugins list, and checking when the plugin id does not resolve to any of our supported formats.
</description><key id="160471815">18896</key><summary>Add did-you-mean for plugin cli</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2016-06-15T16:56:05Z</created><updated>2016-06-17T12:22:49Z</updated><resolved>2016-06-17T12:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java</file></files><comments><comment>Add did-you-mean for plugin cli</comment><comment>This commit adds error messages like: `Unknown plugin xpack, did you mean [x-pack]?`</comment></comments></commit></commits></item><item><title>Improve TimeZoneRoundingTests error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18895</link><project id="" key="" /><description>Currently the error messages for failing tests in the TimeZoneRoundingTests 
are hard to read because they usually report the actual end expected date
in milliseconds utc (e.g. "Expected: &lt;1414270860000L&gt; but: was &lt;1414270800000L&gt;".
This makes failing tests hard to read.

This change introduces a new Matcher that can be used for equality checks for
long dates but reports the error both as a formated date string according to
some time zone and also as the actual long values, so you get messages like
"Expected: 2014-10-26T00:01:00.000+03:00 [1414270860000] but:  was
"2014-10-26T00:00:00.000+03:00 [1414270800000]".

Also clean cleaning up some helper methods and generally simplifying a few test
cases. Otherwise this change shouldn't affect either the scope of the test or
anything about the rounding implementation itself.
</description><key id="160471057">18895</key><summary>Improve TimeZoneRoundingTests error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>enhancement</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T16:52:36Z</created><updated>2016-06-16T08:48:00Z</updated><resolved>2016-06-16T08:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-15T23:12:45Z" id="226346817">LGTM
</comment><comment author="cbuescher" created="2016-06-16T08:47:31Z" id="226425268">On 2.x with 2a0fa5a
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Courier Fetch: Bad Gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18894</link><project id="" key="" /><description>I been getting this error lately as soon as fire up Kibana. Any ideas why this is happening ?
As soon as I hit the Go Back button my queries would populate.

![image](https://cloud.githubusercontent.com/assets/19933089/16049157/af59e2d0-3224-11e6-9a8a-4f99cfdee818.png)

![image](https://cloud.githubusercontent.com/assets/19933089/16049190/c7b95284-3224-11e6-9187-ec72ad1a09de.png)

Here are my ES logs:

[2016-06-15 11:32:38,512][DEBUG][action.admin.cluster.node.stats] [elastic-kibana] failed to execute on node [Cr1WzqKXTAmtVy15adTRbg]
ReceiveTimeoutTransportException[[elasticsearch-1][192.168.1.102:9300][cluster:monitor/nodes/stats[n]] request_id [189677] timed out after [15099ms]]
at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:679)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
[2016-06-15 11:32:38,899][DEBUG][action.admin.cluster.node.stats] [elastic-kibana] failed to execute on node [CIqJQI_SQlK0HbP4naWhkA]
ReceiveTimeoutTransportException[[elasticsearch-2][192.168.1.103:9300][cluster:monitor/nodes/stats[n]] request_id [189678] timed out after [15499ms]]
at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:679)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
[2016-06-15 11:32:59,304][WARN ][transport ] [elastic-kibana] Received response for a request that has timed out, sent [35904ms] ago, timed out [20805ms] ago, action [cluster:monitor/nodes/stats[n]], node [{elasticsearch-1}{Cr1WzqKXTAmtVy15adTRbg}{192.168.1.102}{192.168.1.102:9300}{master=false}], id [189677]
[2016-06-15 11:33:10,304][WARN ][transport ] [elastic-kibana] Received response for a request that has timed out, sent [46904ms] ago, timed out [31405ms] ago, action [cluster:monitor/nodes/stats[n]], node [{elasticsearch-2}{CIqJQI_SQlK0HbP4naWhkA}{192.168.1.103}{192.168.1.103:9300}{master=false}], id [189678]
</description><key id="160456272">18894</key><summary>Courier Fetch: Bad Gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pip00</reporter><labels /><created>2016-06-15T15:46:10Z</created><updated>2016-06-15T16:11:30Z</updated><resolved>2016-06-15T15:48:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T15:48:34Z" id="226230536">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="clintongormley" created="2016-06-15T15:54:14Z" id="226232291">Sorry for closing @pip00 - I see https://github.com/elastic/elasticsearch/issues/18868 and https://github.com/elastic/kibana/issues/7455 as well, but the right place to ask is in the forums.  My best guess is you're having GC issues, but this isn't a bug in Elasticsearch so github is not the right place.
</comment><comment author="pip00" created="2016-06-15T16:11:30Z" id="226237815">No problem. Thank your for your help.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix scroll bug that causes scrolls to never finish.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18893</link><project id="" key="" /><description>Related to #9157.

Closes #18885.
</description><key id="160429975">18893</key><summary>Fix scroll bug that causes scrolls to never finish.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scroll</label><label>bug</label><label>v1.7.6</label></labels><created>2016-06-15T14:02:32Z</created><updated>2016-06-15T14:08:42Z</updated><resolved>2016-06-15T14:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-15T14:04:36Z" id="226197165">LGTM
</comment><comment author="jpountz" created="2016-06-15T14:08:42Z" id="226198256">Merged via 159b8f1ebd53c34a04cee95d3e1f1d3f297829c4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decay functions should allow to specify a value in case a field is missing </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18892</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

When using a decay function in a `function_score` the documentation says in the [offical documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-function-score-query.html#_what_if_a_field_is_missing): "If the numeric field is missing in the document, the function will return 1." In many cases this is not intended since documents missing the field are scored with the highest possible value. 

Similar to `field_value_factor` decay functions should provide a `missing` parameter allowing to define the score in case the field is missing.
</description><key id="160406347">18892</key><summary>Decay functions should allow to specify a value in case a field is missing </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FlorianWilhelm</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label></labels><created>2016-06-15T12:08:52Z</created><updated>2017-07-01T10:47:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T15:30:37Z" id="226224465">I think this makes sense.  @brwe what do you think?
</comment><comment author="brwe" created="2016-06-16T09:08:12Z" id="226430005">Last time we had the discussion we decided we will not do anything because there is a workaround: https://github.com/elastic/elasticsearch/issues/7788 We can re evaluate this decision. Might make sense to make it consistent with `field_value_factor`. I have no strong opinion though. 
</comment><comment author="clintongormley" created="2016-06-16T09:48:23Z" id="226439215">+1 to consistency
</comment><comment author="FlorianWilhelm" created="2016-06-16T16:50:16Z" id="226544977">@brwe Thank you, I haven't thought about that option actually but I guess I am not the only one and consistency is always better. I post your solution for completeness reasons here.

``` json
{
  "query": {
    "function_score": {
      "score_mode": "first",
      "functions": [
        {
          "filter": {
            "exists": {
              "field": "age"
            }
          },
          "gauss": {
            "age": {
              "origin": 22,
              "scale": 5,
              "decay": 0.5
            }
          }
        },
        {
          "script_score": {
            "script": "0"
          }
        }
      ]
    }
  }
}
```
</comment><comment author="magicleo" created="2016-06-17T09:17:29Z" id="226721420">what if I want to use other functions and need  "score_mode": "sum"?
</comment><comment author="FlorianWilhelm" created="2016-06-20T06:59:31Z" id="227064199">@magicleo I would say that `score_mode: first` is only an optimization at that point. If you have `sum` you will just add 0. sometimes.
</comment><comment author="kaka19ace" created="2016-09-28T14:34:12Z" id="250184648">At most usage cases, we disabled the script option for security reason,  so we could not using decay function if the doc field not exists, 

Using missing value  is a good idea :)
</comment><comment author="matthuhiggins" created="2017-01-02T03:19:06Z" id="269931673">kaka19ace - I'm having the same pain. Scripting is not enabled, so the suggested workaround is not available. Makes it tough to use the feature on coordinate fields.</comment><comment author="tuzz" created="2017-02-17T17:12:25Z" id="280709472">I wasn't able to use the workaround above because I have more than one function in `function_score`. Instead, I found another workaround which is to copy the nullable field to a new field in the index mapping to guarantee a value:

```
"mappings": {
  "name_of_type": {
    "field_that_might_be_null": {
      "type": "float",
      "copy_to": "field_that_definitely_wont_be_null"
    },
    "field_that_definitely_wont_be_null" {
      "type": "float",
      "null_value": 0
    }
  }
}
```

Depending on the type of decay, you may need to pick a default value that's far enough out of range to result in a value of 0. Hopefully that helps someone.</comment><comment author="brooks" created="2017-06-08T23:35:52Z" id="307255873">`+1` for missing 👍 </comment><comment author="adamdunkley" created="2017-07-01T10:44:44Z" id="312424904">There is actually a workaround that does not need a null value or scripting to be enabled.

If you make the second score function something that will always yield 0, for example:

```
{
  "query": {
    "function_score": {
      "score_mode": "first",
      "functions": [
        {
          "filter": {
            "exists": {
              "field": "age"
            }
          },
          "gauss": {
            "age": {
              "origin": 22,
              "scale": 5,
              "decay": 0.5
            }
          }
        },
        {
          "gauss": {
            "age": {
              "origin": "0",
              "offset": "0",
              "scale": "100"
            }
          }
        }
      ]
    }
  }
}
```

(where age is never going to be 0)

This still does not solve the issue for where you want multiple functions as it will screw with averages (if using average as the rollup function) but at least it doesn't require scripting or changes to mappings :)</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add microbenchmarking infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18891</link><project id="" key="" /><description>With this commit we add a benchmarks project that contains the necessary build infrastructure and an example benchmark. It is added as a separate project to avoid interfering with the regular build too much (especially sanity checks) and to keep the microbenchmarks isolated.

Microbenchmarks are generated with `gradle :benchmarks:jmhJar` and can be run with `gradle :benchmarks:jmh`.

We intentionally do not use the [jmh-gradle-plugin](https://github.com/melix/jmh-gradle-plugin) as it causes all sorts of problems (dependencies are not properly excluded, not all JMH parameters can be set) and it adds another abstraction layer that is not needed.

Closes #18242
</description><key id="160404901">18891</key><summary>Add microbenchmarking infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Benchmark</label><label>build</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T12:00:58Z</created><updated>2016-06-15T15:34:24Z</updated><resolved>2016-06-15T14:48:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-06-15T12:21:40Z" id="226170927">@rjernst: I pushed initial support for microbenchmarking but I have two specific issues with `benchmarks/build.gradle`:

For some reason transitive dependencies of JMH don't get picked up, so I added them explicitly in the meantime. `gradle :benchmarks:dependencies` shows:

```
------------------------------------------------------------
Project :benchmarks - Elasticsearch subproject :benchmarks
------------------------------------------------------------

_transitive_org.openjdk.jmh:jmh-core:1.12
\--- org.openjdk.jmh:jmh-core:1.12
     +--- net.sf.jopt-simple:jopt-simple:4.6
     \--- org.apache.commons:commons-math3:3.2

_transitive_org.openjdk.jmh:jmh-generator-annprocess:1.12
\--- org.openjdk.jmh:jmh-generator-annprocess:1.12
     \--- org.openjdk.jmh:jmh-core:1.12
          +--- net.sf.jopt-simple:jopt-simple:4.6
          \--- org.apache.commons:commons-math3:3.2

[...]
compile - Dependencies for source set 'main'.
+--- org.elasticsearch:elasticsearch:5.0.0-alpha4-SNAPSHOT -&gt; project :core
[...]
+--- org.openjdk.jmh:jmh-core:1.12
\--- org.openjdk.jmh:jmh-generator-annprocess:1.12
```

As you can see above, in the `compile` scope, the transitive dependencies are gone but [their pom](http://hg.openjdk.java.net/code-tools/jmh/file/448bd18c85f6/jmh-core/pom.xml#l60) looks fine to me and I don't remember that I ever had problems with JMH dependencies (so I'm probably missing something very obvious :)).

The next issue is that JMH generates classes during the build and uses all sorts of tricks which - you might have guessed it - alert our forbidden API check. I have tried various approaches, like a custom `SuppressForbidden` annotation that gets applied to all generated benchmark (sub)classes (-&gt; `@Inherited`) but JMH creates additional classes that don't derive from our classes. Now, we _could_ postprocess the generated Java files before they get compiled but I am not sure it is worth the hassle. Ideally, I'd like to skip the forbidden API check for the benchmarks in general but the "best" option I've found was to set `ignoreFailures` to `true` (and I do not like that). Do you have a better idea?
</comment><comment author="jpountz" created="2016-06-15T13:00:11Z" id="226179541">Since this is for developer use only and will not be shipped, I don't think we need to go through a formal review process. I suggest that we merge it and then iterate directly on master?
</comment><comment author="danielmitterdorfer" created="2016-06-15T13:04:29Z" id="226180575">@jpountz I'm fine with that. @ywelsch also had a short look, so I guess it's ok. I just wait a couple of hours so @rjernst has a chance to answer but if he isn't strongly against merging, I'll merge it on master tomorrow morning.
</comment><comment author="rjernst" created="2016-06-15T14:39:58Z" id="226208035">I agree with @jpountz that since this is just for developers, I think we should just get it in to start, and iterate. I do actually think we should do this more tightly integrated in the build (eg, as a `benchmark` task that can be run inside each project using `elasticsearch.build`, it does not have to be included in `check`). Part of that reason for that is right now it requires sucking in every jar we have if we wanted to benchmark something not in core.  But that can all be worked on over time; this is a good start.

Regarding transitive dependencies: `elasticsearch.build` does magic to not allow transitive dependencies. We do this because for software we ship, we want to know exactly what dependencies we are shipping.  For now, as long as benchmarks is a separate project, you might consider making this a play 'java' project (and you can then apply the randomized testing plugin).
</comment><comment author="danielmitterdorfer" created="2016-06-15T14:47:11Z" id="226210417">@rjernst: Thanks for your explanation. Then I think we're good to start now. 

Just as a general note: While this allows us now to benchmark on our machines, CI integration is not yet done but I'll work together with the infra team on that soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>benchmarks/src/main/java/org/elasticsearch/benchmark/DateBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/HelloBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/AllocationBenchmark.java</file><file>benchmarks/src/main/java/org/elasticsearch/benchmark/routing/allocation/Allocators.java</file></files><comments><comment>Add microbenchmarking infrastructure (#18891)</comment></comments></commit></commits></item><item><title>Cut over settings registration to a pull model</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18890</link><project id="" key="" /><description>Today we have a push model for registering basically anything. All our extension points
are defined on modules which we pass in to plugins. This is harder to maintain and adds
unnecessary dependencies on the modules itself. This change moves towards a pull model
where the plugin offers a getter kind of method to get the extensions. This will also
help in the future if we need to pass dependencies to the extension points which can
easily be defined on the method as arguments if a pull model is used.
</description><key id="160393184">18890</key><summary>Cut over settings registration to a pull model</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>:Settings</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T10:51:35Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-16T13:52:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-15T10:51:49Z" id="226153570">@rjernst can you take a look?
</comment><comment author="rjernst" created="2016-06-15T18:19:21Z" id="226274625">This LGTM. It is an awesome example in starting to remove guice from plugins.

One thought is about the new "shared" setting property. I wonder if it would be simpler to enforce that a single plugin, or ES core, does not register the same setting twice, and otherwise just enforce, similar to mappings, that the setting matches an already registered setting (so two plugins cannot try to register the same setting name with different properties).
</comment><comment author="s1monw" created="2016-06-15T18:23:15Z" id="226275804">&gt; One thought is about the new "shared" setting property. I wonder if it would be simpler to enforce that a single plugin, or ES core, does not register the same setting twice, and otherwise just enforce, similar to mappings, that the setting matches an already registered setting (so two plugins cannot try to register the same setting name with different properties).

I think it's much simpler to explicitly mark the ones that can be duplicates since it has to be done on both ends... (I guess that means my code is wrong :) I should look up the already registered setting and check if its also shared. I think this would protect us from others overriding any settings on us?
</comment><comment author="rjernst" created="2016-06-15T18:33:50Z" id="226278934">My concern is a setting that we don't mark as shared, but then an external plugin wants to read that setting. Now they can't do it until we update our code?
</comment><comment author="s1monw" created="2016-06-15T19:08:40Z" id="226288981">&gt; My concern is a setting that we don't mark as shared, but then an external plugin wants to read that setting. Now they can't do it until we update our code?

they can't register it twice that is true. It think we shouldn't design for this exception it's really only there for the S3 / AWS separation, should we really overdesign this? 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Setting.java</file><file>core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java</file><file>core/src/main/java/org/elasticsearch/env/EnvironmentModule.java</file><file>core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerModule.java</file><file>core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java</file><file>core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/FixedExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ScalingExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPoolModule.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/SettingsFilteringIT.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/SettingsListenerIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java</file><file>core/src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorPlugin.java</file><file>plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java</file><file>plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureDiscoveryClusterFormationTests.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file><file>plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java</file><file>plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java</file><file>plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverTests.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/MapperAttachmentsPlugin.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilterTests.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/index/MockEngineFactoryPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/test/AbstractQueryTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalSettingsPlugin.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file><file>test/framework/src/main/java/org/elasticsearch/test/MockIndexEventListener.java</file><file>test/framework/src/main/java/org/elasticsearch/test/store/MockFSIndexStore.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java</file></files><comments><comment>Cut over settings registration to a pull model (#18890)</comment></comments></commit></commits></item><item><title>group by + Consolidation + Filter + sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18889</link><project id="" key="" /><description>hi, I would like to complete this operation 

data：

```
    {
        "city": "Los Angeles",
        "name": "1",
        "time": "1464240096"
    }   
```

```
    {
        "city": "Los Angeles",
        "name": "2",
        "time": "1464240096"
    }  
```

```
    {
        "city": "Los Angeles",
        "name": "3",
        "time": "1464240096"
    }  
```

```
    {
        "city": "Los Angeles",
        "name": "4",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "5",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "6",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "7",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "8",
        "time": "1464240096"
    }  
```

```
    {
        "city": "Los Angeles",
        "name": "9",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "10",
        "time": "1464240096"
    }  
```

```
    {
        "city": "Los Angeles",
        "name": "11",
        "time": "1464240096"
    } 
```

```
    {
        "city": "Los Angeles",
        "name": "12",
        "time": "1464240096"
    } 
```

```
    {
        "city": "new York",
        "name": "21",
        "time": "1464240091"
    }  
```

```
    {
        "city": "new York",
        "name": "22",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "23",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "24",
        "time": "1464240091"
    }  
```

```
    {
        "city": "new York",
        "name": "25",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "26",
        "time": "1464240091"
    }  
```

```
    {
        "city": "new York",
        "name": "27",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "28",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "29",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "30",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "31",
        "time": "1464240091"
    } 
```

```
    {
        "city": "new York",
        "name": "32",
        "time": "1464240091"
    }

```

then return：

```
[
    {
        "city": "new York",
        "name": "21",
        "time": "1464240091",
        "names": [
            "1",
            "2",
            "3",
            "4",
            "5",
            "6",
            "7",
            "8",
            "9",
            "10",
            "11",
            "12"
        ]
    },
    {
        "city": "Los Angeles",
        "name": "1",
        "time": "1464240096",
        "names": [
            "21",
            "22",
            "23",
            "24",
            "25",
            "26",
            "27",
            "28",
            "29",
            "30",
            "31",
            "32"
        ]
    }
]

```

the ultimate result I need meets : 
firstly: sorted by time  ，
secondly ：merge into 2 records finally

What should I do
</description><key id="160392789">18889</key><summary>group by + Consolidation + Filter + sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brightvip</reporter><labels /><created>2016-06-15T10:49:20Z</created><updated>2016-06-15T10:53:40Z</updated><resolved>2016-06-15T10:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-15T10:53:40Z" id="226153927">You should ask on discuss.elastic.co where we can discuss about it.
We keep this space only for confirmed issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>term query on long type field on version 1.6 and on 2.3 plays completely different</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18888</link><project id="" key="" /><description>on version 1.6 term query on long type field act like a constant score while it's not on 2.3.

the query:
`"query": {
  "bool": {
     "should": [
        {
           "term": {
              "field": {
                 "value": 13310
              }
           }
        }
     ],
     "disable_coord": true
  }`

explain on 1.6:
`"_explanation": {
           "value": 1,
           "description": "ConstantScore(field: \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000g~), product of:",
           "details": [
              {
                 "value": 1,
                 "description": "boost"
              },
              {
                 "value": 1,
                 "description": "queryNorm"
              }
           ]
        }`

explain on 2.3:
`"_explanation": {
           "value": 0.30685282,
           "description": "sum of:",
           "details": [
              {
                 "value": 0.30685282,
                 "description": "weight(field: \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000g~ in 0) [PerFieldSimilarity], result of:",
                 "details": [
                    {
                       "value": 0.30685282,
                       "description": "fieldWeight in 0, product of:",
                       "details": [
                          {
                             "value": 1,
                             "description": "tf(freq=1.0), with freq of:",
                             "details": [
                                {
                                   "value": 1,
                                   "description": "termFreq=1.0",
                                   "details": []
                                }
                             ]
                          },
                          {
                             "value": 0.30685282,
                             "description": "idf(docFreq=1, maxDocs=1)",
                             "details": []
                          },
                          {
                             "value": 1,
                             "description": "fieldNorm(doc=0)",
                             "details": []
                          }
                       ]
                    }
                 ]
              },
              {
                 "value": 0,
                 "description": "match on required clause, product of:",
                 "details": [
                    {
                       "value": 0,
                       "description": "# clause",
                       "details": []
                    },
                    {
                       "value": 3.2588913,
                       "description": "_type:tweet, product of:",
                       "details": [
                          {
                             "value": 1,
                             "description": "boost",
                             "details": []
                          },
                          {
                             "value": 3.2588913,
                             "description": "queryNorm",
                             "details": []
                          }
                       ]
                    }
                 ]
              }
           ]
        }`

Is this known? I couldn't find anything about this change.
</description><key id="160389223">18888</key><summary>term query on long type field on version 1.6 and on 2.3 plays completely different</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Armadilo</reporter><labels /><created>2016-06-15T10:29:14Z</created><updated>2016-06-15T10:49:47Z</updated><resolved>2016-06-15T10:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-15T10:49:47Z" id="226153168">See discussion at #10628 for the background of this change. This was not considered a break since the fact that queries on numerics return constant scores was not documented. Interestingly, these queries will have constant scores again in 5.0 since we are indexing numbers with a different data-stucture which cannot score.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose half-floats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18887</link><project id="" key="" /><description>They have been implemented in https://issues.apache.org/jira/browse/LUCENE-7289.
Ranges are implemented so that the accuracy loss only occurs at index time,
which means that if you are searching for values between A and B, the query will
match exactly all documents whose value rounded to the closest half-float point
is between A and B.
</description><key id="160380292">18887</key><summary>Expose half-floats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>feature</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T09:43:31Z</created><updated>2016-07-04T14:00:07Z</updated><resolved>2016-06-16T07:47:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-15T17:58:27Z" id="226268413">LGTM
</comment><comment author="jpountz" created="2016-06-15T21:31:56Z" id="226326511">Thanks @rjernst!
</comment><comment author="polyfractal" created="2016-06-20T14:31:51Z" id="227159494">![emot-toot](https://cloud.githubusercontent.com/assets/1224228/16198000/300a6c86-36d2-11e6-9e24-d85c19508e6c.gif)
</comment><comment author="jpountz" created="2016-07-04T14:00:07Z" id="230297289">I did some unscientific benchmarking in order to make sure that half floats do not make things too slow. Query times are the same with floats and half floats, which makes sense since queries operate directly on the encoded representation anyway. However, a sum aggregation on the whole dataset runs about 8% slower with half floats due to the fact that half float bits are converted to doubles using software (`HalfFloatPoint.shortBitsToHalfFloat`).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/plain/HalfFloatFielddataTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/NumberFieldTypeTests.java</file></files><comments><comment>Expose half-floats. #18887</comment></comments></commit></commits></item><item><title>Configurable shard weights for shard balancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18886</link><project id="" key="" /><description>The shard balancer currently attributes the same weight to each shard when determining the best balance in the cluster. In situations where some shards are getting a larger share of the ingest or search load in the cluster, we would like to attribute these shards a higher weight so that there is a better overall load distribution. Similarly, attributing weights to shards helps with clusters that have shards varying largely in size and often see some nodes getting way more disk space usage than others.

This PR introduces a new index-level dynamic setting `index.shard.balance.weight_multiplier` that influences the weight of shards for the respective index. A multiplier of 5.0 means that shards of that index are attributed 5 times the weight of a regular shard during the shard balancing action.

The default weight multiplier is 1.0, and only multipliers greater than 1.0 are allowed.

Relates to #7171 and #18819.
</description><key id="160372628">18886</key><summary>Configurable shard weights for shard balancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels /><created>2016-06-15T09:04:55Z</created><updated>2017-06-30T09:46:24Z</updated><resolved>2017-06-30T09:46:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-15T09:20:30Z" id="226133746">I left some comments - looks awesome
</comment><comment author="clintongormley" created="2016-06-15T14:10:04Z" id="226198673">Could we drop `_multiplier` and just call it `index.shard.balance.weight` or maybe even `index.balance.shard.weight`?
</comment><comment author="ywelsch" created="2016-06-15T15:22:20Z" id="226221727">@s1monw I've updated the PR to use ints instead of floats. @clintongormley I've adopted your name change suggestion. Can you also have a look at the docs please?
</comment><comment author="clintongormley" created="2016-06-15T15:41:27Z" id="226228126">Docs LGTM
</comment><comment author="s1monw" created="2016-06-15T18:20:19Z" id="226274943">left one more paranoia comment :)
</comment><comment author="dakrone" created="2016-08-08T20:14:15Z" id="238362156">@ywelsch I think this might also need to update `Balancer.weighShard` to take the shard weights into account?
</comment><comment author="elasticmachine" created="2017-02-23T18:14:45Z" id="282074270">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="dakrone" created="2017-04-07T23:15:58Z" id="292673516">@ywelsch is this still applicable?</comment><comment author="rjernst" created="2017-06-13T21:05:28Z" id="308248360">@ywelsch Is this something you still want to get in?</comment><comment author="ywelsch" created="2017-06-30T09:46:24Z" id="312225028">Closing this PR as I'm not happy with the current solution. I might revisit this at a later point in time.
One issue with the current balancer is that it linearly combines shard and index weight (`theta0 * weightShard + theta1 * weightIndex`) to compute the overall weight, and thus balance. This means that if there is a large enough shard imbalance, index imbalance won't matter, which leads to the situation where, when a fresh node joins the cluster, and a new index is created, all shards of that index go to that node. As this PR can have an even greater influence on the shard balance (i.e. the weight multiplier), this can completely negate the index balance.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using `scroll` search from a node that doesn't hold data (master node or data node) returns data for every call</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18885</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**Steps to reproduce**:
- create a 3-node cluster made of two data nodes and one master node
- create an index like the following

```
POST /test/test/_bulk
{"index":{"_id":1}}
{"event": "BlackBerry device active", "timestamp": "2016-05-19T01:00:00Z", "username": "a"} 
{"index":{"_id":2}}
{"event": "BlackBerry device active", "timestamp": "2016-05-19T02:00:00Z", "username": "a"} 
{"index":{"_id":3}}
{"event": "BlackBerry device active", "timestamp": "2016-05-19T03:00:00Z", "username": "a"} 
{"index":{"_id":4}}
{"event": "BlackBerry device active", "timestamp": "2016-05-19T04:00:00Z"} 
{"index":{"_id":5}}
{"event": "BlackBerry device active", "timestamp": "2016-05-19T05:00:00Z"} 
```
- use the following query using `scroll` and **send the request to the master node**:

```
GET /test_/test/_search?scroll=1m&amp;size=2
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "username": {
        "order": "asc",
        "unmapped_type": "string"
      }
    },
    {
      "timestamp": {
        "order": "asc",
        "unmapped_type": "date"
      }
    }
  ],
  "_source": [
    "username",
    "timestamp"
  ]
}
```
- the first result will show documents with IDs 1 and 2
- the second call `GET /_search/scroll?scroll=1m&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs1OlAxWnFBTEEtUWRTaEJQLUQzLVNxVnc7NTpOTGVNaENDVFNYMm9wSUdjSkhpeGV3OzQ6UDFacUFMQS1RZFNoQlAtRDMtU3FWdzszOk5MZU1oQ0NUU1gyb3BJR2NKSGl4ZXc7NDpOTGVNaENDVFNYMm9wSUdjSkhpeGV3OzA7` (still **to the master node**) will get docs with IDs 3 and 4
- the third call will bring back docs with IDs 4 and 5
- the fourth call will bring back docs with IDs 4 and 5 and so on...

Almost the same behavior can be observed with sending the requests to a data node instead of a master only node, but the data node doesn't have any data from the index (I used `"index.routing.allocation.exclude._name": "NODE_NAME"`) to make that data node (where the requests are sent) not hold data for that specific index.
</description><key id="160366691">18885</key><summary>Using `scroll` search from a node that doesn't hold data (master node or data node) returns data for every call</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-06-15T08:34:37Z</created><updated>2016-06-15T14:08:57Z</updated><resolved>2016-06-15T14:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-15T10:25:07Z" id="226148278">This reproduces for me. I'll dig.
</comment><comment author="jpountz" created="2016-06-15T12:17:40Z" id="226170124">Found it: there is a bug in the hack that we have to merge top hits on the coordinating node. It checks reference equality instead of logical equality, which is an issue since the data might go over the network, so reference equality does not work.

This has been fixed in 2.0+ thanks to the cleanups in #12127 but I cannot backport it since it has some bw breaks. I will open a PR shortly.
</comment><comment author="jpountz" created="2016-06-15T14:08:57Z" id="226198314">Fixed via #18893.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggester with  prefix and regx not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18884</link><project id="" key="" /><description>I have try with Completion Suggest with prefix,regx and text. This is work only with text.  
but I have check with elasticsearch test cases and working fine with prefix and regex (Test class -CompletionSuggestSearchIT).

**Elasticsearch version**:2.3.3

**JVM version**:1.8

**OS version**:windows 10

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:

DELETE /simple_suggest_index
PUT /simple_suggest_index
PUT /simple_suggest_index/simple_type/_mapping
{
"simple_type": {
"properties": {
  "name" : { "type" : "string" },
"suggest": {
"type": "completion",
"analyzer" : "simple",
"search_analyzer": "simple",
"preserve_separators":false,
"preserve_position_increments":false
}
}
}
}

POST /simple_suggest_index/simple_type/
{"name":"1","suggest":{"input":["Foo Fighters"]}}
POST /simple_suggest_index/simple_type/
{"name":"2","suggest":{"input":["Generator"]}}
POST /simple_suggest_index/simple_type/
{"name":"3","suggest":{"input":["Foo Fighters Generator"]}}
POST /simple_suggest_index/simple_type/
{"name":"4","suggest":{"input":["Mr ishara samantha nanayakkara"]}}

POST /simple_suggest_index/_suggest?pretty
{
  "simple_type": {
    "prefix": "m",
    "completion": {
      "field": "suggest",
      "fuzzy": {
        "fuzziness": 2
      }
    }
  }
}
POST /simple_suggest_index/_search?pretty
{
  "suggest" : {
    "simple_type" : {
      "prefix" : "m",
      "completion" : {
        "field" : "suggest",
        "size" : 10,
        "fuzzy" : {
          "fuzziness" : 2,
          "transpositions" : true,
          "min_length" : 3,
          "prefix_length" : 1,
          "unicode_aware" : false,
          "max_determinized_states" : 10000
        }
      }
    }
  }
}

**Describe the feature**:

{
   "_shards": {
      "total": 5,
      "successful": 0,
      "failed": 5,
      "failures": [
         {
            "shard": 0,
            "index": "simple_suggest_index",
            "status": "INTERNAL_SERVER_ERROR",
            "reason": {
               "type": "exception",
               "reason": "failed to execute suggest",
               "caused_by": {
                  "type": "illegal_argument_exception",
                  "reason": "[suggest] does not support [prefix]"
               }
            }
         }
      ]
   }
}
</description><key id="160363396">18884</key><summary>Completion Suggester with  prefix and regx not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ishara</reporter><labels /><created>2016-06-15T08:15:59Z</created><updated>2016-06-15T09:07:53Z</updated><resolved>2016-06-15T09:07:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T09:07:53Z" id="226130860">Hi @ishara 

The new completion suggester, which supports prefix and regex, is only available in 5.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Validate restored index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18883</link><project id="" key="" /><description>Now that index templates are validated on creation/update (#8802), it would be good to validate templates restored from a snapshot too.
</description><key id="160362080">18883</key><summary>Validate restored index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>stalled</label></labels><created>2016-06-15T08:08:06Z</created><updated>2017-07-14T12:59:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="haiguo" created="2016-06-15T14:52:38Z" id="226212252">+1 would be good to get this into 2.x
</comment><comment author="jpountz" created="2016-06-16T10:11:50Z" id="226444308">What should we do if the template is not valid?
</comment><comment author="clintongormley" created="2016-06-16T10:27:05Z" id="226447424">Very good question... You don't want to refuse to restore everything else from the snapshot.  I'm hoping there is a way of reporting: "template Foo skipped because invalid"
</comment><comment author="ywelsch" created="2016-06-16T16:02:47Z" id="226531828">Templates are currently not (re-)validated / upgraded when we upgrade a cluster. I see that as a pre-requisite to this as it would otherwise make the situation possible where a snapshot that was just done in a cluster cannot be restored in the same cluster. The change is thus broader than just "fixing" this for snapshot/restore. If we were to solve this, there is still the question on what to do when we run into an invalid template. A template can be invalid due to a number of reasons (changed naming rules, incompatible mapping or incompatible index settings). 

I see different ways forward: 1) Do the same as we do for the persistent cluster settings, namely just fail to restore. This might be very unsatisfactory for users which have this one template that is not compatible (and only find out when restoring). 2) Do a partial restore and notify the user that some templates could not be restored. This requires adding status fields for partial success and is totally different from an API perspective than what we do for incompatible indices. 3) Take the approach that we use for indices: When we restore indices at the moment, we have the possibility to select which indices should be restored, what settings should be ignored (and what new settings added / overridden). If an index has incompatible mappings, the overall restore operation fails right at the beginning and the user can repeat the restore process by choosing not to include that incompatible index in the restore. The same does not exist for templates (we can neither select which templates should be restored nor can we change their properties upon restore). It's unclear to me whether adding all that complexity to the restore API would be the right thing to do.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Duplicate Data  problem </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18882</link><project id="" key="" /><description>I have problem to push mysql data to elasticsearch using mysql replicator. 

 I have two table for chat__user and chat_message.

chat_user:

```
   id    name  socketid

   1     raj    123
   2     kumar   1234
```

chat_message:

```
  id   chat_from  chat_to  message

   1      123     1235       hello
   2      1235    123        how can i help you? 
   3      123     1235       HOw track my order?
```

 then, John this two tables Using  "chat_message.chat_from = chat_user.socketid OR chat_message.chat_to = chat_user.socketid"

  Myquery:

```
    SELECT * FROM `chat_message` INNER JOIN `chat_user` ON chat_message.chat_from = chat_user.socketid OR chat_message.chat_to = chat_user.socketid
```

Result:

```
chat_from  chat_to  message                    id   name  socketid 

123        1235     hello                      1    raj    123 
1235        123        how can i help you?     1    raj    123 
123        1235     HOw track my order?        1    raj    123 
```

If I push this data to elasticsearch, only push last row data.

```
 123        1235     HOw track my order?        1    raj    123 
```

Because Duplication occur in primary key  I set primary key chat_user id is a primary key in td-agent configuration file. 

Td-Agent Configuration File:

```
    ####
  ## Output descriptions:
  ##
  # HTTP input
  # POST http://localhost:8888/&lt;tag&gt;?json=&lt;json&gt;
  # POST http://localhost:8888/td.myapp.login?json={"user"%3A"me"}
  # @see http://docs.fluentd.org/articles/in_http
  &lt;source&gt;
    @type http
    port 8888
  &lt;/source&gt;

  ## live debugging agent
  &lt;source&gt;
    @type debug_agent
    bind localhost
    port 24230
  &lt;/source&gt;

  ####
  ## Examples:
  ##


  &lt;source&gt;
    @type mysql_replicator
    host localhost
    username root
    password gworks.mobi2
    database livechat
    query SELECT * FROM `chat_message` INNER JOIN `chat_user` ON chat_message.chat_from = chat_user.socketid OR chat_message.chat_to = chat_user.socketid;
    primary_key id 
    interval 10s  
    enable_delete yes
    tag replicator.history5.histestb.${event}.${primary_key}
  &lt;/source&gt;
  &lt;match replicator.**&gt;
   @type stdout
  &lt;/match&gt;

  &lt;match replicator.**&gt;
    @type mysql_replicator_elasticsearch
    host localhost
    port 9200
    tag_format (?&lt;index_name&gt;[^\.]+)\.(?&lt;type_name&gt;[^\.]+)\.(?&lt;event&gt;[^\.]+)\.(?&lt;primary_key&gt;[^\.]+)$
    flush_interval 5s
    max_retry_wait 1800
    flush_at_shutdown yes 
    buffer_type file
    buffer_path /var/log/td-agent/buffer/mysql_replicator_elasticsearch.*
  &lt;/match&gt;
```

I need to push all  data to elasticsearch, Suggest me  How to solve this Problem? .
</description><key id="160358234">18882</key><summary>Duplicate Data  problem </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Git-Rajkumar</reporter><labels /><created>2016-06-15T07:44:53Z</created><updated>2016-06-15T07:53:00Z</updated><resolved>2016-06-15T07:53:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-15T07:53:00Z" id="226114291">For general questions, the [Elastic Discourse forums](https://discuss.elastic.co) are best as Elastic reserves GitHub for verified bug reports and feature requests. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify SubFetchPhase interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18881</link><project id="" key="" /><description>This interface used to have dedicated methods to prevent calling execute
methods. These methods are unnecessary as the checks can simply be
done inside the execute methods itself. This simplifies the interface
as well as its usage.
</description><key id="160356948">18881</key><summary>Simplify SubFetchPhase interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>:Search</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T07:37:09Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-15T13:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-06-15T09:23:13Z" id="226134361">Nice cleanup! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/parent/ParentFieldSubFetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorHighlightSubFetchPhase.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorHighlightSubFetchPhaseTests.java</file></files><comments><comment>Simplify SubFetchPhase interface (#18881)</comment></comments></commit></commits></item><item><title>Expose MMapDirectory.preLoad().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18880</link><project id="" key="" /><description>The MMapDirectory has a switch that allows the content of files to be loaded
into the filesystem cache upon opening. This commit exposes it with the new
`index.store.pre_load` setting.
</description><key id="160353128">18880</key><summary>Expose MMapDirectory.preLoad().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Store</label><label>feature</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T07:11:20Z</created><updated>2016-06-20T11:44:23Z</updated><resolved>2016-06-20T11:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-15T07:13:58Z" id="226106803">Notes/questions to reviewers:
- this removes the deprecated `default` fs to keep things simple
- should the setting be `index.store.pre_load` or `index.store.fs.pre_load`?
- currently the setting has a default value of `false`, should we consider enabling it by default to give better search performance on (re)starts and avoid cold starts?
</comment><comment author="clintongormley" created="2016-06-15T09:01:27Z" id="226129401">I'd vote for `index.store.preload` (without the `_`)
</comment><comment author="jpountz" created="2016-06-15T13:02:20Z" id="226180058">Something else that I forgot to mention: only the terms dictionary, norms, doc values and points will be preloaded. I think this is important to not waste some file system cache on stored fields or term vectors.
</comment><comment author="rmuir" created="2016-06-15T13:39:05Z" id="226189635">I had in mind two cases for the preload option in Lucene:
1. better option than new RAMDirectory(FSDirectory) for folks that want to load the entire index into RAM. This is a common thing people want, they think they are smarter than the OS, but then use RAMDir? that's not smarter :) At least this is going to be better than RAMDir if someone is hell-bent on doing that.
2. more efficient warming than e.g. running a bunch of queries or other things. This could be a concern for those random access datastructures, I agree.

It will touch the first byte of every page. This is still useful as it knows platform-specific crap like page size to do this. Additionally, on linux, before it does that touching, it will call madvise(WILL_NEED).

I'm not sure if we should do this by default: its a fairly serious hammer? Basically you are saying "I think i know better than the operating system" by using this. I worry about doing it by default in cases where these index portions are significantly larger than available RAM, or where the index is considered "cold" by the user anyway. In those cases we could simply cause a ton of unnecessary I/O for no good reason. 

But at the same time, it could be useful for some of these random access data structures (like norms/docvalues) where traditionally they were loaded completely into the heap anyway. Like a middle ground?

Just thoughts, i don't really have strong opinions either way. Maybe we could do some kind of test...
</comment><comment author="jpountz" created="2016-06-15T14:55:54Z" id="226213357">Thanks @rmuir for the feedback. I changed the setting to accept a list of extensions to preload so that users who want to load whole indices into memory can do it without requiring us to have a dedicated setting for it. Also the default value is now to preload only norms and doc values, and I improved the documentation to describe how to disable this behaviour for cold indices or preload more files for hot indices.
</comment><comment author="uschindler" created="2016-06-17T07:38:06Z" id="226701657">-1 to enable by default. That's horrible!
</comment><comment author="jpountz" created="2016-06-17T08:04:41Z" id="226706733">Fair enough, I changed the default back so that it does not pre-load anything.
</comment><comment author="uschindler" created="2016-06-17T08:11:36Z" id="226708085">Thanks Adrien. Maybe others have a different opinion, but those settings have a large effect on opening indexes. It also depends what type of system you have: For some logstash environment its different than an online shop regarding delays while opening new segments.

+1 to make it an array setting. Much cleaner!
</comment><comment author="uschindler" created="2016-06-17T08:13:29Z" id="226708488">For the example Robert brought in: People who want to load everything - do we have a shortcut for that? (RAMDirectory replacment)
</comment><comment author="jpountz" created="2016-06-17T08:15:39Z" id="226708887">Indeed it is probably safer to have it empty by default. Then we can work on documentation, or maybe even a "how to tune for search speed" post like @mikemccand  did for indexing.

There is no shortcut for loading everything into memory, do you think there should be one? I'm wondering it might be better to not make it too easy?
</comment><comment author="uschindler" created="2016-06-17T08:20:44Z" id="226709843">&gt; There is no shortcut for loading everything into memory, do you think there should be one? I'm wondering it might be better to not make it too easy?

I agree it would make it too easy. But on the other hand for those people that really want everything in RAM, there is no clean way to do it - because you would need to list every (maybe not yet known) file extension. My first idea would have been a glob pattern like for indexes - `"*"` as setting matches every file.
</comment><comment author="jpountz" created="2016-06-17T08:34:00Z" id="226712432">@uschindler I pushed a new commit that adds support for `"*"` in order to load everything into memory.
</comment><comment author="uschindler" created="2016-06-17T08:48:42Z" id="226715362">Looks fine. Let's see what @rmuir says!
</comment><comment author="rmuir" created="2016-06-17T13:43:42Z" id="226772066">I don't think we should differ from lucene's defaults here. If there are some interesting numbers, then we can think about what to do.

But lets provide a less horrible alternative to RAMDirectory for stubborn people at least.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/store/FsDirectoryService.java</file><file>core/src/test/java/org/elasticsearch/index/store/FsDirectoryServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/IndexStoreTests.java</file></files><comments><comment>Expose MMapDirectory.preLoad(). #18880</comment></comments></commit></commits></item><item><title>Very minor typo fixed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18879</link><project id="" key="" /><description /><key id="160345299">18879</key><summary>Very minor typo fixed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbritten</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-06-15T06:09:05Z</created><updated>2016-06-16T09:03:13Z</updated><resolved>2016-06-16T09:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T08:59:34Z" id="226128910">Hi @rbritten 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="rbritten" created="2016-06-15T22:43:23Z" id="226341784">CLA signed.
</comment><comment author="clintongormley" created="2016-06-16T09:03:13Z" id="226428838">thanks @rbritten  - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can't work with Chrome/Safari userAgent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18878</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.1

**JVM version**:

```
java -version
openjdk version "1.8.0_71"
OpenJDK Runtime Environment (build 1.8.0_71-b15)
OpenJDK 64-Bit Server VM (build 25.71-b15, mixed mode)
```

**OS version**:

```
lsb_release -a
LSB Version:    :core-4.1-amd64:core-4.1-noarch
Distributor ID: CentOS
Description:    CentOS Linux release 7.1.1503 (Core)
Release:    7.1.1503
Codename:   Core
```

**Description of the problem including expected versus actual behavior**:
work fine on firefox, can't work with Chome/safari

**Steps to reproduce**:
1. Normal user-agent is ok

``` shell
curl -v 'https://xxx.my.host/elasticsearch/_mget?timeout=0&amp;ignore_unavailable=true&amp;preference=1465897339430' -H 'origin: https://xxx.my.host' -H 'user-agent: Rajax/1 Redmi_3/ido Android/5.1.1 Display/LMY47V Eleme/5.10.2 ID/fa0ef395-659a-3150-b347-f258248013af; KERNEL_VERSION:3.10.49-perf-g6241083 API_Level:22 Mozilla/5.0 (Linux; Android 5.1.1; Redmi 3 Build/LMY47V; wv)' -d '{"docs":[{"_index":".kibana","_type":"config","_id":"4.5.1"}]}'
*   Trying 115.x.x.42...
* Connected to xxx.my.host (115.x.x.42) port 443 (#0)
* TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate: *.ele.me
* Server certificate: GeoTrust SSL CA - G3
* Server certificate: GeoTrust Global CA
&gt; POST /elasticsearch/_mget?timeout=0&amp;ignore_unavailable=true&amp;preference=1465897339430 HTTP/1.1
&gt; Host: xxx.my.host
&gt; Accept: */*
&gt; origin: https://xxx.my.host
&gt; user-agent: Rajax/1 Redmi_3/ido Android/5.1.1 Display/LMY47V Eleme/5.10.2 ID/fa0ef395-659a-3150-b347-f258248013af; KERNEL_VERSION:3.10.49-perf-g6241083 API_Level:22 Mozilla/5.0 (Linux; Android 5.1.1; Redmi 3 Build/LMY47V; wv)
&gt; Content-Length: 62
&gt; Content-Type: application/x-www-form-urlencoded
&gt;
* upload completely sent off: 62 out of 62 bytes
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.9.6
&lt; Date: Wed, 15 Jun 2016 03:47:01 GMT
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 116
&lt; Connection: keep-alive
&lt; Vary: Accept-Encoding
&lt;
* Connection #0 to host xxx.my.host left intact
{"docs":[{"_index":".kibana","_type":"config","_id":"4.5.1","_version":1,"found":true,"_source":{"buildNum":9892}}]}
```

 2.
Got 403 forbidden with Chrome/Safari user-agent...

``` shell
curl -v 'https://xxx.my.host/elasticsearch/_mget?timeout=0&amp;ignore_unavailable=true&amp;preference=1465897339430' -H 'origin: https://xxx.my.host' -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5)' -d '{"docs":[{"_index":".kibana","_type":"config","_id":"4.5.1"}]}'
*   Trying 115.x.x.12...
* Connected to xxx.my.host (115.x.x.12) port 443 (#0)
* TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate: *.my.host
* Server certificate: GeoTrust SSL CA - G3
* Server certificate: GeoTrust Global CA
&gt; POST /elasticsearch/_mget?timeout=0&amp;ignore_unavailable=true&amp;preference=1465897339430 HTTP/1.1
&gt; Host: xxx.my.host
&gt; Accept: */*
&gt; origin: https://xxx.my.host
&gt; user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5)
&gt; Content-Length: 62
&gt; Content-Type: application/x-www-form-urlencoded
&gt;
* upload completely sent off: 62 out of 62 bytes
&lt; HTTP/1.1 403 Forbidden
&lt; Server: nginx/1.9.6
&lt; Date: Wed, 15 Jun 2016 03:46:08 GMT
&lt; Transfer-Encoding: chunked
&lt; Connection: keep-alive
&lt;
* Connection #0 to host xxx.my.host left intact
```

**Provide logs (if relevant)**:
above

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
I'm trying to use kibana to connect to elasticsearch, but found that not work with chrome/safari and work fine on firefox. I thought it was kibana's problme, but it seems there is some detail that I don't know about elasticsearch.
</description><key id="160340724">18878</key><summary>Can't work with Chrome/Safari userAgent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Lellansin</reporter><labels><label>feedback_needed</label></labels><created>2016-06-15T05:25:29Z</created><updated>2016-06-15T08:29:44Z</updated><resolved>2016-06-15T08:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-06-15T06:22:42Z" id="226098501">This has been raised on the forums - https://discuss.elastic.co/t/got-403-forbidden-with-chrome-user-agent/52812
</comment><comment author="jasontedor" created="2016-06-15T06:37:32Z" id="226100660">I think this is addressed by #18278 and #18283, released in 2.3.3. Can you upgrade and verify?
</comment><comment author="Lellansin" created="2016-06-15T08:28:30Z" id="226121800">It works after upgrade to 2.3.3, thanks @jasontedor 👍 
</comment><comment author="jasontedor" created="2016-06-15T08:29:44Z" id="226122055">You're welcome!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DateIndexNameProcessor date_rounding needs different values for months and minutes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18877</link><project id="" key="" /><description>The [DateIndexName Processor documentation](https://www.elastic.co/guide/en/elasticsearch/reference/master/date-index-name-processor.html) lists `m` as the value for both minute and month `date_rounding` units. The value should be different for each unit. 

[Date Math uses `M` for month](https://www.elastic.co/guide/en/elasticsearch/reference/master/date-math-index-names.html) so I think this is just a documentation change. Happy to submit PR for this
</description><key id="160330219">18877</key><summary>DateIndexNameProcessor date_rounding needs different values for months and minutes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels><label>docs</label></labels><created>2016-06-15T03:18:16Z</created><updated>2016-06-15T08:58:00Z</updated><resolved>2016-06-15T08:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-06-15T08:57:34Z" id="226128392">thx @russcam that is indeed incorrect! I'll fix it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>docs: fix typo</comment></comments></commit></commits></item><item><title>Emit nicer error message when trying to install unknown plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18876</link><project id="" key="" /><description>When installing plugins, we first try the elastic download service for
official plugins, then try maven coordinates, and finally try the
argument as a url. This can lead to confusing error messages about
unknown protocols when eg an official plugin name is mispelled. This
change adds a heuristic for determining if the argument in the final
case is in fact a url that we should try, and gives a simplified error
message in the case it is definitely not a url.

closes #17226
</description><key id="160329191">18876</key><summary>Emit nicer error message when trying to install unknown plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-15T03:05:56Z</created><updated>2016-06-15T16:56:43Z</updated><resolved>2016-06-15T16:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-06-15T06:12:45Z" id="226097173">@dadoonet I pushed an update.
</comment><comment author="spinscale" created="2016-06-15T07:44:10Z" id="226112604">thx for doing it! If we want to be extra fancy, we could apply a similar logic which we use when parsing setting names, using levenshtein to check for similar plugins in order to help on typos... https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java#L247-L270

LGTM otherwise
</comment><comment author="dadoonet" created="2016-06-15T08:19:03Z" id="226119728">LGTM 2
</comment><comment author="rjernst" created="2016-06-15T16:56:34Z" id="226250372">&gt; If we want to be extra fancy, we could apply a similar logic which we use when parsing setting names, using levenshtein to check for similar plugins in order to help on typos

I like this idea! I opened #18896 for a follow up
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java</file></files><comments><comment>Merge pull request #18876 from rjernst/plugin_install_unknown</comment></comments></commit></commits></item><item><title>Remove only node preference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18875</link><project id="" key="" /><description>This commit removes the search preference _only_node as the same
functionality can be obtained by using the search preference
_only_nodes. This commit also adds a test that ensures that _only_nodes
will continue to support specifying node IDs.

Relates #18822
</description><key id="160298658">18875</key><summary>Remove only node preference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Search</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T22:25:12Z</created><updated>2016-06-17T19:27:54Z</updated><resolved>2016-06-17T19:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T08:53:18Z" id="226127363">With this change, `_only_node` would be silently ignored.  I think we should (at least for 5.x) throw an exception instead?
</comment><comment author="jasontedor" created="2016-06-15T09:17:21Z" id="226133022">I don't think so, I think it will throw as an unrecognized preference.
</comment><comment author="clintongormley" created="2016-06-15T09:32:19Z" id="226136448">Ah sorry, I confused myself with silently ignored unknown QS params.
</comment><comment author="abeyad" created="2016-06-17T19:24:34Z" id="226859478">LGTM
</comment><comment author="jasontedor" created="2016-06-17T19:27:54Z" id="226860251">Thanks @abeyad!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/OperationRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadIT.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceIT.java</file></files><comments><comment>Remove only node preference</comment></comments></commit></commits></item><item><title>index.routing.allocation.total_primary_shards_per_node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18874</link><project id="" key="" /><description>Currently, an index can be configured with [`index.routing.allocation.total_shards_per_node`](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html). We're making heavy use of this to ensure that the shards for our indices are evenly distributed across our available nodes.

However, this control does not extend far enough, we also need to be able to control the allocation of the primaries vs. replicas. So, here's a feature request for `index.routing.allocation.total_primary_shards_per_node`.

---

Today, if we have 2 nodes, and we create 2 indices with 2 primary shards, each with a replica, we can end up in a situation where one node has all 4 primary shards, and the other node has the replicas. This will happen even if we configure `{"index.routing.allocation.total_shards_per_node": 2}` on each index.

If `index.routing.allocation.total_primary_shards_per_node` were implemented, we would be able to create the indices with settings `{"index.routing.allocation.total_primary_shards_per_node": 1, "index.routing.allocation.total_shards_per_node": 2}`, which would result in each node having exactly 2 primary shards and 2 replicas.
</description><key id="160294427">18874</key><summary>index.routing.allocation.total_primary_shards_per_node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2016-06-14T22:00:13Z</created><updated>2016-06-17T09:25:44Z</updated><resolved>2016-06-17T09:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-15T08:45:41Z" id="226125698">Why does it matter?  Primary vs replica is just a role - the shards should be doing the same work.
</comment><comment author="clintongormley" created="2016-06-15T08:47:06Z" id="226125978">Ah, now I've seen your comment in #18866 and I understand more. I think that #18866 is the correct fix here, rather than worrying about how many primaries are on a single node.
</comment><comment author="clintongormley" created="2016-06-17T09:25:43Z" id="226723207">Discussed this in Fix it Friday.  We agree that this setting is not the way to go. Instead we should be working on improving #18866
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MultiMatchQueryIT#testCrossFieldMode test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18873</link><project id="" key="" /><description>This test rarely seems to fail on CI, but I can reproduce it reliably on master with:

```
gradle :core:integTest -Dtests.seed=F953E772EA3B8953 -Dtests.class=org.elasticsearch.search.query.MultiMatchQueryIT -Dtests.method="testCrossFieldMode" -Dtests.security.manager=true -Dtests.locale=hi -Dtests.timezone=America/Bahia_Banderas
```

There seems to be some underlying issues with boosting, as the first search hit returned is supposed to have an id of ultimate1, but the id is 49, which is one of the later documents created in the `init()` phase of the test.
</description><key id="160294164">18873</key><summary>MultiMatchQueryIT#testCrossFieldMode test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Search</label><label>test</label></labels><created>2016-06-14T21:58:54Z</created><updated>2016-09-13T16:38:34Z</updated><resolved>2016-09-13T16:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2016-08-29T19:37:50Z" id="243231278">One more repro seed, FWIW, this one on Azul ZingVM 16.01.7.0 on CentOS 7.2.

```
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=C3DC091D23C57091 -Dtests.class=org.elasticsearch.search.query.MultiMatchQueryIT -Dtests.method="testCrossFieldMode" -Dtests.security.manager=true -Dtests.locale=id -Dtests.timezone=America/Mexico_City
FAILURE 1.45s J2 | MultiMatchQueryIT.testCrossFieldMode &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: searchHit id should be "ultimate1"
   &gt;      but:  was "24"
   &gt;    at __randomizedtesting.SeedInfo.seed([C3DC091D23C57091:CBFC75455976FFFC]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit(ElasticsearchAssertions.java:349)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit(ElasticsearchAssertions.java:284)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit(ElasticsearchAssertions.java:261)
   &gt;    at org.elasticsearch.search.query.MultiMatchQueryIT.testCrossFieldMode(MultiMatchQueryIT.java:575)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="awislowski" created="2016-09-07T16:06:43Z" id="245331646">I have made a PR fixing this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java</file></files><comments><comment>[TEST] fix MultiMatchQueryIT random docs generation so that they don't interfere in score tests</comment></comments></commit></commits></item><item><title>Add search preference to prefer multiple nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18872</link><project id="" key="" /><description>The search preference _prefer_node allows specifying a single node to
prefer when routing a request. This functionality can be enhanced by
permitting multiple nodes to be preferred. This commit replaces the
search preference _prefer_node with the search preference _prefer_nodes
which supplants the former by specifying a single node and otherwise
adds functionality.

Relates #18822
</description><key id="160292592">18872</key><summary>Add search preference to prefer multiple nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Search</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T21:50:21Z</created><updated>2016-06-15T15:08:48Z</updated><resolved>2016-06-15T01:34:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T22:26:40Z" id="226035068">LGTM, but, yeah, breaking label.
</comment><comment author="clintongormley" created="2016-06-15T08:57:53Z" id="226128481">With this change, _prefer_node would be silently ignored. I think we should (at least for 5.x) throw an exception instead?
</comment><comment author="jasontedor" created="2016-06-15T09:11:44Z" id="226131755">@clintongormley I think we throw on unrecognized preference which `_prefer_node` is now. I will verify this with a test later. 
</comment><comment author="jasontedor" created="2016-06-15T09:21:45Z" id="226134042">It throws, built against fa77d4d88527c64d3ff3751e4fe4338c00b37ed0:

```
05:18:26 [jason:~] $ curl -XGET localhost:9200/
{
  "name" : "Ethan Edwards",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha4",
    "build_hash" : "fa77d4d",    "build_date" : "2016-06-15T09:04:24.481Z",
    "build_snapshot" : true,    "lucene_version" : "6.1.0"
  },  "tagline" : "You Know, for Search"
}
05:18:34 [jason:~] $ curl -XGET "localhost:9200/_search?pretty=1&amp;preference=_prefer_node:xyz"
{
  "error" : {
    "root_cause" : [
      {
        "type" : "illegal_argument_exception",
        "reason" : "no Preference for [_prefer_node]"
      }
    ],
    "type" : "illegal_argument_exception",
    "reason" : "no Preference for [_prefer_node]"
  },
  "status" : 400
}
```
</comment><comment author="clintongormley" created="2016-06-15T09:32:39Z" id="226136512">Ah sorry, I confused myself with silently ignored unknown QS params.
</comment><comment author="jasontedor" created="2016-06-15T15:08:48Z" id="226217491">&gt; Ah sorry, I confused myself with silently ignored unknown QS params.

Ah, I thought that maybe you were thinking of preference parameters that do not start with an underscore, which are hashed and used to seed the shard shuffler. In this case, since `_prefer_node` does start with an underscore though, we do attempt to parse it and throw since it's no longer recognized. :smile:
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/OperationRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceIT.java</file></files><comments><comment>Add search preference to prefer multiple nodes</comment></comments></commit></commits></item><item><title>Fix Casting Bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18871</link><project id="" key="" /><description>As Uwe said, Definition.Sort should be redesigned as I'm currently abusing it for casting.  However, for now this is a quick fix to a bug that would allow primitives to be cast to any subclass of Object rather than just Object.
</description><key id="160288906">18871</key><summary>Fix Casting Bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T21:31:29Z</created><updated>2016-06-14T21:45:34Z</updated><resolved>2016-06-14T21:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-14T21:39:48Z" id="226024507">looks good
</comment><comment author="jdconrad" created="2016-06-14T21:45:34Z" id="226025838">Thanks @rmuir !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/AnalyzerCaster.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/WhenThingsGoWrongTests.java</file></files><comments><comment>Merge pull request #18871 from jdconrad/cast</comment></comments></commit></commits></item><item><title>Use sysprop like with es.path.home to pass conf dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18870</link><project id="" key="" /><description>Currently we always pass -E to the the plugin cli with the conf dir, but
this causes a very confusing error message when not giving a specific
command to the plugin cli. This change makes path.conf pass just like
path.home. These are special settings, so passing via sysprops is the
right thing to do (it is all about how we pass between shell and java
cli).

closes #18689
</description><key id="160285287">18870</key><summary>Use sysprop like with es.path.home to pass conf dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-beta1</label><label>v6.0.0-alpha1</label></labels><created>2016-06-14T21:13:15Z</created><updated>2016-12-08T10:45:51Z</updated><resolved>2016-09-14T21:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-09-07T21:10:03Z" id="245420193">@jasontedor I pushed a change to fix the quoting, and also made the equivalent change for windows bat script.
</comment><comment author="jasontedor" created="2016-09-09T18:15:50Z" id="245995147">I think this looks right, and I tested it too. Can you clean up the merge conflicts and I'll give it a final review?
</comment><comment author="rjernst" created="2016-09-12T18:14:19Z" id="246439743">@jasontedor I fixed the merge conflict.
</comment><comment author="jasontedor" created="2016-09-14T20:02:52Z" id="247135998">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Plugins: Use sysprop like with es.path.home to pass conf dir (#18870)</comment></comments></commit></commits></item><item><title>Highlight "*" doesn't work with match _all field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18869</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3
**JVM version**: Java HotSpot(TM) 64-Bit Server VM (25.20-b23 mixed mode windows-amd64 compressed oops)
**OS version**: Windows 7

I'm sending a request like this:

``` json
{
    "from": 0, 
    "query": {
        "match": {
            "_all": "presidencia"
        }
    }
    ,
    "aggs": {
        //... some aggregations 
    }
    ,
    "highlight": {
        require_field_match : false
        ,"fields": {
            "*": {}
        }
    }
}
```

But my response doesn't come with highlight field. Response:

``` json
{
    "took": 68,
    "timed_out": false,
    "_shards": {"total": 15, "successful": 15, "failed": 0},
    "hits": {
        "total": 692785,
        "max_score": 0.48536316,
        "hits": [
            //Some hits WITHOUT the HIGHLIGHT results...
        ]
    },
    "aggregations": {
        //some aggs ...
    }
}
```
</description><key id="160275022">18869</key><summary>Highlight "*" doesn't work with match _all field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">renanleandrof</reporter><labels /><created>2016-06-14T20:22:54Z</created><updated>2016-06-14T20:32:26Z</updated><resolved>2016-06-14T20:24:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T20:24:52Z" id="226004874">See the note about _all in https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html . This is a known limitation of _all and highlighting. The usual way to work around this is to set `require_field_match` to false and then highlight against `*`. That has its problems as well, but it does work around it.
</comment><comment author="renanleandrof" created="2016-06-14T20:29:23Z" id="226006191">Sorry, you replied before I could update the question, I am doing the
required_field_match false thing and still nothing comes.
Em 14 de jun de 2016 17:25, "Nik Everett" notifications@github.com
escreveu:

&gt; See the note about _all in
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html
&gt; . This is a known limitation of _all and highlighting. The usual way to
&gt; work around this is to set require_field_match to false and then
&gt; highlight against *. That has its problems as well, but it does work
&gt; around it.
&gt; 
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18869#issuecomment-226004874,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AGz0aeiM7Cg3c5cDB6CIeOsYXAlgCOlJks5qLw5PgaJpZM4I1uMr
&gt; .
</comment><comment author="nik9000" created="2016-06-14T20:32:26Z" id="226007075">&gt; I am doing the required_field_match false thing and still nothing comes.

Ah! Sorry about that. Then it could be the analyzers producing tokens for your fields that don't match up with _all. That is one of the problems. I'd try to use the [analyze](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html) API to track it down.

It _probably_ isn't a bug so much as a known limitation so this conversation ought to move to discuss.elastic.co.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Courier Fetch: Bad Gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18868</link><project id="" key="" /><description>I been getting this error lately as soon as fire up Kibana. Any ideas why this is happening ?
As soon as I hit the Go Back button my queries would populate.

![image](https://cloud.githubusercontent.com/assets/19933089/16049157/af59e2d0-3224-11e6-9a8a-4f99cfdee818.png)

![image](https://cloud.githubusercontent.com/assets/19933089/16049190/c7b95284-3224-11e6-9187-ec72ad1a09de.png)
</description><key id="160260755">18868</key><summary>Courier Fetch: Bad Gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pip00</reporter><labels /><created>2016-06-14T19:12:31Z</created><updated>2016-06-15T16:07:51Z</updated><resolved>2016-06-14T19:13:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-14T19:13:44Z" id="225986291">There is a [dedicated repository for Kibana](https://github.com/elastic/kibana), maybe it's best to seek an answer there?
</comment><comment author="pip00" created="2016-06-14T19:18:04Z" id="225987441">I don't think this is an issue with Kibana. 
</comment><comment author="jasontedor" created="2016-06-14T19:26:20Z" id="225989536">&gt; I don't think this is an issue with Kibana.

I don't think it's an Elasticsearch issue, I don't think that we return 502 anywhere and anyway even if it is there is no indication where in Elasticsearch the issue could be since you've told us nothing. If you do think that this is an Elasticsearch issue, then your best bet is to ask a question on the [Elastic Discourse forums](https://discuss.elastic.co) in the Elasticsearch category until there is a verified bug here. Sorry, I really think you'll get the most effective help if you try to troubleshoot from the Kibana side.
</comment><comment author="jasontedor" created="2016-06-15T16:07:51Z" id="226236699">&gt; I don't think that we return 502 anywhere

I checked, Elasticsearch never sends a 502.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>improve unary operators and cleanup tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18867</link><project id="" key="" /><description>This is similar to #18849 but for unary operators.

Don't issue casts or boxing but just use `expected` if available to speed these up. I added tests to DefOptimizationTests and unary tests with an expected return type. 

I also split up the massive DefOperationTests, adding tests to each operator.
</description><key id="160260409">18867</key><summary>improve unary operators and cleanup tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T19:10:35Z</created><updated>2016-06-14T19:30:44Z</updated><resolved>2016-06-14T19:30:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-14T19:14:06Z" id="225986382">LGTM.  I love all the tests.  Thanks @rmuir!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AdditionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AndTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicExpressionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ComparisonTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOperationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOptimizationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DivisionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/MultiplicationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/OrTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/PromotionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/RemainderTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ScriptTestCase.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ShiftTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/StringTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/SubtractionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/UnaryTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/XorTests.java</file></files><comments><comment>Merge pull request #18867 from rmuir/more_indy_typing</comment></comments></commit></commits></item><item><title>Snapshot from replicas?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18866</link><project id="" key="" /><description>It would be awesome if we could configure snapshots to run from replicas. Of course, this might not even be technically feasible due to the way Lucene segments are synced during a snapshot. But, I thought I'd ask anyway. I imagine it being an option for the repository (`allow_replicas` or something) which defaults to false. I understand that by enabling such an option, there's a risk that the snapshot process will be snapshotting out of date data. In our case, that risk is acceptable.

**Background:** we run a pretty massive Elasticsearch cluster that is indexing 50k+ events per second (45mbp/s+), all day, every day. The problem is, we run frequent snapshots, and the burden this places on the primary shards can cause our indexing performance to suffer as a result. If the replicas could also be selected as candidates for the snapshot process then it would reduce this pressure.
</description><key id="160254115">18866</key><summary>Snapshot from replicas?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label></labels><created>2016-06-14T18:41:09Z</created><updated>2017-02-10T14:02:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T19:00:43Z" id="225982696">@imotov seems reasonable? Can't we just do this by default?
</comment><comment author="imotov" created="2016-06-14T19:23:42Z" id="225988867">@samcday, @clintongormley sorry, I must be missing something, but I don't really see how it would help. Replication in elasticsearch is document-based, so amount of work performed on primaries is exactly the same as amount of work performed on replicas (unless we run some very heavy update operations with some crazy scripts). We removed async operation option in 2.0, so an indexing operation is now going to always wait for the replica to finish the operation before returning response to the user. By moving "the burden" to a replica we are just going to move the place were we wait. Even with async replication, it doesn't make sense to me, because a slow replica would cause a pileup of in-flight indexing requests, which in turn can lead to running out of memory and cluster instability. Could you give a bit more details on how how this change would improve the overall indexing performance?
</comment><comment author="samcday" created="2016-06-14T20:47:55Z" id="226011289">@imotov here's our problem in more detail:

We have a large cluster holding 9416 primary shards. Each of these shards has one replica in a different AZ (we're deploying in AWS). We've sunk a _lot_ of engineering effort into a pretty elaborate higher-level allocation strategy that ensures the indices that are actively being written to have shards evenly distributed across 4 "indexing tiers" made up of 24 nodes in total. The problem is, short of actually disabling Elasticsearch shard allocation and doing it 100% ourselves, we have no way to control where the primary shards are allocated. We're already using [index.routing.allocation.total_shards_per_node](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html) but this does not stop a large number of primary shards from clustering on a small set of indexing nodes. If the snapshot process randomly selected between the primary or one of the available replicas for the primary,  (the same way queries do today) then it would more evenly spread the snapshotting load across our active nodes.

Before you say it, yes, I know our use-case is crazy. I have virtually every PM and dev at Elastic telling me we're crazy (maybe we are. If we're cognizant of our possible craziness, wouldn't that mean we're definitely not crazy though? hmmm.). Anyway, you could simplify this way down:

Let's say you have 2 indices each with 3 primary shards + 3 replica shards. Allocation looks like this:

```
      NODE A            NODE B
+---------------+ +---------------+
| index_1[0][p] | | index_1[0][r] |
| index_1[1][p] | | index_1[1][r] |
| index_1[2][r] | | index_1[2][p] |
| index_2[0][p] | | index_2[0][r] |
| index_2[1][p] | | index_2[1][r] |
| index_2[2][r] | | index_2[2][p] |
+---------------+ +---------------+
```

In this case, Node A has 4 primary shards, Node B only has 2. So, during a snapshot, NodeA currently has to do 2x more work than Node B.
</comment><comment author="imotov" created="2016-06-14T21:02:50Z" id="226015368">Now, it makes more sense. So, the problem is unbalanced allocation of primary shards, which leads to overloading nodes with disproportionate number of primary shards on operations that are executed only on primaries, which in this particular case the snapshot operation. 

I can clearly see how this can be an issue on two nodes cluster in your example, but I am having a bit of a hard time imagining this situation across 24 nodes unless you just did a rolling restart or using very long living indices. 
</comment><comment author="samcday" created="2016-06-14T21:06:23Z" id="226016309">You hit the nail on the head - we do a lot of rolling restarts to propagate config changes / ES minor version upgrades. For larger changes we do use shard allocation filtering to exclude nodes, wait for the node to be evacuated, and then kill it. In this case shards disperse more evenly, but we don't do this too often, seeing as each indexing node has 1tb+ of data to move around :(
</comment><comment author="samcday" created="2016-06-14T22:01:08Z" id="226029497">Whilst discussing this I've realised that maybe what I'm asking for is a band-aid. I've also raised #18874 as a feature request.

I'd still like to keep this one open to see if others have a similar need.
</comment><comment author="clintongormley" created="2016-06-15T08:49:48Z" id="226126604">&gt; Whilst discussing this I've realised that maybe what I'm asking for is a band-aid. I've also raised #18874 as a feature request.

Actually I don't think this is a band-aid at all.  Primaries and replicas SHOULD be doing similar amounts of work.  It should not matter that all primaries are on one node and all replicas on another.

We should work at reducing reliance on the primary to avoid these hot spots.
</comment><comment author="bleskes" created="2016-06-17T09:28:30Z" id="226723804">the main reason why we always snapshot from a primary is that we want to make sure we snapshot the same shard so we can do proper file based deltas and make snapshoting efficient (files across shards are different). At the time snapshot and restore was written, we only had one way to accomplish this - do it on the primary. These days we have allocation ids and we can explore making each snapshot sticky to the same allocation id (but not necessarily a primary) and distribute the load across copies. @samcday will that work for you?
</comment><comment author="samcday" created="2016-06-18T04:34:26Z" id="226921383">@bleskes yeah that sounds great. How would you decide which shard to use? Randomised selection?
</comment><comment author="bleskes" created="2016-06-18T06:06:12Z" id="226924451">That would be my first attempt, yeah.

On 18 jun. 2016 6:34 AM +0200, Samnotifications@github.com, wrote:

&gt; @bleskes(https://github.com/bleskes)yeah that sounds great. How would you decide which shard to use? Randomised selection?
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly,view it on GitHub(https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226921383), ormute the thread(https://github.com/notifications/unsubscribe/AA9bJxMe4-hj6CLzzP6p4FbMwq1SvPYAks5qM3VkgaJpZM4I1odk).
</comment><comment author="s1monw" created="2016-07-01T09:51:39Z" id="229906584">I think we can remove the discuss label @bleskes please put it back if you feel like it needs more discussion
</comment><comment author="bleskes" created="2016-07-01T10:48:29Z" id="229917430">@s1monw yeah, I should have removed the label. Thanks for cleaning up.
</comment><comment author="apatrida" created="2017-02-09T18:20:41Z" id="278727486">A note for @bleskes  ... we use a model where we add and remove nodes all the time based on load (i.e. app servers come online and receive shards for indexes they use heavily so they can mostly query locally which still having live data, and then scale up or down).  Therefore allocations will change a lot as these extra expansions come and go.   Does this break the following?

&gt; These days we have allocation ids and we can explore making each snapshot sticky to the same allocation id (but not necessarily a primary) and distribute the load across copies. </comment><comment author="bleskes" created="2017-02-10T08:17:36Z" id="278883917">@apatrida if we end up doing this (there are more questions, like for example, are we willing to suffer the costs of a bigger snapshot when a shard relocates - at that point it is copied from the primary, not the original shard) it will be indeed problematic for you as it seems you would like to make sure that certain copies are never snapshotted. Note thought that even today you are not safe- if a primary fails, one of the copies on the app servers may be elected as primary. There is no way to control that and I don't see that happening soon.</comment><comment author="apatrida" created="2017-02-10T09:44:37Z" id="278900478">So the consequence isn't really a breakage of snapshot/restore but more about how much data will need to be copied since maybe it will be less incremental for a given shard that happens to no longer have the same allocation IDs available between runs.  </comment><comment author="bleskes" created="2017-02-10T10:16:18Z" id="278907064">&gt; So the consequence isn't really a breakage of snapshot/restore but more about how much data will need to be copied since maybe it will be less incremental 

Correct.

&gt; that happens to no longer have the same allocation IDs available between runs.

Almost. When shard is moved around it gets a new allocation ID. You can think as an identifier of a disk folder - when the shard moves to a new folder on another node  it gets a new id. When it is moved back, the old data is "deleted" (not really, but conceptually yes, at least now - there are plans to change it) and the shard gets a new allocation id. The problem is more that the different shards have different merge cycles and thus different files. To make snapshot efficient (i.e., small deltas) we need to make it stick to a certain set of files. We currently do so by looking at the primary. If we move to aID we could stick to other shards, but when those shards are moved their file structure will change (it will be a copy of the primary)
</comment><comment author="apatrida" created="2017-02-10T14:02:13Z" id="278948611">thanks.  that helps.  I think our shard manager will be ok in these models,
when compared to the inner workings and one not breaking the other (much).

On Fri, Feb 10, 2017 at 7:16 AM, Boaz Leskes &lt;notifications@github.com&gt;
wrote:

&gt; So the consequence isn't really a breakage of snapshot/restore but more
&gt; about how much data will need to be copied since maybe it will be less
&gt; incremental
&gt;
&gt; Correct.
&gt;
&gt; that happens to no longer have the same allocation IDs available between
&gt; runs.
&gt;
&gt; Almost. When shard is moved around it gets a new allocation ID. You can
&gt; think as an identifier of a disk folder - when the shard moves to a new
&gt; folder on another node it gets a new id. When it is moved back, the old
&gt; data is "deleted" (not really, but conceptually yes, at least now - there
&gt; are plans to change it) and the shard gets a new allocation id. The problem
&gt; is more that the different shards have different merge cycles and thus
&gt; different files. To make snapshot efficient (i.e., small deltas) we need to
&gt; make it stick to a certain set of files. We currently do so by looking at
&gt; the primary. If we move to aID we could stick to other shards, but when
&gt; those shards are moved their file structure will change (it will be a copy
&gt; of the primary)
&gt;
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278907064&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AALIROQIlqIEklMNVeyuc5HqkemsniFwks5rbDkWgaJpZM4I1odk&gt;
&gt; .
&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test: Fix integ test extra config files to work with more than one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18865</link><project id="" key="" /><description>Groovy does some crazy capturing when using closures inside a loop. In
this case, it somehow decided the local loop variable would be
modified, and so each closure was getting a wrapped value that would be
updated on each loop iteration, until all the closures pointed at the
last value. This change fixes the loop to extract the object to be used by
the closures.
</description><key id="160244473">18865</key><summary>Test: Fix integ test extra config files to work with more than one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T17:57:25Z</created><updated>2016-06-14T17:59:26Z</updated><resolved>2016-06-14T17:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T17:58:41Z" id="225963707">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #18865 from rjernst/fix_extra_config_file</comment></comments></commit></commits></item><item><title>How to properly recognize long running jobs in the _tasks reply</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18864</link><project id="" key="" /><description>The new `_tasks` API misses an important feature, namely the ability to tell which task is what. For example if someone launches [a long reindexing job](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html) and later on tries to have an idea about the progress, there is no strong identification of the task. As a human you might make a guess, but with several similar reindex jobs run concurrently, a programmatic decision is impossible.

Therefore I propose that all API calls that may result in entries in the tasks list carry a supplementary parameter, say `task_tag`, that would be tied to the respective task (so that in the `_tasks` reply you may identify it). It's safer than directly specifying a task ID and has less code impact.
</description><key id="160242904">18864</key><summary>How to properly recognize long running jobs in the _tasks reply</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acarstoiu</reporter><labels><label>:Task Manager</label><label>discuss</label></labels><created>2016-06-14T17:50:21Z</created><updated>2016-06-17T12:31:51Z</updated><resolved>2016-06-17T09:29:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T18:02:40Z" id="225964944">In 5.0 `?wait_for_completion=false` on reindex will be less dangerous because the results will persist (merged that about 20 minutes ago) so I expect for long running operations you'll be able to use that.

Rather than a tag I'd prefer to just spit the request that user made as part of the task. I believe that is what the `description` field is for, though I'm fairly sure we aren't using it properly now.

I believe that _tasks come back with a start time so you could use that too....
</comment><comment author="clintongormley" created="2016-06-14T18:39:58Z" id="225976443">&gt; In 5.0 ?wait_for_completion=false on reindex will be less dangerous because the results will persist (merged that about 20 minutes ago) so I expect for long running operations you'll be able to use that.

+1

&gt; Rather than a tag I'd prefer to just spit the request that user made as part of the task. I believe that is what the description field is for, though I'm fairly sure we aren't using it properly now.

+1

If you put the task into the background with `wait_for_completion=false`, you get a task ID which you can use to track the task status.  I don't see the benefit of also specifying a `task_tag` if you're not intending the run the task in the background.
</comment><comment author="acarstoiu" created="2016-06-15T11:27:01Z" id="226160322">First of all, when using `wait_for_completion=false` I should get not one, but several task IDs as there are normally several nodes that perform a certain request. But that's useless now because I would simply lose the outcome :-1: 
Even in the upcoming _5.0_, not waiting for a request's outcome implies that someone will have to keep polling the `_tasks` API until the results arrive, which is not resource friendly. It's monitoring and result getting combined together, but I believe most people will be able to live with that.

Using the quoted request to identify a task is just guessing (one could have simultaneous identical requests), akin the use of the _action_ property.
</comment><comment author="clintongormley" created="2016-06-15T11:42:26Z" id="226163252">&gt; First of all, when using wait_for_completion=false I should get not one, but several task IDs as there are normally several nodes that perform a certain request. 

You should get the parent task ID.

&gt; But that's useless now because I would simply lose the outcome 👎 

In 2.x yes.  

&gt; Even in the upcoming 5.0, not waiting for a request's outcome implies that someone will have to keep polling the _tasks API until the results arrive,

The tasks will be there immediately.  Once the task is complete the outcome will be stored in the tasks index but will be accessible via the same tasks API.

&gt; which is not resource friendly.

How would a task tag be any different?
</comment><comment author="nik9000" created="2016-06-15T12:35:16Z" id="226173772">The task get API in 5.0 (GET /_tasks/taskid) supports
?wait_for_completion=true. So if the task is still running it'll wait for
it to finish. As of now it won't load the result from the task index after
it has finished waiting though if you run it again it will. I'll fix that
though.

I expect that to be fairly resource friendly. You can handle timeouts or a
proxy chopping the connection by retrying the GET.

I'm not 100% sure a user provided tag on the request is a bad thing. I'm
just sure that returning the actual request is a good thing we should do
too.

I like user tags because you can put a session or something in there and
then cancel all cancelable tasks for a session.
On Jun 15, 2016 7:42 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; First of all, when using wait_for_completion=false I should get not one,
&gt; but several task IDs as there are normally several nodes that perform a
&gt; certain request.
&gt; 
&gt; You should get the parent task ID.
&gt; 
&gt; But that's useless now because I would simply lose the outcome 👎
&gt; 
&gt; In 2.x yes.
&gt; 
&gt; Even in the upcoming 5.0, not waiting for a request's outcome implies that
&gt; someone will have to keep polling the _tasks API until the results arrive,
&gt; 
&gt; The tasks will be there immediately. Once the task is complete the outcome
&gt; will be stored in the tasks index but will be accessible via the same tasks
&gt; API.
&gt; 
&gt; which is not resource friendly.
&gt; 
&gt; How would a task tag be any different?
&gt; 
&gt; —
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18864#issuecomment-226163252,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/AANLoqDCMwh_ZGdC3IuXK0X85HG6WNoSks5qL-UngaJpZM4I1lZK
&gt; .
</comment><comment author="nik9000" created="2016-06-15T12:50:24Z" id="226177230">&gt; I like user tags because you can put a session or something in there and then cancel all cancelable tasks for a session.

Like there session is some construct outside of ES.
</comment><comment author="clintongormley" created="2016-06-17T09:29:02Z" id="226723910">Discussed it in fix it friday - we agree that the task ID is the preferred way to retrieve task statuses and see no immediate need for additional tags.  Will close this for now, but we can revisit if this proves popular.
</comment><comment author="acarstoiu" created="2016-06-17T11:50:36Z" id="226749809">Just to answer the question

&gt; &gt; which is not resource friendly.
&gt; 
&gt; How would a task tag be any different?

I have to repeat myself: monitoring and getting the result are _different_ things. Getting the result should be as quick as possible, while the sampling frequency in monitoring doesn't need to be high.

You see, a task tag is an identification decided by a requester willing to wait for the result. While monitoring can take place on another thread ;)
</comment><comment author="clintongormley" created="2016-06-17T12:31:51Z" id="226756894">So why not just use two threads with the task API and the task ID: one with wait_for_completion (for the result) and one without wait_for_completion (for monitoring)?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Specialize the search script API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18863</link><project id="" key="" /><description>Currently the scripting API allows to return arbitrary objects. This is an issue for search scripts because they might be called billions of times in a single search request and missing information about the return type makes things harder to optimize.

So we could look into specializing the search script API so that it would only be allowed to return a double. This would break a couple of things however:
- scripts would not be able to return multiple values per document anymore,
- scripts would not be able to return anything else than a double, so eg. building a string in a script and returning it would be impossible.

This is likely not an issue for the main use cases that we want to support with scripting, such as multiplying the score with the log of the value of some field. However I wolud not be surprised that there are some existing search scripts producing arrays of numbers or strings in the wild.
</description><key id="160216518">18863</key><summary>Specialize the search script API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2016-06-14T15:48:37Z</created><updated>2017-05-30T03:14:05Z</updated><resolved>2017-05-30T03:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-14T15:52:54Z" id="225926934">For us with painless, there are two main issues:
- forces boxing of return values
- means the "root" of our tree is untyped, this hurts us a lot.

Compare the QPS of expressions, which dodges these issues (https://benchmarks.elastic.co/index.html#search_qps_scripts). Look how much faster it is than painless. This is sad, its how much performance we are leaving on the table.

In general, search script and executable script need more separation. One of them gets called millions and billions of times and needs to be fast. The latter case is not really as much of a hotspot.
</comment><comment author="nik9000" created="2016-06-14T16:03:43Z" id="225930321">&gt; In general, search script and executable script need more separation. One of them gets called millions and billions of times and needs to be fast. The latter case is not really as much of a hotspot.

+1
</comment><comment author="uschindler" created="2016-06-14T16:35:12Z" id="225939713">+1

I'd suggest to have 2 possible scripts: One returning Object and one returning just double (unboxed). We can handle that easily in painless. We just add the correct casts and implement the main logic in 2 different methods.

We can also add stuff for covariant returns: So scripts have to implement 2 methods: one returning double and one returning Object. Scripts returning doubles can implement the double method directly and add a covariant bridge to box (returning Object). If a script cannot return a double, it throws ClassCastException in the double returning method and only implements the Object one.
</comment><comment author="nik9000" created="2016-06-14T16:39:11Z" id="225940816">Lots of non-search scripts don't need to return anything. Usually their job is to modify stuff in some `ctx` variable. I guess aggregations have scripts that want to return arbitrary stuff.
</comment><comment author="clintongormley" created="2016-06-14T18:33:52Z" id="225974602">&gt; In general, search script and executable script need more separation. One of them gets called millions and billions of times and needs to be fast. The latter case is not really as much of a hotspot.

Agreed.  I can think of two places where search scripts today need to return strings:
- Script Fields - These can return pretty much anything, and really they should be executable scripts instead of search scripts as they are only called once per hit.  The only problem here is that it would be a breaking change as users would need to use `ctx._source` instead of `_source`, but I think this is worth it.
- Term aggregations - I know users use this to eg combine multiple fields or to emit tags which represent some calculation

The last one is trickier.  With @uschindler's proposal, can we still keep the string return in term aggregations (and suffer the slower execution speed) and use the double version (pretty much?) everywhere else 
</comment><comment author="jpountz" created="2016-06-17T10:08:19Z" id="226732223">Discussed in FixitFriday: there are reservations about removing the ability to produce anything but doubles in all search scripts, so one idea would be to have the following APIs in `ScriptEngineService`:

``` java
    // already exists: used for updates, ingest or search scripts that may need something else than doubles
    Object compile(String scriptName, String scriptSource, Map&lt;String, String&gt; params);

    // new API: used by search scripts that need to produce doubles only like script_score or sorting by script
    // for the script engine, the default implementation could delegate to #compile
    Object compileDouble(String scriptName, String scriptSource, Map&lt;String, String&gt; params);
```

Then it seems like we should easily be able to pick the right one depending on the call site. @jdconrad @uschindler @rmuir would it make things easier to optimize for Painless?
</comment><comment author="uschindler" created="2016-06-17T11:42:35Z" id="226748494">`compileDouble` should of course return double :-) or did I misundserstood something?
</comment><comment author="rmuir" created="2016-06-17T14:13:44Z" id="226779823">I'm not happy about it. Today we have 2 apis, and both are slow. The proposal here would make one of them more complicated.

If we are going to have 2 apis, why not just have one of them be fast?
</comment><comment author="s1monw" created="2016-06-17T15:06:29Z" id="226794639">&gt; If we are going to have 2 apis, why not just have one of them be fast?

can you explain the API and it's consequence ie. does it mean we remove functionality? Does it mean we restrict everything to a double? I mean if we can have 1 fine but how?
</comment><comment author="rmuir" created="2016-06-17T15:15:30Z" id="226797196">No, i can't because these scripting apis are a disaster. Its literally impossible to tell what is going on outside of it. You've got slow shit using the "fast" apis and probably vice versa too.

Its willy nilly which one is used: ExecutableScript or SearchScript. And we don't even tell the script which one it needs to be at compile-time. Forget about return value, the slow hashmaps, etc: we can't even give a nice error if someone tries to use `_score` from an update script, because we don't know which one we are until runtime.

I'm going to exit the issue, I don't think i will be very helpful. 

I just repeat, look here: https://benchmarks.elastic.co/index.html#search_qps_scripts

Why is expressions 50% faster than painless? It generates the equivalent code. The difference is that expressions bypasses the scripting API. 

I find it ironic we don't seem to care about leaving 50% performance on the table, yet at the same time fork a joda-time class (causing immense engineering frustrations) so that a date operation will be 10% faster.
</comment><comment author="jdconrad" created="2016-06-17T20:07:01Z" id="226868922">I would also make other major changes to the scripting API.

1) I would prefer that the ScriptService only be responsible for compiling a script.  The CompiledScript that is handed back should be what actually executes the script.  It seems odd to me that I have to call back into the ScriptService with this object every time I want to execute a script.  (The CompiledScript can know what contexts it's allowed to run under.)
2) I would move caching to the ScriptEngines.  Let the engines decide how to cache instead of having one big blob of random scripts in the service.  This would also prevent the one-off for callbacks into the Groovy ScriptEngine since it already does some form of its own caching.
3) This one is probably unrealistic because of the way we store scripts and how we want to execute a single script against multiple indices, but it would be awesome if we could compile a script against an index so that we know the types of fields coming in.  One big benefit of Expressions is that it can assume all fields are doubles, and pretty much backdoors directly into Lucene to get the field data.  It would be nice if this was available to Painless as well.
</comment><comment author="s1monw" created="2016-06-18T10:47:42Z" id="226935318">@jdconrad you can do whatever you want there. I am all +1 to even deprecate the old API and build a new one that can be used only in search as a start. I am sure we are all on the same page and we want to make it to gain the 50% but I also think you guys know it best what needs to be done. Once the drama queen has calmed down and gets his willy nilly shit together I think everything is possible. Go ahead and make the changes.
</comment><comment author="rjernst" created="2017-05-30T03:14:05Z" id="304765081">I'm going to close this because it duplicates #20426, which is the concrete issue to satisfy this problem.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify FetchSubPhase registration and detach it from Guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18862</link><project id="" key="" /><description>this commit removes FetchSubPhrase registration by class to registration
by instance. No Guice binding needed anymore.
</description><key id="160214556">18862</key><summary>Simplify FetchSubPhase registration and detach it from Guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>:Plugins</label><label>:Search</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T15:40:44Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-15T07:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-14T16:48:02Z" id="225943333">The change to percolator highlighting should be reviewed by Martijn but otherwise LGTM.
</comment><comment author="martijnvg" created="2016-06-14T20:28:14Z" id="226005892">LGTM2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/Highlighters.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorHighlightSubFetchPhase.java</file><file>modules/percolator/src/main/java/org/elasticsearch/percolator/PercolatorPlugin.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorHighlightSubFetchPhaseTests.java</file><file>test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java</file></files><comments><comment>Simplify FetchSubPhase registration and detach it from Guice (#18862)</comment></comments></commit></commits></item><item><title>`deb` package from official repo does not restart the service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18861</link><project id="" key="" /><description>Hi,

I have this repository in Ubuntu 14.04: 'deb https://packages.elastic.co/elasticsearch/2.x/debian stable main'

I did `apt-get upgrade` and it updated Elasticsearch (I had like 2.1.1 and this command updated it to 2.3.0).

The problem is: although, the new Elasticsearch is on the disk, the service is not restarted. That is: when I update the Elasticsearch from the official repo, I have to restart it manually to really update it.

I find this to be a rather confusing and non-standard behaviour, I really think, the official packages should restart on package upgrade. Like nginx, Postgres, Jenkins or many others do.
</description><key id="160209445">18861</key><summary>`deb` package from official repo does not restart the service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chillum</reporter><labels /><created>2016-06-14T15:20:24Z</created><updated>2016-06-14T15:30:36Z</updated><resolved>2016-06-14T15:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-14T15:25:24Z" id="225918113">Sorry, we decided previously that we are not going to restart the service automatically. This is because we do not want shards to automatically start re-allocating during an upgrade without direct intervention by the user. However, you can use the flag [`RESTART_ON_UPGRADE`](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html#_linux) to enable this, if you so desire.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get XContent params from request in Nodes rest actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18860</link><project id="" key="" /><description>Closes #18794
</description><key id="160196796">18860</key><summary>Get XContent params from request in Nodes rest actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:REST</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T14:32:16Z</created><updated>2016-06-16T14:04:28Z</updated><resolved>2016-06-16T14:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T15:58:21Z" id="225928662">Fix looks good. Can you add a test for it? Something in the rest tests (`rest-api-spec/src/main/resources/rest-api-spec/test/`probably.

You can run with with `gradle distribution:integ-test-zip:check`.
</comment><comment author="alexshadow007" created="2016-06-15T15:25:43Z" id="226222871">@nik9000 done
</comment><comment author="nik9000" created="2016-06-16T13:57:36Z" id="226492750">Looks good. I'll test and merge!
</comment><comment author="nik9000" created="2016-06-16T14:04:28Z" id="226494757">&gt; Looks good. I'll test and merge!

And the tests passed. Thanks so much! Merged!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Register Highlighter instances instead of classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18859</link><project id="" key="" /><description>This change detaches highlighter registration from Guice. It's just a
small step into the right direction.
</description><key id="160190760">18859</key><summary>Register Highlighter instances instead of classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Highlighting</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T14:08:29Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-14T15:04:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-14T14:14:32Z" id="225894388">I think this should be fine. If you wanted to go further and refactor highlighters to work like queries then I think the next step is to replace Highlighters with ParseFieldRegistry and rework the parsing a bit. But that is a much more invasive change. This LGTM.
</comment><comment author="nik9000" created="2016-06-14T14:16:25Z" id="225894933">BTW, the [experimental highlighter](https://github.com/wikimedia/search-highlighter/blob/master/experimental-highlighter-elasticsearch-plugin/src/main/java/org/wikimedia/highlighter/experimental/elasticsearch/ExperimentalHighlighter.java) project I started way back when has a no-arg constructor so it'll work with this when @nomoa needs to upgrade to 5.0.
</comment><comment author="nomoa" created="2016-06-14T14:43:11Z" id="225904154">Hey @nik9000 thanks for caring about that! :)
If I understood correctly I'll just have to switch the bindings from `to(ExperimentalHighlighter.class)`  to `toInstance(new ExperimentalHighlighter())`
</comment><comment author="s1monw" created="2016-06-14T14:45:48Z" id="225904999">@nomoa you should use the register method on the SearchModule instead
</comment><comment author="nik9000" created="2016-06-14T14:47:10Z" id="225905478">&gt; @nomoa you should use the register method on the SearchModule instead

Like

```
public class ExperimentalHighlighterPlugin extends Plugin {
  void onModule(SearchModule module) {
    module.register("experimental", new ExperimentalHighlighter());
  }
}
```
</comment><comment author="nomoa" created="2016-06-14T14:48:58Z" id="225906064">Awesome, this is way cleaner like that and I'll be able to remove one class, thanks!
</comment><comment author="PKFresher" created="2016-06-28T03:11:34Z" id="228936882">@nik9000  hello , our ES version 1.6 ,  how can SearchModule  register   my own FetchSubPhaseImpl  class
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/Highlighters.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>core/src/main/java/org/elasticsearch/search/highlight/Highlighters.java</file><file>core/src/test/java/org/elasticsearch/search/SearchModuleTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterPlugin.java</file></files><comments><comment>Register Highlighter instances instead of classes (#18859)</comment></comments></commit></commits></item><item><title>Add support for the find operator (=~) and the match operator (==~)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18858</link><project id="" key="" /><description>Adds support for the find operator (`=~`) and the match operator (`==~`) to painless's regexes. Also whitelists most of the Pattern class and documents regex support in painless.

The find operator (`=~`) constructs a matcher. The operator is called "find" because matchers coerce to booleans by calling `Matcher.find`. So this is valid:

```
if (ctx._source.last =~ /b/)
```

As is this:

```
Matcher m = ctx._source.last =~ /[aeiou]/
```

The match operator (`==~`) returns a boolean, true if the lhs `Matcher.match`es the rhs. So this is valid:

```
if (ctx._source.last ==~ /^[^aeiou].*[aeiou]$/)
```

All three of these things (find operator, match operator, Matcher -&gt; boolean coercion) are "borrowed" from groovy. Groovy has lots more useful stuff around regexes but this is a good start.
</description><key id="160186809">18858</key><summary>Add support for the find operator (=~) and the match operator (==~)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T13:52:02Z</created><updated>2016-06-17T14:21:56Z</updated><resolved>2016-06-16T12:43:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-14T15:09:48Z" id="225912977">@nik9000 This looks pretty good to me.  One more serious comment about functionality and one minor follow up question.
</comment><comment author="jdconrad" created="2016-06-14T15:33:37Z" id="225920744">@nik9000 +1  Thanks for the follow up answers.
</comment><comment author="uschindler" created="2016-06-14T15:55:28Z" id="225927737">Syntax looks file to me.

Question about DEFs: If you have assigned a pattern to def, the boolean coercion and operators won't work, or does it?

I am a bit afraid about the new Sort.MATCHER type. Unless this breaks hardcoded checks I am fine with it, but we have a lot of code like `type.sort == Sort.DEF` or similar. We should check those if statements that they don't forget to also handle MATCHER when they expect an object.
</comment><comment author="nik9000" created="2016-06-14T16:02:21Z" id="225929891">&gt; Question about DEFs: If you have assigned a pattern to def, the boolean coercion and operators won't work, or does it?

Probably not at this point but it should.... I'll have a look.

&gt; I am a bit afraid about the new Sort.MATCHER type. Unless this breaks hardcoded checks I am fine with it, but we have a lot of code like type.sort == Sort.DEF or similar. We should check those if statements that they don't forget to also handle MATCHER when they expect an object.

I don't understand what you mean - is the problem with Sort.MATCHER or Sort.DEF missing the support for the matcher case? Or both? I'm kind of fumbling around in the dark on stuff like Sort.MATCHER - I can tell that it is the normal way to add support for the casing but I'm not sure if there are consequences I hadn't intended.
</comment><comment author="rmuir" created="2016-06-14T16:07:01Z" id="225931339">I think the coersion to boolean will be an issue. I know this is a groovy thing, but we have to be careful here. We can't just easily treat Matcher as a "boxed type" for boolean without a lot of (possibly slow) things. Currently on the dynamic side, we let the JVM take care of all these conversions... I do really think this is the way we should do it :)

Is it possible at first to have one operator that returns boolean and the other that returns Matcher just to keep things simple?
</comment><comment author="nik9000" created="2016-06-14T16:09:09Z" id="225931975">&gt; Is it possible at first to have one operator that returns boolean and the other that returns Matcher just to keep things simple?

Yeah, I can back out the coercion stuff.
</comment><comment author="uschindler" created="2016-06-14T16:30:19Z" id="225938324">&gt; Yeah, I can back out the coercion stuff.

Then we don't need Sort.MATCHER anymore!
</comment><comment author="nik9000" created="2016-06-14T16:33:06Z" id="225939098">&gt; Then we don't need Sort.MATCHER anymore!

I think the idea is that we can get the operators in easy but will have to think more about the coercion. I still want it, or, if we can't have it then I should rework the operators some. Either way is fine with me and this is a good incremental improvement without the coercion.
</comment><comment author="uschindler" created="2016-06-14T16:40:11Z" id="225941073">&gt; I can tell that it is the normal way to add support for the casing but I'm not sure if there are consequences I hadn't intended.

It was just a warning! I don't like how Sort is used throughout the code. At some places it is used to check a type which is wrong. We just have to check all the if statements and verify that we don't miss correct checks. E.g. if we have a check somewhere `sort == Sort.Object` this won't work anymore for Matchers, although they are Objects. That was my warning! I just hate the usage of Sort. I'd like to have marker interfaces on the types and do instanceof checks instead of a Sort enum.
</comment><comment author="nik9000" created="2016-06-14T16:42:54Z" id="225941879">&gt; That was my warning!

Now I understand! Thanks!
</comment><comment author="rmuir" created="2016-06-14T17:16:52Z" id="225951604">&gt; I think the idea is that we can get the operators in easy but will have to think more about the coercion. I still want it, or, if we can't have it then I should rework the operators some. Either way is fine with me and this is a good incremental improvement without the coercion.

If i could see it working in a way right now that didn't involve a runtime check for every conditional, i would try to work something up. But I am currently afraid that is the only solution for the dynamic case, if we want to treat Matcher as boolean (and we are very sparse on type information, so thats the general case right now: lots of `def`). Right now we avoid this, we only use MethodHandles asType and it seems fast :) 

I do really think we should try to avoid this coersion feature in general, i think its an anti-feature of groovy. I know things like java.util.Maps can be treated as a boolean there, but that's very ambiguous.
</comment><comment author="nik9000" created="2016-06-14T17:22:00Z" id="225953026">&gt; I do really think we should try to avoid this coersion feature in general, i think its an anti-feature of groovy. I know things like java.util.Maps can be treated as a boolean there, but that's very ambiguous.

What if we have `=~` just return a boolean as well. That way it can still be the "find" operator. Then if we really want to get more groovy we can work up the coercion stuff later. And if we don't we can just use `/foo/.matcher("bar")` when we want the actual matcher?
</comment><comment author="rjernst" created="2016-06-14T17:23:08Z" id="225953358">I think `=~` and `==~` should both just return `boolean`. If someone wants to get a matcher, they can call Pattern functions. But the simple case of having a condition on "find this pattern in a string" should work:

```
if (doc["myfield] =~ /hello/) {
```
</comment><comment author="nik9000" created="2016-06-14T18:29:31Z" id="225973235">&gt; I think =~ and ==~ should both just return boolean.

I did it!
</comment><comment author="clintongormley" created="2016-06-14T18:56:03Z" id="225981329">I haven't quite followed the conversation above so apologies in advance, but I want one of two things from a regex:
- does it match (boolean)
- give me the captures (array)

@nik9000 tells me that `find` finds the pattern inside a string and `match` ensures that the whole string matches.  The second one seems redundant to me; if I want the whole string to match I'll just use `$ ^` anchors.

I'd much rather have one operator which returns a boolean and another operator which returns an array of captures.
</comment><comment author="nik9000" created="2016-06-14T19:04:17Z" id="225983752">&gt; returns an array of captures

It'd be a java `Matcher` which has methods like `.group` for the whole match and `.group(2)` for second group, etc.

&gt; The second one seems redundant to me; if I want the whole string to match I'll just use $ ^ anchors.

They map to two different methods in java and it _looks like_ `matches` is going to be faster than `find` with anchors. OTOH we could actually measure it and optimize it on our side if it comes out to be faster. As in we could detect that there are anchors on both sides of the pattern and the pattern isn't in multiline mode then we could strip them and use matches....

Just so we're clear, if we get what we want the only syntax that gets you matches is:

```
Matcher m = /foo/i.matcher('cat');
```

or

```
Pattern p = /foo/i;
Matcher m = p.matcher('cat');
```
</comment><comment author="nik9000" created="2016-06-14T19:04:46Z" id="225983884">&gt; what we want

I guess I should rephrase that to "this PR as is and an as yet unwritten flags PR".
</comment><comment author="rmuir" created="2016-06-14T19:25:51Z" id="225989424">&gt; Just so we're clear, if we get what we want the only syntax that gets you matches is:

This isn't necessarily the only syntax. Its the only thing we have today.

But we could work on things like method augmentation.  Imagine giving CharSequence a few of the ones it has in groovy (http://docs.groovy-lang.org/latest/html/groovy-jdk/java/lang/CharSequence.html#findAll%28java.util.regex.Pattern%29)

so e.g. `"foobar".findAll(/something/)` would return a list. They have others.

I am not saying we should do this, it needs a lot of thought, .e.g i dont want to maintain tons of shitty string functions either, at the sametime distrustful of groovy code that e.g. adds an `execute` method to List and other horrible things they do. Nor do I really want to do our own thing and document any kind of "api", maybe just a key subset (e.g. regex and similar features on CharSequence)...

Just something to think about, it avoids the downsides of performance tradeoffs and confusion due to operator explosion.
</comment><comment author="uschindler" created="2016-06-14T21:12:22Z" id="226017838">&gt; I think =~ and ==~ should both just return boolean.

+1 for that and thanks for cleaning up the PR. Looks much better to me.

I agree with @nik9000 and @rmuir: Users that really want to use the Matcher object should simply call `.matcher()` on the pattern. If they need a matcher they will also want to get the groups and so on. In that case a simple syntax using `~=` is contraprocuctive, because them people tend to do horrible shit like `('a'=~/foo/).group(n)` over and over for every group. With having to call matcher they see that there is actually some object returned that can be stored and then group() called on the local stored matcher.
</comment><comment author="uschindler" created="2016-06-14T21:46:55Z" id="226026159">If we need named groups we can add an alias with different name. Like namedGroup(String)
</comment><comment author="nik9000" created="2016-06-14T21:53:24Z" id="226027681">&gt; If we need named groups we can add an alias with different name. Like namedGroup(String)

I'll have a look and see if that is already a thing then I'll set it up.
</comment><comment author="uschindler" created="2016-06-14T22:27:41Z" id="226035294">See getLength() as alias for size() on lists. We need this to support length property 
</comment><comment author="nik9000" created="2016-06-14T22:50:24Z" id="226039666">&gt; See getLength() as alias for size() on lists. We need this to support length property

Got it!
</comment><comment author="nik9000" created="2016-06-15T17:35:27Z" id="226261610">Where've we landed on this? Is this the way we want to go or not? Should I change it, merge it, wait for more review?
</comment><comment author="nik9000" created="2016-06-15T19:18:51Z" id="226291674">&gt; Where've we landed on this? Is this the way we want to go or not? Should I change it, merge it, wait for more review?

I only ask because hacking on painless is fun and I'm excited to go and do flags and don't want to deal with the merge conflicts I'd get if I did it now. Basically I'm excited and lazy at the same time.
</comment><comment author="jdconrad" created="2016-06-15T19:21:45Z" id="226292426">I'm +1 to these changes.  @rmuir or @uschindler did either of you have any other concerns?
</comment><comment author="jdconrad" created="2016-06-16T01:00:43Z" id="226362161">@nik9000 Please go ahead and merge this.  I don't want you to have to deal with any serious conflicts, and if something comes up we can make changes moving forward.  Thanks again for the work here!
</comment><comment author="uschindler" created="2016-06-16T10:09:58Z" id="226443949">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add support to order buckets by `top_hits` aggregation's max score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18857</link><project id="" key="" /><description /><key id="160180953">18857</key><summary>add support to order buckets by `top_hits` aggregation's max score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label></labels><created>2016-06-14T13:27:14Z</created><updated>2017-04-10T16:10:56Z</updated><resolved>2017-04-10T16:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-14T13:50:49Z" id="225886885">I don't like the fact that we need to add a max aggregation to sort the top terms but it can save a lot of memory if the breadth_first mode is used. The change that you propose does not play nicely with the breadth first mode. The good thing about the max aggregation (vs the top hits) is that the memory footprint to compute the max score is bounded. When the breadth_first mode is used we compute the terms aggregation and the max scores in one pass and the top hits are computed in a second pass where only the top terms are replayed. If the terms aggregation needs the top hits then we cannot delay anything and we need to compute the tophits on all the documents that match the query. 
</comment><comment author="jpountz" created="2016-06-14T15:16:18Z" id="225915144">I agree the fact that top hits cannot use breadth_first anymore is a concern.
</comment><comment author="martijnvg" created="2016-06-14T21:15:16Z" id="226018584">Wait, my understanding is that the `breadth_first` optimization can't be used when score is required by an agg ( top_hits sort by score by default). So my understanding was incorrect.

I really don't like that we have to use a `max` with a script in order to make the field collapse use case work. This PR looked like an easy way out... I wonder if top_hits can be a bit smarter and kind of do what `max` agg does for selecting the best buckets and then for those buckets compute TopDocs. (maybe keep track of most relevant ScoreDoc/FieldDoc, and then when building shard level response computing TopDocs only for the top buckets)
</comment><comment author="jimczi" created="2016-06-15T07:19:42Z" id="226107813">@martijnvg the support for `breadth_first` aggs that require scoring has been added recently:
https://github.com/elastic/elasticsearch/pull/18127

&gt; I wonder if top_hits can be a bit smarter and kind of do what max agg does for selecting the best buckets and then for those buckets compute TopDocs. (maybe keep track of most relevant ScoreDoc/FieldDoc, and then when building shard level response computing TopDocs only for the top buckets)

This sounds like `breadth_first` to me ;) I wonder if we could just add a sort mode in the terms agg directly. As @jpountz says in this PR: https://github.com/elastic/elasticsearch/pull/18112 it has an impact on the API/impls but IMO it would simplify the field_collapsing use case.
</comment><comment author="jpountz" created="2016-06-15T07:31:40Z" id="226110152">I am wondering whether we could find a way for the `top_hits` aggregation and the parent terms aggregation to communicate so that the `top_hits` could know that the parent terms agg both sorts by it and runs with breadth-first so that it would only collect the max scores on the first pass.

Another idea would be to add a dedicated aggregation for the `top_hits` under `terms` sorted by score use-case. It could add the max aggregation under the hood, etc. but keep the request and response clean?
</comment><comment author="martijnvg" created="2016-06-15T09:04:01Z" id="226129959">&gt; the support for breadth_first aggs that require scoring has been added recently:
&gt; #18127

That is great @jimferenczi :) I missed that.

&gt; Another idea would be to add a dedicated aggregation for the top_hits under terms sorted by score use-case. It could add the max aggregation under the hood, etc. but keep the request and response clean?

I like this idea. I'll try this out.
</comment><comment author="elasticmachine" created="2017-02-23T18:14:47Z" id="282074275">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="dakrone" created="2017-04-07T23:14:58Z" id="292673381">@martijnvg ping, is this still applicable?</comment><comment author="martijnvg" created="2017-04-10T16:10:56Z" id="292998161">@dakrone No, I never got back to this. Closing and will open a new pr when I get back to this.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mustache: Render Map as JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18856</link><project id="" key="" /><description>~~This commit changes Mustache script engine so that every map is rendered as a JSON string (instead of default Map.toString())~~

This pull request adds two util functions to the Mustache templating engine:
- {{#toJson}}my_map{{/toJson}} to render a Map parameter as a JSON string
- {{#join}}my_iterable{{/join}} to render any iterable (including arrays) as a comma separated list of values like `1, 2, 3`. It's also possible de change the default delimiter (comma) to something else.

closes #18970
</description><key id="160178578">18856</key><summary>Mustache: Render Map as JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-14T13:16:17Z</created><updated>2016-06-29T07:59:08Z</updated><resolved>2016-06-29T07:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-06-17T15:05:14Z" id="226794268">I updated the code to make it work with the `{{#toJson}}ctx.payload.someMap{{/toJson}}` syntax. I also added a `{{#join}}ctx.payload.someIterable{{/join}}` to render things like comma separated values.

It would be great if you can review and test it.

@rjernst Thanks for your review! The pull request was marked as work in progress and I changed the whole implementation after discussing with @spinscale. I took your comment into consideration, I'd be glad if you find the time to review again.
</comment><comment author="rjernst" created="2016-06-18T17:06:22Z" id="226953353">I have no opinion on the technical side of this change. I'm happy this allows rendering json, but I also think the syntax looks funky based on other templating languages I have used in the past. I would have expected something simpler like `{{ctx.payload.someMap.asJson}}` or `{{asJson(ctx.payload.someMap)}}`
</comment><comment author="spinscale" created="2016-06-20T06:53:49Z" id="227063334">I had a quick look but did not find any possibility for having arguments to configure the string joiner char instead of hardcoding it as a comma, that would be nice.

Apart from that there needs to be some documentation in the search template docs for those functions
</comment><comment author="tlrx" created="2016-06-22T15:04:27Z" id="227773258">Thanks @spinscale for the review. I updated the code, changing few things around and added the possibility to set a custom delimiter in the `join` function. Please let me know what you think.

&gt; I have no opinion on the technical side of this change. I'm happy this allows rendering json, but I also think the syntax looks funky based on other templating languages I have used in the past. I would have expected something simpler like {{ctx.payload.someMap.asJson}} or {{asJson(ctx.payload.someMap)}}

@rjernst Yes, I don't have a strong feeling here. I agreed with you when I discussed with @spinscale about implementing a `toJSON()` (you called it `asJson`) method on arrays and maps. 

But the Mustache specs also propose wrapped functions section with `{{#wrapped}}...{{/wrapped}}` so I think having `{{#toJson}}...{{/toJson}}` or `{{#join}}...{{/join}}` syntax is OK. It also has the side effect to allow things like `{{#join}}{{#toJson}}terms{{/toJson}}{{/join}}` which would be more difficult to express using the other syntax.
</comment><comment author="spinscale" created="2016-06-28T08:43:49Z" id="228988591">left two minor comments. I like it, very helpful for mustache users!

LGTM
</comment><comment author="tlrx" created="2016-06-29T07:59:08Z" id="229284281">Thanks @spinscale 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove dead code and dead parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18855</link><project id="" key="" /><description>title says it all
</description><key id="160174404">18855</key><summary>Remove dead code and dead parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2016-06-14T12:56:22Z</created><updated>2016-06-14T13:25:53Z</updated><resolved>2016-06-14T13:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-14T12:58:47Z" id="225873002">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/UidTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Remove dead code and dead parameters (#18855)</comment></comments></commit></commits></item><item><title>Remove size 0 options in aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18854</link><project id="" key="" /><description>This removes the ability to set `size: 0` in the `terms`, `significant_terms` and `geohash_grid` aggregations for the reasons described in https://github.com/elastic/elasticsearch/issues/18838

Closes #18838
</description><key id="160162713">18854</key><summary>Remove size 0 options in aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>breaking</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T11:55:52Z</created><updated>2016-09-19T12:29:35Z</updated><resolved>2016-06-14T14:32:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-14T11:59:22Z" id="225859707">@jpountz do you have time to review this?
</comment><comment author="jimczi" created="2016-06-14T12:30:34Z" id="225866246">Left one minor comment, LGTM otherwise
</comment><comment author="colings86" created="2016-06-14T12:40:14Z" id="225868457">@jimferenczi thanks for the review. I pushed a commit to address your comment. Waiting for tests to pass but will push to master as soon as they do
</comment><comment author="jpountz" created="2016-06-14T12:55:13Z" id="225872057">LGTM2
</comment><comment author="colings86" created="2016-06-14T14:33:51Z" id="225900649">Deprecation logging added to 2.x in https://github.com/elastic/elasticsearch/commit/ca416f26758c406e948e70703b523423771cf146
Deprecation logging added to 2.4 in https://github.com/elastic/elasticsearch/commit/90d7c685122ae505b0c6f85f609f6051ce207968
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoGridAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/search/aggregations/bucket/AbstractTermsTestCase.java</file></files><comments><comment>#18854 Remove size 0 options in aggregations</comment></comments></commit></commits></item><item><title>How to set "index": "not_analyzed" While push data from Mysql using mysql-replicator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18853</link><project id="" key="" /><description>I post my question in elastic search forums https://discuss.elastic.co/t/how-to-set-index-not-analyzed-while-push-data-from-mysql-using-mysql-replicator/52718, This is My Problem, Suggest me  How to do this?. 
</description><key id="160145841">18853</key><summary>How to set "index": "not_analyzed" While push data from Mysql using mysql-replicator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Git-Rajkumar</reporter><labels /><created>2016-06-14T10:18:11Z</created><updated>2016-06-14T10:54:24Z</updated><resolved>2016-06-14T10:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-14T10:54:24Z" id="225847121">discuss is the best place for questions. Let's keep the question there .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updated from parameter description.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18852</link><project id="" key="" /><description>Not sure that my description better but origin description looks very weird,
and i try to make emphasize to offset...

Btw: here i found link to your file CONTRIBUTING.md, but link looks like:
`https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md`
but:

```
curl https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md
{"error":"Not Found"}
```
</description><key id="160135289">18852</key><summary>Updated from parameter description.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cn007b</reporter><labels><label>docs</label></labels><created>2016-06-14T09:25:10Z</created><updated>2016-06-14T13:22:49Z</updated><resolved>2016-06-14T12:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T12:34:33Z" id="225867179">thanks @cn007b - merged, and i fixed the link to contributing too
</comment><comment author="cn007b" created="2016-06-14T13:22:49Z" id="225879001">cool! thx u!)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Updated from parameter description. (#18852)</comment></comments></commit></commits></item><item><title>Fixing typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18851</link><project id="" key="" /><description /><key id="160135126">18851</key><summary>Fixing typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>docs</label></labels><created>2016-06-14T09:24:22Z</created><updated>2016-06-14T12:22:25Z</updated><resolved>2016-06-14T12:22:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T12:22:25Z" id="225864401">thanks @synhershko - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fixing typos (#18851)</comment></comments></commit></commits></item><item><title>Feature Request : Get Nested sub elements in a document in a paginated fashion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18850</link><project id="" key="" /><description>Is it possible to get the nested sub elements in a document in a paginated fashion. Example we have a document as below and we would know the document id

{
"_index": "index1",
"_type": "type1",
"_id": "81cabf3c-51f5-442a-b78d-4324c4344f51",
"_version": 1,
"found": true,
"_source": {
"chiild1" : "child1 value"
"names": [
{
"name": "John"
},
{
"name": "Walter"
},
{
"name": "Justin"
},
{
"name": "Cramer"
}
]
}
}

Can we run a query like below (It does not work but are there other options ?)

GET index1/type1/81cabf3c-51f5-442a-b78d-4324c4344f51?sourceinclude=names.name&amp;start=0&amp;size=2

1.

{
"_index": "index1",
"_type": "type1",
"_id": "81cabf3c-51f5-442a-b78d-4324c4344f51",
"_version": 1,
"found": true,
"_source": {
"names": [
{
"name": "John"
},
{
"name": "Walter"
}
]
}
}

2.

GET index1/type1/81cabf3c-51f5-442a-b78d-4324c4344f51?sourceinclude=names.name&amp;start=2&amp;size=2

{
"_index": "index1",
"_type": "type1",
"_id": "81cabf3c-51f5-442a-b78d-4324c4344f51",
"_version": 1,
"found": true,
"_source": {
"names": [
{
"name": "Justin"
},
{
"name": "Cramer"
}
]
}
}
</description><key id="160120675">18850</key><summary>Feature Request : Get Nested sub elements in a document in a paginated fashion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmishra002</reporter><labels /><created>2016-06-14T08:06:24Z</created><updated>2016-06-14T11:56:43Z</updated><resolved>2016-06-14T11:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T11:56:43Z" id="225859144">Hi @pmishra002 

You can do this with inner hits:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested"
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": [
    {
      "name": "one",
      "order": 1
    },
    {
      "name": "two",
      "order": 2
    },
    {
      "name": "three",
      "order": 3
    },
    {
      "name": "four",
      "order": 4
    },
    {
      "name": "five",
      "order": 5
    },
    {
      "name": "six",
      "order": 6
    },
    {
      "name": "seven",
      "order": 7
    },
    {
      "name": "eight",
      "order": 8
    },
    {
      "name": "nine",
      "order": 9
    }
  ]
}

GET t/_search?_source=false
{
  "query": {
    "nested": {
      "path": "foo",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "sort": ["foo.order"],
        "from": 3
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove casts and boxing for dynamic math</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18849</link><project id="" key="" /><description>#18847 gave us the ability to optimize this stuff when we know types, but we don't use it yet.

This patch removes the manual boxing/promotion/conversion and just hands over what we have for the binary math and comparison operators. For example in the script in the nightly benchmark:

```
(Math.log(Math.abs(doc['population'].value)) + doc['elevation'].value * doc['latitude'].value)/_score
```

We remove two calls to Double.valueOf: one because we know the `_score` is a double argument, we don't need to box it, and another because we know the math function returns a double.

Instead of:

```
    ALOAD 2
    INVOKEVIRTUAL org/apache/lucene/search/Scorer.score ()F
    F2D
    DSTORE 5
    ALOAD 3
    LDC "population"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)D [...]
    INVOKESTATIC java/lang/Math.abs (D)D
    INVOKESTATIC java/lang/Math.log (D)D
    INVOKESTATIC java/lang/Double.valueOf (D)Ljava/lang/Double;
    ALOAD 3
    LDC "elevation"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)Ljava/lang/Object; [...]
    ALOAD 3
    LDC "latitude"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)Ljava/lang/Object; [...]
    INVOKEDYNAMIC mul(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; [...]
    INVOKEDYNAMIC add(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; [...]
    DLOAD 5
    INVOKESTATIC java/lang/Double.valueOf (D)Ljava/lang/Double;
    INVOKEDYNAMIC div(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; [...]
    ARETURN
```

We do:

```
    ALOAD 2
    INVOKEVIRTUAL org/apache/lucene/search/Scorer.score ()F
    F2D
    DSTORE 5
    ALOAD 3
    LDC "population"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)D [ ... ]
    INVOKESTATIC java/lang/Math.abs (D)D
    INVOKESTATIC java/lang/Math.log (D)D
    ALOAD 3
    LDC "elevation"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)Ljava/lang/Object; [ ... ]
    ALOAD 3
    LDC "latitude"
    INVOKEINTERFACE java/util/Map.get (Ljava/lang/Object;)Ljava/lang/Object;
    INVOKEDYNAMIC value(Ljava/lang/Object;)Ljava/lang/Object; [ ... ]
    INVOKEDYNAMIC mul(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; [ ... ]
    INVOKEDYNAMIC add(DLjava/lang/Object;)Ljava/lang/Object; [ ... ]
    DLOAD 5
    INVOKEDYNAMIC div(Ljava/lang/Object;D)Ljava/lang/Object; [ ... ]
    ARETURN
```

I added lots of tests to DefOptimizationTests for parameter and return values, and also correctness tests to DefOperationsTests for when we have partial types. I also found and fixed some bugs along the way (e.g. EComp did not work correctly when only one operand was `def`, it only looked at the RHS).
</description><key id="160097357">18849</key><summary>Remove casts and boxing for dynamic math</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-14T04:56:53Z</created><updated>2016-06-14T17:01:35Z</updated><resolved>2016-06-14T17:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-14T05:04:36Z" id="225780773">With this patch, i see only small differences in that nightly benchmark between static and dynamic now, typically something like this:

```
SEARCH painless_static (median): 0.438902 sec
SEARCH painless_dynamic (median): 0.459875 sec
```
</comment><comment author="uschindler" created="2016-06-14T08:25:13Z" id="225812924">&gt; With this patch, i see only small differences in that nightly benchmark between static and dynamic now

How was it before?
</comment><comment author="uschindler" created="2016-06-14T08:27:33Z" id="225813445">In general I have to closer look into the code!

I know, we cannot use the IDefLink class here, as the operands are SExpression not ALink, so we need a lot of if then else. But the current code just looking for Sort.DEF is fine (as simple). The IDefLink is more needed for the inverse case (links that make an expression at end).

Maybe we find a better solution for the def rewriting, there is room for improvement. As a fisrt step this looks good!
</comment><comment author="rmuir" created="2016-06-14T11:33:34Z" id="225854637">&gt; With this patch, i see only small differences in that nightly benchmark between static and dynamic now
&gt; 
&gt; How was it before?

Before typically like a 15% difference.

&gt; In general I have to closer look into the code!
&gt; 
&gt; I know, we cannot use the IDefLink class here, as the operands are SExpression not ALink, so we need a lot of if then else. But the current code just looking for Sort.DEF is fine (as simple). The IDefLink is more needed for the inverse case (links that make an expression at end).

These E-nodes are different than that case. In this case we just want to not generate the casts and boxing, and instead take the types and expected return values as-is and pass to invokedynamic, thats what the change does. 

If we want to reduce if-then-else (this is pre-existing, from the static case, i am afraid to heavily refactor it here), then the easiest way to do that, would be to refactor nodes to have the same behavior. For example shifts have different promotion than additions, but they are all bundled in one EBinary. 

&gt; Maybe we find a better solution for the def rewriting, there is room for improvement. As a fisrt step this looks good!

I don't really see it as rewriting the node, as it doesnt do that. It just inhibits the explicit casts and boxing. For these cases it can be done on return value and arguments without anything heavy.

Some cases are still to be improved. For example compound assignments in EChain. But see my comment, i think thats too scary to optimize right now as i see bugs looking at it, we need tons of tests for those compound assignments first...
</comment><comment author="jdconrad" created="2016-06-14T16:43:51Z" id="225942128">I'm +1 on these changes.  Great use of the extra type information.  
</comment><comment author="uschindler" created="2016-06-14T16:47:41Z" id="225943215">+1 also from my side!
</comment><comment author="rmuir" created="2016-06-14T17:01:35Z" id="225947303">I'm gonna followup by splitting up that massive DefOperations test too. We should have it for each operator. Some stuff is still missing like compound assignment tests and there are bugs in those for def!!!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOperationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOptimizationTests.java</file></files><comments><comment>Merge pull request #18849 from rmuir/give_indy_ops_types</comment></comments></commit></commits></item><item><title>JBoss Module doc needs updating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18848</link><project id="" key="" /><description>I can probably provide some edits, once I get it working.

https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/_deploying_in_jboss_eap6_module.html

Needs to be updated for 2.3.3.  The page shows 2.0 even on the 2.3.3 release.  Also considering the new structure, probably needs to reflect:
- Jackson upgrade (Wildfly ships w/ 2.5.4 which is not compatible)
- New dependencies on modules (since ES is no longer shaded)
- Updated lucene JARs.
</description><key id="160081503">18848</key><summary>JBoss Module doc needs updating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels><label>:Java API</label><label>adoptme</label><label>docs</label></labels><created>2016-06-14T01:57:16Z</created><updated>2016-06-14T11:30:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T11:27:17Z" id="225853403">@johnament a PR would be greatly appreciated!
</comment><comment author="clintongormley" created="2016-06-14T11:30:25Z" id="225854028">btw, you can define variables in the index.asciidoc file, like:

```
:major-version: 2
```

To use that variable inside a code block, you need something like this:

```
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
echo "deb https://packages.elastic.co/elasticsearch/{major-version}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{major-version}.list
--------------------------------------------------
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor def math</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18847</link><project id="" key="" /><description>This refactors operators on dynamic types to use invokedynamic (and fixes several bugs/consistency issues/tests).

The idea here is that the math operators we have now aren't that slow. Each one has a little hand written decision tree and works ok. But this gives no chance to make things better.

Instead, let the current functions be "fallback" impls, and add impls for the basic types: int, float, long, double, boolean (other types promoted). We limit the cache to a size of 1, if types change, we just revert to what we have today. The idea is to help the 90% case.

These will use all the type information we give it (which today is zero), but it gives us the opportunity to fix that, by just using a more specific indy signature. Even one argument or return value can reduce some boxing, use a simpler guard, and so on.

I fixed some consistency issues and bugs between static and dynamic cases around shifts, unary +, and bitwise operators, and added tests around this.
</description><key id="160064245">18847</key><summary>Refactor def math</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T23:06:34Z</created><updated>2016-06-14T11:57:11Z</updated><resolved>2016-06-13T23:39:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-13T23:21:35Z" id="225737103">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefMath.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Utility.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/AndTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BinaryOperatorTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefOperationTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/OrTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/UnaryTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/XorTests.java</file></files><comments><comment>Merge pull request #18847 from rmuir/refactor_def_math</comment></comments></commit></commits></item><item><title>ESIntegTestCase not documented in compatibility guides</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18846</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_java_api_changes.html

When reviewing this page, the rename of `ElasticsearchIntegrationTest` to `ESIntegTestCase` is not documented.
</description><key id="160026055">18846</key><summary>ESIntegTestCase not documented in compatibility guides</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels /><created>2016-06-13T19:27:41Z</created><updated>2016-06-13T23:20:37Z</updated><resolved>2016-06-13T23:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-13T23:20:37Z" id="225736928">We usually do not document minor Java API changes as your IDE will assist you here. This is especially true for the test framework.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add percolator query extraction support for dismax query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18845</link><project id="" key="" /><description /><key id="160012341">18845</key><summary>Add percolator query extraction support for dismax query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T18:15:45Z</created><updated>2016-06-13T18:27:59Z</updated><resolved>2016-06-13T18:27:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-13T18:21:33Z" id="225665488">LGTM
</comment><comment author="martijnvg" created="2016-06-13T18:27:59Z" id="225667270">thx @nik9000!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/percolator/src/main/java/org/elasticsearch/percolator/ExtractQueryTermsService.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/ExtractQueryTermsServiceTests.java</file></files><comments><comment>Merge pull request #18845 from martijnvg/percolator_dismax_query</comment></comments></commit></commits></item><item><title>Set next version back to alpha4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18844</link><project id="" key="" /><description /><key id="159991749">18844</key><summary>Set next version back to alpha4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2016-06-13T16:29:03Z</created><updated>2016-06-13T17:19:12Z</updated><resolved>2016-06-13T17:19:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-13T17:12:56Z" id="225646692">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file></files><comments><comment>Merge pull request #18844 from rjernst/bump_version_alpha4</comment></comments></commit></commits></item><item><title>No space left on device and then "FileAlreadyExistsException"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18843</link><project id="" key="" /><description>We are using Elasticsearch:

{
  "name": "Guardsman",
  "cluster_name": "elasticsearch",
  "version": {
    "number": "2.0.0",
    "build_hash": "de54438d6af8f9340d50c5c786151783ce7d6be5",
    "build_timestamp": "2015-10-22T08:09:48Z",
    "build_snapshot": false,
    "lucene_version": "5.2.1"
  },
  "tagline": "You Know, for Search"
}

ES hosted on linux server (ubuntu).
The server had 50 GiB volume of storage.

Suddenly ES stopped working at all.

Search queries retrieve the following result:
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; 

ES cluster health status was Red, all the shard status were red as well.

After digging in ES logs I figured out that there are no space on disk,
in order to resolve the issue, I create a new expended storage volume for ES server.

Unfartently, this did not resolve the problem, We still getting errors from ES.
This time FileAlreadyExistsException (while recovering shards according to ES's logs).

Attached logs file of ES from both, before expending the volume and after.

For being operative we deleted all of our indices at ES at then restart ES service,
and this of-curse solved the issue, BUT we lost all of our data.
1. How can we avoid such of incidents in the future?
2. Is there any resolution to "FileAlreadyExistsException"?

Thank in advance.

[es log.zip](https://github.com/elastic/elasticsearch/files/312252/es.log.zip)
</description><key id="159990682">18843</key><summary>No space left on device and then "FileAlreadyExistsException"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">almoghzn</reporter><labels /><created>2016-06-13T16:23:55Z</created><updated>2016-06-13T16:27:30Z</updated><resolved>2016-06-13T16:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-13T16:27:28Z" id="225634049">I see that you already [posted](https://discuss.elastic.co/t/no-space-left-on-device-and-then-filealreadyexistsexception/52638/1) this on the [Elastic Discourse forums](https://discuss.elastic.co). That's probably the best place to get your questions answered as Elastic reserves GitHub for verified bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for /regex/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18842</link><project id="" key="" /><description>Adds `/regex/` as a regex constructor. A couple of fun points:
1. This makes generic the idea of arbitrary stuff adding a constant.
Both SFunction and LRegex create a statically initialized constant.
Both go through Locals to do this because they LRegex isn't directly
iterable from SScript.
2. Differentiating `/` as-in-division from `/` as-in-start-of-regex
is hard. See:
http://www-archive.mozilla.org/js/language/js20-2002-04/rationale/syntax.html#regular-expressions
The javascript folks have a way, way tougher time of it then we do
because they have semicolon insertion. We have the much simpler
delimiter rules. Even with our simpler life we still have to add
a hack to get lexing `/regex/` to work properly. I chose to add
token-level lookbehind because it seems to be a pretty contained hack.
I considered and rejected lexer modes, a lexer member variable,
having the parser set variables on the lexer (this is a fairly common
solution for js, I believe), and moving regex parsing to the parser
level.
3. I've only added a very small subset of java.util.regex to the
whitelist because it is the subset I needed to test LRegex sanely.
More deserves to be added, and maybe more regex syntax like `=~` and
`==~`. Those can probably be added without too much pain.
</description><key id="159983592">18842</key><summary>Add support for /regex/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T15:51:51Z</created><updated>2016-06-28T09:32:06Z</updated><resolved>2016-06-13T22:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-13T15:56:18Z" id="225624928">@jdconrad I wanted to take a crack at regexes "because parsers are fun" (irrelevant ping for @bd808)!

It turned out a bit more complicated than I'd liked because `/` is hard. I think I added some lexer ambiguity by adding the sempreds where I did. I didn't see any slowdown but I'm not looking very hard. I expect we can work around it by iterating on this though.
</comment><comment author="jdconrad" created="2016-06-13T16:29:42Z" id="225634724">@nik9000 I'll make sure to take a look at this at some point today.  Thanks for tackling this!
</comment><comment author="uschindler" created="2016-06-13T16:36:33Z" id="225636645">I like the new constants handling, including the writeConstants method ref :-) I have not closely looked at the antlr code, but the ASM stuff looks fine. I will report once I have done a more close review!
</comment><comment author="nik9000" created="2016-06-13T16:40:18Z" id="225637690">&gt; I like the new constants handling, including the writeConstants method ref :-) I have not closely looked at the antlr code, but the ASM stuff looks fine. I will report once I have done a more close review!

Thanks!
</comment><comment author="uschindler" created="2016-06-13T16:57:46Z" id="225642529">I think all "synthetic constants" should have a "$" in their name, so they are definitely not accessible outside. Javac does the same (e.g. the assertion constant, or lambda implementation methods). Our handles for the DEF case already have this, maybe make it consistent.

A second thing: If the pattern is bullshit, you would only see this in a clinit error. Maybe you should pass the Pattern to Pattern.compile also as a compile-time check (just to try if it compiles).

Another question: How to handle pattern arguments like caseinsensitive? currently you have to pass as "(?i)" as part of the regex. PERL users know this as coming after the second slash, e.g. `/regex/i`
</comment><comment author="jdconrad" created="2016-06-13T17:06:55Z" id="225645094">@nik9000 The constants stuff is awesome.  Very clean.  I'm wondering if we can use modes instead of custom code for the regexes in the lexer.  I just can't help but think we should be able to resolve that divides conflict some other way.
</comment><comment author="nik9000" created="2016-06-13T17:14:04Z" id="225646995">&gt; I think all "synthetic constants" should have a "$" in their name

Sure! I saw that at some point in something else and forgot to adapt. Easy enough to fix.

&gt; If the pattern is bullshit, you would only see this in a clinit error.

Yeah. I wonder if I can compile it when I hit is and just pass the compiled Pattern to the initializer.

&gt; /regex/i

I was hoping to do that in a followup, but, yes, I think we should hadn't flags after the trailing `/`.

&gt; I'm wondering if we can use modes instead of custom code for the regexes in the lexer.

I investigated that. We can do it but it makes the lexer much more complex to read though probably simpler to run. Because modes don't inherit from each other you'd need to copy every rule and rename it. It pretty much quadruples the size of the lexer because you need to specify every mode transition and most token's types. Twice.
</comment><comment author="jdconrad" created="2016-06-13T17:35:35Z" id="225653022">@nik9000 Yeah, you're right.  It would be ridiculous to do this another way.  I'm on board with the grammar changes.  The only question I have left is should the predicates in the lexer be on the right side of the rules?  I think this might have ramifications on how it matches the first and last characters.  (In the lexer they can be anywhere.)  Edit:  The reason I ask this question is right hand predicates seem to be the preferred way in the book.
</comment><comment author="uschindler" created="2016-06-13T17:36:43Z" id="225653285">&gt; Yeah. I wonder if I can compile it when I hit is and just pass the compiled Pattern to the initializer.

That does not work with final variables. Just compiling it while parsing once does not matter in reality, Its just to check that syntax is correct and throw some sytax related exception on parse problem. The compiled pattern is not important. `Pattern.compile()` should be fast, its only slow if you do it millions of times :-)
</comment><comment author="nik9000" created="2016-06-13T18:32:52Z" id="225668609">&gt; The only question I have left is should the predicates in the lexer be on the right side of the rules?

I've moved it to the right side and it seems to be being called far fewer times. I just checked by debugging  it, but that seems like a good thing?
</comment><comment author="nik9000" created="2016-06-13T18:47:36Z" id="225672539">@uschindler and @jdconrad I believe this is ready for another round.
</comment><comment author="uschindler" created="2016-06-13T18:56:39Z" id="225674937">I'd make the type in addConstant also be a Definition.Type for consistency... unless it is too complicated to setup for the MethodHandles special case...
</comment><comment author="uschindler" created="2016-06-13T18:58:07Z" id="225675316">Ignore that it's too complicated! Maybe only add a second overload for the consistency :)
</comment><comment author="nik9000" created="2016-06-13T18:58:10Z" id="225675333">&gt; unless it is too complicated to setup for the MethodHandles special case...

That is why I didn't do it in the first place.
</comment><comment author="nik9000" created="2016-06-13T18:58:34Z" id="225675437">&gt; Maybe only add a second overload for the consistency :)

That I'm happy to do.
</comment><comment author="nik9000" created="2016-06-13T20:02:38Z" id="225691658">&gt; That I'm happy to do.

Done.
</comment><comment author="jdconrad" created="2016-06-13T20:07:54Z" id="225693023">The behavior of the lexer makes sense with the predicate being called fewer times if it's on the right hand side.  It will only test the predicate now if it actually matches one of the tokens.
</comment><comment author="nik9000" created="2016-06-13T20:27:58Z" id="225698170">Morning syntactic predicates....
</comment><comment author="uschindler" created="2016-06-13T21:15:14Z" id="225710906">Ok, from my side all looks fine. I'd just add some tests for more complicated regexes (e.g. those with escaped forward slashes or similar).

Also the renamed constants with a $ similar to the others are still missing. Otherwise LGTM from the ASM side.
</comment><comment author="nik9000" created="2016-06-13T21:43:47Z" id="225718026">&gt; Morning

mourning.... Spelling is hard.
</comment><comment author="uschindler" created="2016-06-13T21:47:15Z" id="225718842">&gt; mourning.... Spelling is hard.

This is why the method had to die! :-)
</comment><comment author="nik9000" created="2016-06-13T21:49:48Z" id="225719479">OK! Since @uschindler is good with it and we're pretty much done with the review part I've prepared it for merge. @jdconrad, are you good with it?
</comment><comment author="jdconrad" created="2016-06-13T21:51:30Z" id="225719839">This looks good to me.  +1 @nik9000 Thanks for adding this!
</comment><comment author="nik9000" created="2016-06-13T22:13:00Z" id="225724638">Thanks for reviewing @jdconrad and @uschindler !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18841</link><project id="" key="" /><description>First time user of ELK. 

In my /var/log/elasticsearch/logstashTesting.log file, all I have are entries that begin with this. 
[indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist]

**Elasticsearch (ELK) version**:
[root@logstash ~]# yum list installed | grep -E '(elasticsearch|logstash|kibana)'
elasticsearch.noarch   2.3.3-1          @elasticsearch-2.x  
kibana.x86_64          4.5.1-1          @kibana-4.5  
logstash.noarch        1:2.3.2-1        @logstash-2.3   

**JVM version**:
[root@logstash ~]# java -version
openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)
[root@logstash ~]# 

**OS version**:
[root@logstash ~]# cat /etc/redhat-release 
CentOS release 6.7 (Final)
[root@logstash ~]# 

**Provide logs (if relevant)**:
`RemoteTransportException[[logstash][&lt;ip_redacted&gt;:9300][indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist];
Caused by: java.lang.IllegalArgumentException: field [@timestamp] doesn't exist
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:166)
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:54)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:282)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:278)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-06-13 10:51:40,146][DEBUG][action.fieldstats        ] [logstash] [.kibana][0], node[o3rmPA87QB2R7bDSvUD9Fw], [P], v[4], s[STARTED], a[id=9YqIdu6LQguolACS17Bo1g]: failed to execute [org.elasticsearch.action.fieldstats.FieldStatsRequest@4b00875d]
RemoteTransportException[[logstash][&lt;ip_redacted&gt;:9300][indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist];
Caused by: java.lang.IllegalArgumentException: field [@timestamp] doesn't exist
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:166)
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:54)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:282)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:278)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-06-13 10:52:17,348][DEBUG][action.fieldstats        ] [logstash] [.kibana][0], node[o3rmPA87QB2R7bDSvUD9Fw], [P], v[4], s[STARTED], a[id=9YqIdu6LQguolACS17Bo1g]: failed to execute [org.elasticsearch.action.fieldstats.FieldStatsRequest@616f19c]
RemoteTransportException[[logstash][&lt;ip_redacted&gt;:9300][indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist];
Caused by: java.lang.IllegalArgumentException: field [@timestamp] doesn't exist
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:166)
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.shardOperation(TransportFieldStatsTransportAction.java:54)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:282)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:278)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
`

**logstash config file** 
`input {
  beats {
    port =&gt; 5044
  }
}

filter {
  date {
    locale =&gt; "en"
    match =&gt; ["mytimestamp", "YYYY-MM-dd HH:mm:ss"]
    target =&gt; "@timestamp"
  }
  grok {
    match =&gt; [ "message", "%{GREEDYDATA:message}"]
  }
}

output {
  stdout {
    codec =&gt; rubydebug
  }
  elasticsearch {
    hosts =&gt; "&lt;ip_redacted&gt;:9200"
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type =&gt; "%{[@metadata][type]}"
  }
  if "OMG" in [message] {
   email {
    from =&gt; "logstash@oracle.com"
    subject =&gt; "logstash alert"
    to =&gt; "robert.m.morris@oracle.com"
    via =&gt; "sendmail"
    body =&gt; "Here is the event line that occured: %{message}"
   }
 }
}
`

**elasticsearch config file** 
`[root@logstash ~]# cat /etc/elasticsearch/elasticsearch.yml 
/# ======================== Elasticsearch Configuration =========================

/#

/# NOTE: Elasticsearch comes with reasonable defaults for most settings.

/#       Before you set out to tweak and tune the configuration, make sure you

/#       understand what are you trying to accomplish and the consequences.

/#

/# The primary way of configuring a node is via this file. This template lists

/# the most important settings you may want to configure for a production cluster.

/#

/# Please see the documentation for further information on configuration options:

/# http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html

/#

/# ---------------------------------- Cluster -----------------------------------

/#

/# Use a descriptive name for your cluster:

/#

cluster.name: logstashTesting

/#

/# ------------------------------------ Node ------------------------------------

/#

/# Use a descriptive name for the node:

/#

node.name: ${HOSTNAME}

/#

/# Add custom attributes to the node:

/#

/# node.rack: r1

/#

/# ----------------------------------- Paths ------------------------------------

/#

/# Path to directory where to store the data (separate multiple locations by comma):

/#

path.data: /var/data/elasticsearch

/#

/# Path to log files:

/#

path.logs: /var/log/elasticsearch

/#

/# ----------------------------------- Memory -----------------------------------

/#

/# Lock the memory on startup:

/#

/# bootstrap.mlockall: true

/#

/# Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory

/# available on the system and that the owner of the process is allowed to use this limit.

/#

/# Elasticsearch performs poorly when the system is swapping the memory.

/#

/# ---------------------------------- Network -----------------------------------

/#

/# Set the bind address to a specific IP (IPv4 or IPv6):

/#

network.host: &lt;ip_redacted&gt;

/#

/# Set a custom port for HTTP:

/#

/# http.port: 9200

/#

/# For more information, see the documentation at:

/# http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html

/#

/# --------------------------------- Discovery ----------------------------------

/#

/# Pass an initial list of hosts to perform discovery when new node is started:

/# The default list of hosts is ["127.0.0.1", "[::1]"]

/#

/# discovery.zen.ping.unicast.hosts: ["host1", "host2"]

/#

/# Prevent the "split brain" by configuring the majority of nodes (total number of nodes / 2 + 1):

/#

/# discovery.zen.minimum_master_nodes: 3

/#

/# For more information, see the documentation at:

/# http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html

/#

/# ---------------------------------- Gateway -----------------------------------

/#

/# Block initial recovery after a full cluster restart until N nodes are started:

/#

/# gateway.recover_after_nodes: 3

/#

/# For more information, see the documentation at:

/# http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html

/#

/# ---------------------------------- Various -----------------------------------

/#

/# Disable starting multiple nodes on a single system:

/#

/# node.max_local_storage_nodes: 1

/#

/# Require explicit names when deleting indices:

/#

/# action.destructive_requires_name: true
`

**kibana log file**
`[root@logstash ~]# cat /opt/kibana/config/kibana.yml 
/# Kibana is served by a back end server. This controls which port to use.
/# server.port: 5601

/# The host to bind the server to.
/# server.host: "0.0.0.0"

/# If you are running kibana behind a proxy, and want to mount it at a path,
/# specify that path here. The basePath can't end in a slash.
/# server.basePath: ""

/# The maximum payload size in bytes on incoming server requests.
/# server.maxPayloadBytes: 1048576

/# The Elasticsearch instance to use for all your queries.
elasticsearch.url: "http://&lt;ip_redacted&gt;:9200"

/# preserve_elasticsearch_host true will send the hostname specified in `elasticsearch`. If you set it to false,
/# then the host you use to connect to _this_ Kibana instance will be sent.
/# elasticsearch.preserveHost: true

/# Kibana uses an index in Elasticsearch to store saved searches, visualizations
/# and dashboards. It will create a new index if it doesn't already exist.
/# kibana.index: ".kibana"

/# The default application to load.
/# kibana.defaultAppId: "discover"

/# If your Elasticsearch is protected with basic auth, these are the user credentials
/# used by the Kibana server to perform maintenance on the kibana_index at startup. Your Kibana
/# users will still need to authenticate with Elasticsearch (which is proxied through
/# the Kibana server)
/# elasticsearch.username: "user"
/# elasticsearch.password: "pass"

/# SSL for outgoing requests from the Kibana Server to the browser (PEM formatted)
/# server.ssl.cert: /path/to/your/server.crt
/# server.ssl.key: /path/to/your/server.key

/# Optional setting to validate that your Elasticsearch backend uses the same key files (PEM formatted)
/# elasticsearch.ssl.cert: /path/to/your/client.crt
/# elasticsearch.ssl.key: /path/to/your/client.key

/# If you need to provide a CA certificate for your Elasticsearch instance, put
/# the path of the pem file here.
/# elasticsearch.ssl.ca: /path/to/your/CA.pem

/# Set to false to have a complete disregard for the validity of the SSL
/# certificate.
/# elasticsearch.ssl.verify: true

/# Time in milliseconds to wait for elasticsearch to respond to pings, defaults to
/# request_timeout setting
/# elasticsearch.pingTimeout: 1500

/# Time in milliseconds to wait for responses from the back end or elasticsearch.
/# This must be &gt; 0
/# elasticsearch.requestTimeout: 30000

/# Time in milliseconds for Elasticsearch to wait for responses from shards.
/# Set to 0 to disable.
/# elasticsearch.shardTimeout: 0

/# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying
/# elasticsearch.startupTimeout: 5000

/# Set the path to where you would like the process id file to be created.
/# pid.file: /var/run/kibana.pid

/# If you would like to send the log output to a file you can set the path below.
/# logging.dest: stdout

/# Set this to true to suppress all logging output.
/# logging.silent: false

/# Set this to true to suppress all logging output except for error messages.
/# logging.quiet: false

/# Set this to true to log all events, including system usage information and all requests.
/# logging.verbose: false
[root@logstash ~]# 
`

**Question**:
So what's going on here? (also, logstash isn't sending the email when the match is found)
</description><key id="159976568">18841</key><summary>[indices:data/read/field_stats[s]]]; nested: IllegalArgumentException[field [@timestamp] doesn't exist]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DreadPirateRob</reporter><labels /><created>2016-06-13T15:21:51Z</created><updated>2016-06-14T12:56:32Z</updated><resolved>2016-06-14T07:56:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="DreadPirateRob" created="2016-06-13T15:28:14Z" id="225616144">I had to put a slash "/" in front of all of the pound signs "#", otherwise the formatting was messed up here on github
</comment><comment author="danielmitterdorfer" created="2016-06-14T07:56:02Z" id="225806383">Hi @DreadPirateRob,

this is no issue with Elasticsearch hence I'll close the ticket. We have a [dedicated discussion forum](https://discuss.elastic.co) for such questions. Can you please post your question there (the [Logstash forum](https://discuss.elastic.co/c/logstash) seems to fit best)? 

As a hint: Elasticsearch does not know about the field `@timestamp` and hence it complains. You need to define a mapping for your index, which is (very loosely speaking) similar to a DDL statement in SQL. You also set `manage_template =&gt; false` in your Logstash configuration. [According to the docs](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-manage_template) this means that Logstash will not create a mapping automatically and you have to provide your own.

Another hint: You can use [Markdown](https://help.github.com/articles/github-flavored-markdown/) for formatting in Github comments.
</comment><comment author="DreadPirateRob" created="2016-06-14T12:46:49Z" id="225869939">Since the error occurs in the elasticsearch logs, I figured this would be the place to put this issue. Also, I did use markdown. 

I will change the `manage_template =&gt; false` configuration in logstash. I'll also post this in the logstash forum as suggested. 
</comment><comment author="danielmitterdorfer" created="2016-06-14T12:53:44Z" id="225871655">&gt; Since the error occurs in the elasticsearch logs, I figured this would be the place to put this issue

Sure, no worries. You're right; it could have also been an issue with Elasticsearch and it is sometimes hard to tell what's the root cause.

&gt; Also, I did use markdown. 

I just referred to the Markdown reference as you mentioned that you had trouble with the hash comments in the yml file. You can use triple backticks to start / end a multi-line block comment:

``````
# this is a multiline ...
# preformatted block
# that will not be broken
# I used ``` to start the block and will now use ``` to end it again.
``````
</comment><comment author="DreadPirateRob" created="2016-06-14T12:55:51Z" id="225872241">for future reference, the question is posted here https://discuss.elastic.co/t/indices-data-read-field-stats-s-nested-illegalargumentexception-field-timestamp-doesnt-exist/52738

Also (for my fellow newbies), the forums only have a 5K character limit, so to post my question with the relevant logs &amp; config files I had to make 5 posts in the thread just to get all the information in. 
</comment><comment author="DreadPirateRob" created="2016-06-14T12:56:32Z" id="225872425">Ahh, see I used a single backtick instead of the triple. fail on my part. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Typo fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18840</link><project id="" key="" /><description /><key id="159974161">18840</key><summary>Typo fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TomyLobo</reporter><labels><label>:Packaging</label><label>Awaiting CLA</label><label>bug</label></labels><created>2016-06-13T15:11:17Z</created><updated>2016-09-13T06:00:18Z</updated><resolved>2016-09-12T21:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T11:10:25Z" id="225850177">Hi @TomyLobo 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-09-12T21:36:33Z" id="246502172">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment><comment author="TomyLobo" created="2016-09-13T06:00:18Z" id="246582979">Signing a CLA seemed to much effort for a typo fix to me...
Do what you want with this PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>index shard should be able to cancel check index on close.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18839</link><project id="" key="" /><description>If someone sets `index.shard.check_on_startup`, indexing start up time can be slow (by design, it diligently goes and checks all data). If for some reason the shard is closed in that time, the store ref is kept around and prevents a new shard copy to be allocated to this node via the shard level locks. This is especially tricky if the shard is close due to a cancelled recovery which may re-restart soon.

This PR adds a cancellable threads instance to each IndexShard and perform index checking underneath it, so it can be cancelled on close. This assumes that:

1) Interrupting a checkIndex is safe.
2) Interrupting a checkIndex will actually make it stop.

Relates to #12011

PS. We are also discussing not doing a full index check on peer recovery but rather just do a checksum check.
</description><key id="159973471">18839</key><summary>index shard should be able to cancel check index on close.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T15:08:39Z</created><updated>2016-06-20T10:16:22Z</updated><resolved>2016-06-20T10:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-13T15:08:51Z" id="225610157">@s1monw wdyt?
</comment><comment author="s1monw" created="2016-06-20T07:59:31Z" id="227074367">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file></files><comments><comment>Revert #18839 as it causes file leaks</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file></files><comments><comment>index shard should be able to cancel check index on close. (#18839)</comment></comments></commit></commits></item><item><title>Remove ability to specify `size: 0` on aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18838</link><project id="" key="" /><description>Currently `size: 0` is used as an alias for `get me all the buckets` on the `terms`, `significant_terms` and `geohash_grid` aggregations. This is very trappy as it hides the fact that this is actually a really bad idea which, for high cardinality fields will use a lot of memory and stream huge amounts of data from the shards back to the coordinating node to reduce. It cost huge amounts in CPU (for the reduce), memory (on both the shards and the reduce node) and network bandwidth. It also has the ability to starve the node of resources potentially destabilising the cluster (either through creating a lot of garbage to be collected which could trigger stop the world GC pauses or through tying up the CPU or network and blocking the liveness pings back to the master.

Having `size: 0` as an option makes it look like there is a short cut here and we can do the `give me all the buckets` case in a different very efficient way, which we can't. Internally we rewrite `size:0` to `Integer.MAX_VALUE`

Removing this option (`size: 0`) and requiring the user to specify a number means it becomes more obvious that asking for huge number of bugs will incur a high cost by the very fact that the user actually has to ask for that high number. 

The only case (IMO) for asking for all buckets of a terms agg back which is not a bad idea is when you have a low cardinality and in that case you almost always know, at least to an order of magnitude, how many values you expect (usually &lt;10, 10s or low 100s). In these cases having to specify a size is not a big burden as you can still set it to a number higher than you expect (e.g. for 10s of values set size to 200) and in the case where there is a bug and your low cardinality field accidentally becomes very high cardinality you are protected against doing a very heavy operation. Other use-cases should not be asking for all buckets but instead should be slicing and analysing the data in a way where they do not need to ask for everything (you can't show that much information to a user anyway).
</description><key id="159964743">18838</key><summary>Remove ability to specify `size: 0` on aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>breaking</label></labels><created>2016-06-13T14:33:22Z</created><updated>2016-10-12T14:52:52Z</updated><resolved>2016-06-17T15:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-17T15:24:12Z" id="226799588">Since this requires some changes in Kibana I've reverted the removal. Though it is now deprecated in 2.x/2.4.
</comment><comment author="jimczi" created="2016-06-17T15:34:41Z" id="226802432">Oups wrong issue ;)
</comment><comment author="GlenRSmith" created="2016-10-12T14:46:56Z" id="253234254">@colings86 
Should this really have been removed from the documentation? Would it have been better to signal the deprecation in the documentation?
</comment><comment author="colings86" created="2016-10-12T14:50:29Z" id="253235457">Deprecation logging was added as commented in #18854. Also in that PR we added it to the breaking changes document for 5.0: https://github.com/elastic/elasticsearch/pull/18854/files#diff-b044a68ebfc190743082f46a3361748cR24
</comment><comment author="colings86" created="2016-10-12T14:51:49Z" id="253235900">We could add a note to the 2.4 documentation as well if you think it will help. Fancy raising a PR to change the 2.4 documentation?
</comment><comment author="GlenRSmith" created="2016-10-12T14:52:52Z" id="253236169">Can do. I need to find a sample page with documented deprecation to use as an example of how to "style" such a deprecation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoGridAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java</file><file>test/framework/src/main/java/org/elasticsearch/search/aggregations/bucket/AbstractTermsTestCase.java</file></files><comments><comment>Remove size 0 options in aggregations</comment></comments></commit></commits></item><item><title>Remove mapper attachments plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18837</link><project id="" key="" /><description>We now have in 5.0.0 `ingest-attachment` plugin.
We should remove `mapper-attachments` plugin in the future.

I propose doing that either in 6.0 or from 5.1. Labelling for now as `6.0.0`.
</description><key id="159946151">18837</key><summary>Remove mapper attachments plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>breaking</label><label>v6.0.0-alpha1</label></labels><created>2016-06-13T13:08:45Z</created><updated>2016-09-19T12:48:24Z</updated><resolved>2016-09-19T12:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-21T14:04:24Z" id="227449826">+1
</comment><comment author="clintongormley" created="2016-06-21T14:06:29Z" id="227450453">Let's add deprecation logging in 5.0
</comment><comment author="dadoonet" created="2016-07-26T10:37:00Z" id="235231493">@clintongormley Deprecation logging is already done with #16948. So we are ready to remove the plugin for 6.0.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/MapperAttachmentsPlugin.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/TikaImpl.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MapperAttachmentsClientYamlTestSuiteIT.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/TikaDocTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/TikaImplTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java</file></files><comments><comment>Remove mapper attachments plugin</comment></comments></commit></commits></item><item><title>Change ingest packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18836</link><project id="" key="" /><description>In core merge `o.e.ingest.core` with `o.e.ingest`. The core package isn't needed.
In ingest-common added `o.e.ingest.common` module and moved all code to this package. To avoid unneeded bugs if package protected methods outside the module are used, which can't be caught by the compiler.
</description><key id="159903643">18836</key><summary>Change ingest packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T09:06:19Z</created><updated>2016-06-21T07:24:57Z</updated><resolved>2016-06-21T07:24:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-13T09:45:24Z" id="225535483">I'm not sure you want to do this: I disagree with tons of packages, but I don't think packages should be shared across modules/plugins, especially across classloaders!

Things like package private access are not going to work as you think, there will be bugs not caught by the compiler and only discovered by REST tests and users.
</comment><comment author="martijnvg" created="2016-06-13T10:05:47Z" id="225540051">@rmuir Good point. I didn't realise this.

I will change the PR to only drop the o.e.ingest.core package instead and revert the changes in the plugin modules.
</comment><comment author="martijnvg" created="2016-06-13T10:07:07Z" id="225540326">Also I'll move all code in the ingest-common module to `o.e.ingest.common`. (right now it uses the `o.e.ingest` package)
</comment><comment author="martijnvg" created="2016-06-13T10:27:32Z" id="225544461">I've changed the PR.
</comment><comment author="talevy" created="2016-06-20T17:51:06Z" id="227217254">LGTM, needs rebase
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove wrong json-data in example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18835</link><project id="" key="" /><description>It seems the json-data for the example copied and pasted wrongly from an earlier example. Since it was even missing the comma delimiter, it seems safe to assume it can simply be omitted. Without it, the example makes more sense again.
</description><key id="159895603">18835</key><summary>Remove wrong json-data in example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lyrixderaven</reporter><labels /><created>2016-06-13T08:19:00Z</created><updated>2016-06-14T07:22:01Z</updated><resolved>2016-06-14T07:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-14T07:21:58Z" id="225799647">Merged - thanks @lyrixderaven 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TransportClient.builder().settings(settings).build() hangs silently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18834</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.3

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:
I am trying to create a transport client in my application. 
TransportClient.builder().settings(settings).build() hangs and there is no error message.

I enabled debug level log and it can't tell me anyting about what's going on
2016-06-13 01:27:06,045 UTC INFO org.elasticsearch.plugins: [Agamotto] modules [], plugins [], sites []
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [force_merge], type [fixed], size [1], queue_size [null]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [percolate], type [fixed], size [8], queue_size [1k]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [fetch_shard_started], type [scaling], min [1], size [16], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [listener], type [fixed], size [4], queue_size [null]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [index], type [fixed], size [8], queue_size [200]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [refresh], type [scaling], min [1], size [4], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [suggest], type [fixed], size [8], queue_size [1k]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [generic], type [cached], keep_alive [30s]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [warmer], type [scaling], min [1], size [4], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [search], type [fixed], size [13], queue_size [1k]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [flush], type [scaling], min [1], size [4], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [fetch_shard_store], type [scaling], min [1], size [16], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [get], type [fixed], size [8], queue_size [1k]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [bulk], type [fixed], size [8], queue_size [50]
2016-06-13 01:27:06,046 UTC DEBUG org.elasticsearch.threadpool: [Agamotto] creating thread_pool [snapshot], type [scaling], min [1], size [4], keep_alive [5m]

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="159892711">18834</key><summary>TransportClient.builder().settings(settings).build() hangs silently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yamap77</reporter><labels /><created>2016-06-13T08:00:29Z</created><updated>2016-06-13T13:25:55Z</updated><resolved>2016-06-13T13:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-13T13:25:55Z" id="225579645">There isn't nearly any detail here to reproduce, but your best bet to get traction on this issue is to open a post on the [Elastic Discourse forums](https://discuss.elastic.co) as Elastic reserves GitHub for verified bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exclude admin / diagnostic requests from HTTP request limiting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18833</link><project id="" key="" /><description>With this commit we exclude certain HTTP requests that are needed to inspect the cluster
from HTTP request limiting to ensure these commands are processed even in critical
memory conditions.

Relates #17951, relates #18145
</description><key id="159891395">18833</key><summary>Exclude admin / diagnostic requests from HTTP request limiting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Network</label><label>enhancement</label><label>resiliency</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-13T07:52:08Z</created><updated>2016-06-20T11:31:17Z</updated><resolved>2016-06-15T12:31:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-13T09:04:25Z" id="225526152">I wonder if we should go from a whitelist to a blacklist, it seems like we are adding more and more exceptions - maybe we should just put the memory hungry ones in there explicitly?
</comment><comment author="danielmitterdorfer" created="2016-06-13T09:10:18Z" id="225527502">Previous exceptions were defined on transport level (OTOH, we have currently 3 exceptions defined on transport level). This PR adds support for HTTP level. I think the better option is to be conservative with exceptions and just use a whitelist but it would be pretty easy to use a blacklist instead (just swap the default and blacklist the relevant actions).
</comment><comment author="s1monw" created="2016-06-14T13:25:10Z" id="225879624">&gt; Previous exceptions were defined on transport level (OTOH, we have currently 3 exceptions defined on transport level). This PR adds support for HTTP level. I think the better option is to be conservative with exceptions and just use a whitelist but it would be pretty easy to use a blacklist instead (just swap the default and blacklist the relevant actions).

ok sure.... I left some comments
</comment><comment author="danielmitterdorfer" created="2016-06-15T05:27:20Z" id="226091356">@s1monw I've pushed a commit that addresses your comments. Could you please have another look?
</comment><comment author="s1monw" created="2016-06-15T07:06:08Z" id="226105494">LGTM
</comment><comment author="danielmitterdorfer" created="2016-06-15T07:06:25Z" id="226105542">Thanks; I'll merge soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>core/src/main/java/org/elasticsearch/rest/RestController.java</file><file>core/src/main/java/org/elasticsearch/rest/RestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestCancelTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/tasks/RestListTasksAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestTests.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpClient.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpRequestSizeLimitIT.java</file><file>core/src/test/java/org/elasticsearch/rest/RestControllerTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/main/RestMainActionTests.java</file><file>core/src/test/java/org/elasticsearch/rest/action/support/RestTableTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/rest/FakeRestRequest.java</file></files><comments><comment>Exclude admin / diagnostic requests from HTTP request limiting</comment></comments></commit></commits></item><item><title>Simplify NodeJoinController to make use of new cluster state batching infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18832</link><project id="" key="" /><description>The NodeJoinController is responsible for processing joins from nodes, both normally and during master election. For both use cases, the class processes incoming joins in batches in order to be efficient and to accumulated enough joins (i.e., &gt;= min_master_nodes) to seal an election and ensure the new cluster state can be committed. Since the class was written, we introduced a new infrastructure to support batch changes to the cluster state at the `ClusterService` level. This PR rewrites NodeJoinController to use that infra and be simpler.

The PR also introduces a new concept to ClusterService allowing to submit tasks in batches, guaranteeing that all tasks submitted in a batch will be processed together (potentially with more tasks).  On top of that I added some extra safety checks to the ClusterService, around potential double submission of task objects into the queue.

This is done in preparation to revive #17811
</description><key id="159851311">18832</key><summary>Simplify NodeJoinController to make use of new cluster state batching infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-12T22:16:20Z</created><updated>2016-06-17T07:22:15Z</updated><resolved>2016-06-17T07:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-14T19:44:13Z" id="225994124">@bleskes It's really beautiful. The changes to `ClusterService` look great, and I gave `NodeJoinController` a super-careful review outside of the diff. I left a few comments, but it LGTM. No need for another round unless you want it.
</comment><comment author="bleskes" created="2016-06-15T04:39:26Z" id="226086143">@jasontedor thx for your review. I pushed another commit addressing all your comments (and responded to some).
</comment><comment author="bleskes" created="2016-06-15T12:01:45Z" id="226166888">@jasontedor thx again. I'll merge this in when I have the cycles to deal with potential test failures
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/test/java/org/elasticsearch/cluster/service/ClusterServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file></files><comments><comment>Simplify NodeJoinController to make use of new cluster state batching infra (#18832)</comment></comments></commit></commits></item><item><title>Array constructor references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18831</link><project id="" key="" /><description>This adds support for stuff like `list.stream().toArray(String[]::new)`

In this case we generate a synthetic method such as:

```
private static synthetic String[] lambda$0(int size) {
  return new String[size];
}
```

Then we just return an ordinary reference to that (`this::lambda$0`).

I also refactored the grammar portion for references so the different types are organized better.
</description><key id="159836966">18831</key><summary>Array constructor references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-12T17:00:27Z</created><updated>2016-06-14T11:18:38Z</updated><resolved>2016-06-13T17:36:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-13T16:04:08Z" id="225627278">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ArrayTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java</file></files><comments><comment>Merge pull request #18831 from rmuir/moreRefs</comment></comments></commit></commits></item><item><title>Added elasticsearch-browser plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18830</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="159834044">18830</key><summary>Added elasticsearch-browser plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OlegKunitsyn</reporter><labels /><created>2016-06-12T15:54:14Z</created><updated>2016-06-12T15:55:17Z</updated><resolved>2016-06-12T15:55:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Safeguard: limit number of terms in TermsQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18829</link><project id="" key="" /><description>**Describe the feature**: I have seen a lot of cases where an application will generate some long list of terms for a `TermsQuery` and pass it off to Elasticsearch. In general, `TermsQuery` performance degrades quickly as more terms are added, and once you get into hundreds (or thousands), you can pretty much forget it. (Unless there's a way to do a terms query with hundreds or thousands of terms and I just don't know the secret.) I think we should consider short-circuiting on the number of terms if it goes beyond some threshold unless some overriding argument is given. Additionally, it might be a good idea for Elastic.co documentation to warn users that many terms can have a substantial impact on latency, and of course point out the default limit/safeguard (supposing it is decided that the number of terms should be limited).

Relates #11511
</description><key id="159805462">18829</key><summary>Safeguard: limit number of terms in TermsQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jonaf</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label><label>resiliency</label></labels><created>2016-06-12T03:25:02Z</created><updated>2017-03-13T07:16:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-13T07:04:37Z" id="225503404">+1 on adding a soft limit to discourage having too many terms in terms queries. However I don't think we should let the query execute with a partial list of terms in that case, we should fail the entire request.
</comment><comment author="jonaf" created="2016-06-13T13:58:46Z" id="225588745">Sorry, by "short-circuiting," I meant abandoning the entire query, not truncating the list of terms.
</comment><comment author="jpountz" created="2016-06-13T14:20:49Z" id="225595314">Cool, thanks for clarifying. +1 then!
</comment><comment author="jpountz" created="2016-06-13T14:22:26Z" id="225595752">@jonaf In case you would like to work on a PR that fixes this issue, feel free to ping me for review.
</comment><comment author="hluu" created="2016-06-13T19:28:14Z" id="225682833">+1
</comment><comment author="jonaf" created="2016-06-15T17:38:40Z" id="226262511">@jpountz Happy to. I just had to get authorization to sign the CLA. Should this contribution be destined for master only, or 2.x as well? (Based on reading contribution guidelines, it looks like I need to open a separate PR for each branch and reference this issue from the PR's?)

Edit: just noticed the #11511 is tagged for v5, so I'll just make these changes from master.
</comment><comment author="jpountz" created="2016-06-15T21:35:17Z" id="226327321">@jonaf Awesome! In general we build PRs for master first and only consider backporting when the change is ready for master.
</comment><comment author="toniov" created="2017-03-13T07:16:06Z" id="286031293">&gt;  In general, TermsQuery performance degrades quickly as more terms are added, and once you get into hundreds (or thousands), you can pretty much forget it.

Would this be applied in case of a Terms Query using the [lookup](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html#query-dsl-terms-lookup) mechanism as well?

In the above link it says:
&gt; When it’s needed to specify a terms filter with a lot of terms it can be beneficial to fetch those term values from a document in an index.

But not sure how much is that `a lot`.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Method references to user functions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18828</link><project id="" key="" /><description>This allows stuff like:

```
int mycompare(int i, int j) { j - i } 
...
list.sort(this::mycompare)
```

It is like a manual transmission lambda. The main idea here is allowing pointers to functions in our own class (whether user explicitly wrote them or we wrote them on their behalf).

So a few cleanups were needed:
1. allow creating FunctionRef to these (lots of refactoring here)
2. allow dynamic code (Def) to access these. We need a runtime whitelist, and we'd like to keep all methods private, avoid reflection/security issues/etc. We add a whitelist to the class itself, a pointer to each function like this:

```
private static final MethodHandle handle$mycompare$2;
```

These are initialized in `&lt;clinit&gt;` with `ldc` (they are simply constants).
We can lookup from this by name and arity exactly, and we have what we need.
3. general cleanup of functions in our ASM code (thanks @uschindler).
</description><key id="159791978">18828</key><summary>Method references to user functions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-11T20:16:40Z</created><updated>2016-06-13T09:06:19Z</updated><resolved>2016-06-12T06:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-06-11T20:33:33Z" id="225392736">I also cleaned up the MethodWriter stuff to no longer nest. The code was ununderstandable! :-)
</comment><comment author="uschindler" created="2016-06-11T20:48:51Z" id="225393384">+1 to merge current state to master! Next is real lambdas!
</comment><comment author="jdconrad" created="2016-06-12T05:49:13Z" id="225411519">Thanks @rmuir and @uschindler! This all looks great to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessLexer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java</file></files><comments><comment>Merge pull request #18828 from rmuir/ownReferences</comment></comments></commit></commits></item><item><title>Add } as a delimiter. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18827</link><project id="" key="" /><description>Add `}` is statement delimiter but only in places where it is
otherwise a valid part of the syntax, specificall the end of a block.
We do this by matching but not consuming it. Antlr 4 doesn't have
syntax for this so we have to kind of hack it together by actually
matching the `}` and then seeking backwards in the token stream to
"unmatch" it. This looks reasonably efficient. Not perfect, but way
better than the alternatives.

I tried and rejected a few options:
1. Actually consuming the `}` and piping a boolean all through the
grammar from the last statement in a block to the delimiter. This
ended up being a rather large change and made the grammar way more
complicated.
2. Adding a semantic predicate to delimiter that just does the
lookahead. This doesn't work out well because it doesn't work (I
never figured out why) and because it generates an _amazing_
`adaptivePredict` which makes a super huge DFA. It looks super
inefficient.

Closes #18821
</description><key id="159776171">18827</key><summary>Add } as a delimiter. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-11T14:30:38Z</created><updated>2016-06-13T09:07:24Z</updated><resolved>2016-06-11T16:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-11T14:30:47Z" id="225365888">Oop, conflicts. One moment.
</comment><comment author="nik9000" created="2016-06-11T14:33:04Z" id="225366074">Rebased to remove conflicts. Parser merge conflicts are impressive but you can just regenerate....
</comment><comment author="nik9000" created="2016-06-11T14:34:03Z" id="225366141">@jdconrad and/or @rmuir: this ready for review when you are ready for it.
</comment><comment author="rmuir" created="2016-06-11T14:50:23Z" id="225367237">thanks for looking into this. It will make more pre-existing simple scripts work as-is (we can never do that perfectly, but it is nice when it works). It should also make lambda syntax be less stupid.

we need @jdconrad to look at the grammar change, as he understands the antlr side best.
</comment><comment author="jdconrad" created="2016-06-11T15:48:47Z" id="225371003">@nik9000 +1.  Thanks for looking into this.  I'm also surprised the semantic predicate didn't work, but this solution looks good too.  
</comment><comment author="nik9000" created="2016-06-11T16:54:06Z" id="225374467">Thanks for reviewing @rmuir and @jdconrad!

Since I didn't make this clear in the description: painless now allows syntax like:

```
if (foo) {return 1}
```

where it used to require:

```
if (foo) {return 1;}
```

Note the extra semicolon in the "before" example.
</comment><comment author="jdconrad" created="2016-06-11T18:28:29Z" id="225381963">@nik9000  Just as a note, I've thought about the predicate situation some more...  The reason this doesn't work is because even though _we_ know the next token must match RBRACK due to the predicate, the path available for ANTLR to make this decision doesn't necessarily require the RBRACK.  All the predicate is doing is making the statements look like this -- 

`| DO block WHILE LP expression RP delimiter` --&gt; `| DO block WHILE LP expression RP delimiter?`

Meaning we no longer require any specific token after delimiter for that rule.  This is what's causing the ambiguities because now there's an opening for any number of paths to match since ANTLR has no knowledge of what the predicate is doing other than opening or closing a path, and it must write the DFA as such.

Your solution (the correct one) forces RBRACK to match closing off the possibility of any other paths.  You'll also note this is different from the ELSE situation which actually closes off other paths in the event of an else forcing a single path.  Unfortunately this is not possible for delimiter because we may optionally include a semicolon before a right bracket.
</comment><comment author="nik9000" created="2016-06-11T18:34:17Z" id="225382247">Right! I was reasonably sure that was it. I think antlr 3 had support for this kind of thing natively...
</comment><comment author="nik9000" created="2016-06-11T18:39:23Z" id="225382478">Indeed, syntactic predicates look like they did this. I'm sure the antlr experts have a better solution than the one I just merged but I'm unsure of a way to do it that wouldn't way overcomplicate the grammar and/or introduce that crazy backtracking.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete multiple ES indexes in one go</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18826</link><project id="" key="" /><description>**Describe the feature**:
There is no inbuilt command for deleting multiple ES indexes in one go. Its a good to have feature
</description><key id="159768460">18826</key><summary>Delete multiple ES indexes in one go</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">temaniarpit27</reporter><labels /><created>2016-06-11T11:04:45Z</created><updated>2016-06-11T11:06:30Z</updated><resolved>2016-06-11T11:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-11T11:06:30Z" id="225353074">Yes you can. `DELETE index1,index2`. Read https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-index.html

Please use discuss.elastic.co where we can better discuss that kind of things.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Previous Versions of a doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18825</link><project id="" key="" /><description>**Describe the feature**: There is no inbuilt feature of ElasticSearch which gives previous versions of a particular doc. Sometimes it becomes difficult to see the new changes. So we can have a version control system (VCS) for each doc so that we can look at different versions of a doc
</description><key id="159768373">18825</key><summary>Previous Versions of a doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">temaniarpit27</reporter><labels /><created>2016-06-11T11:02:40Z</created><updated>2016-06-11T11:05:23Z</updated><resolved>2016-06-11T11:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-11T11:05:23Z" id="225352984">You need to do that by yourself.
For example, add after the doc id its version number, like `doc1-v1`. Then when you add a new version, send a new doc with `_id` `doc1-v2`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Lambda Stub Node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18824</link><project id="" key="" /><description>As the title says.
</description><key id="159741716">18824</key><summary>Add Lambda Stub Node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-10T23:29:36Z</created><updated>2016-06-28T09:32:12Z</updated><resolved>2016-06-11T00:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-11T00:42:45Z" id="225326241">+1 to push. We can flesh this out soon now
</comment><comment author="jdconrad" created="2016-06-11T00:43:03Z" id="225326263">@rmuir Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessLexer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ELambda.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/antlr/ParserTests.java</file></files><comments><comment>Merge pull request #18824 from jdconrad/stubby</comment></comments></commit></commits></item><item><title>Port reindex's groovy tests to painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18823</link><project id="" key="" /><description>This ports the reindex tests that rely on groovy to painless because we expect people to prefer painless over groovy because you can use inline scripts without opening a gaping security hole.
</description><key id="159728806">18823</key><summary>Port reindex's groovy tests to painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v5.0.0-alpha4</label></labels><created>2016-06-10T21:39:32Z</created><updated>2016-06-14T11:17:56Z</updated><resolved>2016-06-13T16:29:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-10T21:43:16Z" id="225303339">I've added WIP because I couldn't port reindex's timeout tests to painless. These tests we (ab)using the script query and Thread.sleep to cause a search side timeout. I still want to test these on the REST layer because I want to make sure the parsing works. I'm not sure of the right way. I'd prefer to have real timeouts rather than simulated ones because simulations would drift out of date.

I'm thinking of making a test plugin that making a language that just sleeps. I know that actually forcing the timeouts like this is generally not good practice but I don't trust us to not break mock timeouts....
</comment><comment author="jdconrad" created="2016-06-10T21:50:04Z" id="225304591">LGTM!  Thanks for updating these with Painless.
</comment><comment author="nik9000" created="2016-06-13T16:06:45Z" id="225628040">@jdconrad I just rebased to pick up the `;` change and added a skip for the timeout tests. I'll have to think more about those. I've also removed the WIP. Can you have another look? Is fairly short so I figured the rebase was ok.
</comment><comment author="jdconrad" created="2016-06-13T16:24:58Z" id="225633319">LGTM!
</comment><comment author="nik9000" created="2016-06-13T16:31:46Z" id="225635317">Thanks for reviewing @jdconrad !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search preference _prefer_node enhancement </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18822</link><project id="" key="" /><description>**Describe the feature**:
- Allow the search preference for `_prefer_node` to use the node name or the node id. 
- Allow the search preference for `_prefer_node` to accept a csv of values (round robin the preferred nodes)

Currently we can only prefer a single node and that single node must be the node's id. The node id is dynamically generated which does not allow this feature to be used with out much effort to maintain it. 

Use case: 
I have a single cluster across multiple data centers and need to force client nodes in data center A to only send search queries to data nodes in data center A. With this proposed enhancement I will be able to automate the preference routing. 
</description><key id="159726258">18822</key><summary>Search preference _prefer_node enhancement </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dev-head</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2016-06-10T21:23:01Z</created><updated>2016-06-17T19:47:38Z</updated><resolved>2016-06-17T19:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-10T21:40:06Z" id="225302684">&gt; Currently we can only prefer a single node and that single node must be the node's id. The node id is dynamically generated which does not allow this feature to be used with out much effort to maintain it.

Nodes will be getting a persistent node IDs in Elasticsearch 5.0.0 (it will be generated the first time a node starts up, persisted to disk, and used forever after that). I don't think that this is the solution to your problem, see below.

&gt; I have a single cluster across multiple data centers

We generally recommend against this.

&gt; force client nodes in data center A to only send search queries to data nodes in data center A

Use the `_only_nodes` preference? You can specify a common attribute on the nodes and use that attribute for the client? This seems to me to be the best option?
</comment><comment author="dev-head" created="2016-06-10T22:01:13Z" id="225306649">`_only_nodes` would cause an error if the nodes are offline for any reason, i do want it to be able to fall back, which is why `_prefer_node` is more helpful in our case. We need to have 99.9999% up time and sub second global latency all while handling an ever growing data set and user base. 

A persistent node id would make life easier but it still means ES is in charge where I want to be in charge of it, mainly for automation needs. 

My use case may not be the normal one, but it does seem to be pretty commonly run into by the community. I need to provide the same data across the globe and ES can do that, right now there just isn't much to support this type of use case for one reason or another. 

We could have multiple clusters running, but it is a bit over kill to have to mange feeding the same data into each cluster and then manage all the collisions it can cause because each cluster has it's own view of the data. 

edit: for clarity
</comment><comment author="nik9000" created="2016-06-10T22:09:53Z" id="225308205">&gt; We need to have 99.9999% up time
&gt; 
&gt; I have a single cluster across multiple data centers

I don't think these statement work well together.

If you need sub-second global latency you'll need a couple of distinct clusters and a system that forks writes to all of them. This is far superior to a single cluster because you can shoot one of the clusters and have your application fail over to the other DC temporarily. Fully separate clusters are what a truly paranoid administrator should want. And you can't have that kind of uptime without massive paranoia. I'm [told](http://uptime.is/99.9999) six nines is 31.6 seconds of downtime a year. So you'll need stuff like automatic failover across DCs too.

I haven't formed an opinion on `_prefer_node`, though.
</comment><comment author="jasontedor" created="2016-06-10T22:21:22Z" id="225310101">&gt; `_only_nodes` would cause an error if the nodes are offline for any reason, i do want it to be able to fall back, which is why `_prefer_node` is more helpful in our case.

Note that you did say

&gt; force client nodes in data center A to _only_ send search queries to data nodes in data center A

but I appreciate the clarification (emphasis added).

&gt; We need to have 99.9999% up time and sub second global latency all while handling an ever growing data set and user base.

This is not going to happen with a single _cluster_, sorry.
</comment><comment author="dev-head" created="2016-06-10T23:14:53Z" id="225317481">There is a need to be able to use that preference to give routing control to the search queries. This is the reason that the preference exists today. Is this feature enhancement, to prefer more than one node and to identify that preference by a node name, a reasonable request, on it's own outside of my use case?

---

&gt; This is not going to happen with a single cluster, sorry.
&gt; I don't think these statement work well together.

challenge accepted. 👍 
</comment><comment author="bleskes" created="2016-06-13T14:30:07Z" id="225598110">&gt; I have a single cluster across multiple data centers and need to force client nodes in data center A to only send search queries to data nodes in data center A.

A short note that if you take away the need to "force" and make it "prefer", you get it all out of the [allocation awareness feature](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/allocation-awareness.html) by default:

&gt; When executing search or GET requests, with shard awareness enabled, Elasticsearch will prefer using local shards — shards in the same awareness group — to execute the request. This is usually faster than crossing racks or awareness zones.
</comment><comment author="clintongormley" created="2016-06-14T08:55:07Z" id="225819870">I agree that running a cluster across two data centres (not availability zones) it very likely to end in tears.  Also, that using allocation awareness provides what @dev-head is after.

That said, I think adding `_prefer_nodes` as an adjunct  to `_only_nodes` makes sense.  I'd even consider removing `_only_node` and `_prefer_node`.
</comment><comment author="jasontedor" created="2016-06-14T22:37:49Z" id="226037278">&gt; That said, I think adding `_prefer_nodes` as an adjunct  to `_only_nodes` makes sense.  I'd even consider removing `_only_node` and `_prefer_node`.

I agree, and opened two little pieces: #18872 and #18875.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Painless: Should } be a delimiter?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18821</link><project id="" key="" /><description>I'm porting reindex's tests from groovy to painless. Yay. One thing I found is that a statement like

```
if (ctx._source.bar != 1) {ctx._source.foo = 1}
```

doesn't compile. It needs to be

```
if (ctx._source.bar != 1) {ctx._source.foo = 1;}
```

with the semicolon between the `1` and the `}`.

Is it worth making that work?
</description><key id="159719926">18821</key><summary>Painless: Should } be a delimiter?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label></labels><created>2016-06-10T20:46:04Z</created><updated>2016-06-11T16:52:18Z</updated><resolved>2016-06-11T16:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-10T20:55:29Z" id="225293314">I think so if it creates no ambiguity in the grammar. The missing semicolons create too much ambiguity and, at the moment, the only place you can have it missing is EOF. But maybe } works.
</comment><comment author="nik9000" created="2016-06-10T21:03:37Z" id="225295136">Cool! I might give that a shot this evening if I get a chance. I've wanted an excuse to play with painless's guts.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/BasicStatementTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/WhenThingsGoWrongTests.java</file></files><comments><comment>Painless: Add } as a delimiter. Kindof.</comment></comments></commit></commits></item><item><title>Make timevalue writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18820</link><project id="" key="" /><description>Writeable is better for immutable objects like TimeValue.

Switches serialization from `writeLong` to `writeZLong` which saves a couple of bytes.
</description><key id="159713767">18820</key><summary>Make timevalue writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-06-10T20:13:40Z</created><updated>2016-06-28T09:51:18Z</updated><resolved>2016-06-10T22:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-10T20:31:24Z" id="225287918">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove allocation commands from the `_reroute` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18819</link><project id="" key="" /><description>The `_reroute` API is pretty handy if a user wants to kick off an allocation round if the cluster (due to a bug) missed to `reroute` shards etc. We had some of them in 1.7 etc when we added delayed allocation. Aside of this is has the following purposes:
- to shoot yourself in the food and work around how the shard-balancer allocated shards
- to move tons of shards at once where you should have used allocation filtering and make your cluster go nuts https://github.com/elastic/elasticsearch/issues/18739
- to cut your left arm off if you accidentally set `allow_primary = true`
- to have a reason to write a script instead of unsetting `index.routing.allocation.disable_allocation` see https://github.com/alicegoldfuss/shardnado/issues/1

I think this stuff needs to go. We can't offer APIs like this where basically nothing in the docs tells you:
- this is an expert API
- 99% of the times you are going to use it you should either user a different API, report a but or just don't mess with the cluster at all
- use a cmd tool to repair state on disc so primaries can be allocated (we don't have that yet I know but you get the drift)

I spend so much time on pulling folks out of the dirt after using this I don't think it's worth it. 
</description><key id="159698282">18819</key><summary>Remove allocation commands from the `_reroute` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>blocker</label><label>v6.0.0</label></labels><created>2016-06-10T18:49:58Z</created><updated>2017-07-20T18:37:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-10T18:59:37Z" id="225267649">In the years I've used Elasticsearch I've had two valid uses for it:
1. To force assign a missing primary and accept that data loss.
2. To jiggle the allocator to make it start when it stopped.

Other than that I've only ever misused it. For those reading along: you don't usually want to move shards around with this API because it doesn't pin them where you put them. If you want to pin a shard someplace you should use [allocation filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html). It is much more flexible and actually works.

I'd be quite happy with an API to force assignment of an empty primary and one to jiggle the allocator. A command line tool is less nice because I'd have to connect to the right node and stuff. Also, I'm not sure it'd work in the case where the data is totally gone?

I remember once I reformatted a few machine when I didn't have any replicas. I wasn't paying good attention.....
</comment><comment author="s1monw" created="2016-06-10T19:22:47Z" id="225272930">&gt; 1. To jiggle the allocator to make it start when it stopped.

this doesn't need an allocation command right? all you need to do is to run `_reroute` with an empty body?

&gt; I'd be quite happy with an API to force assignment of an empty primary and one to jiggle the allocator. A command line tool is less nice because I'd have to connect to the right node and stuff. Also, I'm not sure it'd work in the case where the data is totally gone?
&gt; I remember once I reformatted a few machine when I didn't have any replicas. I wasn't paying good attention.....

I think in such a case we should have a cmd tool that creates such an empty primary shard on disk and we let `_reroute?fetch_stores=true` go and figure out the rest?
</comment><comment author="nik9000" created="2016-06-10T19:54:57Z" id="225279866">&gt; this doesn't need an allocation command right? all you need to do is to run _reroute with an empty body?

I don't believe it is required. I think I always added some small one just in case.

&gt; I think in such a case we should have a cmd tool that creates such an empty primary shard on disk and we let `_reroute?fetch_stores=true` go and figure out the rest?

Sorry, I don't know that API. I still don't like having to pick a machine on which to create the empty shard. That was one of the problems with the allow_primary command - you had to pick a node rather than just let Elasticsearch pick a decent choice. It was just another thing to have to think about when you've busted your cluster.
</comment><comment author="s1monw" created="2016-06-10T20:03:01Z" id="225281674">&gt; Sorry, I don't know that API. I still don't like having to pick a machine on which to create the empty shard. That was one of the problems with the allow_primary command - you had to pick a node rather than just let Elasticsearch pick a decent choice. It was just another thing to have to think about when you've busted your cluster. 

I made that API up :) I think this is something that you should never use a REST endpoint for. something is seriously fucked up lets build one-off tools for these situations.
</comment><comment author="jonaf" created="2016-06-12T04:26:50Z" id="225409530">I have used the reroute API in many cases to get ES "unstuck" from either initializing or unassigned shards. I'm not sure how else I'd recover a cluster besides restarting it (sometimes this means a full cluster restart). Since Elasticsearch is often the primary means of searching for public services, a full cluster restart is rarely an option (in particular, replacing the cluster for a reason such as this is quite expensive from an infrastructure perspective). Restarting individual nodes gets progressively less painful with every release, but I doubt it will ever be a trivial performance cost.

In older versions of Elasticsearch, I found myself using the reroute API more frequently because it was often unclear why ES would refuse to allocate shards. I think this has also gotten better with every release, and particularly in 2.x, the state of shards (or problems allocating them) gets clearer.

I guess the reroute API gives a (false?) sense of security, since it gives the administrator the hammer with which to enforce a shard movement / allocation, and that just makes a guy feel in control.

&gt; I think this is something that you should never use a REST endpoint for. something is seriously fucked up lets build one-off tools for these situations.

I can see your argument for removing this from the REST API and, for the functionality that is not satisfied by existing API's or other means, moving to a command line tool. Of course, to complement this new tool, it would be really nice if Elasticsearch could identify the node(s) that are good candidates for the allocation, or some API to "propose" an allocation where Elasticsearch suggests which nodes are good candidates. That's not more convenient than the REST API, but at least it doesn't suddenly make the process of such allocations go from convenient to painful suddenly.

&gt; That was one of the problems with the allow_primary command - you had to pick a node rather than just let Elasticsearch pick a decent choice. It was just another thing to have to think about when you've busted your cluster.

Hm, I would always just allocate to a random node. ES will rebalance afterwards if it isn't happy with the allocation. But I suppose ES could make the destination node optional and pick something better than "random" for you.
</comment><comment author="aewhite" created="2016-06-13T11:27:10Z" id="225555281">The reroute API has been a critical tool for in keeping the cluster stable when the default allocator over allocated a node. This was a fairly normal case for us before we switched to tempest due to our shard sizes not being equal. 

Balance aside, I've had to unallocate shards and let them "rebuild" because their translogs got out-of-sync and had deletes showing up on one shard but not another. 

In a perfect world, maybe it's not needed, but I don't think ES is there yet. Yeah, it's a really big hammer but I can't think of another tool that would have solved our problems. 
</comment><comment author="s1monw" created="2016-06-13T11:53:45Z" id="225560075">&gt; The reroute API has been a critical tool for in keeping the cluster stable when the default allocator over allocated a node. This was a fairly normal case for us before we switched to tempest due to our shard sizes not being equal.

so you took over the entire shard allocation process and manage everything yourself? I am asking because otherwise the balancer will kick in and reverse your decisions at some random point in time?

&gt; Balance aside, I've had to unallocate shards and let them "rebuild" because their translogs got out-of-sync and had deletes showing up on one shard but not another.

this is a special case which happens rarely I am against an API for, it should be a commandline tool.

&gt; In a perfect world, maybe it's not needed, but I don't think ES is there yet. Yeah, it's a really big hammer but I can't think of another tool that would have solved our problems.

I haven't seen any reason here that convinced me to not remove it. It's too much of a hammer. If the balancer is not smart enough for a usecase we have to fix it. If we need more allocation decider we have to fix it. We can't offer a hammer and expect the user to know how to use it. 
</comment><comment author="kimchy" created="2016-06-13T12:48:01Z" id="225571057">+1 on this suggestion, simple reroute API to kickstart a reroute is handy. A way to (1) force a primary to be allocated (2) stop shard allocation is also very handy, don't mind if it is in an API (with extra protection flag, or conformation based execution based on random token) or a command line tool.
</comment><comment author="aewhite" created="2016-06-13T12:50:58Z" id="225571644">&gt; the balancer will kick in and reverse your decisions at some random point in time

That is not entirely true. It tries to balance by shard _count_ so as long as I swapped a large shard for a small shard the balancer left things alone. 

&gt; I haven't seen any reason here that convinced me to not remove it

As long as the same functionality exists somewhere then I guess I don't really have much of an argument. 

My biggest concern is that a production cluster get's into resource starved or hot-spot-heavy sate and admins have no recourse. Balancers aren't perfect, configuration can miss edge cases, and bugs happens, so make sure the hammer exists somewhere to get the system stable. 
</comment><comment author="s1monw" created="2016-06-13T12:55:39Z" id="225572676">&gt; My biggest concern is that a production cluster get's into resource starved or hot-spot-heavy sate and admins have no recourse. Balancers aren't perfect, configuration can miss edge cases, and bugs happens, so make sure the hammer exists somewhere to get the system stable.

see this is my argument, I can support anybody in the community that just goes and uses that hammer. Hence I have to remove the hammer. Edge cases happen but that doesn't mean I am going to build APIs that work around ANY safety mechanism available. The reason "something might happen" isn't valid here give how many people get into the "something happened" situation because of that hammer?
</comment><comment author="s1monw" created="2016-06-13T12:55:59Z" id="225572733">&gt; As long as the same functionality exists somewhere then I guess I don't really have much of an argument.

yeah no it won't exist. 
</comment><comment author="nik9000" created="2016-06-13T13:03:14Z" id="225574439">&gt; That is not entirely true. It tries to balance by shard count so as long as I swapped a large shard for a small shard the balancer left things alone.

A combination of [allocation filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html) and settings [total_shards_per_node](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html) on the index level is going to be more permanent. It might not be enough to get the cluster safe, but in that case we should address that issue. Anything you do with the reroute API is going to be undone by the balancer eventually because it doesn't set up constraints. That is why it isn't a good API. It works temporarily.
</comment><comment author="aewhite" created="2016-06-13T13:16:13Z" id="225577289">So, what is the the hypothetical solution to the problem of a cluster with a hot spot that is killing bulk loading performance. Or, since ES can't predict the size of shards, put two large shards on the same node and cause resource/hot-spotting issues? These are both very real issues that we have faced and not hypotheticals, and as far as I can tell are unavoidable without a bug-free, configuration perfect, very very advance balancer (or hardware overkill). 

I agree with @nik9000 that the use of reroute is a temporarily solution but the alternative of having NO solution in a production environment is very scary. If a bug is discovered in a live environment, I don't see waiting on a code fix as being feasible. To me, this isn't a "might happen" but more of a "when will it happen" case. 

Anyway, I rest my case, I don't want to take over or derail the conversation here. 
</comment><comment author="s1monw" created="2016-06-13T13:39:17Z" id="225583223">&gt; So, what is the the hypothetical solution to the problem of a cluster with a hot spot that is killing bulk loading performance. Or, since ES can't predict the size of shards, put two large shards on the same node and cause resource/hot-spotting issues? These are both very real issues that we have faced and not hypotheticals, and as far as I can tell are unavoidable without a bug-free, configuration perfect, very very advance balancer (or hardware overkill).

I have been thinking about this for a while and I think the solution to this is to give the user more control how the balancer handles individual indices. The ultimate flexibility here would be a adding a weight to an index that the balancer can take into account. This weight would be updatable such that the user can reduce the weight if the index goes readonly or can raise it if bulk indexing happens. This could be even combined with a node level max weight where certain nodes can only hold shards up to a certain weight. I think what we won't do as a start is to make the weight dynamic in terms of changing it automatically if indexing rate drops, it's too fragile and might change too quickly. But for your situation this seems to be the right solution and the user knows much better if an index is much bigger than another index. It can still be a function of the # of docs or so but it's up to the user I guess
</comment><comment author="jonaf" created="2016-06-16T17:08:35Z" id="226550067">As an update, I ran into a use-case for the reroute API today in Elasticsearch 1.7.3. I had a cluster in yellow state with 2242 unassigned shards. When I looked at the shards API, I saw that they were all the third replica of all of the indices in the cluster. The day before, the cluster had a node go unhealthy and recover during heavy indexing / index creation. I restarted the node that went unhealthy to see if Elasticsearch would start assigning shards again, and it didn't do anything. So I then used the reroute API to assign one of the replicas to a random node -- I expected to get an error with a message from Elasticsearch explaining why the shard couldn't be assigned. To my surprise, though, the replica shard _was_ assigned. So I proceeded to assign the rest in the same way.

These were replica shards, so a command line tool to create empty primaries would not have helped as a replacement for the reroute API. I'm not sure what else I could have done to recover the cluster, except muck with the allocation settings and try to convince Elasticsearch to move shards around in some way and hopefully pick up and assign those other shards.

At the very least, it would have been really helpful if ES had logged errors to indicate the reason why those replicas were never assigned so I could know in what way I needed to adjust my cluster settings and/or topology to satisfy the allocation requirements.

Here's the cluster health API output after I started assigning shards:

``` json
{
  "cluster_name": "anon_polloi_agrippasrc_lyamtestfiltalloca_elasticsearch",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 24,
  "number_of_data_nodes": 18,
  "active_primary_shards": 7548,
  "active_shards": 20295,
  "relocating_shards": 21,
  "initializing_shards": 2,
  "unassigned_shards": 2347,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 1459,
  "number_of_in_flight_fetch": 17040
}
```

Here's what it looked like before I did anything:

``` json
{
  "cluster_name": "anon_polloi_agrippasrc_lyamtestfiltalloca_elasticsearch",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 24,
  "number_of_data_nodes": 18,
  "active_primary_shards": 7548,
  "active_shards": 20222,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 2422,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 17572
}
```
</comment><comment author="s1monw" created="2016-06-17T07:53:53Z" id="226704632">you have to wait until `"number_of_in_flight_fetch": 17572` goes to 0 it's still looking for unallocated version of those shards. See this is exactly why I think this API is trappy and must go away.
</comment><comment author="radu-gheorghe" created="2016-10-20T07:38:02Z" id="255031664">&gt; I think this stuff needs to go. We can't offer APIs like this where basically nothing in the docs tells you:
&gt; - this is an expert API
&gt; - 99% of the times you are going to use it you should either user a different API, report a but or just don't mess with the cluster at all
&gt; - use a cmd tool to repair state on disc so primaries can be allocated (we don't have that yet I know but you get the drift)
&gt; 
&gt; I spend so much time on pulling folks out of the dirt after using this I don't think it's worth it. 

Maybe changing the docs to point these things out, while still leaving users with the flexibility, is a better solution.

Reporting a bug when we find one is definitely a good idea, but if it occurs in production it's nice to have a workaround, you can't wait for the bug to be fixed. Granted, now ES is more stable than it used to be (and thanks so much for the titanic work on making it happen!), but I'm still scared of not having the option to allocate shards manually in various corner cases.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add capturing method references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18818</link><project id="" key="" /><description> #18748 added non-capturing method references of the forms (`Class::virtualMethod`, `Class::staticMethod`, `Class::new`).

This PR adds support for capturing references such as `instance::virtualMethod`. The main benefit is enhancing our lambda support, especially on the dynamic side. I also cleaned up DefBootstrap to be less confusing, it takes varargs (depending on `flavor`), and has error checks around this.

For any lambda we have two main parts: the interface type of the parameter, and the implementation method. For this to work, we add a new flavor to DefBootstrap called `REFERENCE`. Its basically just a dynamic `LambdaMetaFactory`, taking expected interface type as a parameter, but it resolves the implementation method with a dynamic lookup based on receiver's class. Then it calls LambdaMetaFactory. Its cached on receiver's class just like anything else.

There are 4 cases, based on what we know at compile time.
1. interface type is known, implementation method is known:  `type.foo(anothertype::bar)`
2. interface type is known, implementation method is unknown: `type.foo(def::bar)`
3. interface type is unknown, implementation method is known: `def.foo(type::bar)`
4. interface type is unknown, implementation method is unknown: `def.foo(def::bar)`

cases 4 and 3 are just like 2 and 1, respectively, except deferred. The placeholder signature on the stack was extended, to support the number of capture arguments.

case 1 is completely static. We just call `LambdaMetaFactory` like java would, only we capture the reference of the instance.
case 2 is very similar, except instead of calling `LambdaMetaFactory`, we use the new `REFERENCE` DefBootstrap.
case 3 defers the call to LambdaMetaFactory until the interface type is resolved.
case 4 defers the call to `DefBootstrap` until the interface is resolved.

case 4 does mean we nest a cache inside a cache, but its what we need: the two things are really independent. Our scripts are small, its contained and does not recurse, and we don't want bad performance.
</description><key id="159697670">18818</key><summary>Add capturing method references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-10T18:47:36Z</created><updated>2016-06-13T09:07:58Z</updated><resolved>2016-06-10T22:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-10T19:53:43Z" id="225279638">LGTM!  Thanks for tackling this, I realize it's quite complicated.  (Added one important comment after the fact.)
</comment><comment author="rmuir" created="2016-06-10T21:48:06Z" id="225304238">@jdconrad I pushed some commits and merged master.
</comment><comment author="jdconrad" created="2016-06-10T22:12:56Z" id="225308725">+1.  Merge it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Def.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/DefBootstrap.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FeatureTest.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/FunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/ScriptImpl.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECapturingFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/package-info.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/DefBootstrapTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionRefTests.java</file></files><comments><comment>Merge pull request #18818 from rmuir/capturingReferences</comment></comments></commit></commits></item><item><title>Remove parseElementst() method in RescorePhase and SuggestPhase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18817</link><project id="" key="" /><description>A minor clean up. I think the default implementation in SearchPhase should be enough.
</description><key id="159670170">18817</key><summary>Remove parseElementst() method in RescorePhase and SuggestPhase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-10T16:12:45Z</created><updated>2016-06-13T18:32:07Z</updated><resolved>2016-06-13T08:21:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-10T17:29:32Z" id="225245004">LGTM
</comment><comment author="cbuescher" created="2016-06-13T08:21:58Z" id="225516883">@jpountz thanks for the review
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Copy headers and context to individual requests inside a bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18816</link><project id="" key="" /><description>This change copies the headers and context values from the BulkShardRequest to the individual requests that are getting executed, so that the data stored in the headers and context is available during these requests when necessary.

I couldn't find a great way to test this change without a lot of other changes. This solves a [user reported issue](https://discuss.elastic.co/t/problem-with-indexed-groovy-script-bulk-api-and-shield/52234) in the dicsuss forums and only affects 2.x as we have the `ThreadContext` in master.
</description><key id="159664913">18816</key><summary>Copy headers and context to individual requests inside a bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Bulk</label><label>bug</label><label>review</label><label>v2.3.4</label><label>v2.4.0</label></labels><created>2016-06-10T15:47:35Z</created><updated>2016-06-15T15:37:20Z</updated><resolved>2016-06-15T15:37:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-10T19:06:14Z" id="225269252">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Raised IOException on deleteBlob</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18815</link><project id="" key="" /><description>Title is self-explanatory.  Closes #18530.
</description><key id="159641983">18815</key><summary>Raised IOException on deleteBlob</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gfyoung</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-06-10T14:02:25Z</created><updated>2016-08-04T15:12:46Z</updated><resolved>2016-07-01T03:00:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gfyoung" created="2016-06-10T14:02:38Z" id="225190213">Suggestions on how to write tests for these changes?
</comment><comment author="jasontedor" created="2016-06-10T14:28:16Z" id="225197050">&gt; Title is self-explanatory.

It explains the what, but not the why and the why is really important.
</comment><comment author="gfyoung" created="2016-06-10T14:30:13Z" id="225197644">@jasontedor : I understand that these exceptions are not `IOException`, but then is the issue I'm closing a real issue then?  Also, if you look at the two implementations that already throw `IOException` (see &lt;a href="https://github.com/elastic/elasticsearch/issues/18530#issuecomment-225163982"&gt;here&lt;/a&gt;), those caught exceptions are not `IOException` either.
</comment><comment author="gfyoung" created="2016-06-10T14:30:49Z" id="225197871">@jasontedor : I think the issue that I am closing explains the importance.  Do I need to repeat it in the PR?
</comment><comment author="abeyad" created="2016-06-10T15:06:20Z" id="225208152">@gfyoung Regarding writing tests, take a look at `FsBlobStoreContainerTests`.  It extends `ESBlobStoreContainerTestCase` which contains useful general `BlobContainer` tests that can be used for each of the different `BlobContainer` implementations.
</comment><comment author="jasontedor" created="2016-06-10T15:39:07Z" id="225217510">&gt; I understand that these exceptions are not `IOException`, but then is the issue I'm closing a real issue then?

Yes, the leniency is the real issue.
</comment><comment author="jasontedor" created="2016-06-10T15:39:43Z" id="225217673">&gt; I think the issue that I am closing explains the importance. Do I need to repeat it in the PR?

Yes, the why needs to be in the commit message which ends up as the default body for the initial PR comment.
</comment><comment author="gfyoung" created="2016-06-10T15:41:56Z" id="225218279">@jasontedor : Regarding the leniency, sure thing.  I suppose a more careful examination of the possible exceptions thrown needs to be done here.  Regarding the commit body, fair enough.  That can be added.
</comment><comment author="gfyoung" created="2016-06-12T02:36:45Z" id="225406188">1) Added a `deleteBlob` test for the `IOException`.

2) Added `BlobStoreContainer` tests `AzureBlobContainer`, `HdfsBlobContainer`, and `S3BlobContainer`, but I had issues writing their `newBlobStore` methods, especially as simply as the `GoogleCloudStorageBlobStoreContainerTests` for example.

Feedback on how to properly write them would be great!
</comment><comment author="gfyoung" created="2016-06-22T01:28:48Z" id="227619236">Is there any update on whether or not we can use actual repositories for both Azure or S3 for these tests?  If that can be done, that will save the working  of having to mock everything for both repositories.
</comment><comment author="jasontedor" created="2016-06-22T01:30:43Z" id="227619502">&gt; Is there any update on whether or not we can use actual repositories for both Azure or S3 for these tests? 

We can not. These tests need to be able to run from developer machines.
</comment><comment author="gfyoung" created="2016-06-22T01:34:15Z" id="227619894">@jasontedor : Ah, I see.  Hmm...then perhaps some more input from others on how to possibly mock the Azure and S3 repositories?  &lt;strike&gt;The mocking strategy suggested by @abeyad looked promising, but further discussion of it appears to me that it will only hit a dead end AFAICT.&lt;/strike&gt;
</comment><comment author="gfyoung" created="2016-06-23T05:55:36Z" id="227956973">Sometimes a fresh look at a problem is needed.  Took a different approach to mocking S3-related components and found that I really just needed to mock the client as @abeyad had mentioned at some point.  Onto `Azure` and `HDFS` now!
</comment><comment author="gfyoung" created="2016-06-23T06:22:05Z" id="227960529">`AzureStorageServiceMock` really made things a lot easier for the `AzureBlobStoreContainer` tests than I had perceived previously.  Only `HDFS` is left now!
</comment><comment author="gfyoung" created="2016-06-23T07:21:28Z" id="227970125">Having some issues with `HDFS`: instantiating the `BlobStore` results in a permissions error (even when I create a temp directory that I then instantiate `FileContext` with).

An exception is thrown because when `FileContext.getFileContext(...)` is called, it cannot call `UserGroupInformation.getCurrentUser()` via `Subject.getSubject()` (in Hadoop source code) due to permission denials.  How to get around this?
</comment><comment author="abeyad" created="2016-06-23T13:03:58Z" id="228043082">@gfyoung great news, let me know when its ready and I can give it another review.  
</comment><comment author="gfyoung" created="2016-06-23T13:05:54Z" id="228043557">@abeyad : read my comment above - some assistance on the `HDFS` permissions issue would be great!
</comment><comment author="abeyad" created="2016-06-23T13:07:42Z" id="228044010">Not familiar with HDFS but perhaps @jbaiera could help?
</comment><comment author="jbaiera" created="2016-06-23T16:25:48Z" id="228105111">@abeyad Re HDFS: Many of the Hadoop services' authentication is based around Hadoop's `UserGroupInformation` object, which is poked around with even if auth is turned off. When you first request the `UserGroupInformation` object in the Hadoop Client, it goes through it's internal setup steps. One of the steps is registering with a metrics gathering system so that the UGI can emit quantiles for number of login successes and failures. This is for the benefit of reporting and monitoring systems like Ganglia. The snag occurs when UGI registers it's metrics object. The system tries accessing the `UgiMetrics` object's fields using reflection to collect them as metrics. When the wrapper code asks for all declared fields on `UgiMetrics` it trips on the SecurityManager as it does not have the permission for `accessDeclaredMembers`. I'm afraid I'm not too familiar with the Security side of Java to help much further than that... 
</comment><comment author="gfyoung" created="2016-06-23T20:00:31Z" id="228166190">&lt;strike&gt;Hmmm...seems like a stripped down mock of the FileContext might be needed but not sure yet how to go about that...&lt;/strike&gt;
EDIT: Arghhh...private constructor only in `FileContext` --&gt; no extending :cry: 
</comment><comment author="gfyoung" created="2016-06-24T10:15:29Z" id="228309471">You can pass in `-Dtests.security.manager=false` to get past the initial error I was getting, but then it breaks because Java has no permissions to call `chmod` when calling `writeBlob`
</comment><comment author="jasontedor" created="2016-06-24T12:14:55Z" id="228330598">&gt; You can pass in `-Dtests.security.manager=false` to get past the initial error I was getting

This isn't going to be accepted, these plugins will run under the security manager (so should be tested under it) and the security manager is non-optional in Elasticsearch 5.0.0.
</comment><comment author="gfyoung" created="2016-06-24T13:25:04Z" id="228344439">Ah, okay, back to square one it is...is there anyway I can elevate privileges when making these calls or add some other config that will allow me to do so?
</comment><comment author="abeyad" created="2016-06-24T14:18:33Z" id="228358016">@gfyoung I'm not very familiar with how HDFS handles these issues that @jbaiera described, but looking at HdfsBlobStore, the code is already there to handle this very situation you describe: https://github.com/elastic/elasticsearch/blob/master/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobStore.java#L123

Can you verify that code is being invoked when you instantiate the HdfsBlobStore?
</comment><comment author="gfyoung" created="2016-06-24T14:22:53Z" id="228359232">@abeyad : if I use the security manager as @jasontedor described, I can't even get there.  The code throws an exception when I attempt to instantiate `HdfsBlobStore` as I described &lt;a href="https://github.com/elastic/elasticsearch/pull/18815#issuecomment-227970125"&gt;here&lt;/a&gt;.
</comment><comment author="abeyad" created="2016-06-24T14:25:20Z" id="228359936">@gfyoung at what point in the code is an exception thrown?  can you share a stack trace?
</comment><comment author="gfyoung" created="2016-06-25T00:31:22Z" id="228495833">```
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:writeVersionProperties UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.13
  OS Info               : Linux 3.19.0-61-generic (amd64)
  JDK Version           : Oracle Corporation 1.8.0_91 [Java HotSpot(TM) 64-Bit Server VM 25.91-b14]
  JAVA_HOME             : /usr/lib/jvm/java-8-oracle
:core:compileJava UP-TO-DATE
:core:generateModulesList UP-TO-DATE
:core:generatePluginsList UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:jar UP-TO-DATE
:client:rest:compileJava UP-TO-DATE
:client:rest:processResources UP-TO-DATE
:client:rest:classes UP-TO-DATE
:client:rest:jar UP-TO-DATE
:plugins:repository-hdfs:compileJava UP-TO-DATE
:plugins:repository-hdfs:processResources UP-TO-DATE
:plugins:repository-hdfs:classes UP-TO-DATE
:plugins:repository-hdfs:copyPluginPropertiesTemplate
:plugins:repository-hdfs:pluginProperties UP-TO-DATE
:test:framework:compileJava UP-TO-DATE
:test:framework:processResources UP-TO-DATE
:test:framework:classes UP-TO-DATE
:test:framework:jar UP-TO-DATE
:plugins:repository-hdfs:compileTestJava UP-TO-DATE
:plugins:repository-hdfs:processTestResources UP-TO-DATE
:plugins:repository-hdfs:testClasses UP-TO-DATE
:plugins:repository-hdfs:test
   [junit4] &lt;JUnit4&gt; says olá! Master seed: 52D32AD49015465
==&gt; Test Info: seed=52D32AD49015465; jvm=1; suite=1
Suite: org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests
  1&gt; [2016-06-24 10:40:10,770][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=12, reason=Cannot allocate memory
  1&gt; [2016-06-24 10:40:10,790][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
  1&gt; [2016-06-24 10:40:10,790][WARN ][org.elasticsearch.bootstrap] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
  1&gt; [2016-06-24 10:40:10,791][WARN ][org.elasticsearch.bootstrap] These can be adjusted by modifying /etc/security/limits.conf, for example: 
  1&gt;    # allow user 'LINUX-USER-1' mlockall
  1&gt;    LINUX-USER-1 soft memlock unlimited
  1&gt;    LINUX-USER-1 hard memlock unlimited
  1&gt; [2016-06-24 10:40:10,791][WARN ][org.elasticsearch.bootstrap] If you are logged in interactively, you will have to re-login for the new limits to take effect.
&gt; REPRODUCE WITH: gradle :plugins:repository-hdfs:test -Dtests.seed=52D32AD49015465 -Dtests.class=org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests -Dtests.method="testWriteRead" -Dtests.security.manager=true -Dtests.locale=ar-SD -Dtests.timezone=America/Porto_Velho
ERROR   0.53s | HdfsBlobStoreContainerTests.testWriteRead &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.security.AccessControlException: access denied ("javax.security.auth.AuthPermission" "getSubject")
   &gt;    at __randomizedtesting.SeedInfo.seed([52D32AD49015465:F4A0A314C9BC2C55]:0)
   &gt;    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
   &gt;    at java.security.AccessController.checkPermission(AccessController.java:884)
   &gt;    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
   &gt;    at javax.security.auth.Subject.getSubject(Subject.java:287)
   &gt;    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:631)
   &gt;    at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:447)
   &gt;    at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:425)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests.newBlobStore(HdfsBlobStoreContainerTests.java:34)
   &gt;    at org.elasticsearch.repositories.ESBlobStoreContainerTestCase.testWriteRead(ESBlobStoreContainerTestCase.java:49)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/LINUX-USER-1/Repositories/elasticsearch/plugins/repository-hdfs/build/testrun/test/J0/temp/org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests_52D32AD49015465-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=1797, maxMBSortInHeap=6.250442282302583, sim=RandomSimilarity(queryNorm=false,coord=crazy): {}, locale=ar-SD, timezone=America/Porto_Velho
  2&gt; NOTE: Linux 3.19.0-61-generic amd64/Oracle Corporation 1.8.0_91 (64-bit)/cpus=4,threads=1,free=459689360,total=514850816
  2&gt; NOTE: All tests run in this JVM: [HdfsBlobStoreContainerTests]

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':plugins:repository-hdfs:test'.
&gt; There were test failures: 1 suite, 1 test, 1 error [seed: 52D32AD49015465]

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

Completed [1/1] in 5.31s, 1 test, 1 error &lt;&lt;&lt; FAILURES!

Tests with failures:
  - org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests.testWriteRead

   [junit4] JVM J0:     1.02 ..     7.86 =     6.84s
   [junit4] Execution time total: 7.97 sec.
   [junit4] Tests summary: 1 suite, 1 test, 1 error
:plugins:repository-hdfs:test FAILED

BUILD FAILED

Total time: 55.894 secs

BUILD SUCCESSFUL
Total time: 9 seconds
```

Attached the output as a file for future reference too: [debug.txt](https://github.com/elastic/elasticsearch/files/332808/debug.txt)
</comment><comment author="gfyoung" created="2016-06-26T03:49:36Z" id="228582832">```
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:writeVersionProperties UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.13
  OS Info               : Linux 3.19.0-61-generic (amd64)
  JDK Version           : Oracle Corporation 1.8.0_91 [Java HotSpot(TM) 64-Bit Server VM 25.91-b14]
  JAVA_HOME             : /usr/lib/jvm/java-8-oracle
:core:compileJava UP-TO-DATE
:core:generateModulesList UP-TO-DATE
:core:generatePluginsList UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:jar UP-TO-DATE
:client:rest:compileJava UP-TO-DATE
:client:rest:processResources UP-TO-DATE
:client:rest:classes UP-TO-DATE
:client:rest:jar UP-TO-DATE
:plugins:repository-hdfs:compileJava UP-TO-DATE
:plugins:repository-hdfs:processResources UP-TO-DATE
:plugins:repository-hdfs:classes UP-TO-DATE
:plugins:repository-hdfs:copyPluginPropertiesTemplate
:plugins:repository-hdfs:pluginProperties UP-TO-DATE
:test:framework:compileJava UP-TO-DATE
:test:framework:processResources UP-TO-DATE
:test:framework:classes UP-TO-DATE
:test:framework:jar UP-TO-DATE
:plugins:repository-hdfs:compileTestJava
:plugins:repository-hdfs:processTestResources UP-TO-DATE
:plugins:repository-hdfs:testClasses
:plugins:repository-hdfs:test
   [junit4] &lt;JUnit4&gt; says hello! Master seed: 7222BA4654ABC020
==&gt; Test Info: seed=7222BA4654ABC020; jvm=1; suite=1
Suite: org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests
  1&gt; [2016-06-25 23:45:08,938][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=12, reason=Cannot allocate memory
  1&gt; [2016-06-25 23:45:08,939][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
  1&gt; [2016-06-25 23:45:08,939][WARN ][org.elasticsearch.bootstrap] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
  1&gt; [2016-06-25 23:45:08,940][WARN ][org.elasticsearch.bootstrap] These can be adjusted by modifying /etc/security/limits.conf, for example: 
  1&gt;    # allow user 'LINUX-USER-1' mlockall
  1&gt;    LINUX-USER-1 soft memlock unlimited
  1&gt;    LINUX-USER-1 hard memlock unlimited
  1&gt; [2016-06-25 23:45:08,940][WARN ][org.elasticsearch.bootstrap] If you are logged in interactively, you will have to re-login for the new limits to take effect.
  2&gt; SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
  2&gt; SLF4J: Defaulting to no-operation (NOP) logger implementation
  2&gt; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
  2&gt; REPRODUCE WITH: gradle :plugins:repository-hdfs:test -Dtests.seed=7222BA4654ABC020 -Dtests.class=org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests -Dtests.method="testWriteRead" -Dtests.security.manager=true -Dtests.locale=es-GT -Dtests.timezone=Pacific/Efate
ERROR   0.51s | HdfsBlobStoreContainerTests.testWriteRead &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.security.AccessControlException: access denied ("java.io.FilePermission" "/home/LINUX-USER-1/Repositories/elasticsearch/plugins/repository-hdfs/build/testrun/test/J0" "read")
   &gt;    at __randomizedtesting.SeedInfo.seed([7222BA4654ABC020:83AF2BFFD416B810]:0)
   &gt;    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
   &gt;    at java.security.AccessController.checkPermission(AccessController.java:884)
   &gt;    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
   &gt;    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
   &gt;    at java.io.File.exists(File.java:814)
   &gt;    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:517)
   &gt;    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:504)
   &gt;    at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1064)
   &gt;    at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:161)
   &gt;    at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)
   &gt;    at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:730)
   &gt;    at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:726)
   &gt;    at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
   &gt;    at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:733)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore$2.run(HdfsBlobStore.java:66)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore$2.run(HdfsBlobStore.java:63)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore$4.run(HdfsBlobStore.java:136)
   &gt;    at java.security.AccessController.doPrivileged(Native Method)
   &gt;    at java.security.AccessController.doPrivileged(AccessController.java:713)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore.execute(HdfsBlobStore.java:133)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore.mkdirs(HdfsBlobStore.java:63)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStore.&lt;init&gt;(HdfsBlobStore.java:56)
   &gt;    at org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests.newBlobStore(HdfsBlobStoreContainerTests.java:53)
   &gt;    at org.elasticsearch.repositories.ESBlobStoreContainerTestCase.testWriteRead(ESBlobStoreContainerTestCase.java:49)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/LINUX-USER-1/Repositories/elasticsearch/plugins/repository-hdfs/build/testrun/test/J0/temp/org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests_7222BA4654ABC020-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {}, docValues:{}, maxPointsInLeafNode=1614, maxMBSortInHeap=7.760738480461883, sim=ClassicSimilarity, locale=es-GT, timezone=Pacific/Efate
  2&gt; NOTE: Linux 3.19.0-61-generic amd64/Oracle Corporation 1.8.0_91 (64-bit)/cpus=4,threads=1,free=452914336,total=514850816
  2&gt; NOTE: All tests run in this JVM: [HdfsBlobStoreContainerTests]
Completed [1/1] in 2.27s, 1 test, 1 error &lt;&lt;&lt; FAILURES!

Tests with failures:
  - org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests.testWriteRead

   [junit4] JVM J0:     0.66 ..     3.76 =     3.10s
   [junit4] Execution time total: 3.80 sec.
   [junit4] Tests summary: 1 suite, 1 test, 1 error
:plugins:repository-hdfs:test FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':plugins:repository-hdfs:test'.
&gt; There were test failures: 1 suite, 1 test, 1 error [seed: 7222BA4654ABC020]

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 34.647 secs

BUILD SUCCESSFUL
Total time: 4 seconds
```

Attached the output as a file for future reference too: [debug.txt](https://github.com/elastic/elasticsearch/files/333412/debug.txt)
</comment><comment author="gfyoung" created="2016-06-28T09:14:31Z" id="228995567">Finally!  Tests have been successfully added for **ALL** relevant `BlobContainers`.  This is ready for review.
</comment><comment author="abeyad" created="2016-06-28T21:03:58Z" id="229182910">@gfyoung This is looking great!  Thanks for all your hard work and persistence.  I left a few comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobStoreContainerTests.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobContainer.java</file><file>plugins/repository-hdfs/src/test/java/org/elasticsearch/repositories/hdfs/HdfsBlobStoreContainerTests.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockAmazonS3.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStoreContainerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreContainerTestCase.java</file></files><comments><comment>Revert "Raised IOException on deleteBlob (#18815)"</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java</file><file>plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobStoreContainerTests.java</file><file>plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobContainer.java</file><file>plugins/repository-hdfs/src/test/java/org/elasticsearch/repositories/hdfs/HdfsBlobStoreContainerTests.java</file><file>plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockAmazonS3.java</file><file>plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStoreContainerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/repositories/ESBlobStoreContainerTestCase.java</file></files><comments><comment>Raised IOException on deleteBlob (#18815)</comment></comments></commit></commits></item><item><title>[Shield] Exists Filter Does Not Work when Field Level Security is On</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18814</link><project id="" key="" /><description>Sorry if it's not the right place for shield-related tickets, but I haven't found any other place to submit issues on shield. 
Exists query does not work if I turn on field level security in Shield. 
Steps to reproduce:

Shield role setting: 

```
 test_index_user:
       cluster: monitor
      indices:
       'test_index':
          privileges: read,manage
          fields:
           - Date
           - Sales
           - product_group
           - year
           - product_category
           - sku
```

Example data:
     {
          "Sales": 148850,
          "year": 2001,
          "product_group": "Books",
          "sku": "KA111115",
          "Date": "2013-09-17T05:07:00.000Z",
          "product_category": "Arts"
        }, 
   {
          "Sales": 148850,
          "year": 2015,
          "product_group": "Books",
          "sku": "KA111115",
          "Date": "2013-09-17T05:07:00.000Z",
          "product_category": "Arts"
        }
If I sent  this query -- I see groups by year: 
{
   "aggs" : {
        "years" : {
            "terms" : { "field" : "year" }
        }
    }
}
If I add exists filter -- I get empty result

{
  "query": {
    "bool": {
      "must": [
        {
          "exists": {
            "field": "year" 
          }
        }
      ]
    }
  },
   "aggs" : {
        "years" : {
            "terms" : { "field" : "year" }
        }
    }
}
Could you please advice if it's expected behavior or shield issue that should be fixed.
</description><key id="159641361">18814</key><summary>[Shield] Exists Filter Does Not Work when Field Level Security is On</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yana2301</reporter><labels /><created>2016-06-10T13:59:38Z</created><updated>2016-06-14T11:19:34Z</updated><resolved>2016-06-14T11:19:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-06-13T17:39:58Z" id="225654141">@yana2301 Thanks for reporting! This is indeed a bug, which will be fixed in the next release. 

As a workaround for now, if you add `_field_names` to the `fields` section in the `elasticseach.yml` then I expect the `exists` filter/query to work again.
</comment><comment author="clintongormley" created="2016-06-14T11:19:34Z" id="225851914">Nothing further to do here. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removed duplicate deleteBlob methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18813</link><project id="" key="" /><description>Title is self-explanatory.  Closes #18529.
</description><key id="159607504">18813</key><summary>Removed duplicate deleteBlob methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gfyoung</reporter><labels><label>:Snapshot/Restore</label><label>non-issue</label><label>v5.0.0-alpha5</label></labels><created>2016-06-10T10:39:07Z</created><updated>2016-07-29T11:14:26Z</updated><resolved>2016-07-13T18:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-10T15:06:33Z" id="225208218">@gfyoung I reviewed the code and it looks good, I put one minor comment.  Also, it would be better to have a more descriptive commit message.  The title for the commit message is good, but there should be a body for the commit message enumerating the two methods that were removed from the interface and why they were removed (i.e. explaining that its a cleaner interface to have just one method, and we gain nothing by the other two methods as they don't afford us atomic deletes anyway).  

Thanks for taking this on!
</comment><comment author="gfyoung" created="2016-06-10T15:15:54Z" id="225210913">@abeyad : Certainly.  I've updated the commit message for the duplicate `deleteBlob` method removal, and added a more descriptive error message as requested.  Ready to merge if there are no other concerns.
</comment><comment author="abeyad" created="2016-06-10T15:19:45Z" id="225211986">LGTM
</comment><comment author="tlrx" created="2016-06-10T18:54:54Z" id="225266480">I left a comment.
</comment><comment author="s1monw" created="2016-06-10T19:01:03Z" id="225267989">awesome cleanup - I think we need a note in the migration guide and should deprecate these methods in 2.x @abeyad can you take care of this once it's in?
</comment><comment author="abeyad" created="2016-06-10T19:03:04Z" id="225268494">@s1monw will do
</comment><comment author="imotov" created="2016-06-10T19:36:24Z" id="225275874">-1. As @tlrx said before this will have a significant negative impact on performance of the delete operation on slow repositories. I think we should instead take advantage batch deletes on S3 the same way [as we do it on GCE](https://github.com/elastic/elasticsearch/blob/e7eb664c78dce2451dbeb55db0c0b761a65b295b/plugins/repository-gcs/src/main/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobStore.java#L250). Snapshot deletion can lead to potential deletion of a large number of files at ones and we will have to wait for roundtrip for each operation before preceding to delete the next file.
</comment><comment author="s1monw" created="2016-06-10T19:46:31Z" id="225278077">&gt; -1. As @tlrx said before this will have a significant negative impact on performance of the delete operation on slow repositories. I think we should instead take advantage batch deletes on S3 the same way as we do it on GCE. Snapshot deletion can lead to potential deletion of a large number of files at ones and we will have to wait for roundtrip for each operation before preceding to delete the next file.

what is significant? 1 second per file? 100ms per fiel? I think API simplicity should be preferred compared to batching. The interfaces we use for S/R are extremely polluted with optimizations I really wonder if it's worth it.
</comment><comment author="imotov" created="2016-06-10T20:31:02Z" id="225287835">&gt; what is significant? 

I would expect the latency for the delete request to be anywhere between 30ms and 300ms per file depending on location. Assuming, that we delete a snapshot with 100 shards, 20 files per shard with average latency of 50ms per file, we are talking about the difference between 5 sec and 1 minute 40 sec to delete the snapshot. 
</comment><comment author="tlrx" created="2016-06-13T06:16:32Z" id="225497178">&gt; -1. As @tlrx said before this will have a significant negative impact on performance of the delete operation on slow repositories.

@imotov That's right, I agree with this statement. But what convinced me to remove the multiple variations of `deleteBlob` is that batch deletions make exception and error handling complex. Now we have a Task API I think it is OK to have longer snapshot deletion as long as we can regularly check the progress.
</comment><comment author="imotov" created="2016-06-13T13:55:14Z" id="225587758">@tlrx not sure I see how Task API would help with potential 10-20 times slow down in snapshot deletion speed. We can get the performance back by running deletes in parallel but that will increase complexity on other layers and might start triggering throttling on some providers that are not happy with too many small simultaneous requests rushing in. I don't see how this is a good change for S3 and GCE plugins, but if you and @dadoonet like it, I am not going to argue.
</comment><comment author="dadoonet" created="2016-06-13T19:48:55Z" id="225688030">I was looking at this today. Actually this `deleteBlobs()` method has been added because of #12697.
If we remove this method, we can close #12697 as it won't be implemented.

We can remove deleteBlobs for now and reimplement if we need to optimize at some point. It will be easier than years ago because plugins and core code lives now in the same repo.

So I'm not against this cleanup.
</comment><comment author="gfyoung" created="2016-06-15T23:28:03Z" id="226349210">Okay, let's quickly summarise what's going on here (easier for myself to understand):

@abeyad proposed the change to simplify the interface &lt;a href="https://github.com/elastic/elasticsearch/issues/18529"&gt;here&lt;/a&gt; and has given the green light to this PR

@imotov had concerns about latency for S3 and GCE but would not object if @tlrx and @dadoonet both gave the green light to this PR

@tlrx did not believe the latency impact would be too significant to outweigh the benefits of a simplified interface and AFAIU has approved this change

@dadoonet also appears to have given the green light for this change AFAIU

So...can we merge this :question: 
</comment><comment author="tlrx" created="2016-06-16T06:09:29Z" id="226396401">&gt; @tlrx did not believe the latency impact would be too significant and AFAIU has approved this change

Sorry, that's not what I said. I agree with @imotov and I also think that this will have a significant negative impact on performance. On the other hand, Ali, Simon and Robert have the argument that this change make the API cleaner and easier to maintain. I do agree with this too, as well as error handling in batch requests can be tricky to handle correctly too. These points make me think that we can sacrifice some perf at snapshot deletion time in favor of cleaner code if that make our life (and user's life too) better. That's why I'm OK with this change.
</comment><comment author="gfyoung" created="2016-06-16T06:19:13Z" id="226397748">@tlrx : updated my comment to clarify based on what you said

My question still stands though.
</comment><comment author="s1monw" created="2016-06-20T08:02:50Z" id="227074963">maybe we can add a compromise here and try to interpret a wildcard suffix. If somebody calls `delete("foo*")` we try to fetch all files matching the prefix / wildcard and delete them? This would simplify the interface and we can make batching an impl detail?
</comment><comment author="abeyad" created="2016-06-20T14:08:21Z" id="227152460">+1

On Mon, Jun 20, 2016 at 4:03 AM, Simon Willnauer notifications@github.com
wrote:

&gt; maybe we can add a compromise here and try to interpret a wildcard suffix.
&gt; If somebody calls delete("foo*") we try to fetch all files matching the
&gt; prefix / wildcard and delete them? This would simplify the interface and we
&gt; can make batching an impl detail?
&gt; 
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/18813#issuecomment-227074963,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe/ABjkQdbQMqLKI1iNJ04JkBAp9yAsXxXcks5qNklngaJpZM4Iy1Kv
&gt; .
</comment><comment author="gfyoung" created="2016-06-21T08:02:48Z" id="227368691">@s1monw : integrating `deleteBlobsByPrefix` functionality into `deleteBlobs` sounds like an interesting idea.  Depending on how much traction it gets, I wonder though if it might be best served for a separate PR as follow-up?
</comment><comment author="abeyad" created="2016-06-27T21:08:06Z" id="228875991">@gfyoung Sorry I just realized you didn't get a response to this.  Since we are doing the cleanup in this PR, I think it makes more sense to have a deleteBlob with potentially a wildcard prefix string passed in for this PR, because then we won't be getting rid of the bulk updates code, only to reinsert it again in another PR.  Does that make sense?
</comment><comment author="gfyoung" created="2016-06-30T01:52:33Z" id="229539035">@abeyad : if you read your explanation again, you didn't really provide justification for doing it now.

On second look, while the idea proposed by @s1monw is interesting, if you look at how `deleteBlobsByPrefix` is implemented in `AbstractBlobContainer`, you can see that it's really just a `for-loop` in most cases that calls `deleteBlob` in the process.  Suddenly AFAICT, integration is not so simple.  

Unless we have functionality that can do `regex` deletions (which I don't think we can do across ALL implementations), essentially we will have to go through some gymnastics of refactoring the current `deleteBlob` code to make sure that it checks whether we have a wildcard and then somehow integrate a for-loop?

Consequently, my thought process is shifting towards just removing `deleteBlobs` but leaving `deleteBlobsByPrefix` unless a nice, more suitable solution can be proposed instead of the one I just described.
</comment><comment author="s1monw" created="2016-06-30T07:04:57Z" id="229577046">@imotov will this solve you objection ^^
</comment><comment author="imotov" created="2016-07-06T19:30:24Z" id="230881225">Sorry for the delay, I missed this ping somehow. I think `deleteBlobsByPrefix` is a leftover from the times when a blob store was used by gateways and it's not actually used by snapshot/restore. Therefore I don't see how we would be able to take advantage of support for wildcards in `deleteBlob` unless it also supports listing blobs using comma-separated syntax or full regex syntax, which we could use to list multiple blob files in a single call.

Anyway, we just doubled the time and increased by 10-50 times the price of a snapshot deletion on S3 by implementing file existence check in #18815 for a similar gain in resiliency. So, my objections here over loosing _potential_ savings on S3 don't seem to make much sense any more.
</comment><comment author="gfyoung" created="2016-07-06T19:41:18Z" id="230883966">@imotov: Actually, #18815 (also my PR) got reverted but will be re-merged once a nagging test failure has been fixed.  Not sure if that changes your stance.

IINM, are you no longer objecting to the deletions (i.e. we can even remove the byPrefix method as has already been done)?
</comment><comment author="gfyoung" created="2016-07-13T14:34:08Z" id="232374317">@imotov : any response to my comments?

@Everyone : can this be merged if there are no other concerns?
</comment><comment author="abeyad" created="2016-07-13T17:43:31Z" id="232431916">Since none of the current blob container implementations take advantage of batching with deleteBlobsByPrefix, the only one that will is @tlrx 's Google implementation.  As discussed with @s1monw, we will merge this PR now and @tlrx can add an optional wildcard suffix to the single `deleteBlob` interface so batching can be done under the hood in working on the Google storage blob container implementation.
</comment><comment author="abeyad" created="2016-07-13T19:58:00Z" id="232468686">@gfyoung thanks for your work on this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java</file><file>core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java</file><file>core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>plugins/repository-gcs/src/main/java/org/elasticsearch/common/blobstore/gcs/GoogleCloudStorageBlobContainer.java</file></files><comments><comment>Removed duplicate deleteBlob methods (#18813)</comment></comments></commit></commits></item><item><title>remove trailing whitespace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18812</link><project id="" key="" /><description>As most modern text editors remove trailing whitespaces automatically, this just becomes annoying when working with configuration management and other systems. 
</description><key id="159579577">18812</key><summary>remove trailing whitespace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kazgurs</reporter><labels /><created>2016-06-10T07:58:05Z</created><updated>2016-06-10T08:01:16Z</updated><resolved>2016-06-10T08:01:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-10T08:01:16Z" id="225117832">thanks for submitting - the 1.7 branch is not necessarily continued and this problem doesn't exist in master. I also don't think you want to go and sign the CLA for a whitespace change. I will just go and remove that whitespace myself and close this PR instead. Thanks to taking a look anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pluralize "index"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18811</link><project id="" key="" /><description>This doesn't just happen to "an index" unless you're restoring just one.  It reads better this way, IMO.
</description><key id="159533542">18811</key><summary>Pluralize "index"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">untergeek</reporter><labels /><created>2016-06-09T23:52:40Z</created><updated>2016-06-13T18:04:20Z</updated><resolved>2016-06-13T18:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Pluralize "index" (#18811)</comment></comments></commit></commits></item><item><title>Add Functions to Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18810</link><project id="" key="" /><description>Users can now define their own equivalent to static Java methods inside of a script.
</description><key id="159518946">18810</key><summary>Add Functions to Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T22:03:56Z</created><updated>2016-06-10T21:04:40Z</updated><resolved>2016-06-10T21:03:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-10T20:44:08Z" id="225290766">looks great, +1
</comment><comment author="jdconrad" created="2016-06-10T21:04:40Z" id="225295375">@rmuir Thanks for the review!  (This was merged via squash to get rid of worthless commit messages.)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Analyzer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Compiler.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Locals.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Writer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParser.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserBaseVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/PainlessParserVisitor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/antlr/Walker.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ALink.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/AStatement.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBinary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBool.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EBoolean.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ECast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EChain.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EComp.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConditional.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EConstant.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EDecimal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EExplicit.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EFunctionRef.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENull.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/ENumeric.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/EUnary.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LArrayLength.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LBrace.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallInvoke.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCallLocal.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LCast.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefCall.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LDefField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LField.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LListShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LMapShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewArray.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LNewObj.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LShortcut.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LStatic.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LString.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/LVariable.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SBreak.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SCatch.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SContinue.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclBlock.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDeclaration.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SDo.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SEach.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SExpression.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFor.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SFunction.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIf.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SIfElse.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SReturn.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SSource.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SThrow.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/STry.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/SWhile.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/node/package-info.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/FunctionTests.java</file><file>modules/lang-painless/src/test/java/org/elasticsearch/painless/ReservedWordTests.java</file></files><comments><comment>Add functions to Painless.</comment></comments></commit></commits></item><item><title>Better error message when mapping configures null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18809</link><project id="" key="" /><description>Improve the error message when the user sends something like:

``` js
"analyzer": null
```

Closes #18803
</description><key id="159487315">18809</key><summary>Better error message when mapping configures null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T19:21:27Z</created><updated>2016-06-10T13:44:04Z</updated><resolved>2016-06-10T13:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-09T19:21:44Z" id="224999161">@jpountz, can you have a look at this?
</comment><comment author="jpountz" created="2016-06-09T20:52:54Z" id="225022915">LGTM
</comment><comment author="nik9000" created="2016-06-10T13:44:03Z" id="225185469">Thanks for reviewing @jpountz !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make random UUIDs reproducible in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18808</link><project id="" key="" /><description>Today we use a random source of UUIDs for assigning allocation IDs,
cluster IDs, etc. Yet, the source of randomness for this is not
reproducible in tests. Since allocation IDs end up as keys in hash maps,
this means allocation decisions and not reproducible in tests and this
leads to non-reproducible test failures. This commit modifies the
behavior of random UUIDs so that they are reproducible under tests. The
behavior for production code is not changed, we still use a true source
of secure randomness but under tests we just use a reproducible source
of non-secure randomness.

It is important to note that there is a test,
UUIDTests#testThreadedRandomUUID that relies on the UUIDs being truly
random. Thus, we have to modify the setup for this test to use a true
source of randomness. Thus, this is one test that will never be
reproducible but it is intentionally so.
</description><key id="159476688">18808</key><summary>Make random UUIDs reproducible in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>review</label><label>test</label></labels><created>2016-06-09T18:29:47Z</created><updated>2016-06-14T07:39:06Z</updated><resolved>2016-06-10T14:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-09T18:30:41Z" id="224985267">@ywelsch This should prevent the non-reproducible test failure that you experienced from being non-reproducible in the future.
</comment><comment author="nik9000" created="2016-06-09T18:44:12Z" id="224989082">I like it but I think @ywelsch should probably look too?
</comment><comment author="nik9000" created="2016-06-09T20:29:07Z" id="225016419">Still LGTM. If you the @ywelsch will want to look before merging it then wait, otherwise merge when ready.
</comment><comment author="jasontedor" created="2016-06-09T20:31:47Z" id="225017186">Thanks @nik9000. I will wait for @ywelsch to take a peek.
</comment><comment author="ywelsch" created="2016-06-10T08:47:56Z" id="225126626">@jasontedor thanks so much for finding the cause of non-reproducibility for some of the tests 💃.

I have a few questions about the current implementation. For tests, we're creating now a new `SecureRandom` instance per freshly generated uuid string. This new instance is seeded based on non-secure randomness. We use this new instance exactly once and throw it away.
1) Is it maybe good enough for our purposes to fall back to non-secure randomness in tests to create UUIDs? The UUIDs which are addressed by this PR are not used as document ids but cluster state - related things, which don't change that frequently (`j.u.Random` is internally only based on 48 bits while the constructed `SecureRandom` instance has 160 bits).
2) Should we try to emulate what the randomized testing framework does with their randomness and have a per-thread instance?
</comment><comment author="jasontedor" created="2016-06-10T12:24:53Z" id="225168269">&gt; 1) Is it maybe good enough for our purposes to fall back to non-secure randomness in tests to create UUIDs? The UUIDs which are addressed by this PR are not used as document ids but cluster state - related things, which don't change that frequently (`j.u.Random` is internally only based on 48 bits while the constructed `SecureRandom` instance has 160 bits).
&gt; 2) Should we try to emulate what the randomized testing framework does with their randomness and have a per-thread instance?

@ywelsch I considered both of these options. I did not think that we should do the latter, I think it's overkill considering that this is the only place that we need it. I considered the former and the only reason that I decided against it is so that the method could have a return type of `SecureRandom`. What do you think?
</comment><comment author="ywelsch" created="2016-06-10T12:50:47Z" id="225173411">I agree that the second option is too complicated for what we need. I would prefer option 1, the tradeoff being that the return type is not `SecureRandom` but the super-type `Random`. With proper documentation that should be fine.
</comment><comment author="jasontedor" created="2016-06-10T14:01:38Z" id="225189959">@ywelsch I pushed 1f308405298af0537c33a347bdfb252adc72acc3.
</comment><comment author="ywelsch" created="2016-06-10T14:12:19Z" id="225192711">Left a comment on documentation and an optional one on naming. Thanks @jasontedor. LGTM.
</comment><comment author="s1monw" created="2016-06-13T09:24:51Z" id="225530806">@jasontedor I reverted this commit since it caused the same UUIDs to be generated which confused the diff logic on the clusterstate updates. We had situations in some tests that forced master reelection that we generated the same UUID for the clusterstates which confused the diff logic and essentially merges 2 clusterstates that is should not merge:

```
  1&gt; [2016-06-10 15:16:52,701][DEBUG][org.elasticsearch.cluster.service] [node_t3] processing [zen-disco-join(elected_as_master, [3] joins received)]: took [26ms] done applying updated cluster_state (version: 1, uuid: 4_TvEWvyRSqhVDOwy96hyQ) &lt;==== node_t3 is master with CS version 1

  1&gt; [2016-06-10 15:16:55,801][DEBUG][org.elasticsearch.cluster.service] [node_t2] processing [zen-disco-join(elected_as_master, [2] joins received)]: took [41ms] done applying updated cluster_state (version: 2, uuid: 4_TvEWvyRSqhVDOwy96hyQ) &lt;==== node_t2 is master (node_t3 left) with CS version 2 and the same UUID!!!!!

  1&gt; [2016-06-10 15:16:55,826][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [zen-disco-receive(from master [master {node_t3}{4JJp7ZHuSBSTzAwL7kz_XQ}{127.0.0.1}{127.0.0.1:30229} committed version [3]])]: took [21ms] done applying updated cluster_state (version: 3, uuid: 2M4P_YdbQJS33PXUT0GFNw)  &lt;==== node_t0 thinks node_t3 is still master since it merged state 1 and 3
```

I think we need to be more careful here with respect to randomness but I think we should also look into hardening out diff logic here just to be sure we are not messing things up in such a situation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/RandomBasedUUIDGenerator.java</file><file>core/src/main/java/org/elasticsearch/common/Randomness.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterChangedEventTests.java</file><file>core/src/test/java/org/elasticsearch/common/UUIDTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/ReindexSameIndexTests.java</file></files><comments><comment>Make random UUIDs reproducible in tests</comment></comments></commit></commits></item><item><title>Add whitelisted classes groovy.json.JSONOutput </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18807</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3

**JVM version**:  openjdk version "1.8.0_91"
OpenJDK Runtime Environment (build 1.8.0_91-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**: centOS 7

**Description of the problem including expected versus actual behavior**:

I'm having the exact same issue of this [thread](https://discuss.elastic.co/t/webhook-action-hits-collection-not-json/24568): Basically watcher ctx is giving me a java.util.HashMap object and I need a json object.

So, I try to implement the solution but I get the error described in the log section (Summarizing, ScriptException[failed to run inline script [return [ body: groovy.json.JsonOutput.toJson(ctx  .payload.hits.hits)]] using lang [groovy]]; nested: MissingPropertyException[No such property: groovy for class). 

Doing more research, I found this [page](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-security.html) about the security feature of whitelisting classes. I guess i can whitelist in my classloader whitelist in the java security policy, but why not producing json output from elasticsearch scripting for default?

**Steps to reproduce**:
1. Using sense, create a watch like [this](https://gist.github.com/rvalenciano/db701603a1df6051dd9315da64505f8f):

**Provide logs (if relevant)**:

The error is described here;

`[2016-06-09 11:12:09,191][INFO ][cluster.metadata         ] [Tower] [.watch_history-2016.06.09] update_mapping [watch_record]
[2016-06-09 11:12:19,102][ERROR][watcher.transform.script ] [Tower] failed to execute [script] transform for [running_process_watch_1-2016-06-09T17:12:19.083Z]
ScriptException[failed to run inline script [return [ body: groovy.json.JsonOutput.toJson(ctx  .payload.hits.hits)]] using lang [groovy]]; nested: MissingPropertyException[No such property: groovy for class: 703695e74fadee477a3e905ccc4a47d0c085cf47];
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:320)
    at org.elasticsearch.watcher.transform.script.ExecutableScriptTransform.doExecute(ExecutableScriptTransform.java:74)
    at org.elasticsearch.watcher.transform.script.ExecutableScriptTransform.execute(ExecutableScriptTransform.java:60)
    at org.elasticsearch.watcher.transform.script.ExecutableScriptTransform.execute(ExecutableScriptTransform.java:41)
    at org.elasticsearch.watcher.execution.ExecutionService.executeInner(ExecutionService.java:378)
    at org.elasticsearch.watcher.execution.ExecutionService.execute(ExecutionService.java:273)
    at org.elasticsearch.watcher.execution.ExecutionService$WatchExecutionTask.run(ExecutionService.java:438)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: groovy.lang.MissingPropertyException: No such property: groovy for class: 703695e74fadee477a3e905ccc4a47d0c085cf47
    at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:53)
    at org.codehaus.groovy.vmplugin.v7.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:177)
    at 703695e74fadee477a3e905ccc4a47d0c085cf47.run(703695e74fadee477a3e905ccc4a47d0c085cf47:1)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:313)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:310) 
`

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

Is there any possibility for whitelisting groovy.json.JSONOutput in the default class wishlist? An example of this is in this [fork](https://github.com/rmuir/elasticsearch/commit/79467a138a0f435ff3776216aa8df0364787ad00).
</description><key id="159467384">18807</key><summary>Add whitelisted classes groovy.json.JSONOutput </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rvalenciano</reporter><labels /><created>2016-06-09T17:44:33Z</created><updated>2016-09-22T08:35:03Z</updated><resolved>2016-06-09T17:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-09T17:53:22Z" id="224974704">The Groovy dependency has been reduced from the `groovy-all` artifact to `groovy` (just the core). The class `groovy.json.JsonOutput` is no longer available with the `lang-groovy` module so we will not be whitelisting this class. 

Relates #16858
</comment><comment author="rvalenciano" created="2016-06-09T18:05:53Z" id="224978388">@jasontedor , quick question, any other possible way to retrieve a json instead of a java.util.HashMap response from watcher ctx? Or do I have to program using groovy core a parser from HashMap to json?
</comment><comment author="jasontedor" created="2016-06-10T19:48:42Z" id="225278599">@rvalenciano I think that it's best to ask that question on the [Elastic Discourse forums](https://discuss.elastic.co) in the [Watcher category](https://discuss.elastic.co/c/watcher).
</comment><comment author="rvalenciano" created="2016-06-10T20:01:10Z" id="225281252">Thanks @jasontedor 
</comment><comment author="kousiknandy" created="2016-09-22T08:35:02Z" id="248842943">FWIW, I had the same problem, and had to drop [groovy-json-2.4.6.jar](http://central.maven.org/maven2/org/codehaus/groovy/groovy-json/2.4.6/groovy-json-2.4.6.jar) in the ${ELASTICSEARCH_HOME}/modules/lang-groovy/ directory for it to work.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Squash a race condition in RefreshListeners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18806</link><project id="" key="" /><description>It presented as listeners never being called if you refresh at the same
time as the listener is added. It was caught rarely by
testConcurrentRefresh. mostly this is removing code and adding a comment:

```
Note that it is not safe for us to abort early if we haven't advanced the
position here because we set and read lastRefreshedLocation outside of a
synchronized block. We do that so that waiting for a refresh that has
already passed is just a volatile read but the cost is that any check
whether or not we've advanced the position will introduce a race between
adding the listener and the position check. We could work around this by
moving this assignment into the synchronized block below and double
checking lastRefreshedLocation in addOrNotify's synchronized block but
that doesn't seem worth it given that we already skip this process early
if there aren't any listeners to iterate.
```
</description><key id="159459902">18806</key><summary>Squash a race condition in RefreshListeners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T17:06:14Z</created><updated>2016-06-09T17:54:14Z</updated><resolved>2016-06-09T17:49:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-09T17:07:43Z" id="224961609">@ywelsch: thanks for the tip about adding the 10ms sleep! I'll now try that whenever I'm trying to reproduce threading issues. Good tool.

@martijnvg: it wasn't the ArrayList: it was just me misusing a volatile and creating a different race. Hopefully there aren't issues with the ArrayList....
</comment><comment author="martijnvg" created="2016-06-09T17:46:25Z" id="224972586">@nik9000 LGTM (Happy it wasn't the ArrayList :) )
</comment><comment author="nik9000" created="2016-06-09T17:54:14Z" id="224974970">Thanks for the review @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify that using time zones for date rounding (e.g. histograms) can lead to buckets that are not equally sized </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18805</link><project id="" key="" /><description>I think we should clarify in our documentation that when using arbitrary time intervals together with time zones (e.g. in `date_histogram`), we cannot guarantee that the resulting buckets have exactly the specified interval width when DST changes (or other time zone offset changes) happen. 

As a simple example, when you specify an `interval` like e.g. 12h in a `date_histogram` together with a time zone (e.g. "CET") and then use extended bounds, we get nice buckets that align with 00:00 and 12:00 in local time in that time zone:

```
GET test/doc/_search
{
  "query": { "match": {
    "foo": "some_text"
  }}, 
  "aggs" : {
        "my_hist" : {
            "date_histogram" : {
                "field" : "event_date",
                "interval" : "12h",
                "time_zone": "CET",
                "extended_bounds" : {
                    "min" : "2016-03-26",
                    "max" : "2016-03-29"
                }
            }
        }
    }
}
```

```
"buckets": [
        {
          "key_as_string": "2016-03-26T00:00:00.000+01:00",
          "key": 1458946800000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-26T12:00:00.000+01:00",
          "key": 1458990000000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-27T00:00:00.000+01:00",
          "key": 1459033200000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-27T12:00:00.000+02:00",
          "key": 1459072800000,   &lt;--- note: 1459072800000 - 1459033200000 = 39600000ms, thats 11h
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-28T00:00:00.000+02:00",
          "key": 1459116000000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-28T12:00:00.000+02:00",
          "key": 1459159200000,
          "doc_count": 0
        }
```

However, this example covers the day DST starts in CET (27 Mar 2016, +1h at 02:00) so the "2016-03-27T00:00:00.000+01:00" bucket is only 11h "wide". On DST start we would have the reverse situation where the bucket covers 13h. 

This is something that cannot be avoided when at the same time we want e.g. 6h/12h intervals to align nicely with local time, but I think it is not really clear for every user and they might (incorrectly) rely on the assumption that buckets are equal-sized in their further processing. Maybe this should just be stated in the docs clearly, at least for these relatively simple cases. Otherwise we can only really guarantee fixed-size buckets for fixed UTC offset time zones (like +01:00, -06:00).
</description><key id="159443074">18805</key><summary>Clarify that using time zones for date rounding (e.g. histograms) can lead to buckets that are not equally sized </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>adoptme</label><label>docs</label></labels><created>2016-06-09T15:51:38Z</created><updated>2016-11-16T08:40:07Z</updated><resolved>2016-11-16T08:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-10T07:03:18Z" id="225108549">Agreed this is something we should address with documentation.
</comment><comment author="cbuescher" created="2016-06-10T09:10:35Z" id="225131552">We discussed this on FF and think we should write some documentation about it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Clarify date_histogram bucket sizes for DST time zones</comment></comments></commit></commits></item><item><title>"An" is not used before a long "U" sound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18804</link><project id="" key="" /><description>&lt;!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
--&gt;
- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/.github/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?
</description><key id="159436360">18804</key><summary>"An" is not used before a long "U" sound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">poweld</reporter><labels /><created>2016-06-09T15:25:05Z</created><updated>2016-06-13T18:39:04Z</updated><resolved>2016-06-13T18:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-10T19:43:26Z" id="225277422">@poweld Thanks for the contribution; we need the CLA signed before we can merge.
</comment><comment author="clintongormley" created="2016-06-13T18:39:03Z" id="225670220">While "a ubiquitous" has become more common (see http://i.stack.imgur.com/PwVAZ.png) , "an ubiquitous" is perfectly acceptable (and sounds better to my English ears) 

Given that the CLA hasn't been signed, I'm going to close this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>null_pointer_exception during index mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18803</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 16.04 LTS

**Description of the problem including expected versus actual behavior**:
Error response during index mapping

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "null_pointer_exception",
      "reason": null
   },
   "status": 500
}
```

**Steps to reproduce**:
 1.

```
PUT /haystack/
{
  "settings": {
    "analysis": {
      "filter": {
        "haystack_edgengram": {
          "max_gram": 15,
          "type": "edgeNGram",
          "min_gram": 2
        },
        "haystack_ngram": {
          "max_gram": 15,
          "type": "nGram",
          "min_gram": 3
        }
      },
      "tokenizer": {
        "haystack_ngram_tokenizer": {
          "max_gram": 15,
          "type": "nGram",
          "min_gram": 3
        },
        "haystack_edgengram_tokenizer": {
          "max_gram": 15,
          "type": "edgeNGram",
          "side": "front",
          "min_gram": 2
        }
      },
      "analyzer": {
        "edgengram_analyzer": {
          "filter": [
            "haystack_edgengram",
            "lowercase"
          ],
          "type": "custom",
          "tokenizer": "standard"
        },
        "ngram_analyzer": {
          "filter": [
            "haystack_ngram",
            "lowercase"
          ],
          "type": "custom",
          "tokenizer": "standard"
        }
      }
    }
  }
}
```

 2.

```
PUT /haystack/_mapping/modelresult
{
    "properties": {
      "inside_id": {
        "type": "long"
      },
      "text": {
        "type": "string",
        "analyzer": "edgengram_analyzer"
      },
      "django_ct": {
        "include_in_all": false,
        "index": "not_analyzed",
        "type": "string"
      },
      "type_claster_id": {
        "type": "long"
      },
      "closed_at": {
        "type": "date"
      },
      "is_closed": {
        "type": "boolean"
      },
      "geo_type_region_id": {
        "type": "long"
      },
      "priority": {
        "type": "long"
      },
      "shingles": {
        "type": "string",
        "analyzer": "russian"
      },
      "board": {
        "type": "string",
        "analyzer": "russian"
      },
      "subscribers_names": {
        "type": "string",
        "analyzer": "russian"
      },
      "type": {
        "type": "long"
      },
      "claster_code": {
        "type": "long"
      },
      "views": {
        "type": "long"
      },
      "buy_seo_text": {
        "type": "string",
        "analyzer": "russian"
      },
      "updated_at": {
        "type": "date"
      },
      "geo_type_slug": {
        "type": "string",
        "analyzer": "russian"
      },
      "sell_seo_text": {
        "type": "string",
        "analyzer": "russian"
      },
      "geo_type_id": {
        "type": "long"
      },
      "name": {
        "type": "string",
        "analyzer": "edgengram_analyzer"
      },
      "attached": {
        "type": "boolean"
      },
      "ad_type": {
        "type": "string",
        "analyzer": "russian"
      },
      "filters_ids": {
        "type": "string",
        "analyzer": "russian"
      },
      "subject": {
        "type": "string",
        "analyzer": "russian"
      },
      "django_id": {
        "include_in_all": false,
        "index": "not_analyzed",
        "type": "string"
      },
      "is_approved": {
        "type": "boolean"
      },
      "author": {
        "type": "string",
        "analyzer": "russian"
      },
      "content": {
        "type": "string",
        "analyzer": "russian"
      },
      "views_today": {
        "type": "long"
      },
      "owner_id": {
        "type": "long"
      },
      "body": {
        "type": "string",
        "analyzer": "edgengram_analyzer"
      },
      "description": {
        "type": "string",
        "analyzer": "russian"
      },
      "parent": {
        "type": "long"
      },
      "key": {
        "type": "long"
      },
      "slug": {
        "type": "string",
        "analyzer": "russian"
      },
      "last_activity_at": {
        "type": "date"
      },
      "thread": {
        "type": "string",
        "analyzer": "russian"
      },
      "url": {
        "type": "string",
        "analyzer": null
      },
      "created_at": {
        "type": "date"
      },
      "title": {
        "type": "string",
        "analyzer": "edgengram_analyzer"
      },
      "geo_object": {
        "type": "long"
      },
      "attributed": {
        "type": "boolean"
      },
      "published": {
        "type": "boolean"
      },
      "primary_filter_id": {
        "type": "long"
      },
      "geo_type_region_slug": {
        "type": "string",
        "analyzer": "russian"
      }
    }
  }
```

**Provide logs (if relevant)**:

```
[2016-06-09 17:34:49,914][INFO ][cluster.metadata         ] [Micro] [haystack] creating index, cause [api], templates [], shards [5]/[1], mappings []
[2016-06-09 17:34:50,650][INFO ][cluster.routing.allocation] [Micro] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[haystack][4]] ...]).
[2016-06-09 17:34:57,408][DEBUG][action.admin.indices.mapping.put] [Micro] failed to put mappings on indices [[haystack]], type [modelresult]
java.lang.NullPointerException
    at org.elasticsearch.index.mapper.core.TypeParsers.parseAnalyzersAndTermVectors(TypeParsers.java:211)
    at org.elasticsearch.index.mapper.core.TypeParsers.parseTextField(TypeParsers.java:250)
    at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:161)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:305)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:218)
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:139)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:118)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:99)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:508)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:257)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:230)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:468)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-06-09 17:34:57,409][WARN ][rest.suppressed          ] /haystack/_mapping/modelresult Params: {index=haystack, type=modelresult}
java.lang.NullPointerException
    at org.elasticsearch.index.mapper.core.TypeParsers.parseAnalyzersAndTermVectors(TypeParsers.java:211)
    at org.elasticsearch.index.mapper.core.TypeParsers.parseTextField(TypeParsers.java:250)
    at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:161)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:305)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:218)
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:139)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:118)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:99)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:508)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:257)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:230)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:468)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="159423983">18803</key><summary>null_pointer_exception during index mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">Alkalit</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-06-09T14:36:26Z</created><updated>2016-06-10T13:43:40Z</updated><resolved>2016-06-10T13:43:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-09T18:32:57Z" id="224985886">&gt; "analyzer": null

There is your problem! Send a string there....

I'll add a better error message.
</comment><comment author="Alkalit" created="2016-06-10T07:41:55Z" id="225114475">Thanks alot for quick response. I spent almost a day for this error.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/DateFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TextFieldMapperTests.java</file></files><comments><comment>Better error message when mapping configures null</comment></comments></commit></commits></item><item><title>Add Shrink request source parser to parse create index request body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18802</link><project id="" key="" /><description>This commit enforces that the shrink request body contains only `settings` and `aliases` for underlying create index request

Follow up to https://github.com/elastic/elasticsearch/pull/18732#discussion_r66407196
</description><key id="159421049">18802</key><summary>Add Shrink request source parser to parse create index request body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T14:24:50Z</created><updated>2016-06-15T20:53:44Z</updated><resolved>2016-06-15T20:53:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-10T07:36:14Z" id="225113559">Looks good, I think we need to update documentation too?
</comment><comment author="areek" created="2016-06-10T15:23:06Z" id="225212881">@s1monw the documentation looks appropriate:

```
The `_shrink` API is similar to the &lt;&lt;indices-create-index, `create index` API&gt;&gt;
and accepts `settings` and `aliases` parameters for the target index:
```

do you think it makes sense to add anything to it?
</comment><comment author="clintongormley" created="2016-06-13T18:28:50Z" id="225667500">~~@areek Sorry for the late comment but I think that the rollover API should accept a full create-index body, rather than be limited to settings and aliases, otherwise you have to use index templates regardless.~~
</comment><comment author="clintongormley" created="2016-06-14T14:55:13Z" id="225908146">Sorry, my last comment was meant for the rollover API, not shrink
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/shrink/ShrinkRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestShrinkIndexAction.java</file></files><comments><comment>Merge pull request #18802 from areek/enhancement/shrink_request_parser</comment></comments></commit></commits></item><item><title>Unable to connect to ElasticSearch on a remote server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18801</link><project id="" key="" /><description>**Elasticsearch version**: 5.0

**JVM version**: 1.8

**OS version**: Windows Server 2012 R2

**Description of the problem including expected versus actual behavior**: 
After googling I made some changes to the logging.yml file like changing the network.host: 0, setting http.port, network.bind_host: 0 and also set the max and min heap size to be the same. But still ES works locally and refuses any remote connections.
</description><key id="159419352">18801</key><summary>Unable to connect to ElasticSearch on a remote server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arunkumar987</reporter><labels /><created>2016-06-09T14:17:51Z</created><updated>2016-06-09T14:23:27Z</updated><resolved>2016-06-09T14:23:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-09T14:23:27Z" id="224910179">@arunkumar987 Sorry for the trouble, if you head over to the [Elastic Discourse forums](https://discuss.elastic.co) it's likely that someone from the community can help you. Elastic reserves GitHub for verified bug reports and feature requests, and this looks like a configuration issue. Note that it would be really helpful if you provided some logging and more detail about exactly what you did and are trying to do.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix invalid rounding value for TimeIntervalRounding close to DST transitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18800</link><project id="" key="" /><description>There are edge cases where rounding a date to a certain interval using a time
zone with DST shifts can currently cause the rounded date to be bigger than the
original date. This happens when rounding a date closely after a DST start and
the rounded date falls into the DST gap.

Here is an example for CET time zone, where local time is set forward by one
hour at 2016-03-27T02:00:00+01:00 to 2016-03-27T03:00:00.000+02:00:

The date 2016-03-27T03:01:00.000+02:00 (1459040460000) which is just after the
DST change is first converted to local time (1459047660000). If we then apply
interval rounding for a 14m interval in local time, this  takes us to
1459047240000, which unfortunately falls into the DST gap.  When converting
this back to UTC, joda provides options to throw exceptions on illegal dates
like this, or correct this by adjusting the date to the new time zone offset.
We currently do the later, but this leads to converting this illegal date back
to 2016-03-27T03:54:00.000+02:00 (1459043640000), giving us a date that is
larger than the original date we wanted to round.

This change fixes this by using the "strict" option of 'convertLocalToUTC()'
to detect rounded dates that fall into the DST gap. If this happens, we can use
the time of the DST change instead as the interval start.

Even before this change, intervals around DST shifts like this can be shorter
than the desired interval.  This, for example, happens when the requested
interval width doesn't completely fit into the remaining time span when the DST
shift happens. For example, using a 14m interval in UTC+1 (CET before DST
starts) leads to the following valid rounding values around the time where DST
happens:

2016-03-27T01:30:00+01:00
2016-03-27T01:44:00+01:00
2016-03-27T01:58:00+01:00
2016-03-27T02:12:00+01:00
2016-03-27T02:26:00+01:00
...

while the rounding values in UTC+2 (CET after DST start) are placed like this
around the same time:

2016-03-27T02:40:00+02:00
2016-03-27T02:54:00+02:00
2016-03-27T03:08:00+02:00
2016-03-27T03:22:00+02:00
...

From this we can see then when we switch from UTC+1 to UTC+2 at 02:00 the last
rounding value in UTC+1 is at 01:58 and the first valid one in UTC+2 is at
03:08, so even if we decide to put all the dates in between into one rounding
interval, it will only cover 10 minutes. With this change we choose to use the
moment of DST shift as an aditional interval separator, leaving us with a 2min
interval from [01:58,02:00) before the shift and an 8min interval from
[03:00,03:08) after the shift.

This change also adds tests for the above example and adds randomization to the
existing TimeIntervalRounding tests.
</description><key id="159416100">18800</key><summary>Fix invalid rounding value for TimeIntervalRounding close to DST transitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T14:04:01Z</created><updated>2016-06-14T15:10:46Z</updated><resolved>2016-06-14T14:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-14T10:50:33Z" id="225846417">@cbuescher the documentation change in https://github.com/elastic/elasticsearch/issues/18805 is enough.  
LGTM
</comment><comment author="cbuescher" created="2016-06-14T13:13:51Z" id="225876703">@jimferenczi thanks for the review, I ended up finding another way to detect the edge case without the try-catch logic which I think is much better and added that change in the last commit. Mind to take another quick look only at that change before I merge?
</comment><comment author="jimczi" created="2016-06-14T13:21:26Z" id="225878645">@cbuescher left one minor comment.
LGTM
</comment><comment author="cbuescher" created="2016-06-14T15:10:46Z" id="225913284">Merged to 2.x branch with 780cfb1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file></files><comments><comment>Merge pull request #18800 from cbuescher/fix-interval-rounding-uneven</comment></comments></commit></commits></item><item><title>es5 alpha fails to make progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18799</link><project id="" key="" /><description>**Elasticsearch version**:

5.0.0 alpha1/2/3 and v5.0.0-alpha3-216-g6202b63

**JVM version**:

java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)

**OS version**:

Linux 107r01pc 4.2.0-36-generic # 42~14.04.1-Ubuntu SMP Fri May 13 17:27:22 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

A cluster with 3 data nodes and an index with 2 replicas can become
unavailable with the right order of node restarts. Other orderings of
restarts result in eventual availability.

I expected that 2 data nodes are always enough to service an index with
replication factor 3.

Other issues can be reproduced by very similar steps:
- with unsafe minimum master nodes restarts can cause data loss
- when only a quorum of the replicas are present incomplete replicas
  won't get fixed. it would be nice to see earlier replica healing.
  It seems to be safer and easier to move replicas if every working
  quorum of shards would heal themselves, as the missing nodes may
  crashed hard, and couldn't be restarted in any way.

**Steps to reproduce**:

There are some scripts to reproduce these issues, but attaching them is
not allowed, so, please, see
https://github.com/SweetNSourPavement/es5issue.

The gist of the modus operandi is:
1. create an index with replication 3
2. shut down one data node
3. insert a doc, which should be successful, as a quorum is present
4. stop and start nodes
5. observe the result
</description><key id="159397019">18799</key><summary>es5 alpha fails to make progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SweetNSourPavement</reporter><labels><label>:Cluster</label></labels><created>2016-06-09T12:36:29Z</created><updated>2017-05-09T08:15:24Z</updated><resolved>2016-06-17T19:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-13T17:49:28Z" id="225656657">@ywelsch could you take a look at this please
</comment><comment author="ywelsch" created="2016-06-14T09:30:21Z" id="225828390">@SweetNSourPavement thanks for your interest in testing the ES 5.0 alpha releases and reporting back to us. Let me give some explanations for the observations you've made with ES. Setting `minimum_master_nodes` to a wrong value will indeed lead to data loss (I will later in this post explain why). We've taken a number of steps to make users aware of this:
- The default configuration has a section that talks about "split brain" and the setting `minimum_master_nodes`, pointing to our documentation.
- Since v5.0.0, we enforce that `minimum_master_nodes` is explicitly set in production mode (#17288)
- Since v5.0.0, we log warnings when we detect that `minimum_master_nodes` is set to less than a quorum of master-eligible nodes in the cluster (#15625)
- We are also discussing stricter rules to enforce that `minimum_master_nodes` is set to a quorum of master-eligible nodes (#18573).

The reason data loss occurs in case of split brains has to do with how we replicate data in ES. Data replication in ES is based on the primary/backup model. This means that there is a single shard copy that is the source of truth (called `the primary`), replicating it's contents to other shard copies. In case of the primary failing, one of the secondary copies is chosen as the new primary. Recovery on cluster restart is simple. The master choses a non-stale copy of the data as primary. The other shard copies simply resync their data from the new primary. As you might have noticed in this description, ES does not do quorum-based recoveries (it does not look at multiple shard copies to determine what data should be part of the new primary). The advantage of primary/backup replication is that this step is not required, making it a simple yet powerful model that also works in scenarios where users want to have only 2 copies of the data.

Let me explain now what happened in the specific scenario which you outlined above: node 1 (when it was shut down) thinks that all three (empty at that point) shard copies of the data are non-stale. After node 1 left, both node 2 and node 3 know that only their shard copies are good (and the copy of node 1 is stale). When restarting the cluster, node 1 gets up first, however, and elects itself as master (before node 2 and 3 are up). This can happen because `minimum_master_nodes` is wrongly set to 1. Node 1 thinks that it’s own copy of the data is good and makes it the primary shard. Node 2 and node 3 then join the cluster and resync their copies of the data from the primary, effectively removing the indexed document.

The other scenarios you describe can be explained in the same way and follow directly from the data replication model we use. If you want me to clarify on some points, I'm happy to explain further.
</comment><comment author="SweetNSourPavement" created="2016-06-14T13:21:25Z" id="225878638">Thank you for the explanation. My theories, which you now confirmed, led me to poke around replica quorums, and after realizing that this data loss scenario kinda works like intended I probed on.

The main issue I try to raise here is the inability of es5 to recover an index, even in a seemingly healthy enough state. The script `progress`, when run with parameter `red` configures the cluster correctly, I think:
- has 3 master nodes
- all masters stay up all the time, so they have the ability to witness all
- `minimum_master_nodes` is 2
- has 3 data nodes
- 2 data nodes are running when queries are made

and yet, the index state remains red.
</comment><comment author="ywelsch" created="2016-06-14T13:46:37Z" id="225885643">The index state remaining red is expected behavior in this case. When the data nodes shut down one after the other, only the last one will be seen as having a non-stale copy of the data. The reason is that the master nodes can only witness nodes leaving the cluster but not data being written to the shards. Assume for example that after the first two data nodes have left the cluster a document is indexed (and acknowledged) into the shard on the third node just before that one is shut down. To prevent data loss, the master nodes can only resurrect the shard on the third node as primary.
</comment><comment author="SweetNSourPavement" created="2016-06-14T14:58:13Z" id="225909199">I'm confused now. The expected behaviour is this:
- There's no guarantee that an index remains available (in the long run) if nodes stop and start?
- If an index remains unavailable, the best solution is to stop and start nodes until the correct start order is hit upon?

What am I missing? Availability seems not that high when an _expected_ single hard failure can cause unbounded unavailability, and human intervention is required.
</comment><comment author="ywelsch" created="2016-06-14T15:13:24Z" id="225914189">A single hard failure does not cause unbounded unavailability. In your scenario, there was a moment where 2 out of the 3 data nodes were unavailable (when restarted). This means that only the data on the third node was non-stale. Now by shutting down the third node and starting only the first two nodes this single remaining "good" copy became unavailable to the cluster...

The scenario with 3 data nodes and 3 shard copies supports two nodes to fail and the data to still be available.
</comment><comment author="SweetNSourPavement" created="2016-06-14T15:19:26Z" id="225916162">Only 1 of the data nodes is stopped, the other 2 are up and running, so the write should be a quorum write.
</comment><comment author="clintongormley" created="2016-06-14T15:23:29Z" id="225917462">@SweetNSourPavement That would be the case if all 3 nodes were running then one left.  However, you are recovering from scratch and the node holding the freshest shard copy has not yet appeared, which means that you might lose data unless you wait for it.

If that node is never coming back, there is an API which allows you to choose one of the stale shard copies as the new primary and to recover anyway, but you run the risk of losing data.
</comment><comment author="ywelsch" created="2016-06-14T15:27:32Z" id="225918819">&gt; Only 1 of the data nodes is stopped, the other 2 are up and running, so the write should be a quorum write.

The two remaining data nodes are not stopped in perfect synchrony. There is a short moment in time where a write request can sneak in. Write requests do not have to be quorum, that is configurable on a per-request basis.
</comment><comment author="SweetNSourPavement" created="2016-06-14T15:34:28Z" id="225921008">So, I try to sum it up, maybe I will be clearer.

Here's what the script does:
- starts 3 master nodes
- starts 3 data node (in the script they are referenced as 4, 5 and 6)
- creates the index
- stops one data node (say 4)
- makes one insert
- stops the remaining data nodes (5 and 6)
- starts 2 data nodes (4 and 5)

and I expect the insert to be a quorum write, after that some unavailability while the nodes cycle through their business, and after that an index which can be read.

The way the nodes are chosen influence the final outcome, red or yellow index state.
</comment><comment author="ywelsch" created="2016-06-14T16:06:46Z" id="225931275">I'll reiterate what I said above. There is a moment in time in your scenario where only one data node is up (namely when you stop both node 5 and 6, which does not happen in perfect synchrony). The master node observes this and marks the shard on this last node as the only one to have non-stale data, thereby preventing data loss. Although no additional writes have happened in your scenario while stopping the data nodes 5 and 6, that's something the master node is unaware of (it does not participate in data replication).
</comment><comment author="SweetNSourPavement" created="2016-06-14T16:23:44Z" id="225936339">That's maybe true, I've not read anything substantial on es replication yet, but doesn't help anybody with a stuck index. Power cycles and crashes are normal. Is my test misconfigured? What can I do to have some real higher availability?
</comment><comment author="clintongormley" created="2016-06-14T17:15:54Z" id="225951286">&gt; Power cycles and crashes are normal. Is my test misconfigured? What can I do to have some real higher availability?

With a power cycle, you expect all nodes to rejoin, in which case your index will go green.  With a running cluster where one or two nodes crash, the freshest primary is still online, in which case you can still index with `write_consistency=1` (which is one more node you can afford to lose than with a quorum).  If you do a full cluster restart and don't bring up all the nodes, then you may have lost data.  In this case a human needs to say "i'd rather lose data than keep on waiting".
</comment><comment author="SweetNSourPavement" created="2016-06-16T12:14:42Z" id="226467815">Do you plan improving on this?
</comment><comment author="clintongormley" created="2016-06-16T12:42:16Z" id="226473599">&gt; Do you plan improving on this?

How would you suggest doing so?
</comment><comment author="ywelsch" created="2016-06-16T12:51:13Z" id="226475652">Yes, we're currently discussing a solution that involves not marking a shard copy as stale on node shutdown as long as none of the other active shards have received acknowledged writes. I've opened an issue for this here (#18919) so you can track progress on that.
</comment><comment author="jasontedor" created="2016-06-17T19:53:18Z" id="226866096">Closing in favor of #18919
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Experiment: Module for computing prec@ on queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18798</link><project id="" key="" /><description>Before continuing to read: Consider this to be an experiment, WIP, an early
draft, including a few questions as even the tests will fail occasionally. Putting
this up as PR to start a discussion off of some code - so please do feel free to
take things apart ;)

The implementation itself has been lying around in my GitHub repo for a while which you can easily see from the commit history, I took some time to clean it up and adjust it to the current state of things recently - turns out that thanks to the query refactoring efforts, the introduction of modules and various other changes we made in past months the original code could be trimmed quite a bit.

This PR adds a module to add facilities to compute ranking quality metrics on a
set of pre-annotated query/result sets. Currently only Prec@ is supported as a
metric, adding new metrics is as easy as extending the Evaluator/
RankedListQualityMetric interface.

At the moment there's no REST end point implemented. For illustration purposes
on how to use this stuff look at PrecisionAtRequestTest. Essentially the way it
works is as following:
- Assuming you've indexed a bunch of documents and want to understand how well
  your chosen ranking function performs for a set of queries:
- Create a set of queries, for each query add a set of documents you deem
  relevant to each query.
- Submit this stuff, queries are going to be executed against your index, for
  each query the returned documents are checked against the set of relevant
  documents you supplied, Prec@ is computed and each document that you
  didn't explicitly submit as relevant that was still returned for the query is
  being returned as well.

Caveats:
- Currently the whole set of annotated queries and documents has to be sent over
  the wire. I believe it would make sense to store this internally.
- The naming of some classes, methods and fields could use quite a bit of love
  and consistency.
- There's not REST endpoint yet.
- There's no integration with the task mgmt framework whatsoever.
- There's a dependency between this module and the :lang:mustache module that
  I'm pretty sure could be handled in a better way.
- The integration test I'm referring to above fails in roughly half of the cases
  with a NullPointerException in
  oe.client.transport.support.TransportProxyClient:66 when looking for my
  RankedEvalAction - I assume I forgot to register that someplace important but
  after looking around for some time this morning couldn't figure out where. Any
  enlightening appreciated.
</description><key id="159379069">18798</key><summary>Experiment: Module for computing prec@ on queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>feature</label><label>WIP</label></labels><created>2016-06-09T10:48:46Z</created><updated>2016-07-01T11:35:24Z</updated><resolved>2016-06-30T18:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-09T11:03:51Z" id="224863898">this is a great initiative. What I would be interested in is how you imagine the end-user interface would look like. For instance can you provide example request / response json bodies (no need to be final), this would be a tremendous help!

&gt; Currently the whole set of annotated queries and documents has to be sent over the wire. I believe it would make sense to store this internally.

I think sending them over the wire is fine for now. We might go further in a next step and let it fetch queries via a scan / scroll but that is something that can come way later!

&gt; There's no integration with the task mgmt framework whatsoever.

this is implicit already it's just not cancelable. I wonder if @nik9000 can help here

&gt; There's a dependency between this module and the :lang:mustache module that I'm pretty sure could be handled in a better way.

I think that is ok to have since it's a module? I would focus on usability - dependencies to modules are just fine in such a case?
</comment><comment author="MaineC" created="2016-06-13T12:44:45Z" id="225570341">&gt; this is a great initiative. 

Thanks for the encouraging feedback.

&gt; What I would be interested in is how you imagine the end-user interface would look like. For 
&gt; instance can you provide example request / response json bodies (no need to be final), this 
&gt; would be a tremendous help!

Yeah sure - that was the next step on my list. I know this is kind of working backwards, that's mostly for historical reasons: I first wanted to see if getting the stuff I had to a working state would take more than a couple days (it didn't), post it for feedback to see if it's completely off the rails (apparently it's not complete and utter nonsense) and only add more code on top after that (looking into it now)

Isabel
</comment><comment author="MaineC" created="2016-06-14T09:58:16Z" id="225835208">Added example request and response json that should about capture what is needed as docs to the RestRankEvalAction class comment.

Again listing caveats explicitly: 
- Those are essentially whiteboarded proposals - other than running them through a json linter there's no validation that they actually work and are sufficient.
- In the current proposal there's currently just the possibility to compute one metric per request template, arguably it might make sense to compute more then one.
</comment><comment author="s1monw" created="2016-06-14T10:37:26Z" id="225843729">I added some comments. I think we should add at least a second maybe a 3rd eval method to this PR before we move on. There is also some API work that needs to be done where @clintongormley needs to jump in but I think we can easily start with something like this.
</comment><comment author="MaineC" created="2016-06-14T10:53:03Z" id="225846855">Those comments do make sense.

About having more than one eval method: Huge +1, should give us an understanding what's missing for more sophisticated evaluations.

API work: For sure. That's what I expected.
</comment><comment author="clintongormley" created="2016-06-15T15:17:07Z" id="226220071">Hi @MaineC - this is looking very interesting.

I took a look at the REST API and I think it is almost there.  I think the `request` should accept anything that a search request would accept (eg `index`, `type`, etc).  This would support using https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-template-query.html[query templates].

We could even support full https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-template.html[search templates] by accepting `request_template` instead of `request`.  Later we could support fetching test cases from an index by accepting `stored_requests` in place of `requests` (but all of this later).

For the `ratings` I think we need to use an array with the full index/type/id, because docs can be returned from multiple indices, eg something like this:

```
{
  "requests": [
    {
      "id": "amsterdam_query",
      "request": {
        "index": [
          "foo",
          "bar"
        ],
        "size": 10,
        "query": {
          "bool": {
            "must": [
              {
                "match": {
                  "beverage": "coffee"
                }
              },
              {
                "term": {
                  "browser": {
                    "value": "safari"
                  }
                }
              },
              {
                "term": {
                  "time_of_day": {
                    "value": "morning",
                    "boost": 2
                  }
                }
              },
              {
                "term": {
                  "ip_location": {
                    "value": "ams",
                    "boost": 10
                  }
                }
              }
            ]
          }
        }
      },
      "ratings": [
        {
          "_index": "foo",
          "_type": "some_type",
          "_id": "1",
          "rating": 1
        },
        {
          "_index": "foo",
          "_type": "some_type",
          "_id": "2",
          "rating": 0
        },
        {
          "_index": "foo",
          "_type": "some_type",
          "_id": "3",
          "rating": 1
        },
        {
          "_index": "foo",
          "_type": "some_type",
          "_id": "4",
          "rating": 1
        }
      ]
    }
  ]
}
```

I was also thinking that there are two ways of using this API:
- You want good results from all queries, ie to make sure that you're making things better not worse
- You want the queries to compete with each other to find the better query, eg we could automatically try to include/exclude/change boost/weight different parts of a single query to improve rankings
</comment><comment author="MaineC" created="2016-06-16T07:31:02Z" id="226409317">&gt; We could even support full https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-
&gt; template.html[search templates] by accepting request_template instead of request.

That would be my personal preference (that is, without having run this through a couple of examples this sounds like a good approach as the parameter conceptually remains the same for users.)

&gt; For the ratings I think we need to use an array with the full index/type/id, because docs can be
&gt; returned from multiple indices, eg something like this:

Makes sense.

&gt; You want the queries to compete with each other to find the better query, eg we could 
&gt; automatically try to include/exclude/change boost/weight different parts of a single query to 
&gt; improve rankings

While this sounds like a good idea, I think the term "query" is overloaded multiple times in this proposal. So let me phrase this in a slightly different way to see how this use case fits into the current API (and to see if what I think I understood you saying is actually what you did say):

So one example of the problem we are talking about is the following: We are running a shop system. Users enter what I will call end_user_query like "soldering station", "led", "battery", "egg bot". There are more matching products in our backend than fit on one single page. Each product comes with product_properties like price, weight, availability, colour, maybe some quality score ("suitable for professional use" vs. "for hobbyists only"). We want to come up with an elasticsearch_query that uses the end_user_query and product_properties such that those products are listed on top that are highly likely to be perceived as high quality by the end user and end up being purchased. For an example (taken from code.talks commerce talk, which I attended earlier this year as a speaker) of how this might look like: http://project-a.github.io/on-site-search-design-patterns-for-e-commerce/#data-driven-ranking

So in your proposal you would let multiple elasticsearch_queries compete against each other. For each competition run, you would use a set of maybe 100 end_user_queries. This could be the same set of sampled\* end_user_queries for each elasticsearch_query variation (leading to less work when annotating search results assuming there is some overlap in how products are being ranked).

With the API as is I believe this would mean you'd have to restart the process with each elasticsearch_query variation you want to try out. I'm not sure if we should wrap this use case into the API itself.

Hope this makes sense,
Isabel
- For downstream users there's all sorts of sampling headaches to take into account: Sample over all queries, focus only on "electronics parts" end_user_queries, focus only on queries coming from the US, focus only on queries issued during summer vs. winter etc.
</comment><comment author="clintongormley" created="2016-06-16T07:52:27Z" id="226413376">&gt; With the API as is I believe this would mean you'd have to restart the process with each elasticsearch_query variation you want to try out. I'm not sure if we should wrap this use case into the API itself.

Agreed - this would be implemented by an application.  I just mentioned the two use cases in case it affects how we return the results.
</comment><comment author="MaineC" created="2016-06-20T06:58:51Z" id="227064081">&gt; &gt; With the API as is I believe this would mean you'd have to restart the process with each 
&gt; &gt; elasticsearch_query variation you want to try out. I'm not sure if we should wrap this use case
&gt; &gt; into the API itself.
&gt; 
&gt; Agreed - this would be implemented by an application.

OK.

&gt; I just mentioned the two use cases in case it affects how we return the results.

Makes sense. (And I just wanted to figure out, whether or not it should be part of the request API ;) )
</comment><comment author="clintongormley" created="2016-06-23T12:48:09Z" id="228039213">@MaineC what are the next steps on this PR?
</comment><comment author="MaineC" created="2016-06-27T04:49:39Z" id="228653416">Currently @cbuescher is looking at it, he already suggested a few additional changes. He also suggested to move this PR into a separate branch so we can work on changes in parallel, I think this would make sense.

@s1monw proposed to add at least one more evaluation metric before considering to move forward to see if this fits what we need.

On a related note, @nik9000 put both @cbuescher and me in touch with a guy at Wikimedia who had some input on how they are doing ranking QA, I got permission to share his viewpoint publicly so I guess it makes sense to open a more general issue to track ideas around this PR, no?
</comment><comment author="MaineC" created="2016-06-30T18:33:28Z" id="229748885">Closing in favor of collaborating on this code in branch https://github.com/elastic/elasticsearch/tree/feature/rank-eval
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Should range aggregations support the `lt/gt/lte/gte` option?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18797</link><project id="" key="" /><description>Besides the options `to` and `from`, it would be interesting to also be able to use `lt`, `lte`, `gt`, `gte`.
Currently the following does not fail, but it ignores the range and aggregate on `*-*`.

Request:

```
GET logstash-*/_search
{
  "size": 0, 
  "aggs": {
    "response_codes": {
      "range": {
        "field": "response",
        "ranges": [
          {
            "lte": 100
          },
          {
            "gte":100,
            "lt":200
          }
        ]
      }
    }
  }
}
```

Response:

```
  "aggregations": {
    "response_codes": {
      "buckets": [
        {
          "key": "*-*",
          "doc_count": 299999
        },
        {
          "key": "*-*",
          "doc_count": 299999
        }
      ]
    }
  }
```
</description><key id="159376232">18797</key><summary>Should range aggregations support the `lt/gt/lte/gte` option?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">pmusa</reporter><labels><label>:Aggregations</label></labels><created>2016-06-09T10:32:23Z</created><updated>2016-06-09T11:23:11Z</updated><resolved>2016-06-09T10:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-09T10:55:55Z" id="224862405">Duplicates #17079, duplicates #15198, duplicates #5249
</comment><comment author="pmusa" created="2016-06-09T11:23:11Z" id="224867463">just looked into the open tickets :/ thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Table with Elasticsearch static and dynamic settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18796</link><project id="" key="" /><description>It would be great to have a very simple page to sum up all cluster settings (static and dynamic). I know you can currently see all available cluster settings here although you have to go through each sub page and read through to see if the setting is static or dynamic. For example, adding a table similar to this on the modules page would be very convenient:

| Setting | Module | Type | Documentation URL |
| --- | --- | --- | --- |
| indices.fielddata.cache.size | Indices | static | https://www.elastic.co/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html |
</description><key id="159374883">18796</key><summary>Table with Elasticsearch static and dynamic settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hartfordfive</reporter><labels><label>:Settings</label><label>adoptme</label></labels><created>2016-06-09T10:24:27Z</created><updated>2016-11-28T14:10:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-09T10:32:18Z" id="224858073">I agree we should do something like this. I think the except of the doc link we have all of it avaliable.. maybe we can make it a CAT API?
</comment><comment author="hartfordfive" created="2016-06-09T12:47:14Z" id="224883937">@s1monw, having it part of the CAT API could also be a great idea, probably even better than my last suggestion. 👍   
</comment><comment author="s1monw" created="2016-06-10T09:16:18Z" id="225133134">I removed the discuss label and added adopt me. I think we should not make the doc link a requirement for the first iteration but get a CAT API out there, users can still put the setting into the serach interface to find it.
</comment><comment author="synhershko" created="2016-11-28T14:10:21Z" id="263280119">+1</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>gradle/intellij import build Fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18795</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
   2.3.3
**JVM version**:
   1.8
**OS version**:
   windows 8
**Description of the problem including expected versus actual behavior**:
1. checkout elasticsearch source code 
2. make empty project in intellj
3. import module from exist source
4. you must gradle idea from the root of elasticsearch before importing into intellij
5. cmd + gradle clean idea
   **Steps to reproduce**:
   1.
   2.
   3.
# **Provide logs (if relevant)**:
# Elasticsearch Build Hamster says Hello!

  **Gradle Version        : 2.8**
  OS Info               : Windows 8 6.2 (amd64)
  JDK Version           : Oracle Corporation 1.8.0_77 [Java HotSpot(TM) 64-Bit
erver VM 25.77-b03]
  JAVA_HOME             : C:\Program Files\Java\jdk1.8.0_77
- What went wrong:
  A problem occurred evaluating project ':build-tools'.
  &gt; Failed to apply plugin [id 'elasticsearch.build']
  &gt; **Gradle 2.13 or above i**s required to build elasticsearch
  &lt;!--
  If you are filing a feature request, please remove the above bug
  report block and provide responses for all of the below items.
  --&gt;

**Describe the feature**:
</description><key id="159331457">18795</key><summary>gradle/intellij import build Fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sjyun</reporter><labels /><created>2016-06-09T05:34:12Z</created><updated>2016-06-09T07:55:58Z</updated><resolved>2016-06-09T07:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-06-09T05:46:18Z" id="224805434">Well, the output says that you use Gradle 2.8 but Gradle 2.13 is required to build Elasticsearch. Can you please update to Gradle 2.13 and retry?

Also, I think the Elasticsearch version that you have provided in your report is not correct. Gradle has been introduced only on master (that is 5.0-SNAPSHOT).
</comment><comment author="sjyun" created="2016-06-09T07:14:07Z" id="224817840">clear
i have been confused elasticSearch branch version
 https://www.elastic.co/downloads/elasticsearch
final release version is 2.3.3 so i think master version 2.3.3 cause i use master branch only for release 

thanks 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>flat_settings ignored on nodes info in 5.0.0-alpha3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18794</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha3

**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:

`flat_settings` is not respected on nodes info.

**Steps to reproduce**:
1. call `/_nodes/_all/settings?flat_settings=true&amp;timeout=2s&amp;pretty=true`
2. observe the settings response is not flat e.g.

``` json
{
  "_nodes" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "cluster_name" : "sniffroledetection-cluster-b857db",
  "nodes" : {
    "0f3jW5LyR4K2lYQl8arDAw" : {
      "name" : "sniffroledetection-node-b857db",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1",
      "version" : "5.0.0-alpha3",
      "build_hash" : "cad959b",
      "http_address" : "127.0.0.1:9200",
      "roles" : [ "master", "ingest" ],
      "attributes" : {
        "testingcluster" : "true"
      },
      "settings" : {
        "cluster" : {
          "name" : "sniffroledetection-cluster-b857db"
        },
        "node" : {
          "name" : "sniffroledetection-node-b857db",
          "attr" : {
            "testingcluster" : "true"
          },
          "data" : "false",
          "master" : "true"
        },
        "path" : {
          "logs" : "C:/Users/russ/AppData/Roaming/NEST/5.0.0-alpha3/elasticsearch-5.0.0-alpha3/logs",
          "home" : "C:\\Users\\russ\\AppData\\Roaming\\NEST\\5.0.0-alpha3\\elasticsearch-5.0.0-alpha3",
          "repo" : "C:\\Users\\russ\\AppData\\Roaming\\NEST\\5.0.0-alpha3\\repositories"
        },
        "client" : {
          "type" : "node"
        },
        "threadpool" : {
          "watcher" : {
            "queue_size" : "1000",
            "size" : "40",
            "type" : "fixed"
          }
        },
        "script" : {
          "inline" : "true",
          "stored" : "true"
        },
        "xpack" : {
          "security" : {
            "enabled" : "false"
          }
        }
      }
    }
  }
}
```

`flat_settings` is honored in 5.0.0-alpha1 and 5.0.0-alpha2 so it appears to be a regression in 5.0.0-alpha3.
</description><key id="159327392">18794</key><summary>flat_settings ignored on nodes info in 5.0.0-alpha3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha4</label></labels><created>2016-06-09T04:47:26Z</created><updated>2016-06-16T14:03:51Z</updated><resolved>2016-06-16T14:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Having trouble to emit data to elastic search using transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18793</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 
2.3.2

**JVM version**:
25.91-b14
**OS version**:

**Description of the problem including expected versus actual behavior**:
We have an application which using transport client to emit data to elastic search. We were using 1.7.4 version for the elastic search and a pretty old version of elastic search 1.3.2 in our application. We were upgrade the elastic search cluster to 2.3.2. But the data can not been emitted successfully. Initially we saw this error:
2016-06-08 22:35:00,123 UTC [INFO](elasticsearch[Speedball][transport_client_worker][T#16]{New I/O worker #16}) org.elasticsearch.client.transport: [Speedball
] failed to get local cluster state for [#transport#-1][ip-xxx][inet[localhost/127.0.0.1:8193]], disconnecting...
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:173)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.StreamCorruptedException: Unsupported version: 1
        at org.elasticsearch.common.io.ThrowableObjectInputStream.readStreamHeader(ThrowableObjectInputStream.java:46)
        at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:301)
        at org.elasticsearch.common.io.ThrowableObjectInputStream.&lt;init&gt;(ThrowableObjectInputStream.java:38)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:170)
        ... 23 more

After some research, we thought it might be related to the old version elastic search in our application. Then we tried to upgrade the elastic search. Then we tried upgrade the ES in our application to 2.3.2. But the data was not emitted successfully, too. There is no exception in the log. The only one I saw is:
2016-06-08 09:18:12,137 UTC [INFO](main) org.elasticsearch.plugins: [Star Stalker] modules [], plugins [], sites []

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="159304108">18793</key><summary>Having trouble to emit data to elastic search using transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yamap77</reporter><labels /><created>2016-06-09T00:11:04Z</created><updated>2016-06-09T02:02:18Z</updated><resolved>2016-06-09T02:02:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-09T02:02:18Z" id="224782637">Please ask questions like this on discuss.elastic.co.
Definitely the best place to get help.

We reserve github for confirmed issues.
Also provide all logs please.

Thanks!

Also
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Version conflict engine exception sometimes returns non-existing version number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18792</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_65

**OS version**: OS X 0.11.5 (15F34)

When trying to delete an object with a specified version number and the requested object does not exist, Elasticsearch throws a version conflict engine exception stating that the object is out of date since the current version is -1 (non-existent). However, if the object did still exist just moments ago the exception gives an actual version number (latest version + 1). I assume this is due to the object being only marked as deleted before it's actually thrown out. This state lasts about one minute and can't be avoided by a forced refresh. I get that it may be better performance-wise to mark objects as deleted until some sort of garbage collector comes by. With that exception though, it's not possible to tell whether the object was deleted or updated. Is it possible to set the version number of objects marked as deleted to -1?

**Steps to reproduce**:
1. Index an object.
2. Delete that object.
3. Try to delete it again while specifying the version number (1 in this case).
   1. Get a version conflict engine exception stating that the current version number is 2.
   2. Wait about one minute and try to delete the object again: You'll get a version conflict engine exception stating that the current version number is -1.
</description><key id="159296281">18792</key><summary>Version conflict engine exception sometimes returns non-existing version number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zzzFelix</reporter><labels /><created>2016-06-08T23:04:45Z</created><updated>2016-06-09T02:17:47Z</updated><resolved>2016-06-09T02:17:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-09T02:17:47Z" id="224784320">Deletes and versions are tricky. If a delete operation is version 1024 of a document, you don't want a versioned update with version 1023 to bring the document back to life. Thus, Elasticsearch needs to keep the delete around so that it can resolve a version conflict like this. Alas, keeping deletes around forever would eventually lead to an exhaustion of resources. So, Elasticsearch strikes a balance and does garbage collection as you correctly surmised.

So, where are we? Elasticsearch has to keep versions on deletes, or it can't resolve updates, but it has to eventually garbage collect those deletes. Thus, your proposal of always returning -1 for the version of a deleted document is untenable.

Yet, there is an out. The time that Elasticsearch will hold on to a delete before garbage collecting it is an index-level setting: `index.gc_deletes`. The default is sixty seconds, as your experimentation discovered, but if you set this to zero, then a deleted document will be immediately garbage collected and it's as if the version is -1, non-existent.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Align description with implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18791</link><project id="" key="" /><description>[float value must now be greater than 0, or use `"unlimited"` for no throttle](https://github.com/elastic/elasticsearch/blob/a1172d816cc3d8404347b8770c162b4f938fc486/modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractBaseReindexRestHandler.java#L149)
</description><key id="159287400">18791</key><summary>Align description with implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels><label>docs</label></labels><created>2016-06-08T22:17:21Z</created><updated>2016-08-12T16:36:59Z</updated><resolved>2016-08-12T11:08:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="russcam" created="2016-06-08T23:08:16Z" id="224758092">I think that unlimited should be interpreted from a float value, otherwise the `type` on the rest api spec is not reflective of the acceptable values.

How about `-1` for unlimited requests per second, similar to how `-1` represents disabling `refresh_interval`? float positive infinity may be represented differently in different languages, which may affect ES clients implementing the API
</comment><comment author="nik9000" created="2016-06-09T13:21:54Z" id="224892360">&gt; I think that unlimited should be interpreted from a float value, otherwise the type on the rest api spec is not reflective of the acceptable values.

It is Float.POSITIVE_INFINITY internally and in the transport client.

&gt; How about -1 for unlimited requests per second, similar to how -1 represents disabling refresh_interval?

Elasticsearch core is slowly getting more explicit. This particular argument has already been played out. 0 used to mean unlimited but it just wasn't worth all special cases.

ES is slowly moving towards stuff like this. I'm happy to change the rest spec any way you like to make it easier to generate a proper client. Maybe we change it so they type is `unlimited|float` or something? I'm not sure.
</comment><comment author="nik9000" created="2016-06-09T13:22:18Z" id="224892461">The change LGTM, btw.
</comment><comment author="clintongormley" created="2016-06-13T17:54:17Z" id="225657896">&gt; Elasticsearch core is slowly getting more explicit. This particular argument has already been played out. 0 used to mean unlimited but it just wasn't worth all special cases.

I'm not sure I agree with this.  We use `-1` in a lot of places (eg refresh interval) while `unlimited` is only used in this one place.  Using `-1` is simple to understand, allows for a single value type, and I don't have to remember the special word which means "turn this off".  Honestly i'd prefer `-1` to `unlimited`.
</comment><comment author="bleskes" created="2016-06-24T09:14:45Z" id="228297229">we discussed this on fix it friday - the consensus is that people like the consistency of -1 with other settings and we see no good reason to change the "standard".
</comment><comment author="clintongormley" created="2016-06-27T12:33:20Z" id="228732568">I've opened an issue to get the reindex API changed here https://github.com/elastic/elasticsearch/issues/19089
</comment><comment author="jasontedor" created="2016-08-12T11:08:47Z" id="239420303">Superseded by #19101
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make transport port config more flexible in discovery-aws plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18790</link><project id="" key="" /><description>As a follow up for http://stackoverflow.com/q/37668141

Ultimately the proposal was to add something similar to Google Cloud Engineer's [`--metadata=es_port`](https://github.com/elastic/elasticsearch-cloud-gce#changing-default-transport-port)  to discovery-aws.

This change will help support ES in containerized environment.
</description><key id="159256161">18790</key><summary>Make transport port config more flexible in discovery-aws plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xavi-</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label></labels><created>2016-06-08T19:53:26Z</created><updated>2016-09-29T18:05:53Z</updated><resolved>2016-09-28T17:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-06-15T14:24:48Z" id="226203295">I am against adding this, as the typical solution in containerized environments is to use a cluster manager (see also [my answer in the related Discuss thread](https://discuss.elastic.co/t/overriding-tcp-publish-port-breaks-clustering-when-elasticsearch-is-in-a-container/52135/2?u=danielmitterdorfer)).
</comment><comment author="dadoonet" created="2016-06-15T14:31:44Z" id="226205481">@danielmitterdorfer So we should remove support for this in GCE as well? Basically remove those lines: https://github.com/elastic/elasticsearch/blob/master/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java#L227-L236
</comment><comment author="danielmitterdorfer" created="2016-06-15T14:35:41Z" id="226206747">This would be consistent IMHO.
</comment><comment author="dadoonet" created="2016-09-28T17:26:43Z" id="250237193">I'm going to close this issue as per @danielmitterdorfer answer. I'll going to open a PR to remove this "feature" in GCE then.
</comment><comment author="xavi-" created="2016-09-28T20:19:17Z" id="250287346">Sorry lost track of this thread.  Posted a reply here: https://discuss.elastic.co/t/overriding-tcp-publish-port-breaks-clustering-when-elasticsearch-is-in-a-container/52135/2
</comment><comment author="xavi-" created="2016-09-29T18:05:53Z" id="250545233">In case my reply on elastic.co wasn't clear, I feel the GCE changes are a bad idea.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>painless: exempt LocalDate from bridge method checks on java 9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18789</link><project id="" key="" /><description>The painless whitelist has a lot of self-checking, in this case, it checks
for missing covariant overrides. It fails on java 9, because LocalDate.getEra()
now returns IsoEra instead of Era: https://bugs.openjdk.java.net/browse/JDK-8072746

To our checker, it thinks we were lazy with whitelisting :)

This means painless works on java 9 again
</description><key id="159255746">18789</key><summary>painless: exempt LocalDate from bridge method checks on java 9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2016-06-08T19:51:20Z</created><updated>2016-06-08T21:38:28Z</updated><resolved>2016-06-08T21:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-06-08T21:36:21Z" id="224736474">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Definition.java</file></files><comments><comment>Merge pull request #18789 from rmuir/java9_localdate_you_just_had_to_do_this_didnt_you</comment></comments></commit></commits></item><item><title>Improve performance of applyDeletedShards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18788</link><project id="" key="" /><description>This commit addresses a performance issue in
IndicesClusterStateService#applyDeletedShards. Namely, the current
implementation is O(number of indices \* number of shards). This is
because of an outer loop over the indices and an inner loop over the
assigned shards, all to check if a shard is in the outer index. Instead,
we can group the shards by index, and then just do a map lookup for each
index.

Testing this on a single-node with 2500 indices, each with 2 shards,
creating an index before this optimization takes 0.90s and after this
optimization takes 0.19s.

Relates #18776
</description><key id="159253444">18788</key><summary>Improve performance of applyDeletedShards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T19:39:29Z</created><updated>2016-06-09T14:00:30Z</updated><resolved>2016-06-08T20:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-08T19:44:06Z" id="224705340">Left a minor but LGTM.
</comment><comment author="s1monw" created="2016-06-08T19:44:29Z" id="224705438">LGTM
</comment><comment author="nik9000" created="2016-06-08T19:53:54Z" id="224707947">LGTM too.
</comment><comment author="ywelsch" created="2016-06-09T07:05:55Z" id="224816518">This is already addressed by #17270 (https://github.com/elastic/elasticsearch/pull/17270/files/f71f7de3e85191ae0ac31191d2b6f1ba173c73bd#diff-e3d064cfd24df0c1f53af98e6f9bc66dR372) which also does not even require building an extra map :-) Interestingly, both reviewed by @s1monw ;-) This one is still useful, however, if we want to backport to 2.4.x.
</comment><comment author="s1monw" created="2016-06-09T08:17:15Z" id="224829143">&gt;  both reviewed by @s1monw ;-) 

this one had a different focus :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Improve performance of applyDeletedShards</comment></comments></commit></commits></item><item><title>Documentation for UpdateByQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18787</link><project id="" key="" /><description>The new UpdateByQuery is awesome!  But lacks Java API documentation.   

@debadair ,

Speaking with @nik9000 , I formulated &amp; tested this:

```
UpdateByQueryRequestBuilder updateByQuery = UpdateByQueryAction.INSTANCE.newRequestBuilder(client);

updateByQuery.source("cool_things")
        .filter(termQuery("level", "awesome"))
        .script(new Script("ctx._source.awesome = \"absolutely\""));

BulkIndexByScrollResponse response = updateByQuery.get();
```
</description><key id="159242379">18787</key><summary>Documentation for UpdateByQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Reindex API</label><label>docs</label></labels><created>2016-06-08T18:45:18Z</created><updated>2016-08-18T09:39:58Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PhaedrusTheGreek" created="2016-06-16T13:09:08Z" id="226479863">I forgot to mention a few important points

You must have the reindex module installed.  Here's the POM example for 2.3.2:

```
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch.module&lt;/groupId&gt;
            &lt;artifactId&gt;reindex&lt;/artifactId&gt;
            &lt;version&gt;2.3.2&lt;/version&gt;
        &lt;/dependency&gt;
```

And you have to add the ReindexPlugin class to the client:

```
Client client = TransportClient.builder()
        .settings(settings)
        .addPlugin(org.elasticsearch.index.reindex.ReindexPlugin.class)
        .build() 
        .addTransportAddress(new InetSocketTransportAddress(new InetSocketAddress("localhost", 9300)));
```
</comment><comment author="NicolasYDDER" created="2016-08-18T09:39:58Z" id="240673992">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to lucene-6.1.0-snapshot-3a57bea.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18786</link><project id="" key="" /><description>I would like to upgrade before we release Lucene 6.1 so that Lucene gets some integration testing.
</description><key id="159234955">18786</key><summary>Upgrade to lucene-6.1.0-snapshot-3a57bea.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T18:09:29Z</created><updated>2016-06-28T09:50:51Z</updated><resolved>2016-06-10T14:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-08T18:15:42Z" id="224680608">Note that this is a work in progress, there are many failing tests, especially geo tests, probably one of the reasons is that I got confused about whether to use MortonEncoder.encode or GeoPoint.encodeLatLon is some places. @nknize could you help?

I also have test failures in StoreRecoveryTests, probably because I did not set the permissions correctly, @s1monw maybe you could have a look?
</comment><comment author="jpountz" created="2016-06-09T08:28:33Z" id="224831509">OK, I made some progress, I now have all core tests passing except `GeoBoundingBoxIT.testLimitsBoundingBox`, which seems to be due to  quantization, as this test tries to verify that points that are on the edges of the bbox match.
</comment><comment author="s1monw" created="2016-06-09T09:03:35Z" id="224839237">@nknize can you please make sure @jpountz can make progress on this.
</comment><comment author="mikemccand" created="2016-06-09T15:05:13Z" id="224924229">&gt; I now have all core tests passing except GeoBoundingBoxIT.testLimitsBoundingBox

I dug into this and it looks like it might be a regression in Lucene 6.1 from Lucene 6.0: https://issues.apache.org/jira/browse/LUCENE-7325

@nknize is digging ...
</comment><comment author="nknize" created="2016-06-09T23:18:21Z" id="225054815">I updated the lucene issue and muted the angry test for now to keep from blocking the PR. I think its important to get CI chewing on 6.1.
</comment><comment author="jpountz" created="2016-06-10T07:50:25Z" id="225115872">Thanks @nknize. I fixed the last remaining TODO I had so this PR is now ready for review.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consider making the `*_bytes_per_sec` settings for a repository dynamically updateable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18785</link><project id="" key="" /><description>At the moment these settings can only be specified when the repository is created and changing them requires stopping any on-going (and possibly too slow) restore/snapshot, deleting the repository and adding it back again.

It could be useful for these settings to be dynamically changeable (to make the most of the hardware at hand - disks, cpu, network) and not needing to lose the data already transferred and the already spent time with.
</description><key id="159209458">18785</key><summary>Consider making the `*_bytes_per_sec` settings for a repository dynamically updateable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2016-06-08T16:22:50Z</created><updated>2016-06-17T09:48:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Better handling of an empty shard's segments_N file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18784</link><project id="" key="" /><description>When trying to restore a snapshot of an index created in a previous
version of Elasticsearch, it is possible that empty shards in the
snapshot have a segments_N file that has an unsupported Lucene version
and a missing checksum.  This leads to issues with restoring the
snapshot.  This commit handles this special case by avoiding a restore
of a shard that has no data, since there is nothing to restore anyway.

Closes #18707
</description><key id="159199281">18784</key><summary>Better handling of an empty shard's segments_N file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T15:47:40Z</created><updated>2016-06-10T13:58:53Z</updated><resolved>2016-06-10T13:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-08T15:48:09Z" id="224633146">@s1monw FYI
</comment><comment author="s1monw" created="2016-06-09T15:57:58Z" id="224941083">left some comments - looks great!
</comment><comment author="abeyad" created="2016-06-09T17:19:44Z" id="224964905">@s1monw pushed up a new commit that addresses code review comments: https://github.com/elastic/elasticsearch/pull/18784/commits/f02e1ada947bde8eaa01ce5113407324a81f0146

Also, I reverted back to using `Lucene.parseVersionLenient` because using `Version.parseLeniently` is a forbidden API and `Lucene.parseVersionLenient` is the one central place its allowed.  Also, the `StoreFileMetaData` throws an NPE regardless in its constructor if `writtenBy` is `null`.
</comment><comment author="s1monw" created="2016-06-10T09:58:42Z" id="225143091">LGTM
</comment><comment author="abeyad" created="2016-06-10T13:55:40Z" id="225188400">@s1monw thanks for the review!
</comment><comment author="abeyad" created="2016-06-10T13:58:53Z" id="225189218">Closed by https://github.com/elastic/elasticsearch/commit/43e07c0c886223c65d26a4e3bdb7815ff15abe7d
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade geoip processor's dependencies and database files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18783</link><project id="" key="" /><description>The database files have been doubled in size compared to the previous files being used.
For this reason the database files are now gzip compressed, which required using
`GZIPInputStream` when loading database files.
</description><key id="159175874">18783</key><summary>Upgrade geoip processor's dependencies and database files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T14:20:48Z</created><updated>2016-06-13T17:05:22Z</updated><resolved>2016-06-08T16:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-06-08T14:54:24Z" id="224615211">oh, cool!

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add an index setting to limit the maximum number of slices allowed in a scroll request.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18782</link><project id="" key="" /><description>This is a follow up for https://github.com/elastic/elasticsearch/pull/18237#discussion_r66096836
</description><key id="159168189">18782</key><summary>Add an index setting to limit the maximum number of slices allowed in a scroll request.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Scroll</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T13:49:24Z</created><updated>2016-06-10T07:49:04Z</updated><resolved>2016-06-10T07:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-08T14:01:36Z" id="224598311">left some comments! thanks for doing this!
</comment><comment author="jimczi" created="2016-06-08T20:36:10Z" id="224719232">@s1monw I pushed another commit to address your comments. The default maximum number of slices is now equal to the number of shards in the requested index.  
</comment><comment author="s1monw" created="2016-06-09T10:29:58Z" id="224857679">&gt; @s1monw I pushed another commit to address your comments. The default maximum number of slices is now equal to the number of shards in the requested index.

this might actually not work 😞 if we go through an alias we may have way more slices than shards in a single index. and then we fail half way. So I think we need something different (as you had)? Maybe `1024` ? @jpountz WDTY - sorry for not thinking about it earlier
</comment><comment author="jimczi" created="2016-06-09T11:23:52Z" id="224867599">Don't _shrink my scroll ! ;) 
Yep aliases are dangerous. The simple fact that each slice is an independent request is dangerous.
You may also argue that the index could be deleted and re-created between two sliced scroll.
+1 for `1024` and I'll add a note in the docs about aliases and index creation
</comment><comment author="jpountz" created="2016-06-09T13:01:11Z" id="224887185">+1 to a fixed limit, I think it is easier to reason about
</comment><comment author="jimczi" created="2016-06-10T07:45:36Z" id="225115065">@jpountz @s1monw I changed the default to 1024. 
</comment><comment author="s1monw" created="2016-06-10T07:46:34Z" id="225115230">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/test/java/org/elasticsearch/search/simple/SimpleSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/slice/SearchSliceIT.java</file></files><comments><comment>Merge pull request #18782 from jimferenczi/max_slices_per_scroll</comment></comments></commit></commits></item><item><title>Enormous number of file descriptors for translog file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18781</link><project id="" key="" /><description>Sorry, but I couldn't reproduce this bug(?). I'm just wondering if someone else has encountered that and if is fixed. Thanks.

**Elasticsearch version**:
2.0 (elasticsearch docker image)

**JVM version**:
openjdk 1.8.0_72-internal (elasticsearch docker image)

**OS version**:
Ubuntu 14.04.3 LTS

**Description of the problem including expected versus actual behavior**:
Around 1 million of file descriptors for the translog file.

**Steps to reproduce**:
 Sorry, didn't find how.

**Provide logs (if relevant)**:

```
...
java    3897 pollinate *297r   REG              202,1 210700529 3035985 /usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2016.05.18/3/translog/translog-1.tlog
java    3897 pollinate *298r   REG              202,1 210700529 3035985 /usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2016.05.18/3/translog/translog-1.tlog
java    3897 pollinate *299r   REG              202,1 210700529 3035985 /usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2016.05.18/3/translog/translog-1.tlog
java    3897 pollinate *300r   REG              202,1 210700529 3035985 /usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2016.05.18/3/translog/translog-1.tlog
... (around 1M of this lines)
```
</description><key id="159146317">18781</key><summary>Enormous number of file descriptors for translog file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alon-argus</reporter><labels /><created>2016-06-08T12:00:00Z</created><updated>2016-06-08T12:03:36Z</updated><resolved>2016-06-08T12:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-08T12:03:36Z" id="224568963">it might be this: https://github.com/elastic/elasticsearch/pull/15762

can you please upgrade I am sure this is fixed now. If you can reproduce in a newer version please reopen. 

btw. there is no official (elasticsearch docker image)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix problem with TimeIntervalRounding on DST end</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18780</link><project id="" key="" /><description>Due to an error in our current TimeIntervalRounding, two dates can round to the same key, even when they are 1h apart when using
short interval roundings (e.g. 20m) and a time zone with DST change.

Here is an example for the CET time zone:

On 25 October 2015, 03:00:00 clocks are turned backward 1 hour to 02:00:00 local standard time. The dates
"2015-10-25T02:15:00+02:00" (1445732100000) (before DST end) and
"2015-10-25T02:15:00+01:00" (1445735700000) (after DST end) are thus 1h apart, but currently they round to the same value
"2015-10-25T02:00:00+01:00" (1445734800000).

This violates an important invariant of rounding, namely that the rounded value must be less or equal to the value that is rounded. 
It also leads to wrong histogram bucket counts because documents in [02:00:00+02:00, 02:20:00+02:00) go to the same rounding 
bucket as documents from [02:00:00+01:00, 02:20:00+01:00).

The problem happens because in TimeIntervalRounding#roundKey() we need to perform the rounding operation in local time, but on
converting back to UTC we don't honor the original values time zone offset. This fix changes that and adds tests both for DST start and
DST end as well as a test that demonstrates what happens to bucket sizes when the dst change is not evently divisibly by the interval.
</description><key id="159141170">18780</key><summary>Fix problem with TimeIntervalRounding on DST end</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T11:29:03Z</created><updated>2016-06-08T13:47:36Z</updated><resolved>2016-06-08T12:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-06-08T11:31:30Z" id="224562750">@jpountz this is the first follow-up PR to #18589, now dealing with TimeIntervalRounding. Can you take a look at this? 
</comment><comment author="jpountz" created="2016-06-08T12:43:44Z" id="224577187">LGTM
</comment><comment author="cbuescher" created="2016-06-08T13:47:35Z" id="224593837">Merged to 2.x branch with 1d37a01
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Automatically set the collection mode to breadth_first in the terms aggregation when the cardinality of the field is unknown or smaller than the requested size.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18779</link><project id="" key="" /><description>For fields with big cardinality this can save a lot of memory and speed up the overall aggregations.
For instance with an index of 100,000 documents and two fields (f1 and f2) with a cardinality of 10,000, the following query:

```
"aggs": {
  "f1": {
    "terms": {
       "field": "f1"
    },
    "aggs": {
       "f2": {
          "terms": {
            "field": "f2
         }
      }
    }
  }
}
```

... takes 100ms to finish with the depth_first mode and 10ms with breadth_first. 

For fields with small cardinality the breadth_first mode can use more memory than the depth_first.
This PR does not handle this case yet.

closes #9825
</description><key id="159139622">18779</key><summary>Automatically set the collection mode to breadth_first in the terms aggregation when the cardinality of the field is unknown or smaller than the requested size.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>release highlight</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T11:19:31Z</created><updated>2016-09-01T00:04:41Z</updated><resolved>2016-06-16T12:28:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-06-08T14:16:35Z" id="224602987">LGTM. The heuristic is simple, I like it.

As a follow-up, we might want to change the heuristic for the default shard size (BucketUtils.suggestShardSideQueueSize). Currently it is very aggressive (`size * min(10, num_shards)`), which would probably cause `breadth_first` to be used quite often in practice with this change? For instance Solr is doing `size*1.5+10`, which looks more reasonable to me?
</comment><comment author="jpountz" created="2016-06-08T14:28:49Z" id="224606999">Sorry I was thinking backwards, this aggressivity actually favors _depth first_. Yet I think there is still an argument that by making the default shard size less aggressive, we could save memory since there would be more opportunities to use breadth_first (while still having good accuracy for zipfian distributions).
</comment><comment author="rayward" created="2016-09-01T00:04:40Z" id="243938842">Any chance of this getting backported to 2.x? 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/BestBucketsDeferringCollector.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactoryTests.java</file></files><comments><comment>Merge pull request #18779 from jimferenczi/breadth_first_heuristic</comment></comments></commit></commits></item><item><title>default values for BM25 Similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18778</link><project id="" key="" /><description>added the lucene default values for BM25 similarity - assuming elastic search uses them
</description><key id="159126364">18778</key><summary>default values for BM25 Similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">eratio08</reporter><labels><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-06-08T10:06:05Z</created><updated>2016-06-13T16:57:09Z</updated><resolved>2016-06-13T16:57:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-06-08T12:04:40Z" id="224569171">@clintongormley can you take a look
</comment><comment author="clintongormley" created="2016-06-13T16:57:09Z" id="225642360">thanks @eratio08 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>default values for BM25 Similarity (#18778)</comment></comments></commit></commits></item><item><title>[feature request]scroll search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18777</link><project id="" key="" /><description>currently there is scroll/scan mechinsm, which maintain a context for interator overall documents. 
this request is the same thing: maintain a context for search overall documents and comquer memory issue for deep paginate.
</description><key id="159111497">18777</key><summary>[feature request]scroll search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-06-08T08:53:00Z</created><updated>2016-06-14T01:54:20Z</updated><resolved>2016-06-08T09:28:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-08T09:28:02Z" id="224537020">You need to be more specific. From what I understand you're asking for something that already exists.
Closing.
</comment><comment author="clintongormley" created="2016-06-13T16:55:44Z" id="225641965">Perhaps this is what you're after? https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-search-after.html#search-request-search-after
</comment><comment author="makeyang" created="2016-06-14T01:54:20Z" id="225759378">@clintongormley  yeah, this is exactly what I need
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Creating Index painfully slow on cluster with large indices </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18776</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

I have a cluster with 2 nodes and approximate 10000 index.Each index has one replication. Creating index with some preset mapping on this cluster is painfully slow( about 1 minute to create a index with 1 replication). There is sufficient  memory , java heap , cpu and disk when creating index. I use hot_threads api and find that 95% period of time is spended on running the following code on master node:

```
    at com.google.common.collect.Iterators$3.hasNext(Iterators.java:164)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedShards(IndicesClusterStateService.java:256)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:167)
    - locked &lt;0x00000000f96ea8b0&gt; (a java.lang.Object)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

is this a bug? Can I avoid this by setting any configuration? 
**Elasticsearch version**:
2.3.3
**JVM version**:
1.8
**OS version**:
debian7
</description><key id="159097178">18776</key><summary>Creating Index painfully slow on cluster with large indices </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">puuzll</reporter><labels /><created>2016-06-08T07:29:40Z</created><updated>2016-10-11T03:28:03Z</updated><resolved>2016-06-08T20:31:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-08T19:46:47Z" id="224706011">&gt; is this a bug?

When you create an index, that causes a change to the routing table. A cluster state update task is submitted to the nodes in the cluster. When that cluster state update task arrives, each node must process the new routing table to see if they need to remove indices, delete shards, start shards, etc. Currently, applying deleted shards is O(number of indices \* number of shards). I opened #18788 to address this.

However, you're still going to be hurting here. Having 10000 indices on two nodes with one replica is asking for pain. This means that you have at a minimum 10000 shards on each node if you have one shard per index, and maybe 50000 shards on each node if you're using the default number of shards per index. Either way, this is way too many shards. So #18788 is not meant to address your issue directly, just improve performance for the general case. You'll still need to do something about how many indices and shards that you have.

&gt; Can I avoid this by setting any configuration? 

No.
</comment><comment author="jilen" created="2016-10-08T01:33:15Z" id="252394235">@jasontedor   I suffered from this. Is there any way to improve the index creation speed ?
</comment><comment author="jasontedor" created="2016-10-10T11:00:55Z" id="252587762">@jilen Creating an index requires a cluster state update which can be a slow thing indeed. The issue here was about the degradation in index-creation speed as the number of indices increased; that's what #18788 addressed. I'd say that if you rely on index creation being fast, you probably have an architecture that needs to be reconsidered.
</comment><comment author="bleskes" created="2016-10-10T12:04:45Z" id="252599410">@jilen to quantify what @jasontedor said (which is very true) - index creation is slow when compared to data level operations like indexing and search. You should expect it to run within a couple of seconds. Also note that we now wait (since 5.0) for the primaries to be fully allocated before responding to the call.
</comment><comment author="jilen" created="2016-10-11T02:22:00Z" id="252794397">@jasontedor  @bleskes I am now applying `one-index-per-user pattern`, there are actually more than 20k shards. 

Parallel automatically index creationg(via bulk or update api) actually makes the cluster dead(no response).

What do you suggest for my situation ? Disable automatically index creation ?
</comment><comment author="nik9000" created="2016-10-11T03:28:03Z" id="252802247">Don't have an index per user.

On Oct 10, 2016 10:22 PM, "jilen" notifications@github.com wrote:

&gt; @jasontedor https://github.com/jasontedor I am now applying one-index-per-user
&gt; pattern, there are actually more than 20k shards.
&gt; 
&gt; Parallel automatically index creationg(via bulk or update api) actually
&gt; makes the cluster dead(no response).
&gt; 
&gt; What do you suggest for my situation ? Disable automatically index
&gt; creation ?
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/18776#issuecomment-252794397,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLovR2fcqDQQkHcEd0qSWIhqX2DI-9ks5qyvLNgaJpZM4IwrBH
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>snapshot delete getting stuck on 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18775</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**JVM version**: 1.8.0_91

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

I am running a 8 node cluster, with 5 master eligible nodes. The cluster is configured to take snapshots to an azure container. When a snapshot process was taking too long I ran the delete command and the curl request got stuck. While the snapshot state shows "IN PROGRESS" and the some indices of the cluster shows "DONE" and some "ABORTED".
By running the delete command, an ongoing snapshot should have stopped and deleted the files. Now I can't delete or stop the snapshot process, nor can I start a new snapshot process.

**Steps to reproduce**:
1. Start a snapshot process.
2. Run Delete command while the snapshot process is running.
3. Try Starting a new Snapshot

**Provide logs (if relevant)**:

**curl -XGET "localhost:9200/_snapshot/my_backup/_all?pretty"**
{
  "snapshots" : [ {
    "snapshot" : "20160606-114625",
    "version_id" : 2030299,
    "version" : "2.3.2",
    "indices" : [ ".marvel-es-1-2016.06.06", "igdata_archive_v1", ".kibana", "fbdata_archive_v1", ".marvel-es-data-1", "twitterv1" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2016-06-06T11:46:36.198Z",
    "start_time_in_millis" : 1465213596198,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  } ]
}

**curl -XGET "localhost:9200/_snapshot/my_backup/_status?pretty"**

{
  "snapshots" : [ {
    "snapshot" : "20160606-114625",
    "version_id" : 2030299,
    "version" : "2.3.2",
    "indices" : [ ".marvel-es-1-2016.06.06", "igdata_archive_v1", ".kibana", "fbdata_archive_v1", ".marvel-es-data-1", "twitterv1" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2016-06-06T11:46:36.198Z",
    "start_time_in_millis" : 1465213596198,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  } ]
}
frrole@es-prod-0:~$ curl -XGET "localhost:9200/_snapshot/my_backup/_status?pretty"{
  "snapshots" : [ {
    "snapshot" : "20160606-114625",
    "repository" : "my_backup",
    "state" : "ABORTED",
    "shards_stats" : {
      "initializing" : 0,
      "started" : 0,
      "finalizing" : 0,
      "done" : 19,
      "failed" : 8,
      "total" : 27
    },
    "stats" : {
      "number_of_files" : 2569,
      "processed_files" : 1414,
      "total_size_in_bytes" : 894619893816,
      "processed_size_in_bytes" : 273115600981,
      "start_time_in_millis" : 1465213597658,
      "time_in_millis" : 2580664
    },
    "indices" : {
      "igdata_archive_v1" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 8,
          "failed" : 0,
          "total" : 8
        },
        "stats" : {
          "number_of_files" : 315,
          "processed_files" : 315,
          "total_size_in_bytes" : 640498637,
          "processed_size_in_bytes" : 640498637,
          "start_time_in_millis" : 1465213597710,
          "time_in_millis" : 17707
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 42,
              "processed_files" : 42,
              "total_size_in_bytes" : 78022449,
              "processed_size_in_bytes" : 78022449,
              "start_time_in_millis" : 1465213597745,
              "time_in_millis" : 14713
            }
          },
          "1" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 49,
              "processed_files" : 49,
              "total_size_in_bytes" : 78835393,
              "processed_size_in_bytes" : 78835393,
              "start_time_in_millis" : 1465213597774,
              "time_in_millis" : 17218
            }
          },
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 32,
              "processed_files" : 32,
              "total_size_in_bytes" : 82103826,
              "processed_size_in_bytes" : 82103826,
              "start_time_in_millis" : 1465213597755,
              "time_in_millis" : 12653
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 61,
              "processed_files" : 61,
              "total_size_in_bytes" : 80134234,
              "processed_size_in_bytes" : 80134234,
              "start_time_in_millis" : 1465213597763,
              "time_in_millis" : 17654
            }
          },
          "4" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 29,
              "processed_files" : 29,
              "total_size_in_bytes" : 82384445,
              "processed_size_in_bytes" : 82384445,
              "start_time_in_millis" : 1465213598887,
              "time_in_millis" : 12094
            }
          },
          "5" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 36,
              "processed_files" : 36,
              "total_size_in_bytes" : 79855636,
              "processed_size_in_bytes" : 79855636,
              "start_time_in_millis" : 1465213597738,
              "time_in_millis" : 13382
            }
          },
          "6" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 33,
              "processed_files" : 33,
              "total_size_in_bytes" : 77978824,
              "processed_size_in_bytes" : 77978824,
              "start_time_in_millis" : 1465213597710,
              "time_in_millis" : 13688
            }
          },
          "7" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 33,
              "processed_files" : 33,
              "total_size_in_bytes" : 81183830,
              "processed_size_in_bytes" : 81183830,
              "start_time_in_millis" : 1465213597747,
              "time_in_millis" : 13045
            }
          }
        }
      },
      ".marvel-es-data-1" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 1,
          "failed" : 0,
          "total" : 1
        },
        "stats" : {
          "number_of_files" : 4,
          "processed_files" : 4,
          "total_size_in_bytes" : 5120,
          "processed_size_in_bytes" : 5120,
          "start_time_in_millis" : 1465213597655,
          "time_in_millis" : 1192
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 4,
              "processed_files" : 4,
              "total_size_in_bytes" : 5120,
              "processed_size_in_bytes" : 5120,
              "start_time_in_millis" : 1465213597655,
              "time_in_millis" : 1192
            }
          }
        }
      },
      "fbdata_archive_v1" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 8,
          "failed" : 0,
          "total" : 8
        },
        "stats" : {
          "number_of_files" : 530,
          "processed_files" : 530,
          "total_size_in_bytes" : 1284798959,
          "processed_size_in_bytes" : 1284798959,
          "start_time_in_millis" : 1465213597772,
          "time_in_millis" : 68473
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 83,
              "processed_files" : 83,
              "total_size_in_bytes" : 252055047,
              "processed_size_in_bytes" : 252055047,
              "start_time_in_millis" : 1465213611060,
              "time_in_millis" : 37487
            }
          },
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 38,
              "processed_files" : 38,
              "total_size_in_bytes" : 58202386,
              "processed_size_in_bytes" : 58202386,
              "start_time_in_millis" : 1465213597774,
              "time_in_millis" : 13730
            }
          },
          "1" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 49,
              "processed_files" : 49,
              "total_size_in_bytes" : 160284115,
              "processed_size_in_bytes" : 160284115,
              "start_time_in_millis" : 1465213597781,
              "time_in_millis" : 22071
            }
          },
          "4" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 49,
              "processed_files" : 49,
              "total_size_in_bytes" : 85370756,
              "processed_size_in_bytes" : 85370756,
              "start_time_in_millis" : 1465213612519,
              "time_in_millis" : 15931
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 78,
              "processed_files" : 78,
              "total_size_in_bytes" : 165212008,
              "processed_size_in_bytes" : 165212008,
              "start_time_in_millis" : 1465213597772,
              "time_in_millis" : 25008
            }
          },
          "6" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 67,
              "processed_files" : 67,
              "total_size_in_bytes" : 129844814,
              "processed_size_in_bytes" : 129844814,
              "start_time_in_millis" : 1465213597786,
              "time_in_millis" : 20858
            }
          },
          "5" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 64,
              "processed_files" : 64,
              "total_size_in_bytes" : 117159311,
              "processed_size_in_bytes" : 117159311,
              "start_time_in_millis" : 1465213615057,
              "time_in_millis" : 19244
            }
          },
          "7" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 102,
              "processed_files" : 102,
              "total_size_in_bytes" : 316670522,
              "processed_size_in_bytes" : 316670522,
              "start_time_in_millis" : 1465213615536,
              "time_in_millis" : 50709
            }
          }
        }
      },
      ".kibana" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 1,
          "failed" : 0,
          "total" : 1
        },
        "stats" : {
          "number_of_files" : 11,
          "processed_files" : 11,
          "total_size_in_bytes" : 51305,
          "processed_size_in_bytes" : 51305,
          "start_time_in_millis" : 1465213597658,
          "time_in_millis" : 2041
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 11,
              "processed_files" : 11,
              "total_size_in_bytes" : 51305,
              "processed_size_in_bytes" : 51305,
              "start_time_in_millis" : 1465213597658,
              "time_in_millis" : 2041
            }
          }
        }
      },
      ".marvel-es-1-2016.06.06" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 1,
          "failed" : 0,
          "total" : 1
        },
        "stats" : {
          "number_of_files" : 22,
          "processed_files" : 22,
          "total_size_in_bytes" : 1381698,
          "processed_size_in_bytes" : 1381698,
          "start_time_in_millis" : 1465213611530,
          "time_in_millis" : 3997
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 22,
              "processed_files" : 22,
              "total_size_in_bytes" : 1381698,
              "processed_size_in_bytes" : 1381698,
              "start_time_in_millis" : 1465213611530,
              "time_in_millis" : 3997
            }
          }
        }
      },
      "twitterv1" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 0,
          "failed" : 8,
          "total" : 8
        },
        "stats" : {
          "number_of_files" : 1687,
          "processed_files" : 532,
          "total_size_in_bytes" : 892693158097,
          "processed_size_in_bytes" : 271188865262,
          "start_time_in_millis" : 1465213597920,
          "time_in_millis" : 2580402
        },
        "shards" : {
          "6" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 221,
              "processed_files" : 65,
              "total_size_in_bytes" : 124216528136,
              "processed_size_in_bytes" : 41597344471,
              "start_time_in_millis" : 1465213597987,
              "time_in_millis" : 2580283
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "7" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 218,
              "processed_files" : 69,
              "total_size_in_bytes" : 127543286629,
              "processed_size_in_bytes" : 38811410641,
              "start_time_in_millis" : 1465213610744,
              "time_in_millis" : 2567483
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "2" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 227,
              "processed_files" : 74,
              "total_size_in_bytes" : 127390735566,
              "processed_size_in_bytes" : 36220956572,
              "start_time_in_millis" : 1465213597877,
              "time_in_millis" : 2580272
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "3" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "4" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 245,
              "processed_files" : 111,
              "total_size_in_bytes" : 125007807167,
              "processed_size_in_bytes" : 41813102390,
              "start_time_in_millis" : 1465213597920,
              "time_in_millis" : 2580319
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "5" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 265,
              "processed_files" : 65,
              "total_size_in_bytes" : 134373968301,
              "processed_size_in_bytes" : 39058558697,
              "start_time_in_millis" : 1465213598117,
              "time_in_millis" : 2580205
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "0" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 256,
              "processed_files" : 86,
              "total_size_in_bytes" : 130201274206,
              "processed_size_in_bytes" : 39822216979,
              "start_time_in_millis" : 1465213598039,
              "time_in_millis" : 2580168
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          },
          "1" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 255,
              "processed_files" : 62,
              "total_size_in_bytes" : 123959558092,
              "processed_size_in_bytes" : 33865275512,
              "start_time_in_millis" : 1465213628544,
              "time_in_millis" : 2549621
            },
            "reason" : "IndexShardSnapshotFailedException[Aborted]"
          }
        }
      }
    }
  } ]
}
</description><key id="159088168">18775</key><summary>snapshot delete getting stuck on 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devarajphukan</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>feedback_needed</label></labels><created>2016-06-08T06:24:00Z</created><updated>2017-03-31T14:47:52Z</updated><resolved>2017-03-31T14:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-13T16:54:33Z" id="225641641">@imotov @dadoonet any ideas what might be happening here?
</comment><comment author="imotov" created="2016-06-13T17:10:54Z" id="225646158">@devarajphukan did you wait for the delete operation to finish before starting a new snapshot?
</comment><comment author="devarajphukan" created="2016-06-13T17:34:51Z" id="225652840">@imotov, the curl request for delete was still hanging on when I tried a new snapshot process. Had to do a rolling restart of the whole cluster to stop the delete request and start a new snapshot. Correct me if I am wrong but from the documentation what I understood was we can make a delete request on an ongoing snapshot process to stop it. 
Thanks. 
</comment><comment author="imotov" created="2016-06-13T18:16:00Z" id="225663960">@devarajphukan yes, the delete request was in the process of cancelling the running snapshot but got stuck for some reason. How long was the delete operation hanging before you tried to start a new snapshot? Did you have restart the whole cluster? I would expect just restarting the current master should have been enough.
</comment><comment author="devarajphukan" created="2016-06-13T18:20:18Z" id="225665122">@imotov, the delete request was hanging on for more than 8 hours. I did not know that only a restart of the current master would have solved it, so did it for the whole cluster.
</comment><comment author="imotov" created="2016-06-17T23:41:27Z" id="226905754">It's possible that there are still some issues with azure plugins that causing it get stuck in waiting for the response forever. However since you have restarted the cluster, it's hard to tell what exactly was going on there. If it will happen again, please run the following command on the cluster where the snapshot is stuck and include its result with your report:

```
curl "http://localhost:9200/_nodes/hot_threads?threads=10000&amp;ignore_idle_threads=false"
```
</comment><comment author="devarajphukan" created="2016-06-22T05:37:50Z" id="227648394">@imotov the above curl request works well to stop the delete process which is stuck. This command stopped both the taking snapshot and deleting snapshot processes and the status is "FAILED". After this that particular snapshot needs to be deleted again.
</comment><comment author="imotov" created="2016-06-22T18:23:20Z" id="227833448">@devarajphukan yes, the `DELETE` command is supposed to stop the current snapshot, it might take a while but it shouldn't hang for 8 hours. What I am saying is, if you will have a situation when you will have a snapshot stuck and `DELETE` command will hang for a few hours, please execute the `hot_nodes` command that I mentioned above and paste here the result. Does it make sense?
</comment><comment author="devarajphukan" created="2016-06-22T19:03:24Z" id="227844990">@imotov, yes sure. Will get back with the results, when I get the same issue. Anyway, thanks for the help.
</comment><comment author="devarajphukan" created="2016-06-22T19:05:42Z" id="227845596">For anyone getting the similar issue here's a script in python you can run as a cron job that takes snapshots, waits for it to complete and deletes older ones. https://github.com/devarajphukan/ElasticSearchSnapshotting/blob/master/take_snapshot_cron_script.py 
</comment><comment author="colings86" created="2017-03-31T14:47:51Z" id="290732824">No further feedback. Please reopen if this issue is still reproducable</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query/Filter performance loss from 1.4.2 to 2.3.3 on dynamically updated index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18774</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.3

**JVM version**: 1.8.0_31-b13

**OS version**: Ubuntu 14.04.1 LTS

We are upgrading from 1.4.2 to 2.3.3 and are seeing some significant performance loss for several of our filtered queries against a dynamically updated index. For simplicity, I'll focus on just one query for this issue.

The query in question: https://gist.github.com/gpstathis/f96c048d0afe8f2bfce4753ffc9d555b

I am testing this same query on two different clusters, one 1.4.2 and one 2.3.3, loaded with the same data and getting the same real-time updates (new docs arriving at about 20 o 50 per second). Both clusters are using the same machine specs and the indices are sized with the same number of shards.

On a corpus of about 1 billion docs this query/filter yields about 88k hits. It takes about 5 seconds to execute on 1.4.2 while on 2.3.3 the time has more than doubled to 13 seconds. 

Here are the hot threads for the query on 1.4.2: https://gist.github.com/gpstathis/12ef17f104e3e2d7fafbae10d6eb9267

And now hot threads for the query on 2.3.3: https://gist.github.com/gpstathis/9196625858d4ca0b4311e5d288bae55e

From what I can tell, we are not benefiting from caching in either environment so I am not clear why 2.3.3, despite the new autocaching, would be penalized so much.

Now the real head scratcher is this: the query times and hot threads above are from production—we also have a QA 2.3.3 cluster with a similar size corpus as prod but sized with half the nodes to save on cost (same number of shards). This QA cluster does not receive real-time updates like the prod cluster does. Even with half the number of nodes, the smaller 2.3.3 QA cluster performs similarly to our old 1.4.2 prod cluster and is therefore **faster** than the larger 2.3.3 cluster.

From the above, it seems that the real-time updates have a much more adverse effect on 2.3.3 compared to pre-2.x versions?
</description><key id="159086575">18774</key><summary>Query/Filter performance loss from 1.4.2 to 2.3.3 on dynamically updated index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">gpstathis</reporter><labels><label>:Search</label></labels><created>2016-06-08T06:10:00Z</created><updated>2016-06-17T14:02:30Z</updated><resolved>2016-06-16T10:03:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gpstathis" created="2016-06-08T16:55:43Z" id="224656141">Here is the profiler output:

https://gist.github.com/gpstathis/d284e2ad8b8ac3830901549c558ce36b

Because of #18693, take the numbers with a grain of salt but one can still see that the majority of the time is spent on iterating over the matches which is validated when reducing the date range in the query: the shorter the date range the faster the response times.
</comment><comment author="nik9000" created="2016-06-08T17:45:22Z" id="224671698">I don't know much about GlobalOrdinals, but it looks like they come up in this issue. Does anyone who knows more want to take a look?
</comment><comment author="gpstathis" created="2016-06-09T04:35:34Z" id="224797964">Thanks @nik9000. I don't know if this is only for Aggregations, but according to https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html#global-ordinals:

&gt; Once global ordinals have been rebuilt, they will be reused until the segments in the index change: after a refresh, a flush, or a merge

So if we update the index frequently and keep merging, we don't really give global ordinals the chance to build up. In theory then, if we extend `refresh_interval` we should be able to see the benefits from global ordinals. I tested this by setting `refresh_interval` to `120s`—there was no difference in the response times.
</comment><comment author="jpountz" created="2016-06-09T08:23:15Z" id="224830379">I could be wrong, but since documents are only allowed to have a single parent, you should be able to run this query with a single join instead of 3.

``` javascript
{
  "from": 0,
  "size": 25,
  "query": {
    "constant_score": {
      "filter": {
        "bool": {
          "filter": [
            {
              "has_parent": {
                "query": {
                  "bool": {
                    "filter": [
                      {
                        "terms": {
                          "itype": [
                            "person"
                          ]
                        }
                      },
                      {
                        "term": {
                          "tags": "bf0c5985e9644cf3:infs/acct/"
                        }
                      },
                      {
                        "term": {
                          "tags": "bf0c5985e9644cf3:infs/proj/21694/"
                        }
                      }
                    ]
                  }
                },
                "parent_type": "influencer"
              }
            },
            {
              "range": {
                "publishedDate": {
                  "from": "2015-04-29T00:00:00-0400",
                  "to": "2016-06-02T23:59:59-0400",
                  "include_lower": true,
                  "include_upper": true
                }
              }
            }
          ]
        }
      }
    }
  },
  "fields": "*",
  "sort": [
    {
      "publishedDate": {
        "order": "desc"
      }
    }
  ]
}
```

Given that the hot threads suggest that the bottleneck is the join, this should help on both versions and might also help reduce the gap. 
</comment><comment author="gpstathis" created="2016-06-09T13:46:07Z" id="224899193">Thanks @jpountz . These queries are dynamically constructed so it looks like we need to go back and take closer look at the code that does the joins on `_parent`.

Unfortunately, I tested it, and while rewriting it does shave about 1 second from the 13 seconds it used to take on 2.3.3, it's still a 2x performance degradation from 1.4.2. 

Here are the hot threads while running it: https://gist.github.com/gpstathis/1b508d08e4e612cf9dd0f0224bfabfcd

So there is still a very large gap that we are striving to explain here.
</comment><comment author="jpountz" created="2016-06-09T14:15:58Z" id="224908003">I agree the fact that it is still more than 2x slower than 1.x is puzzling. Can you tell us:
- how many parents and children documents the index has
- how many documents match each individual query (the range, the has_parent, the bool under the has_parent, and each query under the bool)
- are there parts of the query that are often the same?
</comment><comment author="gpstathis" created="2016-06-09T17:33:35Z" id="224968813">@jpountz sent the numbers to your Gmail address.
</comment><comment author="jpountz" created="2016-06-09T21:15:51Z" id="225029215">Thanks, these numbers will help me reproduce the problem locally. Could you also give again the output of the profiler, but with the new query this time?
</comment><comment author="gpstathis" created="2016-06-09T21:30:37Z" id="225032893">@jpountz here you go: https://gist.github.com/gpstathis/e2e4f3bacd9462ebfd63672a35db278a
</comment><comment author="jpountz" created="2016-06-10T18:15:28Z" id="225256558">I have been digging a bit, there seem to be several factors at play, but the main one seems to me about how we run conjunctions. We used to have optimizations for very specific cases through FilteredQuery that got lost when we replaced it with BooleanQuery and switched to different bitset impls for the cache (roaring instead of plain bit sets, and FilteredQuery/BooleanFilter had optimizations for the bitset case, look for eg. `RandomAccessFilterStrategy` in the 1.x hot threads). I started working on some changes in Lucene (see https://issues.apache.org/jira/browse/LUCENE-7330 and https://issues.apache.org/jira/browse/LUCENE-7332) but they won't be available until elasticsearch 5.x.

The fact that parent/child runs on doc values rather than in-memory fielddata in 2.x probably also explains part of the performance hit, but it is unclear how much. I would expect the contribution to be less than the conjunction thing though.
</comment><comment author="gpstathis" created="2016-06-10T23:52:58Z" id="225321826">Thanks for taking a look @jpountz .
</comment><comment author="jpountz" created="2016-06-16T10:03:10Z" id="226442462">I also opened https://issues.apache.org/jira/browse/LUCENE-7339, which I think also explains the slow down that you have been seeing. Even though the benchmark on the issue sees a 6% jump, this happens with a range that is _not_ cached, so it would likely be better with a cached range (which I think could be the case in your queries since you said the range should get reused).

If you don't mind I will now close this issue. Things will hopefully get better in 5.0.
</comment><comment author="gpstathis" created="2016-06-16T20:48:06Z" id="226609185">Thanks @jpountz . Is 5.0 the earliest version that will benefit from these patches?
</comment><comment author="jpountz" created="2016-06-17T08:06:43Z" id="226707140">I expect these changes to be in Lucene 6.2, which means they should be in elasticsearch 5.1 in the worst case, but likely in elasticsearch 5.0. However it won't be in any 2.x release.
</comment><comment author="gpstathis" created="2016-06-17T14:02:29Z" id="226776893">@jpountz ok thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change the default of `include_global_state` from true to false for snapshot restores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18773</link><project id="" key="" /><description>Resolves #18569
</description><key id="159025526">18773</key><summary>Change the default of `include_global_state` from true to false for snapshot restores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T21:10:48Z</created><updated>2016-06-08T17:15:16Z</updated><resolved>2016-06-08T17:15:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-08T09:19:24Z" id="224534974">Left some comments. I think the intention of #18569 was to change the `includeGlobalState` default only for restores.
</comment><comment author="dakrone" created="2016-06-08T16:14:36Z" id="224643135">@ywelsch you're right, I just made the migration note incorrect. I've updated the title of this as well as the migration note to be clear about that, thanks for looking!
</comment><comment author="ywelsch" created="2016-06-08T16:19:54Z" id="224644735">@dakrone can you also update the snapshot / restore documentation (`snapshots.asciidoc`)? The first section there reads `By default, all indices in the snapshot as well as cluster state are restored.` and continues talking about `include_global_state`.
</comment><comment author="dakrone" created="2016-06-08T16:30:44Z" id="224647822">@ywelsch good idea, I've updated it.
</comment><comment author="ywelsch" created="2016-06-08T16:43:59Z" id="224651993">2 more minor comments, LGTM o.w.. Feel free to push once addressed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add sequence numbers to cat shards API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18772</link><project id="" key="" /><description>This commit adds the max sequence number and local and global
checkpoints to the cat shards API.
</description><key id="159006978">18772</key><summary>Add sequence numbers to cat shards API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Sequence IDs</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T19:40:18Z</created><updated>2016-06-08T12:21:38Z</updated><resolved>2016-06-08T12:21:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-07T19:51:39Z" id="224394014">This gives output like:

```
15:50:54 [jason:~] $ curl -XGET "localhost:9200/_cat/shards?h=index,shard,prirep,docs,seq_no.*&amp;v"
index shard prirep docs seq_no.max seq_no.local_checkpoint seq_no.global_checkpoint
i     4     p       111        221                     221                      221
i     4     r       111        221                     221                      221
i     1     r        97        193                     193                      193
i     1     p        97        193                     193                      193
i     2     p       108        215                     215                      215
i     2     r       108        215                     215                      215
i     3     r        98        195                     195                      195
i     3     p        98        195                     195                      195
i     0     p        98        195                     195                      195
i     0     r        98        195                     195                      195
```
</comment><comment author="bleskes" created="2016-06-08T05:49:59Z" id="224494787">LGTM. Do you use it for debugging?
</comment><comment author="tlrx" created="2016-06-08T07:19:27Z" id="224509244">Left a comment - otherwise LGTM
</comment><comment author="jasontedor" created="2016-06-08T10:40:22Z" id="224552858">&gt; Do you use it for debugging?

@bleskes Yes, I just find it easier than filtering the indices stats API.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Add sequence numbers to cat shards API</comment></comments></commit></commits></item><item><title>Add Method to Get New MethodWriters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18771</link><project id="" key="" /><description>Add a method in MethodWriter to allow other MethodWriters to be created for adding static methods to the Executable class on-the-fly.
</description><key id="158991568">18771</key><summary>Add Method to Get New MethodWriters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T18:25:56Z</created><updated>2016-06-28T09:32:17Z</updated><resolved>2016-06-07T18:47:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-06-07T18:47:13Z" id="224376914">+1
</comment><comment author="jdconrad" created="2016-06-07T18:47:27Z" id="224376975">Thanks for the review @rmuir.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/MethodWriter.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/Writer.java</file><file>modules/lang-painless/src/main/java/org/elasticsearch/painless/WriterConstants.java</file></files><comments><comment>Merge pull request #18771 from jdconrad/methods</comment></comments></commit></commits></item><item><title>elasticsearch removing black slash when running plugin command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18770</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.3

**JVM version**: 1.7.0_71-b14

**OS version**: Windows 7

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Add an environment variable called JAVA_HOME, and set D:\Java7u71\jdk.
2. Set %JAVA_HOME%\bin in the PATH variable
3. Go to the directory path of where the elasticsearch folder is. And run the following command: ./bin/plugin -i elasticsearch/marvel/latest [This is from the "Get Started" video]
   1. I see the following error: ./bin/plugin: line 113: D:Java7u71jdk/bin/java: No such file or directory
   2. I did change my path to forward slash and it worked, but I felt it should have worked either way.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: I am just getting started with Elastic Search. I am following the video from the link: https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43138086 and I am currently running into an issue where the elasticsearch plugin is not properly getting the environment variable name correctly where it is removing all the blackslash. D:\Java7u71\jdk\bin. I would like to also note that the command to install the plugin no longer works and feels like there should have been some kind of comments left below the video or something.
</description><key id="158970892">18770</key><summary>elasticsearch removing black slash when running plugin command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mhkim91</reporter><labels /><created>2016-06-07T16:44:44Z</created><updated>2016-06-07T18:08:37Z</updated><resolved>2016-06-07T17:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-07T17:07:25Z" id="224347676">Something does not seem right here. For example `./bin/plugin` should immediately error in a traditional Windows command prompt because `/` is not a valid path separator there. It should say something to the effect of `.` not being recognized as a command.

So, my question for you is what terminal are you attempting to use? I think that you should just use the traditional command prompt.

&gt; ./bin/plugin -i elasticsearch/marvel/latest

Ignoring the oddity with the slashes, this is old syntax; it should be `bin\plugin install marvel-agent`:

```
C:\Users\IEUser\elasticsearch\elasticsearch-2.3.2&gt;set JAVA_HOME=C:\Program Files\Java\jdk1.8.0_74

C:\Users\IEUser\elasticsearch\elasticsearch-2.3.2&gt;bin\plugin install marvel-agent
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.2/marvel-agent-2.3.2.zip ...
Downloading ..........DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.2/marvel-agent-2.3.2.zip checksums if available ...
Downloading .DONE
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission setFactory
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
Installed marvel-agent into C:\Users\IEUser\elasticsearch\elasticsearch-2.3.2\plugins\marvel-agent

C:\Users\IEUser\elasticsearch\elasticsearch-2.3.2&gt;
```
</comment><comment author="mhkim91" created="2016-06-07T17:39:37Z" id="224356832">@jasontedor Sorry. I forgot to point out that I am using MINGW32 or Git Bash shell on my window machine.
</comment><comment author="jasontedor" created="2016-06-07T17:48:36Z" id="224359499">&gt; Sorry. I forgot to point out that I am using MINGW32 or Git Bash shell on my window machine.

That is as I suspected; I'm sorry, but we do not support that and it's not going to work. For example, the `plugin` script is going to check for the existence of `$JAVA_HOME/bin/java` but on Windows it should be `java.exe` because Windows.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Installation and removal instructions for Ingest Plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18769</link><project id="" key="" /><description>There's no place in the v5 docs so far that correctly identifies the plugin IDs to be used to install the Geoip and Attachment plugins into Elasticsearch. This adds the instructions to the documentation.
</description><key id="158968388">18769</key><summary>Installation and removal instructions for Ingest Plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>docs</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T16:32:41Z</created><updated>2016-06-08T19:55:59Z</updated><resolved>2016-06-08T19:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-07T17:59:41Z" id="224362772">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Installation and removal instructions for Ingest Plugins (#18769)</comment></comments></commit></commits></item><item><title>Cache FieldStats in the request cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18768</link><project id="" key="" /><description>This caches FieldStats at the field level. For one off requests or for
few indicies this doesn't save anything, but when there are 30 indices,
5 shards, 1 replica, 100 parallel requests this is about twice as fast
as not caching. I expect lots of usage won't see much benefit from this
but pointing kibana to a cluster with many indexes and shards, will be
faster.

Closes #18717
</description><key id="158968240">18768</key><summary>Cache FieldStats in the request cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cache</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T16:32:01Z</created><updated>2016-06-14T18:15:43Z</updated><resolved>2016-06-14T17:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-06-08T06:28:58Z" id="224500920">@nik9000 I left some minor comments. LGTM otherwise.
</comment><comment author="nik9000" created="2016-06-08T20:26:28Z" id="224716652">@s1monw I pulled out some of IndexShardCacheEntity for nice unit testing without too much trouble.

Making FieldStats use opaque values is a bit more trouble. The stickiest bit is about apply IndexConstraints. I could probably do it but it'd be a bit ugly.
</comment><comment author="nik9000" created="2016-06-13T17:44:36Z" id="225655426">@s1monw done with the last round.
</comment><comment author="s1monw" created="2016-06-14T07:11:07Z" id="225797753">LGTM - left a minor no further review rounds needed
</comment><comment author="nik9000" created="2016-06-14T18:15:43Z" id="225968841">Thanks for all the reviewing @jimferenczi and @s1monw !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove explicit parallel new GC flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18767</link><project id="" key="" /><description>Unless explicitly disabled, the parallel new collector is enabled
automatically as soon as the CMS collector is enabled:

``` c++
  void Arguments::set_cms_and_parnew_gc_flags() {
    assert(
      !UseSerialGC &amp;&amp; !UseParallelOldGC &amp;&amp; !UseParallelGC,
      "Error");
    assert(UseConcMarkSweepGC, "CMS is expected to be on here");

    // If we are using CMS, we prefer to UseParNewGC,
    // unless explicitly forbidden.
    if (FLAG_IS_DEFAULT(UseParNewGC)) {
      FLAG_SET_ERGO(bool, UseParNewGC, true);
    }
```

While it's fine to be explicit, the UseParNewGC flag is deprecated in JDK
8 and produces warning messages in JDK 9:

```
  Java HotSpot(TM) 64-Bit Server VM warning: Option UseParNewGC was
  deprecated in version 9.0 and will likely be removed in a future
  release.
```

Thus, we can and should just remove this flag from the default JVM
options.
</description><key id="158961799">18767</key><summary>Remove explicit parallel new GC flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T16:02:40Z</created><updated>2016-06-07T16:12:42Z</updated><resolved>2016-06-07T16:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-06-07T16:03:40Z" id="224328751">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove explicit parallel new GC flag</comment></comments></commit></commits></item><item><title>Fix sync flush total shards statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18766</link><project id="" key="" /><description>When sync flush fails for a shard with a certain shard id, we currently report the failure with the total of shards of all indices for which sync flush was called instead of only the total of shards with that shard id.
</description><key id="158959452">18766</key><summary>Fix sync flush total shards statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Stats</label><label>bug</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T15:53:22Z</created><updated>2016-06-23T16:44:08Z</updated><resolved>2016-06-10T11:39:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-07T16:26:24Z" id="224335702">Is there a test that can be added?
</comment><comment author="bleskes" created="2016-06-07T16:28:39Z" id="224336382">LGTM. left a minor suggestion. A test will be great.
</comment><comment author="ywelsch" created="2016-06-08T08:18:34Z" id="224520758">@jasontedor @bleskes I've added an integration test as I could not find a good way to test this (it only manifests when there are transport errors).
</comment><comment author="s1monw" created="2016-06-08T09:55:07Z" id="224543358">isn't disruption a bit of an overkill for this? Maybe look at SyncedFlushSingleNodeTests and create an index with 2 shards and close one shard manually before running the request?
</comment><comment author="ywelsch" created="2016-06-08T10:03:25Z" id="224545287">@s1monw a closed shard will not trigger the broken code here. AFAICS the only possible way to trigger this code is on network disruptions.
</comment><comment author="s1monw" created="2016-06-08T10:18:34Z" id="224548495">ok I still wonder if we should do it given the _heaviness_ of the test
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java</file></files><comments><comment>Fix sync flush total shards statistics (#18766)</comment></comments></commit></commits></item><item><title>Move search template to lang-mustache module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18765</link><project id="" key="" /><description>Related to  #17906
</description><key id="158957665">18765</key><summary>Move search template to lang-mustache module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>breaking-java</label><label>v5.0.0-alpha4</label></labels><created>2016-06-07T15:46:09Z</created><updated>2016-07-29T11:58:35Z</updated><resolved>2016-06-23T07:37:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-06-07T16:06:26Z" id="224329644">I think this'll need the `BREAKING` label because it changes the way you use templates on the transport client.
</comment><comment author="tlrx" created="2016-06-08T12:42:20Z" id="224576872">Thanks for your reviews - I just updated the code according to your comments, let me know what you think!
</comment><comment author="s1monw" created="2016-06-08T12:50:22Z" id="224578722">thanks you for working on this!
</comment><comment author="martijnvg" created="2016-06-08T16:30:02Z" id="224647613">@tlrx This is looking good! I left some comments.
</comment><comment author="tlrx" created="2016-06-09T12:24:25Z" id="224879001">@martijnvg Thanks! I updated the code.
</comment><comment author="martijnvg" created="2016-06-09T14:34:48Z" id="224913972">@tlrx I left some comments. The fact that the render api has been replaced with the simulate option should documented in the breaking changes and also the template msearch api should be briefly documented in `search-template.asciidoc`. This getting closer. Lets replaced the WIP label with review label?
</comment><comment author="tlrx" created="2016-06-10T11:01:39Z" id="225154210">@martijnvg Thanks again! Code is ready for another round of review.

Note: as a follow up PR I think we should clean the remaining messy tests in the lang-mustache module.
</comment><comment author="tlrx" created="2016-06-10T12:10:58Z" id="225165740">Ooops, forgot to push the doc for the multi search template, sorry.
</comment><comment author="martijnvg" created="2016-06-10T12:24:59Z" id="225168290">Thanks @tlrx! LGTM 

&gt; as a follow up PR I think we should clean the remaining messy tests in the lang-mustache module.

Agreed, these tests should either not depends on mustache, if they they then they should become rest tests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>error when performing the sum aggregation on integer field on ES v1.7.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18764</link><project id="" key="" /><description>Elasticsearch version: 1.7.0

`"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[k9c_iGB6TM-myDJQElFHDA][bucketname][0]: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }`

I'm running into the above error when executing the "sum" aggregation on a field that is defined as integer.  However, when I changed the mapping from integer to long, the "sum" aggregation worked without any issue.

I can change the mapping from integer to long to work around my issue but is the "sum" aggregation on integer type field not supported on ES v1.7.0? I couldn't find any documentation regarding this restriction/limitation.

Thanks.
</description><key id="158940571">18764</key><summary>error when performing the sum aggregation on integer field on ES v1.7.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">david-ts</reporter><labels /><created>2016-06-07T14:40:49Z</created><updated>2016-06-07T16:08:57Z</updated><resolved>2016-06-07T16:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T16:08:57Z" id="224330423">Hi @david-ts 

This sounds like you've run into a mapping bug, where the field mapping is different either in different types (visible via the mappings API) or different per shard (not visible via the mapping).

When you specified the mapping explicitly and reindexed, it would have fixed the mapping conflict.  I suggest upgrading - we no longer have these issues in ES 2.3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty response when Origin, U-A and Referer set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18763</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.2

**JVM version**: 8

**Description of the problem including expected versus actual behavior**:
Certain requests lead to `net::ERR_EMPTY_RESPONSE` or equivalent `curl: (52) Empty reply from server` when all Referer, Origin and User-Agent headers are specified.

**Steps to reproduce**:
1. Start a server with allow_cors: true and allow_cors_origin: "*"
2. Try to use server at IP:9200 (works)
3. Try to use from a different host - fails

It seems something fishy is going on with the header parsing. Combination of Referrer, Origin and User-Agent headers make it fail. Just two of the three headers work as far as I can tell.

**Provide logs (if relevant)**:
### CURL attempts:

```
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000' -H 'Save-Data: on' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36' --compressed
curl: (52) Empty reply from server
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36' --compressed
curl: (52) Empty reply from server
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
curl: (52) Empty reply from server
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000'
{
  "name" : "Bonita Juarez",
  "cluster_name" : "myesdb",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
[madhav:~/Code/es-test] $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000'
{
  "name" : "Bonita Juarez",
  "cluster_name" : "myesdb",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
[madhav:~/Code/es-test] $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
curl: (52) Empty reply from server
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'Origin: http://localhost:8000' -H 'User-Agent: Mozilla/5.0'
curl: (52) Empty reply from server
[madhav:~/Code/es-test] $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Origin: http://localhost:8000' -H 'User-Agent: Mozilla/5.0'
curl: (52) Empty reply from server
[madhav:~/Code/es-test] 52 $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
{
  "name" : "Bonita Juarez",
  "cluster_name" : "myesdb",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
[madhav:~/Code/es-test] $ curl 'http://REDACTED:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost:8000/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
{
  "name" : "Bonita Juarez",
  "cluster_name" : "myesdb",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
```
### Relevant (seeming) log message:

```
[2016-06-07 13:15:21,461][WARN ][http.netty               ] [Bonita Juarez] Caught exception while handling client http traffic, closing connection [id: 0xd2005f5f, /10.188.0.1:14847 =&gt; /10.188.0.4:9200]
java.lang.IllegalStateException: cannot send more responses than requests
    at org.jboss.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:101)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:292)
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:105)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.jboss.netty.channel.Channels.write(Channels.java:704)
    at org.jboss.netty.channel.Channels.write(Channels.java:671)
    at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:348)
    at org.elasticsearch.http.netty.cors.CorsHandler.forbidden(CorsHandler.java:130)
    at org.elasticsearch.http.netty.cors.CorsHandler.messageReceived(CorsHandler.java:84)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="158922209">18763</key><summary>Empty response when Origin, U-A and Referer set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">codebreach</reporter><labels><label>:REST</label></labels><created>2016-06-07T13:25:23Z</created><updated>2016-06-07T20:55:34Z</updated><resolved>2016-06-07T20:50:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-06-07T16:48:15Z" id="224342108">@codebreach Using a 2.3.2 installation, I set `http.cors.enabled` to `true` and `http.cors.allow-origin` to `"*"` in `elasticsearch.yml`.  I subsequently ran all of your commands listed, and all of them returned to me a successful response.  CORS handling does not take into account the `Referrer` and as of v2.3.3, the `User-Agent` is not taken into account either (https://github.com/elastic/elasticsearch/pull/18283).

Could you kindly elaborate on what you mean by trying to use on IP:9200 works but on a different server it fails?

Your stack trace is disconcerting as it seems to indicate the netty channel is responding twice to the same request.  

Are you able to reproduce this reliably?
</comment><comment author="codebreach" created="2016-06-07T18:25:24Z" id="224370344">Regarding the IP: _sorry got too lost copying over stack traces_

If I talk directly to IP:9200 (using chrome at http://IP:9200/) everything works just fine
BUT if I use something like elasticsearch-head running in its own server (basically try to hit the end point IP:9200 from a different origin), all hell breaks lose

I can diagnose more if you have any tips. As I am pretty sure this 'bug' affects a large enough surface area that it should have been reported already, so it could be something else causing it.

FWIW, I am running in kubernetes behind a loadbalancer (on google container engine). Not sure if that can make this blow up. I am going to try to hit the node directly without the LB and see if that changes anything.
</comment><comment author="codebreach" created="2016-06-07T18:29:29Z" id="224371545">Also it seems I have `cors_allow_origin` set to `localhost` and not `"*"` as reported earlier.
</comment><comment author="abeyad" created="2016-06-07T18:38:59Z" id="224374533">@codebreach if you are trying to hit the end point from a different origin, and you only have the `http.cors.allow-origin` set to `localhost`, then the request should be denied, right?
</comment><comment author="codebreach" created="2016-06-07T19:50:54Z" id="224393826">Yes I have `http.cors.allow-origin` set to `localhost` and have tried `curl` with `Origin` and `Referer` set to `localhost` with same result.
Also I tried to set `http.cors.allow-origin` to `"*"` and also ran `curl` from my laptop; within my cluster (both with and without LB) with same results:

```
$ kubectl exec curl-deployment-2188210394-dlwy4 -- curl 'http://10.191.246.9:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost/' -H 'Origin: http://localhost/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (52) Empty reply from server
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 52


$ kubectl exec curl-deployment-2188210394-dlwy4 -- curl 'http://10.188.0.6:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost/' -H 'Origin: http://localhost/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (52) Empty reply from server
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 52
```
</comment><comment author="codebreach" created="2016-06-07T19:51:15Z" id="224393918">I also played around a bit and found something shady:

#### This works:

```
curl 'http://IPADDRESS:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost/' -H 'Origin: http://localhost/' -H 'User-Agent: Mozill'
```

#### This doesn't:

```
curl 'http://IPADDRESS:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://localhost/' -H 'Origin: http://localhost/' -H 'User-Agent: Mozilla'
```

Note the only difference is `'User-Agent: Mozill'` and `'User-Agent: Mozilla'` 
`Mozills` `MozillGIBBRISH` and anything that doesn't start with Mozilla works!!?
</comment><comment author="abeyad" created="2016-06-07T20:00:49Z" id="224396435">&gt; Note the only difference is 'User-Agent: Mozill' and 'User-Agent: Mozilla' 
&gt; Mozills MozillGIBBRISH and anything that doesn't start with Mozilla works!!?

That's correct.  All browsers send `Mozilla` followed by something else, hence the check.  The `User-Agent` check is actually completely removed in v2.3.3.

&gt; Also I tried to set http.cors.allow-origin to "*" and also ran curl from my laptop; within my cluster (both with and without LB) with same results:

So if you run instead (taking out the `Referrer`):

```
$ kubectl exec curl-deployment-2188210394-dlwy4 -- curl 'http://10.188.0.6:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Origin: http://localhost/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
```

Then you are saying that works fine for you?  
</comment><comment author="codebreach" created="2016-06-07T20:12:15Z" id="224399414">Without `Referer` 👎 
`$ kubectl exec curl-deployment-2188210394-dlwy4 -- curl 'http://10.188.0.6:9200/' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Origin: http://localhost/' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'`
    curl: (52) Empty reply from server
</comment><comment author="abeyad" created="2016-06-07T20:14:52Z" id="224400089">So what is the command that actually works for you, in the exact same setup in which you are running these commands?  I ask because you mentioned in the beginning that specifying 2 out of the 3 headers works for you.
</comment><comment author="codebreach" created="2016-06-07T20:17:16Z" id="224400743">The commands as listed are straight from the terminal. User-Agent seems to be the culprit.

Aside: is there a way to see the config (cors and everything) that the ES server is running with?
</comment><comment author="abeyad" created="2016-06-07T20:24:33Z" id="224402713">The settings you are probably looking for are:

```
curl -XGET "localhost:9200/_nodes?pretty"
```

What do you mean by User-Agent is the culprit?  Could you try just setting `User-Agent` to `Mozilla XYZ` instead of `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36` and see if that makes a difference?
</comment><comment author="codebreach" created="2016-06-07T20:50:08Z" id="224409933">Thanks - I just double checked, CORS is enabled in config:

``` json
       "http" : {
          "compression" : "true",
          "enabled" : "true",
          "cors" : {
            "allow_origin" : "*",
            "enabled" : "true"
          }
        },
```

HOWEVER 
there are no CORS headers! I looked at #9031 and #16689 and seems they would get CORS headers when UA was Mozilla and not otherwise, but I am getting an empty response

```
$ curl -H "User-Agent: Mozilla" -H "Origin: http://example.com" -i http://localhost:9200/
curl: (52) Empty reply from server
$ curl -H "User-Agent: Mozill" -H "Origin: http://example.com" -i http://localhost:9200/
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 309

{
  "name" : "Urthona",
  "cluster_name" : "myesdb",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="codebreach" created="2016-06-07T20:50:16Z" id="224409978">Andd I just wasted your time because of a typo :(
`allow_origin` was specified and not `allow-origin`

Still might be worth it to look at why that causes the server to start throwing exceptions. But I'm going to close this out.
Thanks for all the help investigating. 
</comment><comment author="abeyad" created="2016-06-07T20:55:34Z" id="224411511">@codebreach No problem!  Glad it didn't end up being a blocker for you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix grammatical error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18762</link><project id="" key="" /><description /><key id="158888157">18762</key><summary>Fix grammatical error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">suryatech</reporter><labels /><created>2016-06-07T10:19:38Z</created><updated>2016-06-07T15:53:11Z</updated><resolved>2016-06-07T15:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T15:53:11Z" id="224325381">thanks @suryatech - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fails to bootstrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18761</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**2.3.0**:

**OpenJDK 64-Bit Server VM (build 9-internal+0-2016-04-14-195246.buildd.src, mixed mode)**:

**Ubuntu 16.04 x64**:

**Elasticsearch fails to start**:

I installed java9, elasticsearch 2.3.0 under non-root (elasticserch) user.
I gave the user sudo privileges

When I try to access elasticsearch I get 

``` bash
 curl localhost:9200/_cat/aliases
```

curl: (7) Failed to connect to localhost port 9200: Connection refused

This is what I see in the /var/log/elasticsearch/elasticsearch.log

`[2016-06-07 03:00:26,614][WARN ][env                      ] [Ape-X] max file descriptors [65535] for elasticsearch process likely too low, consider increasing to at least [65536]
[2016-06-07 03:00:26,663][ERROR][bootstrap                ] Exception
java.lang.Error: java.security.AccessControlException: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
        at com.twitter.jsr166e.Striped64.&lt;clinit&gt;(Striped64.java:319)
        at org.elasticsearch.common.metrics.CounterMetric.&lt;init&gt;(CounterMetric.java:28)
        at org.elasticsearch.common.util.concurrent.EsAbortPolicy.&lt;init&gt;(EsAbortPolicy.java:30)
        at org.elasticsearch.common.util.concurrent.EsExecutors.newFixed(EsExecutors.java:80)
        at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:482)
        at org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:400)
        at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(ThreadPool.java:232)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:170)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.security.AccessControlException: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
        at java.security.AccessControlContext.checkPermission(java.base@9-internal/AccessControlContext.java:468)
        at java.security.AccessController.checkPermission(java.base@9-internal/AccessController.java:894)
        at java.lang.SecurityManager.checkPermission(java.base@9-internal/SecurityManager.java:541)
        at java.lang.reflect.AccessibleObject.checkPermission(java.base@9-internal/AccessibleObject.java:74)
        at java.lang.reflect.Field.setAccessible(java.base@9-internal/Field.java:163)
        at com.twitter.jsr166e.Striped64$1.run(Striped64.java:340)
        at com.twitter.jsr166e.Striped64$1.run(Striped64.java:336)
        at java.security.AccessController.doPrivileged(java.base@9-internal/Native Method)
        at com.twitter.jsr166e.Striped64.getUnsafe(Striped64.java:335)
        at com.twitter.jsr166e.Striped64.&lt;clinit&gt;(Striped64.java:312)
        ... 12 more
`

I googled this issue, but could not find the fix. Could you give me some clue on how to fix it please?
</description><key id="158858783">18761</key><summary>Fails to bootstrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Hirurg103</reporter><labels><label>:Packaging</label></labels><created>2016-06-07T07:40:02Z</created><updated>2016-06-10T19:46:01Z</updated><resolved>2016-06-10T19:45:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-07T15:40:25Z" id="224321430">Hi @Hirurg103 

It's quite likely that 2.x won't work with Java9.  Could you try with Java8 and see what happens?
</comment><comment author="Hirurg103" created="2016-06-07T15:47:15Z" id="224323525">Ok @clintongormley. Don't have my working machine with me, I will try Java8 in 3-4 hours
</comment><comment author="jasontedor" created="2016-06-07T15:47:58Z" id="224323737">I think it should work fine, but can you please try a more recent build of JDK 9; that build that you're using is old in a world of at-least weekly EA builds.
</comment><comment author="Hirurg103" created="2016-06-07T20:51:14Z" id="224410305">@clintongormley I installed java 8 and elastic works fine now. Thank you very much for the help! @jasontedor I will try to install elastic 2.3 with the latest version of JDK 9
</comment><comment author="jasontedor" created="2016-06-10T19:45:57Z" id="225277929">Thanks @Hirurg103. I think that we can close this issue then? Please let me know if you think otherwise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping changes in elasticSearch 2.3.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18760</link><project id="" key="" /><description>**Elasticsearch version:2.3.3

**JVM version**:1.7.0_55"

**OS version**:ubuntu 14.04

*_While upgrading ES to 2.2.0 from 1.4.0 ,it says"analyzer on field [contains] must be set when search_analyzer is set" although i had changed "index_analyzer" to "analyzer"?Same changes was working on other record. What was i missing? 
*_:

**{"error":{"root_cause":[{"type":"mapper_parsing_exception","reason":"analyzer on field [contains] must be set when search_analyzer is set"}],"type":"mapper_parsing_exception","reason":"analyzer on field [contains] must be set when search_analyzer is set"},"status":400}**:
</description><key id="158839931">18760</key><summary>Mapping changes in elasticSearch 2.3.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgc445</reporter><labels /><created>2016-06-07T05:11:46Z</created><updated>2016-06-07T06:13:11Z</updated><resolved>2016-06-07T06:13:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-07T05:44:59Z" id="224184141">Could you provide the mapping you are sending?
</comment><comment author="sgc445" created="2016-06-07T06:03:07Z" id="224187270">Mapping is given below:
"xyz": {
           "type": "multi_field",
           "fields": {
             "xyz": {
               "type": "string",
               "index": "not_analyzed"
             },
             "contains": {
               "type": "string",
               "analyzer": "containsAnalyzer",
               "search_analyzer": "standardAnalyzer"
             }
           }
        },

Thanks,
</comment><comment author="dadoonet" created="2016-06-07T06:13:11Z" id="224189070">Multifield format changed.
Look at the guide.

Not an issue IMO. Closing.

Please use discuss.elastic.co if you need more help about this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item></channel></rss>