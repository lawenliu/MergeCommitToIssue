<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Elasticsearch snapshot,want to snapshot a specific index,but snapshot all indices.Is it a bug？</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11557</link><project id="" key="" /><description>I use the API as follow,

curl -XPUT XXXX/_snapshot/indexBackup/tranpmindex_v1ccbs-2015-05-09?wait_for_completion=true -d '{"indices":"tranpmindex_v1ccb-2015-05-09","include_global_state":"false"}'

The index of tranpmindex_v1ccb-2015-05-09 is only about 90M. However, after 15minutes the API executed, the directory of indexBackup increase to 250G.There are a lot of other indices under this dir.

Why?is it a bug? the version of ES is 1.4.4

Thanks!
</description><key id="86637051">11557</key><summary>Elasticsearch snapshot,want to snapshot a specific index,but snapshot all indices.Is it a bug？</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Judyccb</reporter><labels><label>feedback_needed</label></labels><created>2015-06-09T15:25:35Z</created><updated>2015-06-12T16:28:45Z</updated><resolved>2015-06-12T16:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-10T13:17:07Z" id="110746904">@Judyccb That's strange. I just tried reproducing it with 1.4.4 and everything seems to work as expected - only one index is copied to the repository. What do you get back as a response to this operation?  
</comment><comment author="Judyccb" created="2015-06-10T15:59:06Z" id="110810950">After I input the command and clicked enter, there is no output.I waited for just 5 minutes, then got the warn message that the NFS was full.I had to kill the snapshot process.In the ES log,there is only "no disk free,snapshot failed".The log level is INFO.This is our production environment.I tested snapshot in our test environment,there is no problem. But our production environment and test environment is different. Is it possible because of the environment? what circumstances the snapshot did not consider the index name?
</comment><comment author="imotov" created="2015-06-10T16:01:21Z" id="110811732">@Judyccb as long as the body of the request is correctly supplied, the list of indices is always considered.
</comment><comment author="Judyccb" created="2015-06-10T16:07:30Z" id="110813969">Hi  Motov,
Is there the body any problem as follow?
 curl -XPUT XXXX/_snapshot/indexBackup/tranpmindex_v1ccbs-2015-05-09?wait_for_completion=true -d '{"indices":"tranpmindex_v1ccb-2015-05-09","include_global_state":"false"}'
This index of tranpmindex_v1ccb-2015-05-09 was reindexed by Python
</comment><comment author="imotov" created="2015-06-10T16:11:03Z" id="110815225">@Judyccb I don't see any issues with your request. I created and an index with the same name and I wasn't able to reproduce the issue with your command - everything worked fine for me. Do you have any filters or proxy setup between your machine and the elasticsearch server? Is it possible that this proxy somehow cuts off the body or something like this?
</comment><comment author="Judyccb" created="2015-06-10T16:26:18Z" id="110822243">Hi Motov,
The machine I executed the command is one node of our ES cluster.Our ES cluster has 43 nodes. We have a team working on this cluster.I am not sure whether there is any filters or proxy on this machine.How to check it? Could you give any suggestion?
</comment><comment author="imotov" created="2015-06-10T17:24:48Z" id="110843413">I think the best way would be to ask the team working on this cluster. You can try specifying some bogus parameters in the body to see if they will generate a parse error (the error means the body was indeed sent and parsed by elasticsearch). However, if body will not reach elasticsearch you will end up creating a complete snapshot again.
</comment><comment author="Judyccb" created="2015-06-11T05:19:52Z" id="111001162">Hi Motov,

I think the body was indeed sent and parsed by elasticsearch. The es log  is as follow,
[2015-06-09 16:14:39,007][WARN ][snapshots                ] [by10esapap1009_0] [[patrol_v1-2015-01-16][2]] [indexBackup:tranpmindex_v1ccbs-2015-05-09] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [patrol_v1-2015-01-16][2] Failed to perform snapshot (index files)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:503)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:139)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
        at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:818)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No space left on device
        at java.io.FileOutputStream.close0(Native Method)
        at java.io.FileOutputStream.close(FileOutputStream.java:393)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
        at org.elasticsearch.common.blobstore.fs.FsBlobContainer$1.close(FsBlobContainer.java:92)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:560)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:501)
        ... 6 more
[2015-06-09 16:14:39,007][WARN ][snapshots                ] [by10esapap1009_0] [[patrol_v1-2015-03-01][3]] [indexBackup:tranpmindex_v1ccbs-2015-05-09] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [patrol_v1-2015-03-01][3] Failed to perform snapshot (index files)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:503)
/patrol-2015-03-29
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [patrol_v1-2015-02-23][3] Failed to perform snapshot (index files)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:503)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:139)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
        at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:818)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /home/ap/appmon/esdata/snapshot/indices/patrol_v1-2015-02-23/3/__0 (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:221)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:171)
        at org.elasticsearch.common.blobstore.fs.FsBlobContainer.createOutput(FsBlobContainer.java:85)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:555)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:501)
        ... 6 more
[2015-06-09 16:14:44,505][WARN ][snapshots                ] [by10esapap1009_0] [[patrol-2015-03-29][2]] [indexBackup:tranpmindex_v1ccbs-2015-05-09] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [patrol-2015-03-29][2] Failed to perform snapshot (index files)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:503)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:139)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
        at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:818)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /home/ap/appmon/esdata/snapshot/indices/patrol-2015-03-29/2/__0 (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:221)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:171)

---

every record includes "tranpmindex_v1ccbs-2015-05-09"which is the index I want to snapshot,but include the index I do not want to snapshot too,eg, patrol_v1-2015-01-16.How the index I do not want to snapshot parsed by elasticseach?
</comment><comment author="imotov" created="2015-06-11T15:41:52Z" id="111177471">@Judyccb these are just the errors that indicate that you ran out of disk space while trying to snapshot every index in your cluster. This is the default behavior when the elasticsearch server doesn't receive a list of indices specified in the request and therefore creates a snapshot of all available indices. 
</comment><comment author="Judyccb" created="2015-06-11T16:13:11Z" id="111189001">Hi Motov,

## I ran the snapshot command at 15:57. ES snapshot all available indices from 15:57 to 16:09,then no disk and error message as above. ‘tranpmindex_v1ccbs-2015-05-09’ in each error message is the index I want to snapshot,which proved the ES has received the index name from request but not parsed correctly. Do you agree?

##  [2015-06-09 16:14:39,007][WARN ][snapshots ] [by10esapap1009_0] [[patrol_v1-2015-01-16][2]] [indexBackup:tranpmindex_v1ccbs-2015-05-09] failed to create snapshot

curl -XPUT XXXX/_snapshot/indexBackup/tranpmindex_v1ccbs-2015-05-09?wait_for_completion=true -d '{"indices":"tranpmindex_v1ccb-2015-05-09","include_global_state":"false"}'
</comment><comment author="imotov" created="2015-06-11T16:19:01Z" id="111191041">The **tranpmindex_v1ccbs-2015-05-09** in each error message is the name of the snapshot: 

curl -XPUT XXXX/_snapshot/indexBackup/**tranpmindex_v1ccbs-2015-05-09**?wait_for_completion=true

which happens to be the same as the name of the index that you intended to snapshot in your case.
</comment><comment author="Judyccb" created="2015-06-12T05:35:54Z" id="111366118">Hi Motov,
I reproduced the problem in our test environment.If I input the command as follow,
curl -XPUT XXXX/_snapshot/indexBackup/tranpmindex_v1ccbs-2015-05-09wait_for_completion=true
- d '{"indices":"tranpmindex_v1ccb-2015-05-09","include_global_state":"false"}'
  In SecureCRT, when "- d '{"indices":"tranpmindex_v1ccb-2015-05-09","include_global_state":"false"}'"  appears on the second line, then all indices will be snapshot.if "-d"  is on the same line of "curl -XPUT XXXX/_snapshot/indexBackup/tranpmindex_v1ccbs-2015-05-09wait_for_completion=true",the result will be OK.
  I  pasted all the command to the ultraedit,there is no special charactor.Besides that, I use the history|grep snapshot command,the result appeared complete command.
  It's so strange! And I execute other OS command like this ,it's OK. I am not sure whether it's the problem about Restful API. And I think it's a high risk to user because  the snapshot command snapshots all indices without special tag or hint. It's not enough to distinguish snapshot all or a just by '-dXXX '.
</comment><comment author="imotov" created="2015-06-12T15:56:57Z" id="111534511">@Judyccb I think the main use case is making a backup of the entire cluster with all indices at the same time. We are trying to make this use case as simple as possible hence the default behavior. This is also consistent with other APIs in elasticsearch - for example if you don't specify any search request, it will perform search of all indices.
</comment><comment author="Judyccb" created="2015-06-12T16:20:06Z" id="111542116">Hi Motov,
I tested the command using SecureCRT and XShell, the problem appeared all.I doubt the CURL command a little.What do you think? How to avoid this?
Besides that, I open a new issue about restore,#11633,expected your response.
Thanks!!
</comment><comment author="imotov" created="2015-06-12T16:28:42Z" id="111547016">@Judyccb we trying to use github for bug reports and enhancement requests. I think a much better place to ask for help with using the curl command would be our discussion forum https://discuss.elastic.co/. Since what you are describing doesn't seem to be an elasticsearch bug, I am going to close this issue. Let's continue on our discussion forum.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>More information about 'Copy field to'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11556</link><project id="" key="" /><description /><key id="86621392">11556</key><summary>More information about 'Copy field to'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>docs</label></labels><created>2015-06-09T14:38:18Z</created><updated>2015-06-12T13:51:54Z</updated><resolved>2015-06-12T13:51:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11556 from robin13/master</comment></comments></commit></commits></item><item><title>Sub Aggregations in parent-child relationship on field value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11555</link><project id="" key="" /><description>Hi,

I have a parent child mapping in the following format:-
The children are linked to the parent based on the _id field in this case

Parent:-

```
POST http://localhost:9200/_bulk
{ "index" :  {"_index":"estester","_type":"document","_id":"00005777b25343f0b7c03e54c0411c1b"} }
{}
```

Children:-

```
POST http://localhost:9200/_bulk
{
    "index": {
        "_index": "estester",
        "_type": "documentmetadatavalue",
        "_parent": "00005777b25343f0b7c03e54c0411c1b"
    }
}
{
    "fieldid": "DOCUMENT_GUID",
    "fieldvalue": [
        "00005777b25343f0b7c03e54c0411c1b"
    ],
    "parent": "00005777b25343f0b7c03e54c0411c1b"
},
{
    "fieldid": "FILETYPE",
    "fieldvalue": [
        "PDF "
    ],
    "parent": "00005777b25343f0b7c03e54c0411c1b"
},
{
    "fieldid": "CREATEDDATE",
    "fieldvalue": [
        "06/08/2015"
    ],
    "parent": "00005777b25343f0b7c03e54c0411c1b"
}
```

I'm trying to create sub-aggregations which are 2 levels deep based on the fieldid:- CREATEDDATE and FILETYPE.

However the aggregation below fails to create child buckets more than 1 level deep:-

```
GET /estester/documentmetadatavalue/_search?search_type=count
{
    "size": 100,
    "aggs": {
        "field_aggregation": {
            "filter": {
                "term": {
                    "fieldid": "CREATEDDATE"
                }
            },
            "aggs": {
                "field_aggregation_values": {
                    "terms": {
                        "field": "fieldvalue.original",
                        "size": 0
                    },
                    "aggs": {
                      "field_aggregation": {
                          "filter": {
                              "term": {
                                  "fieldid": "FILETYPE"
                              }
                          },
                          "aggs": {
                              "field_aggregation_values": {
                                  "terms": {
                                      "field": "fieldvalue.original",
                                      "size": 0
                                  }
                              }
                          }
                      }
                    }
                }
            }
        }
    }
}
```

Response:-

```
{
   "took": 8,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 196,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "field_aggregation": {
         "doc_count": 4,
         "field_aggregation_values": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
               {
                  "key": "02/27/2015",
                  "doc_count": 1,
                  "field_aggregation": {
                     "doc_count": 0,
                     "field_aggregation_values": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": []
                     }
                  }
               },
               {
                  "key": "05/15/2015",
                  "doc_count": 1,
                  "field_aggregation": {
                     "doc_count": 0,
                     "field_aggregation_values": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": []
                     }
                  }
               },
               {
                  "key": "06/08/2015",
                  "doc_count": 1,
                  "field_aggregation": {
                     "doc_count": 0,
                     "field_aggregation_values": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": []
                     }
                  }
               },
               {
                  "key": "06/09/2015",
                  "doc_count": 1,
                  "field_aggregation": {
                     "doc_count": 0,
                     "field_aggregation_values": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": []
                     }
                  }
               }
            ]
         }
      }
   }
}
```
</description><key id="86611327">11555</key><summary>Sub Aggregations in parent-child relationship on field value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels><label>feedback_needed</label></labels><created>2015-06-09T14:09:00Z</created><updated>2015-06-15T08:25:38Z</updated><resolved>2015-06-15T08:25:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T13:50:34Z" id="111498656">Hi @vineet85 

You've uploaded half a recreation which is missing a bunch of stuff.  I can't tell where the problem is without a working example. Please could you update.
</comment><comment author="vineet85" created="2015-06-14T20:52:33Z" id="111875444">Sorry - here is the complete mapping and steps to re-create:-

Mapping:-
{
  "settings": {
    "index": {
      "number_of_replicas": 1,
      "number_of_shards": 5,
      "analysis": {
        "analyzer": {
          "customfieldvalue": {
            "tokenizer": "whitespace",
            "filter": [
              "lowercase"
            ],
            "type": "custom"
          },
          "customfullText": {
            "tokenizer": "standard",
            "filter": [
              "lowercase",
              "porter_stem"
            ],
            "type": "custom"
          },
          "customfullTextExact": {
            "tokenizer": "whitespace",
            "type": "custom"
          }
        }
      }
    }
  },
  "mappings": {
    "document": {
      "_all": {
        "enabled": false
      },
      "properties": {}
    },
    "documentmetadatavalue": {
      "_source": {
        "enabled": true,
        "excludes": [
          "fulltext"
        ]
      },
      "_all": {
        "enabled": false
      },
      "_parent": {
        "type": "document"
      },
      "properties": {
        "fieldid": {
          "index": "not_analyzed",
          "doc_values": true,
          "type": "string"
        },
        "fieldvalue": {
          "type": "multi_field",
          "fields": {
            "fieldvalue": {
              "analyzer": "standard",
              "type": "string"
            },
            "original": {
              "index": "not_analyzed",
              "doc_values": true,
              "type": "string"
            },
            "stemmed": {
              "analyzer": "english",
              "type": "string"
            },
            "exact": {
              "analyzer": "customfieldvalue",
              "type": "string"
            },
            "date": {
              "index": "not_analyzed",
              "format": "date",
              "ignore_malformed": true,
              "type": "date"
            },
            "integer": {
              "index": "not_analyzed",
              "doc_values": true,
              "ignore_malformed": true,
              "type": "double"
            },
            "boolean": {
              "index": "not_analyzed",
              "type": "boolean"
            }
          }
        },
        "parent": {
          "type": "string"
        },
        "fulltext": {
          "type": "multi_field",
          "fields": {
            "fulltext": {
              "analyzer": "customfullText",
              "type": "string"
            },
            "exact": {
              "analyzer": "customfullTextExact",
              "type": "string"
            }
          }
        }
      }
    }
  }
}

Re-creation:-
POST estester/document/1
{

}
POST estester/document/2
{

}
POST estester/document/3
{

}

POST /estester/documentmetadatavalue/1?parent=1
{
  "parent": "1",
  "fieldid": "CREATEDDATE",
  "fieldvalue": "06/14/2015"
}

POST /estester/documentmetadatavalue/2?parent=2
{
  "parent": "2",
  "fieldid": "CREATEDDATE",
  "fieldvalue": "06/15/2015"
}

POST /estester/documentmetadatavalue/3?parent=3
{
  "parent": "3",
  "fieldid": "CREATEDDATE",
  "fieldvalue": "06/17/2015"
}

POST /estester/documentmetadatavalue/4?parent=1
{
  "parent": "1",
  "fieldid": "FILETYPE",
  "fieldvalue": "Microsoft Word"
}

POST /estester/documentmetadatavalue/5?parent=2
{
  "parent": "2",
  "fieldid": "FILETYPE",
  "fieldvalue": "PDF"
}

POST /estester/documentmetadatavalue/6?parent=3
{
  "parent": "3",
  "fieldid": "FILETYPE",
  "fieldvalue": "text"
}

GET /estester/documentmetadatavalue/_search?search_type=count
{
    "size": 100,
    "aggs": {
        "field_aggregation": {
            "filter": {
                "term": {
                    "fieldid": "CREATEDDATE"
                }
            },
            "aggs": {
                "field_aggregation_values": {
                    "terms": {
                        "field": "fieldvalue.original",
                        "size": 0
                    },
                    "aggs": {
                      "field_aggregation": {
                          "filter": {
                              "term": {
                                  "fieldid": "FILETYPE"
                              }
                          },
                          "aggs": {
                              "field_aggregation_values": {
                                  "terms": {
                                      "field": "fieldvalue.original",
                                      "size": 0
                                  }
                              }
                          }
                      }
                    }
                }
            }
        }
    }
}
</comment><comment author="clintongormley" created="2015-06-15T08:25:37Z" id="111975607">Thanks @vineet85 - You're making a number of mistakes here, but none of them point to bugs in Elasticsearch. A better place to ask this question is in the forum: http://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return empty CommitID from ShadowEngine#flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11554</link><project id="" key="" /><description>this change removes CommitID reading for 1.x indices to prevent raceconditions on shared FS for shadow engines where null is returned if a race happens. We now simply return an empty commit ID which is not needed on shadow engines anyway.
</description><key id="86560861">11554</key><summary>Return empty CommitID from ShadowEngine#flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Shadow Replicas</label><label>bug</label><label>v1.6.1</label></labels><created>2015-06-09T11:21:30Z</created><updated>2015-06-12T14:14:42Z</updated><resolved>2015-06-10T10:33:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-09T11:21:57Z" id="110321778">here is the CI link http://build-us-00.elastic.co/job/es_core_16_strong/37/consoleFull
</comment><comment author="s1monw" created="2015-06-09T15:55:00Z" id="110412867">@dakrone @rmuir I am happy to remove the loop and just return an empty CommitID here it doesn't make a difference anyway? Thoughs
</comment><comment author="dakrone" created="2015-06-09T16:00:51Z" id="110415314">@s1monw that sounds fine to me, if we don't need the CommitID why even read it at all then? Why not always return an empty one?
</comment><comment author="bleskes" created="2015-06-09T18:24:35Z" id="110455081">+1 on an empty commit ID all the time. The only use case for it now is the synced flush which isn't supported anyway on ShadowEngines.
</comment><comment author="s1monw" created="2015-06-10T07:57:05Z" id="110642120">@bleskes @rmuir @dakrone I pushed a new commit - I think it's ready
</comment><comment author="bleskes" created="2015-06-10T08:43:27Z" id="110656978">LGTM. Left one trivial comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport: allow to de-serialize arbitrary objects given their name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11553</link><project id="" key="" /><description>As part of the query refactoring, we want to be able to serialize queries by having them extend Writeable, rather than serializing their json. When reading them though, we need to be able to identify which query we have to create, based on its name.

For this purpose we introduce a new abstraction called NamedWriteable, which is supported by StreamOutput and StreamInput through writeNamedWriteable and readNamedWriteable methods. A new NamedWriteableRegistry is introduced also where named writeable prototypes need to be registered so that we are able to retrieve the proper instance of query given its name and then de-serialize it calling readFrom against it.

This PR replaces #11355 and goes against the feature/query-refactoring branch.
</description><key id="86555910">11553</key><summary>Transport: allow to de-serialize arbitrary objects given their name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-09T11:04:00Z</created><updated>2015-06-10T11:00:25Z</updated><resolved>2015-06-10T11:00:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-09T11:32:08Z" id="110326178">I left some comments but this looks very good IMO.
</comment><comment author="javanna" created="2015-06-09T14:13:31Z" id="110368519">updated according to review
</comment><comment author="s1monw" created="2015-06-10T10:25:45Z" id="110686757">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteableRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportModule.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java</file><file>core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleTests.java</file><file>core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java</file></files><comments><comment>Transport: allow to de-serialize arbitrary objects given their name</comment></comments></commit></commits></item><item><title>Fix argument parsing on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11552</link><project id="" key="" /><description>Currently our argument parsing on startup is very brittle, without the possibility to exit on problems (for example when including a space in a path, many other arguments might break as well and es still starts up)

The used `getopt` function fails on whitespaces, which is also mentioned in its manpage. There does not seem to be any fix for this. The alternative would be to call `getopts` (note the `s`) which has an improved parsing and is also supported by the common shells (even dash under ubuntu). However, the problem is our parsing is much more than simple getopt parsing. We support the following
- long options like `--network.host 127.0.0.1` must be supported and converted to a JVM system property `-Des.network.host=127.0.0.1`
- system properties like `-Dfoo.bar=spam` must be supported

The `getopts` parser does not support the last syntax, when spaces are involved, because the key is not supposed to be `foo.bar`, but `-D`. The `commons-cli` has support for this as well, and we already include it.

In order to simplify this handling and make it testable I think it makes sense to get rid of the parsing in the shell/bat scripts and do this in java. A couple of solutions come to mind here (one of the key features needs to be speed, calling `bin/elasticsearch -v` should still be fast.
- Have a very small class, that parses the getopts arguments and exits on parsing problems. If parsing is successful, call the `Bootstrap` class
- Put everything in the bootstrap class
- Have a small main class that parses all arguments, that then fires up another JVM (Tika is doing this, the call it a forked parser). The interesting approach about this one might be, that we could actually configure the heap size in the `elasticsearch.yml` file as well
- If we do not want to go this route, we may want to think about ditching a lot of our options...
</description><key id="86553517">11552</key><summary>Fix argument parsing on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-06-09T10:52:57Z</created><updated>2016-01-18T20:15:09Z</updated><resolved>2016-01-18T20:15:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T13:42:13Z" id="111496417">It definitely sounds like the parsing should be done in java land.  I like the idea of being able to configure the heap size in the config file, but would be interested to hear what @rmuir thinks about this approach.
</comment><comment author="rmuir" created="2015-06-12T13:58:43Z" id="111501359">I like the testability aspect, but keep in mind that executing processes from java has its own set of different problems. For example instead of whitespace being a problem, then it will be non-ascii paths from processbuilder (maybe only on windows) or whatever.

As far as bootstrap, personally i would prefer if "forking" was somewhere else. Or rather if we call this part bootstrap then if the current bootstrap logic was moved. Its already kind of a mess and really complicated stuff, if we can keep the forking part separate it would prevent it from being more confusing.
</comment><comment author="clintongormley" created="2016-01-18T20:15:09Z" id="172642261">Argument parsing has been fixed and we've decided against the forking option. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: refactored LimitQueryBuilder and Parser and added test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11551</link><project id="" key="" /><description>Split the parse method into a parsing and a query building part, adding serialization and hashCode(), equals() for better testing.
Add basic unit test for Builder and Parser.

PR goes agains query-refacoring feature branch.
</description><key id="86542119">11551</key><summary>Query refactoring: refactored LimitQueryBuilder and Parser and added test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-09T10:20:35Z</created><updated>2015-06-09T13:20:35Z</updated><resolved>2015-06-09T13:20:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-09T12:32:21Z" id="110340523">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11551 from cbuescher/feature/query-refactoring-limit</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file></files><comments><comment>Query refactoring: refactored LimitQueryBuilder and Parser and added test</comment></comments></commit></commits></item><item><title>Always allocate primaries first during recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11550</link><project id="" key="" /><description>During recovery users can set `cluster.routing.allocation.enable` to only allow primaries to be allocated (recovered).

Currently if this is set to `all` we will do any primaries for an index, but in large clusters we have seen instances where instead of recovering the next unallocated primary, we work on the replica of a now allocated primary.

This obviously delays the progress in getting a cluster back to a green state and we should always be treating primaries as first class at all times. It'd be good if the default behaviour here was to recover all primaries first and then move to replicas as applicable.
</description><key id="86468710">11550</key><summary>Always allocate primaries first during recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Recovery</label></labels><created>2015-06-09T06:15:50Z</created><updated>2015-06-09T23:25:13Z</updated><resolved>2015-06-09T23:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-09T06:52:55Z" id="110252596">The master should allocate primaries first, based on the current allocation restrictions (maximum recoveries per node). Once all possible primaries are assigned (but not per-se started) it proceeds to allocate any replicas if possible. This repeats it self with any change to the current status (shard started/failed/node joined etc.). Can you give some more information about what happened? 
</comment><comment author="kimchy" created="2015-06-09T07:41:18Z" id="110262380">primaries today are allocated first, and only when the limit is reached (as per Boaz note) we will go and try and allocate replicas. I suspect that this came from the sync nature of remote listing of shards during allocation which is a problem with very slow file systems, so this will become a non issue in 1.6 (well, the file system will still be slow, but it won't cause perceived notions of hiccups on the master)
</comment><comment author="markwalkom" created="2015-06-09T08:06:41Z" id="110268480">Great, thanks both for the comments :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11549</link><project id="" key="" /><description>'good' vs 'well' typo
</description><key id="86393852">11549</key><summary>Typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ChelseaLura</reporter><labels /><created>2015-06-09T01:06:02Z</created><updated>2015-06-09T07:25:56Z</updated><resolved>2015-06-09T07:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-09T01:08:35Z" id="110187562">@ChelseaLura can you please [sign the CLA](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)? :)
</comment><comment author="ChelseaLura" created="2015-06-09T01:18:20Z" id="110188488">@markwalkom, Sorry about that, I signed it. Do I need to send my copy somewhere? 
</comment><comment author="bleskes" created="2015-06-09T07:25:56Z" id="110258017">@ChelseaLura merged this in. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Doc: Typo 'good' vs 'well' typo</comment></comments></commit></commits></item><item><title>Allow number_of_shards to be set via API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11548</link><project id="" key="" /><description>Currently the only way to set `index.number_of_shards` is via the elasticsearch.yml file, if you try to set this via the API it doesn't work;

```
$ curl -XPUT 0:9200/_cluster/settings -d '{ "persistent": { "index.number_of_shards": 5}}'
{"acknowledged":true,"persistent":{},"transient":{}}
$ curl 0:9200/_cluster/settings
{"persistent":{"discovery":{"zen":{"minimum_master_nodes":"1"}}},"transient":{}}
```

And this is reported in the log;

```
[2015-06-09 09:30:44,854][WARN ][action.admin.cluster.settings] [cluster] ignoring persistent setting [index.number_of_shards], not dynamically updateable
```

It seems like a bit of a gap between forcing a user to either set this on each node, which can potentially result in different values across nodes, or using a template, which may get confusing as well.

Is there any reason we cannot allow this to be set via a call to `_cluster/settings`?
</description><key id="86389247">11548</key><summary>Allow number_of_shards to be set via API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Settings</label><label>adoptme</label><label>deprecation</label><label>low hanging fruit</label></labels><created>2015-06-09T00:46:16Z</created><updated>2016-01-18T20:14:24Z</updated><resolved>2016-01-18T20:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T09:53:11Z" id="111433067">To be clear, this is about setting the default number of shards in a new index, which currently is only settable in the config. You can already do this using index templates.

What we should actually do is remove this setting from the config.
</comment><comment author="clintongormley" created="2016-01-18T20:14:24Z" id="172642100">The solution here is to set up an index template that matches all indices.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Too many bloked threads in elasticsearch java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11547</link><project id="" key="" /><description>We are using elasticsearch 1.4.2 with Spring data. TransportClient object is used to establish connection, execute the query and fetch results (only search requests).
Following code closes connection:
`//There is an instanceof test above this block so no class cast exception here`
`((TransportClient)element.getObjectValue()).close();`
`((TransportClient)element.getObjectValue()).threadPool().shutdownNow();`

Sometimes, we get the following exception while executing the query or closing the connection:

```
org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (shutting down) on org.elasticsearch.transport.netty.NettyTransport$2@489e4aa5
//Long stack trace
```

However, this is intermittent, but the main issue is, after some hours (or days), the application completely hangs, (resulting in some 110% CPU and 80% memory) and all the upcoming search requests fail with 'Too many open files' exception. Below is the stack trace:

```
org.elasticsearch.common.netty.channel.ChannelException: Failed to create a selector.
at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)
at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.&lt;init&gt;(AbstractNioSelector.java:100)
at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.&lt;init&gt;(AbstractNioWorker.java:52)
at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.&lt;init&gt;(NioWorker.java:45)
at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)
at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)
at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:143)
at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:81)
at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:39)
at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:33)
//many more

Caused by: java.io.IOException: Too many open files
at sun.nio.ch.EPollArrayWrapper.epollCreate(Native Method)
at sun.nio.ch.EPollArrayWrapper.&lt;init&gt;(EPollArrayWrapper.java:87)
at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:68)
```

We tried to get thread dump before restarting the api. Below stack trace appeared in it:

```
Thread 12480: (state = BLOCKED)
 java.util.concurrent.FutureTask.&lt;init&gt;(java.lang.Runnable, java.lang.Object) @bci=5, line=92 (Compiled frame)
 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.&lt;init&gt;(java.util.concurrent.ScheduledThreadPoolExecutor, java.lang.Runnable, java.lang.Object, long) @bci=8, line=207 (Compiled frame)
 java.util.concurrent.ScheduledThreadPoolExecutor.schedule(java.lang.Runnable, long, java.util.concurrent.TimeUnit) @bci=33, line=527 (Compiled frame)
 org.elasticsearch.threadpool.ThreadPool.schedule(org.elasticsearch.common.unit.TimeValue, java.lang.String, java.lang.Runnable) @bci=37, line=245 (Compiled frame)
 org.elasticsearch.client.transport.TransportClientNodesService$ScheduledNodeSampler.run() @bci=41, line=323 (Compiled frame)
 java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Compiled frame)
 java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)
 java.lang.Thread.run() @bci=11, line=724 (Interpreted frame)
```

There were 93 such threads in blocked state which caused the application to fail. This has happened multiple times, however we are not able to nail down the root cause.
We don't have any thread pool limit placed on ES node. Don't know whether that would improve the situation. 
</description><key id="86385389">11547</key><summary>Too many bloked threads in elasticsearch java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darshanmehta10</reporter><labels /><created>2015-06-09T00:32:00Z</created><updated>2016-01-18T20:13:42Z</updated><resolved>2016-01-18T20:13:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-09T00:38:30Z" id="110182317">&gt; We don't have any thread pool limit placed on ES node. Don't know whether that would improve the situation.

This is probably causing you a lot of problems!
I'd suggest you join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add node setting to send SegmentInfos debug output to System.out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11546</link><project id="" key="" /><description>This produces very expert / low level output, explaining how/why Lucene picked a given segments_N file to open from the index.
</description><key id="86359338">11546</key><summary>Add node setting to send SegmentInfos debug output to System.out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T22:42:32Z</created><updated>2015-06-12T13:21:31Z</updated><resolved>2015-06-08T22:48:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-08T22:44:34Z" id="110164845">LGTM, can we backport to 1.6 and 1.x?
</comment><comment author="mikemccand" created="2015-06-08T22:45:04Z" id="110164915">&gt; LGTM, can we backport to 1.6 and 1.x?

OK will do!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file></files><comments><comment>Merge pull request #11546 from mikemccand/segment_infos_trace</comment></comments></commit></commits></item><item><title>Add an API to locate unrecovered shards and their state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11545</link><project id="" key="" /><description>This API provides store information for shard copies of indices.
Store information reports on which nodes shard copies exist, the shard
copy version, indicating how recent they are, and any exceptions
encountered while opening the shard index or from earlier engine failure.

By default, only lists store information for shards that have at least one
unallocated copy. When the cluster health status is yellow, this will list
store information for shards that have at least one unassigned replica.
When the cluster health status is red, this will list store information
for shards, which has unassigned primaries.

Endpoints include shard stores information for a specific index, several
indices, or all:

``` bash
curl -XGET 'http://localhost:9200/test/_shard_stores'
curl -XGET 'http://localhost:9200/test1,test2/_shard_stores'
curl -XGET 'http://localhost:9200/_shard_stores'
```

The scope of shards to list store information can be changed through
`status` param. Defaults to 'yellow' and 'red'. 'yellow' lists store information of
shards with at least one unassigned replica and 'red' for shards with unassigned
primary shard.
Use 'green' to list store information for shards with all assigned copies.

``` bash
curl -XGET 'http://localhost:9200/_shard_stores?status=green'
```

Response:

The shard stores information is grouped by indices and shard ids.

``` bash
{
    ...
   "0": { &lt;1&gt;
        "stores": [ &lt;2&gt;
            {
                "sPa3OgxLSYGvQ4oPs-Tajw": { &lt;3&gt;
                    "name": "node_t0",
                    "transport_address": "local[1]",
                    "attributes": {
                        "enable_custom_paths": "true",
                        "mode": "local"
                    }
                },
                "version": 4, &lt;4&gt;
                "allocation" : "primary" | "replica" | "unused", &lt;6&gt;
                "store_exception": ... &lt;5&gt;
            },
            ...
        ]
   },
    ...
}
```

&lt;1&gt; The key is the corresponding shard id for the store information
&lt;2&gt; A list of store information for all copies of the shard
&lt;3&gt; The node information that hosts a copy of the store, the key
    is the unique node id.
&lt;4&gt; The version of the store copy
&lt;5&gt; The status of the store copy, whether it is used as a
    primary, replica or not used at all
&lt;6&gt; Any exception encountered while opening the shard index or
    from earlier engine failure

closes #10952
</description><key id="86358967">11545</key><summary>Add an API to locate unrecovered shards and their state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Stats</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T22:41:20Z</created><updated>2015-07-18T09:53:41Z</updated><resolved>2015-07-16T22:43:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-06-08T22:47:41Z" id="110165279">@s1monw This is still a WIP in terms of documentation and testing, would appreciate a review.
</comment><comment author="s1monw" created="2015-06-09T19:10:17Z" id="110467330">wow @areek this looks pretty awesome. I left some comments
</comment><comment author="areek" created="2015-06-10T03:57:56Z" id="110579411">Thanks for the review @s1monw! Addressed all the comments. 

Was wondering if there are any tests that I can look at to get the cluster to have a bunch of unassigned nodes (currently just stoping random nodes)? 
I see `CorruptedFileTest#corruptRandomPrimaryFile`, was wondering if there is a easier way to test out the corruption exception for the response?
</comment><comment author="bleskes" created="2015-06-10T07:39:37Z" id="110636962">@areek I think this as awesome api. Left some comments here and there. I think we need to beef up the tests to check for the actual content for the shard responses (check it finds stuff and check that it detects corruption etc.) . I'll respond to the naming part on the ticket..
</comment><comment author="bleskes" created="2015-06-11T07:34:03Z" id="111029813">@areek change looks good. Did you see my comment about beefing up the testing?
</comment><comment author="areek" created="2015-06-23T02:09:20Z" id="114330228">@bleskes @clintongormley, I have updated the description with the new API, thoughts? It turned out to be a bit different from what we have discussed before, in terms of default behaviour. It would be good to have this reviewed.
</comment><comment author="clintongormley" created="2015-06-23T19:05:16Z" id="114612270">Description in the docs looks good to me
</comment><comment author="bleskes" created="2015-06-25T13:32:57Z" id="115259597">I went through the code and left some comments - looking good. A couple of other things:

1) I miss the rest spec changes
2) I miss a rest test
3) I think using `shards=allocated` and `shards=unallocatred` as a shard filter is confusing as it is not clear about it's about shard copies that are used or about shards that have any copy allocated. Talked this through with @clintongormley and we settled on having a `status` parameter than can take a comma separated list out of `red,yellow,green,all` and use that to decide which shards will be looked for.
4) "primary_allocated": true in the output seems to be one level too high. What we want to know is whether a store is being used by the cluster or not. As such the information should be on the same level as the store_exception . Naming should also be `allocation: "primary"` or `allocation: "replica"` or `allocation: "unused"` .

Makes sense?
</comment><comment author="areek" created="2015-07-01T03:37:20Z" id="117426173">@bleskes would be awesome to have a review on this again. Let me know your thoughts on how the status params determine which shards to get store info for, it should be pretty easy to change this behaviour. Otherwise, I think this is close.
</comment><comment author="bleskes" created="2015-07-07T11:30:51Z" id="119177127">Thx @areek . Left some more comments.. 
</comment><comment author="areek" created="2015-07-09T04:41:23Z" id="119816669">@bleskes Thanks for the review, addressed all the comments. another review?
</comment><comment author="bleskes" created="2015-07-14T13:17:36Z" id="121235238">I left some final minor comments. I think we are getting close! 
</comment><comment author="areek" created="2015-07-14T17:34:06Z" id="121317482">@bleskes Thanks for the review, addressed all your comments
</comment><comment author="bleskes" created="2015-07-15T15:44:58Z" id="121656963">Left some final suggestions. Thx @areek 
</comment><comment author="areek" created="2015-07-15T18:07:45Z" id="121697029">@bleskes Thanks for the review, updated the PR addressing all your comments.
</comment><comment author="bleskes" created="2015-07-16T09:03:42Z" id="121888567">LGTM. Left some very minor comment. Thx for all the hard word :+1: 
</comment><comment author="areek" created="2015-07-16T22:43:50Z" id="122124021">merged to master https://github.com/elastic/elasticsearch/commit/7a21d846bb920b25101a2a958dbfb44f872cfeb1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move null value handling into MappedFieldType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11544</link><project id="" key="" /><description>In order for exists queries to use the null value for
a field, null value needs to be part of the field type (should
differ between document types). This change moves null value
into the field type, as well as simplifies the null value
methods available to remove supportsNullValue().
</description><key id="86349994">11544</key><summary>Move null value handling into MappedFieldType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T22:09:02Z</created><updated>2015-06-12T13:55:22Z</updated><resolved>2015-06-09T16:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-09T07:52:38Z" id="110264793">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file></files><comments><comment>Merge pull request #11544 from rjernst/refactor/null-value</comment></comments></commit></commits></item><item><title>failed action with response of 400, dropping action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11543</link><project id="" key="" /><description>Today I've updated my ES cluster to the ES version 1.5.2 , as well I've updated the logstash to the 1.5.0 .
Ony of my logstash outputs is elasticsearch.

Java version is 1.8.0_45 .

And since today I'm spammed with this kind of messages:

&gt; {:timestamp=&gt;"2015-06-08T23:37:34.955000+0200", :message=&gt;"failed action with response of 400, dropping action: [\"index\", {:_id=&gt;nil, :_index=&gt;\"logstash-2015.06.08\", :_type=&gt;\"yyy\", :_routing=&gt;nil}, #&lt;LogStash::Event:0x36be02ca @metadata={\"retry_count\"=&gt;0}, @accessors=#&lt;LogStash::Util::Accessors:0x1191c365 @store={\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, @lut={\"[message]\"=&gt;[{\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, \"message\"], \"[type]\"=&gt;[{\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, \"type\"], \"message\"=&gt;[{\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, \"message\"], \"[tags]\"=&gt;[{\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, \"tags\"], \"type\"=&gt;[{\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, \"type\"]}&gt;, @data={\"message\"=&gt;\"iii\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-06-08T21:37:31.836Z\", \"host\"=&gt;\"www\", \"type\"=&gt;\"yyy\", \"client\"=&gt;\"qqq\", \"country\"=&gt;\"es\", \"level\"=&gt;\"INFO\", \"data\"=&gt;{\"userUuid\"=&gt;\"zzz\", \"appUuid\"=&gt;\"ppp\", \"tags\"=&gt;#Java::JavaUtil::ArrayList:0x6e500547, \"sessionId\"=&gt;\"63784859\", \"login\"=&gt;\"xxx\", \"result\"=&gt;{\"passwordMetadata\"=&gt;{\"name\"=&gt;\"ooo\", \"mask\"=&gt;false}}}}, @metadata_accessors=#&lt;LogStash::Util::Accessors:0x777eb8cf @store={\"retry_count\"=&gt;0}, @lut={}&gt;, @cancelled=false&gt;]", :level=&gt;:warn}

I have no idea how to overcome that. Some messages are ok, and ES accepts them and indexes, but some lead to this message.

Any idea what should I check to make them pass thorough ES?
Why in the error message, the original message takes place several time?

The same topic takes place [inside the logstash ES plugin](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/144#issuecomment-110072257), but I haven't found a solution there.

Why the ES is dropping the action?

Any hint is appreciated.

Regards,
</description><key id="86346307">11543</key><summary>failed action with response of 400, dropping action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PavelPolyakov</reporter><labels /><created>2015-06-08T21:54:51Z</created><updated>2015-09-18T20:07:59Z</updated><resolved>2015-06-09T17:09:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="T0aD" created="2015-06-09T12:41:57Z" id="110344794">+1
</comment><comment author="PavelPolyakov" created="2015-06-09T14:15:45Z" id="110369825">I have the next suggestion currently.

My logs doesn't have any mapped format in elasticsearch, they are dynamic.

The log could look like this:

``` javascript
{
    message: "message",
    error: false
}
```

and like this

``` javascript
{
    message: "message"
    error: {
        message: "Error"
    }
}
```

As soon as 1st log type would hit the elasticsearch, the error field would be mapped as string. Since that time, any of the 2nd type logs would not be accepted. And would produce the error from the topic heading.

Could anyone confirm this?
</comment><comment author="clintongormley" created="2015-06-09T17:09:50Z" id="110436128">Hi @PavelPolyakov 

The `error` field can't be both a boolean (first example) and an object (second example).  Whichever format is seen first sets the field type, and all documents with the other format are rejected.

You'll need to add a rule to logstash to change the field name of one of the formats.
</comment><comment author="clintongormley" created="2015-06-09T17:11:54Z" id="110436769">The alternative is to map the `error` field as:

```
{ "type": "object", "enabled": false }
```

With this mapping, it won't look at the contents of the `error` field at all.  Documents won't be rejected, but of course the `error` (and `error.message`) fields won't be indexed either, and so won't be searchable.
</comment><comment author="BZen" created="2015-06-21T05:45:07Z" id="113865537">I was getting the same error message(failed with a response of 400). I was using json codec in my input. But after I removed it I din't get any errors. It worked fine. Is there a problem with the json codec?? Btw my error logs are so big and I dont think they will help.
</comment><comment author="tlastowka" created="2015-08-20T20:39:30Z" id="133163915">@BZen 
Its probably still a mapping issue.  The first time I saw the 400 error was using the json codec, but it wasn't the codec at fault.  Pipe this through the json codec, and then output to elasticsearch.  First will be fine.  Second will cause an error:

--this will be fine--
{ "gets_mapped_as_string" : "string" }
{ "gets_mapped_as_string" : 1 }

--this will cause an error--
{ "gets_mapped_as_int" : 1 }
{ "gets_mapped_as_int" : "string" }
</comment><comment author="raedsh" created="2015-09-18T20:07:59Z" id="141553582">What fixed my 400 error was changing the index name from all Upper case letters to lower case letters.
Works:
index =&gt; "snow-%{+YYYY.MM}"

Errors out:
index =&gt; "SNOW-%{+YYYY.MM}"
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Handle upserts failing when document has already been created by another process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11542</link><project id="" key="" /><description>fix for #11506 and https://github.com/elastic/elasticsearch/issues/9821  
</description><key id="86278341">11542</key><summary>Handle upserts failing when document has already been created by another process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">vedil</reporter><labels><label>:CRUD</label><label>bug</label></labels><created>2015-06-08T18:15:27Z</created><updated>2015-07-09T14:17:54Z</updated><resolved>2015-07-09T00:25:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vedil" created="2015-06-08T22:45:55Z" id="110165034">now the failures are down to 1 
Tests with failures:
- org.elasticsearch.index.engine.InternalEngineTests.testVersioningCreateExistsExceptionWithFlush

checking if the version is sent as parameter and throwing exception only in that case
</comment><comment author="vedil" created="2015-06-09T00:34:47Z" id="110181283">now i got a clean build and also got my test to run and succeed by using mvn test -Des.node.mode=local -Dtests.class=org.elasticsearch.action.IndicesRequestTests "-Dtests.method=_Update_" -Dtests.filter="@slow"  -e
</comment><comment author="vedil" created="2015-07-07T17:51:57Z" id="119282363">just to re-iterate, there is nothing more that i can do to get this merged?
</comment><comment author="clintongormley" created="2015-07-08T16:49:44Z" id="119656187">@vedil we just need to get it reviewed. sorry for the delays

@jasontedor could you take a look please?
</comment><comment author="jasontedor" created="2015-07-08T22:34:36Z" id="119750492">@vedil Thank you for raising this to our attention; it does seem like there might be a race condition here. Before we take any action on this, would you mind 
- removing the [commit 1c5f8de](https://github.com/vedil/elasticsearch/commit/1c5f8deccc14b55a670e05550ca3b82bb8a1f47d) from this pull request that is germane to #9821 as that issue is closed
- removing the [commit e15e2a2](https://github.com/vedil/elasticsearch/commit/e15e2a2cded61dc19f10a708073ea4c6bd1c486a) as it's only code formatting
- squashing the remaining commits into one

Thanks!
</comment><comment author="vedil" created="2015-07-09T01:10:08Z" id="119775559">messed up some thing while re-basing, so created another pull request with just the change and the test case https://github.com/elastic/elasticsearch/pull/12137
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default english stemmer uses obselete algorithm from 1980</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11541</link><project id="" key="" /><description>I've just noticed https://github.com/elastic/elasticsearch/issues/6330 and the associated PR https://github.com/elastic/elasticsearch/pull/6452. I noticed this because on upgrading the install of elasticsearch on www.gov.uk searches including the word "news" started returning results containing only the word "new".

The changes between porter2 and porter are relatively small in terms of the number of words affected, but include changes to fix some pretty common problems like this based on observation of probems with the algorithm in 20 years of production systems.  In general the problems fixed are problems with over-stemming words - conflating too many different forms together.  I know that a lot of people end up using the "light_english" (an alias for "kstem") stemmer, which stems a lot less strongly, and I suspect part of the reason for that is that they've been using the "porter" stemmer initially instead of the "english" stemmer.

I'd encourage you to consider returning to "porter2" the default algorithm when people ask for english (perhaps at elasticsearch 2.0 for compatibility reasons), because it really is quite noticeably better for result quality.  We made "porter2" the algorithm returned when snowball is asked for an english stemmer for this reason.

Martin Porter's opinion last time I talked to him about this (and also my opinion) is that the "porter" algorithm is significantly worse than the "porter2" algorithm.  The only reason we kept the "porter" algorithm in snowball is that a lot of academic evaluations have historically been done using the "porter" stemmer, and it's convenient for them to be able to use the same algorithm for new tests for accurate comparison of results.  I would never deliberately use "porter" in a production system.
</description><key id="86275435">11541</key><summary>Default english stemmer uses obselete algorithm from 1980</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2015-06-08T18:04:17Z</created><updated>2015-06-08T22:53:14Z</updated><resolved>2015-06-08T21:29:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-08T18:06:36Z" id="110094125">&gt; I'd encourage you to consider returning to "porter2" the default algorithm when people ask for english (perhaps at elasticsearch 2.0 for compatibility reasons), because it really is quite noticeably better for result quality.

Unfortunately, its also like twice as slow. Last time i benchmarked, the default porter (non-snowball, just the java impl) has a 2x overall indexing performance advantage, so overall I think we make the right tradeoff. 
</comment><comment author="rboulton" created="2015-06-08T18:11:44Z" id="110095166">The snowball stemmers do tend to have a significant speed overhead compared to hand-coded algorithms, yes - I can believe a 2x speed difference for the actual stemming call.  I wouldn't have expected the cost of stemming to be a significant part of the total indexing speed, though.

Perhaps a bit of work on improving the speed on the porter2 algorithm would be worthwhile.  I don't believe it's had any significant effort put in to performance tuning, so there's probably some very easy gains there.
</comment><comment author="rmuir" created="2015-06-08T18:18:33Z" id="110096502">&gt; I wouldn't have expected the cost of stemming to be a significant part of the total indexing speed, though.

Yeah, I did this a while ago and afterwards actually spent time with snowball and tried to speed it up (and had some success). So lucene's snowball code has already diverged from snowball repository itself: perhaps there must be different performance goals in mind? What other explanation for things like (https://github.com/snowballstem/snowball/blob/master/java/org/tartarus/snowball/SnowballProgram.java#L31-L37)? Sorry that kind of thing is just terrible for performance :)

To me, IR is like, the main reason you'd use a stemmer, I'd be happy to try to fold this stuff back into snowball and improve the performance. But IIRC there were some api changes needed too (maybe some stuff moved from string -&gt; charsequence), and I am confused about the status of snowball.
</comment><comment author="rmuir" created="2015-06-08T18:34:55Z" id="110101817">to me the most promising approach could be: take the current PorterStemmer.java and create a Porter2Stemmer.java (still hand-written, just reflecting the changes). Its not very sexy but currently the hand-written code just performs much better. And its easy to test it against the snowball compiled version for correctness.
</comment><comment author="rboulton" created="2015-06-08T18:45:35Z" id="110104371">The reason for things like that which are terrible for performance is that those lines of code were written 11 years ago (when I readily admit I had little experience with trying to write performant Java) with an aim of getting something working, and never revisited to improve performance. Take a look at Lucene code from the same time and you'll find some similar shockers! As I say, I'm sure there's lots of scope for improving performance.

The approach of hand writing a Porter2Stemmer.java is reasonable, using the snowball code as a reference implementation. Though it would be nicer to improve the snowball code generator so that non-english stemmers got the benefit too, the code generator is rather hard to work with, being a translation of a code generator for C, and written in not-very clean C.

What _is_ well documented here is the algorithm, and in particular the differences between porter and porter2.
</comment><comment author="rmuir" created="2015-06-08T19:18:54Z" id="110111671">Yeah, I guess to me given the stated differences of 5% of words impacted, and reading through the documentation, it just doesn't seem particularly crucial. I guess in my opinion, english is messy and exceptions like news can be handled with exceptions, which we let the user do easily independent of the stemmer. Any IR tests i have done have shown no significant difference, but the performance is pretty important.
</comment><comment author="rboulton" created="2015-06-08T21:29:15Z" id="110147128">The 5% of words which are affected tend to be more frequently encountered words (because the changes were made based on frequently reported problems), so the changes have more effect than might be thought.  I agree that an exception list is a good idea (we have one for gov.uk here https://github.com/alphagov/rummager/blob/master/config/schema/stems.yml, for example); it just seems better to use the best starting point available.

Out of interest, have you done IR tests comparing something like kstem to porter (1 or 2), and have those shown any significant difference?  (Also, comparing to not stemming at all?)

In any case, I think your performance arguments are quite solid; I have different views of the relative importance of quality and speed (mainly because I'm currently working on a pretty small dataset, but one where result quality is paramount), so I'm probably not going to agree with your choice of default.

In order not to clog your tracker up with open issues, I'll close this ticket now - thank you for your feedback. If I ever get time to do performance improvements on the "porter2" stemmer (either to the code generator, or by hand-coding one), I'll try and push that into Lucene and open a new ticket, because other than the speed I think we've agreed on twitter that porter2 is better than porter (though agreeing whether either is good is a different question!).
</comment><comment author="rmuir" created="2015-06-08T21:46:28Z" id="110151268">&gt; Out of interest, have you done IR tests comparing something like kstem to porter (1 or 2), and have those shown any significant difference? (Also, comparing to not stemming at all?)

Yes. kstem has never done well in my tests. Porter does well. just removing plural/possessive also works reasonable too. I think the "weak porter" that terrier has is also interesting (but have not tested myself). Usually improvements are small (&lt;= 15%) vs. not stemming with english: to me that's expected. I don't read too far into this stuff, but when i see consistent patterns across several corpora, i start to feel consensus :)
</comment><comment author="rboulton" created="2015-06-08T22:53:14Z" id="110165986">Thanks for that datapoint; interesting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typo in javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11540</link><project id="" key="" /><description /><key id="86274058">11540</key><summary>Fix typo in javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels /><created>2015-06-08T17:58:37Z</created><updated>2015-06-22T08:48:11Z</updated><resolved>2015-06-12T13:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T13:16:08Z" id="111488687">Already fixed in another commit. thanks anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve exception message when shard has a partial commit (segments_N file) due to prior disk full</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11539</link><project id="" key="" /><description>ES checks the latest segments_N file to see if it was written by Lucene 3.x and upgrade if so, but if that file is truncated (e.g. 0 bytes) due prior disk full, we now give a confusing "failed to upgrade 3x segments" exception when you likely don't have 3.x segments_N nor segments.

This change just breaks apart the exception handling so we do a better job separating "I could not read the latest segments_N file" from "I did read it and it was ancient (Lucene 3.x) and then when I tried to upgrade it, bad things happened".

Closed #11249
</description><key id="86243335">11539</key><summary>Improve exception message when shard has a partial commit (segments_N file) due to prior disk full</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Exceptions</label><label>bug</label><label>v1.6.0</label></labels><created>2015-06-08T16:23:39Z</created><updated>2015-06-12T13:10:38Z</updated><resolved>2015-06-08T16:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-08T16:25:51Z" id="110064183">looks good
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix a typo in the documentation: six_hun -&gt; "narrower"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11538</link><project id="" key="" /><description>This was introduced in https://github.com/elastic/elasticsearch.github.com/commit/defaf4f0, probably
as a search-and-replace mistake.
</description><key id="86238601">11538</key><summary>Fix a typo in the documentation: six_hun -&gt; "narrower"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels /><created>2015-06-08T16:12:31Z</created><updated>2015-07-08T17:22:11Z</updated><resolved>2015-07-08T17:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T17:22:11Z" id="119669553">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11538 from Collaborne/docs-sort-sr-typo</comment></comments></commit></commits></item><item><title>Corrections to example CURL commands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11537</link><project id="" key="" /><description>There were numerous errors in your CURL commands. Foremost, the `-X` parameter only changes the text of the HTTP method sent by curl, not other aspects of the behaviour of the tool (such as whether to accept a cached response). Curl has two main behaviours, GET-like and POST-like. The latter is triggered by the presence of a `-d` parameter. The presence of this parameter also changes the default method from GET to POST (unless overridden by `-X`). `-XGET` is completely unnecessary, as is `-XPOST -d …`. `-XDELETE` will behave like GET (such as sending If-Modified-Since when trying to delete something!), so these have also been changed to pass `-d`. If the body text is empty, the `-d` parameter is moved to the canonical position before the URL (the canonical form of the command is `curl [options] url`).
</description><key id="86211758">11537</key><summary>Corrections to example CURL commands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nickshanks</reporter><labels><label>docs</label></labels><created>2015-06-08T15:02:02Z</created><updated>2015-06-19T16:08:45Z</updated><resolved>2015-06-19T16:08:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickshanks" created="2015-06-08T15:18:59Z" id="110036546">I have now signed the CLA, though I still see an X here. Hopefully just a temporary issue.
</comment><comment author="clintongormley" created="2015-06-12T08:55:18Z" id="111419029">Hi @nickshanks 

Thanks for the PR.  I'm on the edge about it though for a few reasons.  The curl syntax is used for documentation purposes (although obviously these commands are runnable from the command line as well). Most users will use some client API or the Sense or some other UI.

For this reason, it is often useful to include explicit GET / POST / DELETE.  Secondly, Elasticsearch doesn't support caching, so the presence or absence of the `If-Modified-Since` header is meaningless.  

Honestly, I think I'd prefer converting these requests to Sense format, instead of curl, with the added advantage of enabling a "View in Sense" link at the bottom of each code block.
</comment><comment author="nickshanks" created="2015-06-12T09:15:18Z" id="111425038">On 12 Jun 2015, at 09:56, Clinton Gormley notifications@github.com wrote:

&gt; Hi @nickshanks
&gt; 
&gt; Thanks for the PR. I'm on the edge about it though for a few reasons. The curl syntax is used for documentation purposes (although obviously these commands are runnable from the command line as well). Most users will use some client API or the Sense or some other UI.

Yep, my main concern was that the -X argument was being used as if it changed any request into any other request, and people who haven't read the curl man page might make incorrect assumptions (as my colleague did, which is what prompted the PR).

&gt; For this reason, it is often useful to include explicit GET / POST / DELETE. Secondly, Elasticsearch doesn't support caching, so the presence or absence of the If-Modified-Since header is meaningless.

What Elasticsearch supports isn't really relevant though, its intermediaries (i.e. caching proxies) for which I am concerned, as they might return cached results without Elasticsearch even seeing a request. Also, I do not know what other behaviours differ between curl's GET-mode and it's POST-mode, so was just trying to play it safe.

&gt; Honestly, I think I'd prefer converting these requests to Sense format, instead of curl, with the added advantage of enabling a "View in Sense" link at the bottom of each code block.

I don't know what Sense is, so leave it to you.

## 

Nicholas.
</comment><comment author="clintongormley" created="2015-06-12T11:34:44Z" id="111463521">&gt; Yep, my main concern was that the -X argument was being used as if it changed any request into any other request, and people who haven't read the curl man page might make incorrect assumptions (as my colleague did, which is what prompted the PR).

Could you expand on this comment?  What incorrect assumption did your colleague make, with what end result?
</comment><comment author="nickshanks" created="2015-06-12T12:06:24Z" id="111469990">&gt; On 12 Jun 2015, at 12:35, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; Yep, my main concern was that the -X argument was being used as if it changed any request into any other request, and people who haven't read the curl man page might make incorrect assumptions (as my colleague did, which is what prompted the PR).
&gt; 
&gt; Could you expand on this comment? What incorrect assumption did your colleague make, with what end result?

This was at a previous job and unrelated to your API — I just elided over that as I didn't feel it was worth mentioning, but it stuck in my mind as "dangers of blindly using curl without knowing what the options mean", and prompted me to go and learn for myself. When I saw your documentation a couple of days ago, all the old alarm bells started ringing.

Incidentally, he was trying to pass data on a GET request using -d and not understanding why he needed to specify -XGET (we had -XPOST -d 'foo' in our scripts, so naturally someone reading them and knowing HTTP but not curl would assume -X specified the method and -d only provided the body). It was an in-house, very unrestful RESTful API, and I am glad I am not there any more :-)

## 

Nicholas.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Convert curl examples to Sense for snapshot restore</comment></comments></commit></commits></item><item><title>Fix recovered translog ops stat counting when retrying a batch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11536</link><project id="" key="" /><description>#11363 introduced a retry logic for the case where we have to wait on a mapping update during the translog replay phase of recovery. The retry throws or recovery stats off as it may count ops twice.

See http://build-us-00.elastic.co/job/es_g1gc_master_metal/8381/ for an example failure
</description><key id="86183698">11536</key><summary>Fix recovered translog ops stat counting when retrying a batch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T13:45:39Z</created><updated>2015-06-08T16:17:51Z</updated><resolved>2015-06-08T16:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-08T15:21:05Z" id="110037985">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file></files><comments><comment>Merge pull request #11536 from bleskes/recovery_translog_op_count_on_mapping_retry</comment></comments></commit></commits></item><item><title>Adds eclipse settings files to set correct encoding and complier preferences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11535</link><project id="" key="" /><description /><key id="86163170">11535</key><summary>Adds eclipse settings files to set correct encoding and complier preferences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T12:47:13Z</created><updated>2015-06-10T08:19:06Z</updated><resolved>2015-06-10T08:19:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-08T12:49:27Z" id="109981566">Maybe we could try to use the [additionalConfig](https://maven.apache.org/plugins/maven-eclipse-plugin/eclipse-mojo.html#additionalConfig) option in the parent pom to avoid duplicating this option in every module?
</comment><comment author="jpountz" created="2015-06-08T13:46:44Z" id="110001347">Left some minor comments, but I like having everything factored out in the parent pom!
</comment><comment author="colings86" created="2015-06-08T13:52:33Z" id="110002859">@jpountz fixed your comments and pushed an update.
</comment><comment author="jpountz" created="2015-06-08T13:53:24Z" id="110003334">LGTM!
</comment><comment author="jpountz" created="2015-06-09T17:57:50Z" id="110447056">@colings86 It seems to me like this change has been pushed already?
</comment><comment author="colings86" created="2015-06-10T08:19:03Z" id="110647166">Yes, it has, not sure why it didn't auto-close
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11534</link><project id="" key="" /><description /><key id="86127638">11534</key><summary>Upgrade to Lucene 5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-06-08T10:52:48Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-06-08T13:52:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-08T11:47:29Z" id="109960606">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11534 from rmuir/lucene_5_2</comment></comments></commit></commits></item><item><title>[CI] two_nodes_should_run_using_public_ip fails due to missing node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11533</link><project id="" key="" /><description>Stack trace:

```
java.lang.AssertionError: expected:&lt;2&gt; but was:&lt;1&gt;
    at __randomizedtesting.SeedInfo.seed([6E452F021F50AE3F:1E58450F06FBF158]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.junit.Assert.failNotEquals(Assert.java:743)
    at org.junit.Assert.assertEquals(Assert.java:118)
    at org.junit.Assert.assertEquals(Assert.java:555)
    at org.junit.Assert.assertEquals(Assert.java:542)
    at org.elasticsearch.discovery.azure.AbstractAzureComputeServiceTest.checkNumberOfNodes(AbstractAzureComputeServiceTest.java:51)
    at org.elasticsearch.discovery.azure.AzureTwoStartedNodesTest.two_nodes_should_run_using_public_ip(AzureTwoStartedNodesTest.java:78)
```

repro line:

```
mvn test -Pdev -Dtests.seed=6E452F021F50AE3F -Dtests.class=org.elasticsearch.discovery.azure.AzureTwoStartedNodesTest -Dtests.slow=true -Dtests.method="two_nodes_should_run_using_public_ip" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=it -Dtests.timezone=Asia/Kashgar
```

Build url: http://build-us-00.elastic.co/job/es_core_master_centos/5094/
</description><key id="86124913">11533</key><summary>[CI] two_nodes_should_run_using_public_ip fails due to missing node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-06-08T10:44:23Z</created><updated>2016-01-18T20:13:14Z</updated><resolved>2016-01-18T20:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-11T06:59:18Z" id="111017322">Another similar failure for two_nodes_should_run_using_private_ip : http://build-us-00.elastic.co/job/es_g1gc_master_metal/8693/testReport/junit/org.elasticsearch.discovery.azure/AzureTwoStartedNodesTest/two_nodes_should_run_using_private_ip/
</comment><comment author="clintongormley" created="2016-01-18T20:13:14Z" id="172641846">Long time since we've seen this. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate addQuery methods that are going to be removed in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11532</link><project id="" key="" /><description>Also fix the generics in `IndicesQueriesModule#addQuery(Class)` so that it can be used as the single way to register custom query parsers.

Relates to #11481
</description><key id="86103013">11532</key><summary>Deprecate addQuery methods that are going to be removed in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-06-08T09:34:03Z</created><updated>2015-06-10T16:23:54Z</updated><resolved>2015-06-08T10:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-08T09:41:35Z" id="109931571">LGTM
</comment><comment author="s1monw" created="2015-06-08T10:09:58Z" id="109941182">LGTM good to go into 1.6 as well
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>already flushing issue after upgrade to 1.5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11531</link><project id="" key="" /><description>When I update the elastic to 1.5.2, it usually occur the issue as below:

[2015-06-07 00:00:07,870][DEBUG][action.admin.indices.flush] [pay_es_node0] [_river][1], node[nopuwJy_Q76PK1AwibE7Dw], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.flush.FlushRequest@22debddf]
org.elasticsearch.transport.RemoteTransportException: [pay_es_node1][inet[/_._._._:9300]][indices:admin/flush[s]]
Caused by: org.elasticsearch.index.engine.FlushNotAllowedEngineException: [_river][1] already flushing...
        at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:620)
        at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:593)
        at org.elasticsearch.index.shard.IndexShard.flush(IndexShard.java:675)
        at org.elasticsearch.action.admin.indices.flush.TransportFlushAction.shardOperation(TransportFlushAction.java:109)
        at org.elasticsearch.action.admin.indices.flush.TransportFlushAction.shardOperation(TransportFlushAction.java:49)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:339)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:325)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Before that, I never encounter this, so I think it should be the upgraded version's issue, which cause the bulk insert operation failed. Anybody can tell me the reason ? 

Thanks
</description><key id="86087688">11531</key><summary>already flushing issue after upgrade to 1.5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leesword</reporter><labels /><created>2015-06-08T08:46:32Z</created><updated>2015-06-08T09:47:07Z</updated><resolved>2015-06-08T09:47:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-08T09:47:04Z" id="109934278">The flush process is in charge of commiting (fsyncing) any pending changes to disk so that the transaction log can be cleared. There can only be one ongoing flush at a time.  We trigger flushes internally but you can also trigger it externally with the flush API. If there is already an ongoing flush when the external API is called, the API will fail and tell you that with this exception. You can ask it wait for on going flushes before returning by using the `wait_if_ongoing` parameter.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html#flush-parameters

I'm closing this now because it seems that everything works as designed. If something is acting up, please feel fee to reopen.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> there possible delete before the same field data in every week?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11530</link><project id="" key="" /><description>Insert one data, `" @ timestamp, score: 80, ip: 192.8.8.8 "`, later time insert multiple pieces of data, including one for, `" @ timestamp, score: 90, ip: 192.8.8.8 "`, and for the same week, there possible delete before the same week`"ip: 192.8.8.8"`record?
</description><key id="86082143">11530</key><summary> there possible delete before the same field data in every week?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zcola</reporter><labels /><created>2015-06-08T08:29:41Z</created><updated>2015-06-10T16:21:52Z</updated><resolved>2015-06-10T16:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-10T16:21:49Z" id="110820758">Hi @zcola 

Please ask questions like these on the forum instead: https://discuss.elastic.co/c/elasticsearch

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>similarity model is not parsed at mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11529</link><project id="" key="" /><description>Dear ES developers, I have been troubled by this issue for two days -- really need help
it's about use custom similarity module in ES. 
I wrote my own SimilarityProvider module as here, https://github.com/aimeida/tryES2/tree/master/src/main/java/com/unsilo/paysim
compiled it to a jar file and throw to ES lib path. 

then I try send the following index:

``` javascript
PUT /test
{
  "settings" : {
  "similarity" : {
    "my_similarity1" : {
       "type": "com.unsilo.paysim.index.PayWeightSimilarityProvider"
      },
    "my_similarity2" : {
          "type" : "DFR",
          "basic_model": "g",
            "after_effect" : "l",
            "normalization" : "h2",
            "normalization.h2.c" : "3.0"
      }
  }
  }
}

PUT /test/docs/_mapping
{
      "properties" : {
        "content" : {
          "type": "string",
          "similarity" : "my_similarity2"
        }
      }
}
```

everything works fine. Now I want to see the mappings that has been parsed, 

``` javascript
GET /test/_mappings
```

gives me the right results (note that my_similarity2 is part of the ES similarity modules)

``` javascript
 {
   "test": {
      "mappings": {
         "docs": {
            "properties": {
               "content": {
                  "type": "string",
                  "similarity": "my_similarity2"
               }
            }
         }
      }
   }
}
```

However, when I try to set the mapping using my own similarity modules,

``` javascript
PUT /test/docs/_mapping
{
      "properties" : {
        "content" : {
          "type": "string",
          "similarity" : "my_similarity1"
        }
      }
}
```

ES doesn't report any error, so I check the mapping again:

``` javascript
GET /test/_mappings
```

It's obvious that my similarity setting is completely ignored!!

``` javascript
 {
   "test": {
      "mappings": {
         "docs": {
            "properties": {
               "content": {
                  "type": "string"
               }
            }
         }
      }
   }
}
```

I really don't understand what happens... the settings seems to be right

``` javascript
{
   "test": {
      "settings": {
         "index": {
            "creation_date": "1433694379354",
            "number_of_shards": "5",
            "number_of_replicas": "1",
            "version": {
               "created": "1050199"
            },
            "uuid": "0v33M9g5TYaKVWxymyhNLQ",
            "similarity": {
               "my_similarity1": {
                  "type": "com.unsilo.paysim.index.PayWeightSimilarityProvider"
               },
               "my_similarity2": {
                  "basic_model": "g",
                  "type": "DFR",
                  "normalization": "h2",
                  "normalization.h2": {
                     "c": "3.0"
                  },
                  "after_effect": "l"
               }
            }
         }
      }
   }
}
```

I haven't been able to run ES in debug mode yet, so can't figure out what is wrong. Moreover, I also tried writing the similarity module as a plugin, the same thing happens -- ES doesn't register(properly) or call my module at all...

Please share your opinions and ideas !
</description><key id="85916716">11529</key><summary>similarity model is not parsed at mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">aimeida</reporter><labels /><created>2015-06-07T16:41:42Z</created><updated>2015-06-09T10:46:07Z</updated><resolved>2015-06-09T10:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-06-09T07:42:47Z" id="110262580">I think you are missing an @Assisted annotation here https://github.com/aimeida/tryES2/commit/b8e521846b1ddb3d71d7817719869f047722e10e#commitcomment-11582316 and therefore the name is not picked up. 
Can you confirm this fixes the problem?

I wonder if we can automatically check for unannotated parameters like in https://github.com/elastic/elasticsearch/pull/10763/ but need to understand guice better first.
</comment><comment author="aimeida" created="2015-06-09T10:46:06Z" id="110316393">Thanks ! problem solved ! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Near Real Time Updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11528</link><project id="" key="" /><description>Hi,
My question relates to the near real time nature for updates.

Assume that I set a refresh interval of 1 min.

I index a document with a single field of type integer, that looks like this:
{
  "age":10
}

I am returned the following id: 'AU3LAx8AleL5uDCGX0DQ'

I will not be able to see this document for 1 min for any search requests.

However, within this 1 min, suppose I do an update on this document, with the following script.

ctx._source.age += age_pr
Where age_pr is reference to an 'age' parameter = 3.

What should I expect to see after a minute?
10 or 13 or 3.

I ask because I plan to be deploy a relatively large cluster with multiple shards and indices(distributed) and do not know what I can expect in the above scenario.

Regards,
Bhargav.
</description><key id="85898219">11528</key><summary>Near Real Time Updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rrphotosoft</reporter><labels /><created>2015-06-07T12:27:42Z</created><updated>2015-06-07T12:57:47Z</updated><resolved>2015-06-07T12:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-07T12:57:46Z" id="109749188">Please ask this on the mailing list: discuss.elastic.co.

I think it should be 13. A GET will also give 13 before the end of this minute BTW.

Closing. Not an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk update doesn't support `fields` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11527</link><project id="" key="" /><description>The `fields` parameter in the update API allows the user to retrieve the updated `_source` field.  This support is missing in the bulk update API:

```
PUT test/test/1
{
  "foo": 1
}

POST /test/test/1/_update?fields=_source
{
  "doc": {
    "foo": 2
  }
}
```

Neither the QS param nor in the body return the _source field:

```
POST /test/test/_bulk?fields=_source
{"update": { "_id": 1}}
{"fields": ["_source"], "doc": {"foo": 3}}
```

And throws an exception if specified in the metadata line:

```
POST /test/test/_bulk
{"update": { "_id": 1, "fields": "_source"}}
{"doc": {"foo": 3}}
```
</description><key id="85894953">11527</key><summary>Bulk update doesn't support `fields` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Bulk</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-07T11:38:35Z</created><updated>2015-07-08T16:25:35Z</updated><resolved>2015-07-08T16:25:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>Merge pull request #12114 from jasontedor/feature/11527</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>Add support for retrieving fields in bulk updates</comment></comments></commit></commits></item><item><title>JVM running elasticsearhc gets stuck in GC loop on Haswell nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11526</link><project id="" key="" /><description>Hi,

Our farm consists of ~ 900 Linux nodes with elasticsearch configured to use 1 GB heap, locked memory and these parameters (apart from enabling GC logging):
-XX:NewSize=500m -XX:MaxNewSize=600m" 

Roughly half of the cluster are recently purchased (Haswell architecture, 12x2 cores dual socket E5-2680 with 64 GB RAM). We are running Scientific Linux 6.6, which is a RHEL6 equivalent.

Only on these nodes we experience random lockups seen as very long GC where virtually all CPUs are spinning endlessly and never get out of that loop.

There is a way to get the process out of this by either attaching and detaching debugger (or runnning gstack against the process). Then the GC finally finishes, printing a very large elapsed time.
See for example:

2015-06-06T02:10:54.752+0200: 659391.663: [GC (Allocation Failure)  658250K-&gt;167958K(987136K), 0.0734349 secs]

2015-06-06T02:25:42.360+0200: 660279.271: [GC (Allocation Failure)  659441K-&gt;167230K(987136K), 25788.3097757 secs]

2015-06-06T09:35:41.052+0200: 686077.962: [GC (Allocation Failure)  658735K-&gt;202317K(987136K), 0.0538601 secs]

Elasticsearch version is 1.4.2, but I don't think that it is relevant in this case. Initially we used java-1.7.0-openjdk (v75). We now migrated to java-1.8.0-oracle-headless v45, and the problem persists.

We don't have much memory prssure on nodes, and no unusual patter appears with long GC as above.
Perhaps we are hitting JVM, or even Linux, or evne hardeware bug here?

ps -L shows many, usually O(10-30) threads using a lot of CPU, most likely in GC. 

Essentially we are posting it here to share out problem, in case someone else has seen this problem, or if we could be pointed to better methods to investigate (gdb is not very helpful since it "fixes" the problem).
</description><key id="85782793">11526</key><summary>JVM running elasticsearhc gets stuck in GC loop on Haswell nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smorovic</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2015-06-06T17:15:01Z</created><updated>2015-06-12T10:18:18Z</updated><resolved>2015-06-12T10:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-06T17:21:15Z" id="109619690">Is it possible you are hitting http://www.infoq.com/news/2015/05/redhat-futex ?  Also a scary Linux, Haswell-only bug.

But, it doesn't sound like you can un-stick the process when that bug strikes...
</comment><comment author="smorovic" created="2015-06-06T18:12:23Z" id="109627134">Hi,
Thanks for this info. It's possible that this is the case. I am not sure how SLC6 kernel version numbering corresponds to RedHat, but I will check this with sysadmins if we're affected (our kernel is 2.6.32-504.3.3.el6.x86_64).

Yes, unstacking always seem to work. In fact it is simpler (qucker) to do it using gstack. I supposed that it is a synchronisation problem, so if trapping threads enforces synchronisation, it gets them out of that loop. Also they actually use a lot of CPU (spin over the lock I guess..).
</comment><comment author="clintongormley" created="2015-06-12T09:55:27Z" id="111433591">@smorovic have you managed to confirm whether this is the futex bug yet?
</comment><comment author="smorovic" created="2015-06-12T10:18:18Z" id="111438956">Hi,
We suffer from exactly the same symptoms as described here, so apparently it is a futex bug (we will however update out machines with a new kernel next week). I'll close it. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Worker-json.js not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11525</link><project id="" key="" /><description>Topic says it...when attempting to edit a Saved Search I get:

Error: Script file not found: worker-json.js (:0)

This is kibana-4.0.2-linux-x86.tar.gz downloaded just this morning.
</description><key id="85782752">11525</key><summary>Worker-json.js not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DigiAngel</reporter><labels /><created>2015-06-06T17:14:22Z</created><updated>2015-06-08T10:49:36Z</updated><resolved>2015-06-08T10:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-08T10:49:35Z" id="109948475">@DigiAngel This is a Kibana issue so I have moved it to the Kibana issues list. You can track the new issue here: https://github.com/elastic/kibana/issues/4137
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add option to `_cat/indices` to return index creation date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11524</link><project id="" key="" /><description>Would be nice to provide an additional option for admins to return index.creation_date (from the index metadata) for the indices that are returned from the _cat/indices api. 
</description><key id="85649070">11524</key><summary>Add option to `_cat/indices` to return index creation date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-05T22:36:18Z</created><updated>2015-09-30T18:05:58Z</updated><resolved>2015-09-30T18:05:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-16T14:24:46Z" id="112449235">I took a stab at implementing this, mainly to verify my setup. Please see the pull request above, let me know if anything else is required.
</comment><comment author="johtani" created="2015-09-30T18:05:58Z" id="144493419">This already closed by #11688
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Merge pull request #11688 from szroland/master</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Add option to `_cat/indices` to return index creation date #11524</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Add option to `_cat/indices` to return index creation date #11524</comment></comments></commit></commits></item><item><title>[maven] split packaging into modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11523</link><project id="" key="" /><description>**Important**: don't merge this PR as is. It requires more testing.
I'm opening it only to let people discuss on it and share thoughts.

The idea is to have elasticsearch core project responsible of providing a JAR and have other modules packaging this JAR and some dependencies to produce `zip`, `tar.gz`, `deb` and `rpm`.

This change creates a proper `distribution` modules in which we have today packaging for:
- zip
- tar.gz
- rpm
- deb

It creates a new `/distribution` dir in elasticsearch project.
This one contains:

```
├── deb
│   └── src
│       └── packaging
│           ├── init.d
│           ├── lintian
│           └── scripts
├── rpm
│   └── src
│       └── packaging
│           └── init.d
├── src
│   ├── main
│   │   ├── assemblies
│   │   └── resources
│   │       ├── bin
│   │       ├── config
│   │       └── lib
│   └── packaging
│       ├── env
│       ├── scripts
│       └── systemd
│           └── sysctl
├── tar
│   └── src
│       └── main
│           └── assemblies
└── zip
    └── src
        └── main
            └── assemblies
```

And then, if you run `cd distribution; mvn clean install`, you get:

```
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Elasticsearch Distribution ........................ SUCCESS [5.665s]
[INFO] Elasticsearch ZIP Distribution .................... SUCCESS [3.389s]
[INFO] Elasticsearch TAR Distribution .................... SUCCESS [2.621s]
[INFO] Elasticsearch DEB Distribution .................... SUCCESS [2.481s]
[INFO] Elasticsearch RPM Distribution .................... SUCCESS [0.842s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
```

Adding a new packaging should be adding a new module and define in its `pom.xml` any specific needs.
</description><key id="85611867">11523</key><summary>[maven] split packaging into modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Packaging</label><label>build</label></labels><created>2015-06-05T19:16:18Z</created><updated>2015-08-18T11:27:44Z</updated><resolved>2015-07-27T15:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-11T08:56:16Z" id="111048164">@dadoonet nice work! Here is a first round of comments:
1. I don't think that the package names should be changed (before: elasticsearch-2.0.0-SNAPSHOT.zip, in this pull request: elasticsearch-zip-2.0.0-SNAPSHOT.zip). Same thing for Deb/Tar/Zip packages.
2. Classpath in `elasticsearch.in.*` scripts is modified and does not seem right (before: `ES_CLASSPATH="$ES_CLASSPATH:$ES_HOME/lib/elasticsearch-2.0.0-SNAPSHOT.jar:...`, in this pull request: `ES_CLASSPATH="$ES_CLASSPATH:$ES_HOME/lib/elasticsearch-tar-2.0.0-SNAPSHOT.jar:...`
3. There's something I don't get about libs. The current master core project generates Zip/TarGz packages that includes many libs (ex: netty, commons-cli, jackson...). But the 1.6 Zip/TarGz does not include those libs, neither the Deb package generated by this pull request...
4. `/elasticsearch/core/src/packaging` directory does not seem to be removed... Or maybe it's just my IDE?
5. That would be great if `/elasticsearch/distribution/src` could be renamed into `/elasticsearch/distribution/common` because it really contains common files between packaging types.
6. I can't generate the RPM package, command `mvn clean package rpm:rpm -DskipTests -Dgpg.passphrase=none` produces error message `Source location /home/tanguy/Github/elasticsearch/distribution/rpm/config does not exist`
7. I supposed the release script will need to be updated as well since artefacts will be located in different places.

Once those points are resolved, I'll be able to run automated tests for each packaging types.
</comment><comment author="tlrx" created="2015-06-15T09:38:10Z" id="111997809">@dadoonet Point 3 is resolved by #11664
</comment><comment author="dadoonet" created="2015-06-15T16:14:45Z" id="112124224">@tlrx I updated with some new commits.

I fixed points 1, 2, 4

`5.` I used maven convention here so every source file should be under `src/`.
`6.` I tried to run the same but I don't have Ubuntu running. I guess I should try to launch a VM on my machine and run that from it?
`7.` Yes. That's is still on the TODO list (same for plugins BTW).

Let me know what you think about the new commits.
</comment><comment author="dadoonet" created="2015-06-22T09:10:01Z" id="114046713">@tlrx any chance you could review this again? It's a blocker for #11587. Thanks!
</comment><comment author="tlrx" created="2015-06-30T12:46:03Z" id="117163847">@dadoonet great work, I think we're getting close to terminate this.

I tested TAR and DEB packages on Debian 7.8, Debian 8, Ubuntu 12.04, Ubuntu 14.04 and Ubuntu 15.04: it works like a charm.

ZIP package is identical to TAR so I skipped it.

RPM can now be built with the `mvn install` command but it has a wrong naming: `elasticsearch-rpm-2.0.0-SNAPSHOT20150630120151.noarch.rpm` instead of elasticsearch-2.0.0-....rpm` which makes it impossible for me to run the automated tests. I checked its content and everything looks fine except the file name.

I also noticed that the `jsr166e-1.1.0.jar` is missing in the packages but exists right now in master... maybe it has been added in the meanwhile?
</comment><comment author="dadoonet" created="2015-06-30T16:11:51Z" id="117241137">&gt; RPM can now be built with the mvn install command but it has a wrong naming: elasticsearch-rpm-2.0.0-SNAPSHOT20150630120151.noarch.rpm instead of elasticsearch-2.0.0-....rpm` which makes it impossible for me to run the automated tests. I checked its content and everything looks fine except the file name.

Argh! Good catch. Going to fix.

&gt; I also noticed that the jsr166e-1.1.0.jar is missing in the packages but exists right now in master... maybe it has been added in the meanwhile?

Ah! yeah. I probably missed something! Thank you so much for catching it.

Also, I'll need to rebase again my work because new changes have been added to RPM in master. It will require some other tests.

BTW: I'm looking for volunteers to also update the release script when this PR will be merged in or by forking my branch and contributing there. We will need after this PR to look for RPM/DEB/ZIP/TARGZ in their own `distribution/xxx/target/release` dirs and also adapt the signed/unsigned RPM stuff...

I might be able to look at it but probably not before thursday morning.
</comment><comment author="dadoonet" created="2015-07-01T19:50:22Z" id="117806015">@tlrx I did some change and some new tests based on your feedback.
Sounds like we are making big progress! :)

This new change now publish an artifact named elasticsearch-2.0.0-SNAPSHOT.rpm but this name is only set when maven copy the artifact to `.m2`. Not sure yet how I can copy it back to target/releases as this sounds our convention.

That being said, I'm curious to see if the one in the `.m2` can pass your tests.

BTW, I also added an icon for the RPM :)
</comment><comment author="dadoonet" created="2015-07-03T07:28:01Z" id="118263584">@tlrx ping! I'd really like to move that forward and merge it soonish. So if you can test it or share with me your test scripts that would be awesome!
</comment><comment author="dadoonet" created="2015-07-03T08:36:04Z" id="118276741">Just adding for reference here a pending change that needs to be applied here as well if merged: https://github.com/elastic/elasticsearch/pull/12010
</comment><comment author="tlrx" created="2015-07-03T09:45:12Z" id="118301732">@dadoonet On my laptop the RPM package still has a wrong filename:
- Maven project: 
  - `elasticsearch-2.0.0-SNAPSHOT.rpm-2.0.0-SNAPSHOT20150703093546.noarch.rpm`
- Maven local repository: 
  - `elasticsearch-rpm-2.0.0-SNAPSHOT.rpm`

If we change `&lt;name&gt;elasticsearch-${project.version}.rpm&lt;/name&gt;` to `&lt;name&gt;elasticsearch&lt;/name&gt;` in the pom.xml, we end with:
- Maven project: 
  - `elasticsearch-2.0.0-SNAPSHOT20150703094032.noarch.rpm` **&lt;&lt; ok**
- Maven local repository: 
  - `elasticsearch-rpm-2.0.0-SNAPSHOT.rpm` **&lt;&lt; wrong**

It seems difficult to have a coherent naming between the built artefact and the installed artefact.

With this last change in the pom.xml,  RPM works on Fedora21, CentOS6.6, CentOS7,OpenSUSE13 (I did not test tar.gz/zip/deb packages)

The `jsr166e-1.1.0.jar` is still missing in the packages... did you find out if it has been added in master?
</comment><comment author="dadoonet" created="2015-07-06T08:07:04Z" id="118767528">@tlrx Could you try to build my branch again? I don't understand why my VM does not accept it anymore and fails...
</comment><comment author="spinscale" created="2015-07-06T14:35:01Z" id="118874416">any trick to build the RPM as part of `mvn package` in the root dir? Is there anything specific that fails for you or should be tested?
</comment><comment author="spinscale" created="2015-07-06T14:37:34Z" id="118874972">@clintongormley @s1monw I think that this one is a requirement to split out the plugin manager into its own sub-project. Reason for this is, that the plugin-manager source has a dependency to the core, but when packaging everything together, the plugin manager jar needs to be included in the distribution. I do not see a good way doing this, when creating the different packages is part of the core building process.
</comment><comment author="dadoonet" created="2015-07-06T14:51:04Z" id="118878772">@spinscale if your have rpm installed on your machine, then when running mvn package, rpm module is automatically added to the build.

See https://github.com/dadoonet/elasticsearch/blob/maven/distribution/distribution/pom.xml#L152-163

I tested `/usr/bin/rpmbuild`. Do you have it on your machine?
</comment><comment author="tlrx" created="2015-07-06T15:06:35Z" id="118884295">&gt; @tlrx Could you try to build my branch again? I don't understand why my VM does not accept it anymore and fails...

I can build it on my laptop (commit dcc34b5e672204e86b5452ded1db8ba34e1a3d1d).
</comment><comment author="spinscale" created="2015-07-06T15:30:43Z" id="118892440">@dadoonet installed rpm via brew on osx, thus it is `/usr/local/bin/rpmbuild`
</comment><comment author="dadoonet" created="2015-07-06T15:35:46Z" id="118896746">@spinscale oh? I was not aware that we can have rpm on OSX! I spent too much time trying to work with a VM! I guess you saved me here! Thanks. So I'll will adapt the PR to check in both dirs. For now, you can edit the `pom.xml` file and link to your dir.
</comment><comment author="spinscale" created="2015-07-06T15:36:51Z" id="118897516">maybe have a property as well `${packaging.rpm.rpmbuild}`, so one can configure via the commandline and no need to change files in case it is somewhere else?
</comment><comment author="dadoonet" created="2015-07-06T15:37:19Z" id="118897851">+1 @spinscale 
</comment><comment author="spinscale" created="2015-07-07T13:25:12Z" id="119201800">can you rebase this against master? It misses the changes of [#11938](https://github.com/elastic/elasticsearch/pull/11938), that removes the gpg stuff from the plugin configuration and just keeps it in the profile, that needs to be activated specifically
</comment><comment author="spinscale" created="2015-07-07T13:44:35Z" id="119205830">the debian package filename is `elasticsearch-deb-2.0.0-SNAPSHOT.deb`? that deb in the middle seems not needed. Also I got this exception when starting up (doesnt happen on master, so I suppose this is because it is on an old state?). From `/var/log/elasticsearch/elasticsearch.log`

```
[2015-07-07 13:38:36,839][ERROR][bootstrap                ] Exception
java.nio.file.AccessDeniedException: /var/lib/puppet
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:426)
        at java.nio.file.Files.newDirectoryStream(Files.java:413)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:179)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
        at java.nio.file.Files.walkFileTree(Files.java:2602)
        at java.nio.file.Files.walkFileTree(Files.java:2635)
        at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:136)
        at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:69)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:169)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:284)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```
</comment><comment author="dadoonet" created="2015-07-07T16:09:41Z" id="119253555">@spinscale I rebased, fixed the deb naming issue and also move rpm plugin to 2.1.3 as you did in a previous change.
Will work on fixing rpm working as well on MacOSX now and other remaining stuff.
</comment><comment author="dadoonet" created="2015-07-07T17:40:39Z" id="119279516">I added 2 new changes:
- RPM can now work also on MacOSX (thanks @spinscale for telling me I can run `brew install rpm`)
- rpm final file is also copied with the right name under `target/releases`.

Note that when artifact SHA1 will be computed, we need to make sure this is still working as expected.

That being said, may be we should simply adapt our release tool to publish everything which has been installed in the local `~/.m2` as those are the right files... I think (but unsure) that `mvn deploy` actually deploy files from this `~/.m2` dir and not from `target` or `target/releases`.

@tlrx @spinscale Could you look at this please? 

Also if you can share with me what exact commands you are using to test deb packages that will be easier for me to test and fix instead of pinging you all the time :).
</comment><comment author="spinscale" created="2015-07-13T20:28:44Z" id="121046855">ok, two things. I needed to fix the packages to contain the working directory for `start-stop-daemon` and `systemd`. This is the diff

```
diff --git a/distribution/deb/src/main/packaging/init.d/elasticsearch b/distribution/deb/src/main/packaging/init.d/elasticsearch
index 0c5e91d..19198c9 100755
--- a/distribution/deb/src/main/packaging/init.d/elasticsearch
+++ b/distribution/deb/src/main/packaging/init.d/elasticsearch
@@ -173,7 +173,7 @@ case "$1" in
        fi

        # Start Daemon
-       start-stop-daemon --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
+       start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
        return=$?
        if [ $return -eq 0 ]
        then
diff --git a/distribution/src/main/packaging/systemd/elasticsearch.service b/distribution/src/main/packaging/systemd/elasticsearch.service
index a4c2699..b857d61 100644
--- a/distribution/src/main/packaging/systemd/elasticsearch.service
+++ b/distribution/src/main/packaging/systemd/elasticsearch.service
@@ -13,6 +13,8 @@ Environment=LOG_DIR=${packaging.elasticsearch.log.dir}
 Environment=PID_DIR=${packaging.elasticsearch.pid.dir}
 EnvironmentFile=-${packaging.env.file}

+WorkingDirectory=${packaging.elasticsearch.home.dir}
+
 User=${packaging.elasticsearch.user}
 Group=${packaging.elasticsearch.group}
```

One bigger issue  remains. The `distribution/pom.xml` and all the pom files below contains a SNAPSHOT parent and each sub-project contains a version of the distribution pom. Who is updating those to stable values and ensure that they do not run out of sync?

```
    &lt;parent&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;

// each sub project pom.xml
    &lt;parent&gt;
        &lt;groupId&gt;org.elasticsearch.distribution&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch-distribution&lt;/artifactId&gt;
        &lt;version&gt;2.0.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
```

Apart from that I tested the packages and they look good.
</comment><comment author="spinscale" created="2015-07-27T15:52:26Z" id="125252344">obsoleted by #12286
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[BUILD] Don't shade core artifacts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11522</link><project id="" key="" /><description>This commit adds an additioal jar that is shaded and keeps all the
artifacts that are used by default on the server-side unshaded. Users
that need a shaded jar can now use the `shaded` classifyer to pull
the shaded minimized jar in instead. Including the shaded jar in a
downstream project looks like this:

``` XML
&lt;dependency&gt;
  &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
  &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
  &lt;classifier&gt;shaded&lt;/classifier&gt;
&lt;/dependency&gt;
```
</description><key id="85609463">11522</key><summary>[BUILD] Don't shade core artifacts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T19:09:41Z</created><updated>2015-06-05T19:54:16Z</updated><resolved>2015-06-05T19:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-05T19:43:35Z" id="109417957">+1

It gives the user the choice (shaded or unshaded) and conceptually really simplifies the build. I also like that we run the server unshaded which is the way we test!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[Packaging] Fix missing dependencies for RPM/DEB packages</comment></comments></commit></commits></item><item><title>[maven] remove migration script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11521</link><project id="" key="" /><description>The migration script was not mean to be merged in master branch.
We can remove it as it has been run once.
</description><key id="85602248">11521</key><summary>[maven] remove migration script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2015-06-05T18:42:15Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-06-09T18:19:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-08T21:35:55Z" id="110148347">LGTM
</comment><comment author="dadoonet" created="2015-06-09T16:14:02Z" id="110418717">@s1monw It sounds like you are still using this script. So should we keep it around? May be we should move it in dev-tools?
</comment><comment author="s1monw" created="2015-06-09T18:19:26Z" id="110453995">yeah lets keep it for now
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>{1.5.2}: Initialization Failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11520</link><project id="" key="" /><description>I encountered this error when I had a power failure, possibly in the middle of an indexing process.

I haven't made out much from the errors yet, and have just picked out the error exceptions and seeing if I find any search results from them. Such as, `codec footer mismatch`

I'm hoping someone has already seen this before and can help savage what's there. I would really just like to keep the Searches, Visualizations, and Dashboards that I created, and the `logstash-*` indexes can be deleted. However, for the sake of completeness, it should be best if a full solution for this error can be created for more serious cases; where the system must be recoverable without resorting to backups.

```
Failed to configure logging...
org.elasticsearch.ElasticsearchException: Failed to load logging configuration
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:139)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:89)
    at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:100)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:184)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
    at java.nio.file.Files.readAttributes(Files.java:1686)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
    at java.nio.file.Files.walkFileTree(Files.java:2602)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:123)
    ... 4 more
log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{1.5.2}: Initialization Failed ...
1) ElasticsearchException[codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))]
    CorruptStateException[codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))]
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
  at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState
    for parameter 4 at org.elasticsearch.gateway.local.LocalGateway.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.LocalGateway
  while locating org.elasticsearch.gateway.Gateway
    for parameter 1 at org.elasticsearch.gateway.GatewayService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.GatewayService
Caused by: org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:297)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadIndexState(LocalGatewayMetaState.java:400)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadState(LocalGatewayMetaState.java:388)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(LocalGatewayMetaState.java:164)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.gateway.local.state.meta.CorruptStateException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
    at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:207)
    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.read(MetaDataStateFormat.java:168)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:287)
    ... 48 more

2) Error injecting constructor, org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
  at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState
    for parameter 4 at org.elasticsearch.gateway.local.LocalGateway.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.LocalGateway
  while locating org.elasticsearch.gateway.Gateway
Caused by: org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:297)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadIndexState(LocalGatewayMetaState.java:400)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadState(LocalGatewayMetaState.java:388)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(LocalGatewayMetaState.java:164)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.gateway.local.state.meta.CorruptStateException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
    at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:207)
    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.read(MetaDataStateFormat.java:168)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:287)
    ... 39 more

3) Error injecting constructor, org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
  at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState
Caused by: org.elasticsearch.ElasticsearchException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:297)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadIndexState(LocalGatewayMetaState.java:400)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadState(LocalGatewayMetaState.java:388)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(LocalGatewayMetaState.java:164)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.gateway.local.state.meta.CorruptStateException: codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
    at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
    at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:207)
    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.read(MetaDataStateFormat.java:168)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:287)
    ... 29 more

3 errors
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```
</description><key id="85600871">11520</key><summary>{1.5.2}: Initialization Failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">naisanza</reporter><labels><label>feedback_needed</label></labels><created>2015-06-05T18:36:11Z</created><updated>2016-01-18T10:44:21Z</updated><resolved>2016-01-18T10:44:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-10T16:00:11Z" id="110811325">Hi @naisanza 

First thing is to make sure you're configuring logging properly - see your first exception about not being able to find the config file.

Then for this exception:

```
codec footer mismatch: actual footer=0 vs expected footer=-1071082520 (resource: BufferedChecksumIndexInput(SimpleFSIndexInput(path="/usr/share/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.09/_state/state-0.st")))
at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
```

This state file is written as a temp file and only once written and fsync'ed is it atomically renamed to `state-0.st`.  However, this file has been truncated, which indicates a hardware problem (eg your disk actually cached the writes instead of really doing an fsync).

What you can try to do is to delete that file. With it out of the way, the node should start and (hopefully, if you have replicas on another node) find another  copy of the shard elsewhere.  If you don't have replicas then you have probably lost the data in that shard.
</comment><comment author="clintongormley" created="2016-01-18T10:44:21Z" id="172494718">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GatewayAllocator: reset rerouting flag after error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11519</link><project id="" key="" /><description>After asynchronously fetching shard information the gateway allocator issues a reroute via  a cluster state update task. #11421 introduced an optimization trying to avoid submitting unneeded reroutes when results for many shards come in together. This is done by having a rerouting flag, indicating a pending reroute is coming and thus any new incoming shard info doesn't need to issue a reroute. This flag wasn't reset upon an error in the reroute update task. Most notably - if a master node had to step during to a min_master_node violation, it could reject an ongoing reroute. Lacking to reset the flag causing it to skip any future reroute, when the node became master again.

Example failure: http://build-us-00.elastic.co/job/es_core_1x_metal/9122/testReport/junit/org.elasticsearch.cluster/MinimumMasterNodesTests/multipleNodesShutdownNonMasterNodes/
</description><key id="85598421">11519</key><summary>GatewayAllocator: reset rerouting flag after error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>blocker</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T18:22:42Z</created><updated>2015-06-05T19:21:39Z</updated><resolved>2015-06-05T19:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-05T18:37:23Z" id="109391494">LGTM
</comment><comment author="kimchy" created="2015-06-05T18:42:42Z" id="109392791">nice catch @bleskes!, LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>GatewayAllocator: reset rerouting flag after error</comment></comments></commit></commits></item><item><title>[maven] update maven-assembly-plugin to 2.5.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11518</link><project id="" key="" /><description>``` xml
&lt;plugin&gt;
 &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
 &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
 &lt;version&gt;2.5.5&lt;/version&gt;
&lt;/plugin&gt;
```
## Release Notes - Maven Assembly Plugin - Version 2.5.5
- Bug
  - [MASSEMBLY-767] - Schema missing from the web site
  - [MASSEMBLY-768] - JarInputStream unable to find  manifest
    created by version 2.5.4
  - [MASSEMBLY-769] - ZIP fileMode permissions not properly set with
    dependencySet and unpackOptions
</description><key id="85586326">11518</key><summary>[maven] update maven-assembly-plugin to 2.5.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T17:21:57Z</created><updated>2015-06-05T18:35:00Z</updated><resolved>2015-06-05T18:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-05T18:14:24Z" id="109384362">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Blob store shouldn't try deleting the write.lock file at the end of the restore process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11517</link><project id="" key="" /><description>Since we are creating write.lock earlier now, blob store shouldn't attempt deleting this file during clean up at the end of the restore process. The file is locked and the blog store doesn't succeed, but it generates a lot of useless warnings "failed to delete file [write.lock] during snapshot cleanup".
</description><key id="85584651">11517</key><summary>Blob store shouldn't try deleting the write.lock file at the end of the restore process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T17:12:51Z</created><updated>2015-06-08T00:34:01Z</updated><resolved>2015-06-05T19:07:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-05T18:38:54Z" id="109391872">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>Snapshot/Restore: blob store shouldn't try deleting the write.lock file at the end of the restore process</comment></comments></commit></commits></item><item><title>Add delete-by-query plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11516</link><project id="" key="" /><description>This pull request adds a new plugin called "delete-by-query" which implements the now deprecated delete-by-query feature using scan/scroll/bulk requests.

Notes:
- `size` parameter controls the scroll shard_size and the number of actions in bulk requests (defaults to 1000)
- `timeout` parameter can be used to stop scrolling documents after a given time
- response now looks like this (here a node is killed during the DBQ execution):

``` json
{  
   "took":60866,
   "timed_out":false,
   "_indices":{  
      "_all":{  
         "found":531046,
         "deleted":79901,
         "missing":0,
         "failed":301702
      },
      "disposants-2014":{  
         "found":375702,
         "deleted":74000,
         "missing":0,
         "failed":301702
      },
      "beer":{  
         "found":5901,
         "deleted":5901,
         "missing":0,
         "failed":0
      }
   },
   "failures":[  
      {  
         "shard":-1,
         "index":null,
         "reason":{  
            "type":"node_not_connected_exception",
            "reason":"[Puck][inet[/192.168.1.16:9300]] Node not connected"
         }
      }
   ]
}
```

Since the process involves the execution of a scan request (which can fail), then successive async scroll requests (which can also fail) we may imagine a better failure reporting. When a scroll request succeed, the scrolled documents are added to a Bulk request executed in an async manner. If the bulk fails, all documents are reported as `failed` documents in the counter.

Rest API documentation and test will be added later.
</description><key id="85580591">11516</key><summary>Add delete-by-query plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T16:55:31Z</created><updated>2015-06-18T16:53:57Z</updated><resolved>2015-06-17T13:49:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-06T18:08:31Z" id="109626800">I left some minor comments around logging and usage of thread pool (not needed I think).

I could't follow why we need a semaphore and such, I think I a missing something. My thought was that we do search -&gt; bulk -&gt; search -&gt; .... until there are no more results, so always async callback execution type chain until we are done.
</comment><comment author="tlrx" created="2015-06-09T10:10:57Z" id="110300928">@kimchy thanks for your review! Your comments make sense, no need to use semaphore stuff... I rebased and updated the code, it is way simpler now.

I'll add some rest tests too.
</comment><comment author="tlrx" created="2015-06-10T12:09:09Z" id="110722910">@s1monw thanks for your review. I updated the code following your comment and added a REST test. Can you please have another look if possible? Thanks :)

Documentation will be added in another PR.
</comment><comment author="s1monw" created="2015-06-12T12:48:35Z" id="111479604">looks pretty good though. I left a bunch of comments
</comment><comment author="tlrx" created="2015-06-16T11:13:35Z" id="112387362">@s1monw thanks a lot for your review, very valuable. I updated the code following your comments, please let me know if there are still things to improve.

I'd love to have your help on writing documentation for this plugin, since I'm not sure to be able to explain all fallacies of the previous implementation.
</comment><comment author="s1monw" created="2015-06-16T18:34:17Z" id="112523037">&gt; I'd love to have your help on writing documentation for this plugin, since I'm not sure to be able to explain all fallacies of the previous implementation.

lets get this in as is and open an issue for the documentation I will take a look at comment on it what aspects I would take into account?
</comment><comment author="s1monw" created="2015-06-16T18:34:36Z" id="112523109">oh yeah so here is my LGTM ;)
</comment><comment author="tlrx" created="2015-06-17T13:52:03Z" id="112808875">@s1monw thanks!

I created https://github.com/elastic/elasticsearch/issues/11723 for the java doc aspect.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add documentation for delete by query plugin (see #11516)</comment></comments></commit></commits></item><item><title>Allow for backwards compatibility for unix timestamp in pre 2.x indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11515</link><project id="" key="" /><description>In order to be backwards compatible, indices created before 2.x must support
indexing of a unix timestamp and its configured date format. Indices created
with 2.x must configure the `epoch_millis` date formatter in order to
support this. Also the `numeric_resolution` parameter is not needed anymore,
as this is configured by either using `epoch_millis` or `epoch_seconds`, however
it must still be supported for older indices.

Relates #10971
</description><key id="85561904">11515</key><summary>Allow for backwards compatibility for unix timestamp in pre 2.x indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T15:40:40Z</created><updated>2015-06-25T15:23:04Z</updated><resolved>2015-06-25T15:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-08T22:31:08Z" id="110163071">@spinscale Thanks for adding this bwc. I left a couple comments.
</comment><comment author="spinscale" created="2015-06-09T09:06:22Z" id="110283661">After a discussion with Boaz, we should add a configurable mapping option, that supports parsing a unix timestamp by default, but can be disabled via the mapping in order to not break existing applications
</comment><comment author="jpountz" created="2015-06-09T17:16:29Z" id="110437747">@spinscale @bleskes Can you elaborate on why we need an option? We've been making lots of efforts to reduce options on mappings so adding a new one feels a bit like going backwards?
</comment><comment author="bleskes" created="2015-06-09T18:22:18Z" id="110454597">@jpountz it is my understanding the idea is not to introduced another param but rather change the default date parsing such that it does what the old code used to do (give preference to unix time stamps) but this time all the parsing goes through the proper channels (field mappers), making it simpler and allowing people which don't like it the ability to change it.
</comment><comment author="jpountz" created="2015-06-09T18:26:48Z" id="110455654">If we only change the default behaviour based on when the index was created then I'm good.
</comment><comment author="bleskes" created="2015-06-09T18:42:11Z" id="110460363">@jpountz we do it all the time as we used to di this hard coded in code. Now it will be configurable (and the same by default)
</comment><comment author="s1monw" created="2015-06-22T18:14:34Z" id="114205047">@spinscale any idea when you can work on this / get this in?
</comment><comment author="spinscale" created="2015-06-23T12:56:29Z" id="114489625">rebased against master and fixed some tests, need to add more tests to ensure expected behaviour
</comment><comment author="spinscale" created="2015-06-24T12:09:53Z" id="114845685">ok added tests, that do some of the scenarios explained in https://github.com/elastic/elasticsearch/issues/10971 (will leave the dynamic date detection in its own PR)

would be happy go get another review here, and an agreement if we should leave the version specific date formatter in the mapper or move it out. thx! /cc @rjernst 
</comment><comment author="rjernst" created="2015-06-24T16:35:01Z" id="114935176">@spinscale I left some more comments.
</comment><comment author="spinscale" created="2015-06-25T11:08:50Z" id="115211949">worked on all your review comments, except the documentation one, which I think should be kept there to not confuse users trying to look `numeric_resolution` up
</comment><comment author="rjernst" created="2015-06-25T13:52:56Z" id="115264627">LGTM, thanks! I left one small comment on the wording for migration docs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make prompt placeholders consistent with existing placeholders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11514</link><project id="" key="" /><description>In #10918, we introduced the prompt placeholders. These had a different format
than our existing placeholders. This changes the prompt placeholders to follow the
format of the existing placeholders.

Relates to #11455 

Note: This PR is targeted to 1.x to make porting to 1.x and 1.6 easier. Will manually port to master.
</description><key id="85556159">11514</key><summary>Make prompt placeholders consistent with existing placeholders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.6.0</label></labels><created>2015-06-05T15:14:18Z</created><updated>2015-06-08T15:46:18Z</updated><resolved>2015-06-06T15:03:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-06-05T23:53:55Z" id="109485704">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>for fields with numeric type 'byte', elasticsearch accepts values greater than 127 and indexes them wrongly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11513</link><project id="" key="" /><description>using elasticsearch 1.5.2 I can index values greater than 127 for a byte field without getting any error. The values just overflow internally and the resulting numbers can be searched for.
e.g. indexing 250 ends up as -6
indexing 260 ends up as 4

```
curl -XPUT localhost:9200/test -d '
{
    "mappings" : {
        "foo" : {
            "properties" : {
                "field1" : { "type" : "byte" }
            }
        }
    }
}'

curl -XPUT localhost:9200/test/foo/one -d '
{
  "field1" : 127
}'

curl -XPUT localhost:9200/test/foo/two -d '
{
  "field1" : 250
}'

curl -XPUT localhost:9200/test/foo/three -d '
{
  "field1" : 260
}'
```

now searching like this...

```
curl -XGET localhost:9200/test/_search?search_type=count -d '
{
    "aggs" : {
        "values" : {
            "terms" : { "field" : "field1" }
        }
    }
}'
```

returns the following resultset

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 3,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "values": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": -6,
               "doc_count": 1
            },
            {
               "key": 4,
               "doc_count": 1
            },
            {
               "key": 127,
               "doc_count": 1
            }
         ]
      }
   }
}
```

trying the same with a 'short' number, you get an error message for document 'two'

```
curl -XPUT localhost:9200/test -d '
{
    "mappings" : {
        "foo" : {
            "properties" : {
                "field1" : { "type" : "short" }
            }
        }
    }
}'


curl -XPUT localhost:9200/test/foo/one -d '
{
  "field1" : 31000
}'

curl -XPUT localhost:9200/test/foo/two -d '
{
  "field1" : 33000
}'
```

```
{
   "error": "RemoteTransportException[[dev-test-mon-es-02][inet[/192.168.110.83:9300]][indices:data/write/index]]; nested: MapperParsingException[failed to parse [field1]]; nested: JsonParseException[Numeric value (33000) out of range of Java short\n at [Source: UNKNOWN; line: 2, column: 19]]; ",
   "status": 400
}
```
</description><key id="85519992">11513</key><summary>for fields with numeric type 'byte', elasticsearch accepts values greater than 127 and indexes them wrongly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">passing</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>discuss</label></labels><created>2015-06-05T12:24:21Z</created><updated>2016-04-14T15:57:09Z</updated><resolved>2016-04-14T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-05T14:12:17Z" id="109305213">Even with  `coerce:false`, the out-of-range byte value doesn't fail, and the short shouldn't fail with `coerce:true`, but does...
</comment><comment author="szroland" created="2015-06-16T22:40:22Z" id="112594511">Fields mapped as byte actually get read as short and then cast to byte. E.g. if you map the field as byte and send in the 33000 value, you get the same error (e.g. out of range for **short**), e.g.:

```
at org.elasticsearch.common.xcontent.support.AbstractXContentParser.shortValue(AbstractXContentParser.java:108)
at org.elasticsearch.index.mapper.core.ByteFieldMapper.innerParseCreateField(ByteFieldMapper.java:289)
```

For consistency, `AbstractXContentParser.byteValue()` method could be created that ultimately could call `JsonParser.getByteValue()`, which does include byte range check, assuming this behavior of the range check in the parser is desired. However, this is why coerce has no effect.

Coerce works converting String values, but not for down-casting numeric values for short/byte fields, since the range check is in JsonParser in the latter case, e.g. `ensureNumberConversion()` is too late, the parser will have thrown the exception by then if the incoming value is numeric.

JsonParser only checks range for short and byte (e.g. not for int). So an approach could be to always parse the value as int and then cast down for short/byte fields. `ensureNumberConversion()` would then ensure range enforcement as needed. I think this approach would be more consistent overall than relying on `JsonParser.getShortValue()`/`JsonParser.getByteValue()` for short/byte, and `ensureNumberConversion()` for int/float range check.

If you agree, I can implement this.

Coerce documentations also doesn't explicitly mention the behavior of casting down, it only mentions truncating fractions, which is not exactly the same thing, strictly speaking. Maybe it should be spelled out that not only will your 10.2 be converted to 10, but also 266 will become 10 for byte fields.

Another consistency issue is that even if coerce is set and String values are accepted, fractions cause `NumberFormatException`, so while `330.12` and `"330"` are accepted, `"330.12"` is not. JsonParser explicitly checks for the `.` to decide between `long` and `double`, the same approach could be used here as well.
</comment><comment author="jpountz" created="2015-06-18T13:02:00Z" id="113148495">@szroland +1 this proposal makes sense to me.

&gt; If you agree, I can implement this.

Feel free to ping me when you have a PR ready.
</comment><comment author="szroland" created="2015-06-18T22:06:30Z" id="113301886">Looking at this a bit more I think we might want to be just a little bit careful, and make sure the semantics of coerce is fully intended. Bytes are problematic, because they are read as shorts and then casted to byte. So to illustrate the issue, let me use a short field, and the value 32768, which is `Short.MAX_VALUE` + 1, e.g. fits only into an int.

Current behavior:

| Input | Output (coerce=true) | Output (coerce=false) |
| --- | --- | --- |
| 42 | 42 | 42 |
| 42.12 | 42 | IllegalArgumentException |
| "42" | 42 | IllegalArgumentException |
| "42.12" | NumberFormatException | IllegalArgumentException |
| 32768 | JsonParseException | JsonParseException |
| 32768.12 | JsonParseException | JsonParseException |
| "32768" | NumberFormatException | IllegalArgumentException |
| "32768.12" | NumberFormatException | IllegalArgumentException |
| true | JsonParseException | JsonParseException |
| "true" | NumberFormatException | IllegalArgumentException |

The issue is that 42.12 is accepted, but "42.12" is not, even though coerce also supposed to mean to accept the number as a string.
Other issue is that overflow is sometimes an IllegalArgumentException, which can be ignored using ignore_malformed, sometimes it is JsonParseException, which can not be ignored.
With bytes, there is also the automatic cast, which results in even the integer of the number changing (e.g. from 266 to 10).

So one possible behavior is that the integer part must be in range, but when `coerce` is true the number can have a fractional part which is cut off and/or it can also be input as string.

| Input | Output (coerce=true) | Output (coerce=false) |
| --- | --- | --- |
| 42 | 42 | 42 |
| 42.12 | 42 | IllegalArgumentException |
| "42" | 42 | IllegalArgumentException |
| "42.12" | **42** | IllegalArgumentException |
| 32768 | **IllegalArgumentException** | **IllegalArgumentException** |
| 32768.12 | **IllegalArgumentException** | **IllegalArgumentException** |
| "32768" | NumberFormatException | IllegalArgumentException |
| "32768.12" | NumberFormatException | IllegalArgumentException |
| true | **IllegalArgumentException** | **IllegalArgumentException** |
| "true" | NumberFormatException | IllegalArgumentException |

The alternative is to allow the implicit casting of out-of-range values when coerce is true, not only for bytes but for other types as well, but that can be a little bit confusing e.g. turning into negative value in this case.

| Input | Output (coerce=true) | Output (coerce=false) |
| --- | --- | --- |
| 42 | 42 | 42 |
| 42.12 | 42 | IllegalArgumentException |
| "42" | 42 | IllegalArgumentException |
| "42.12" | **42** | IllegalArgumentException |
| 32768 | :boom: **-32768** | **IllegalArgumentException** |
| 32768.12 | :boom: **-32768** | **IllegalArgumentException** |
| "32768" | :boom: **-32768** | IllegalArgumentException |
| "32768.12" | :boom: **-32768** | IllegalArgumentException |
| true | **IllegalArgumentException** | **IllegalArgumentException** |
| "true" | NumberFormatException | IllegalArgumentException |

Reading back I'm getting mixed signals on this casting aspect of coerce from @passing and @clintongormley.
</comment><comment author="anujgandharv" created="2015-11-29T09:29:53Z" id="160393842">When is it planned to be fixed? We have a use case where we need this functionality.
</comment><comment author="martinhynar" created="2016-02-23T18:46:25Z" id="187837031">It seems that not only byte allows decimals, I see the same behavior with short, integer, and long too.
With this mapping

```
{ "mappings": {
    "example": {
      "properties": {
        "number_long": { "type": "long", "coerce": true },
        "number_integer": { "type": "integer", "coerce": true },
        "number_short":  { "type": "short", "coerce": true }}}}}
```

and document

```
{ "number_long": 42.5,
  "number_integer": 42.5,
  "number_short": 42.5 }
```

I am getting values that are not integers. `_search?fields=number_long,number_integer,number_short,_source`

```
"hits": [{
        "_index": "test-integers",
        "_type": "example",
        "_id": "1",
        "_score": 1,
        "_source": {
          "number_long": 42.5,
          "number_integer": 42.5,
          "number_short": 42.5 },
        "fields": {
          "number_long": [ 42.5 ],
          "number_integer": [ 42.5 ],
          "number_short": [ 42.5 ]}}]
```

Behavior is same on 1.7.4 and 2.2.0.

@szroland How you managed to get 42 from 42.12 with coercion enabled?
</comment><comment author="jpountz" created="2016-02-24T05:40:17Z" id="188083931">@martinhynar those values are retrieved from the `_source`, which elasticsearch does not modify. So if the source contains 42.5 for a field value, trying to read this value from the source will always return 42.5 regardless of the mappings.
</comment><comment author="martinhynar" created="2016-02-24T07:58:17Z" id="188131321">@jpountz Thanks for explanation.
I updated my example locally by adding `"store":true` to all fields and now the values returned by search are all 42. I did not realized that store is false by default and therefore values are extracted from `_source`.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/CustomDocValuesField.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/KeywordFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyDateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyDoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyFloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyIntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyLongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyNumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LegacyTokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TextFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/BaseGeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperLegacy.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/LegacyIpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionBuilder.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/search/DocValueFormat.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/boost/CustomBoostMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/DateFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/DateFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyByteFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyDateFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyDoubleFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyFloatFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyIntegerFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyLongFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyShortFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LegacyTokenCountFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/NumberFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/NumberFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/LegacyDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/TimestampFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ip/IpFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ip/IpFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ip/LegacyIpMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/numeric/LegacyNumericTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractTermQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchPhrasePrefixQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchPhraseQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MultiMatchQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IpTermsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/simple/SimpleSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/sort/AbstractSortTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java</file><file>modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java</file><file>plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java</file><file>plugins/mapper-murmur3/src/main/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapper.java</file><file>plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/SizeFieldMapper.java</file><file>plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file></files><comments><comment>Use the new points API to index numeric fields. #17746</comment></comments></commit></commits></item><item><title>Execute Scripting Engine before searching for inner templates in template query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11512</link><project id="" key="" /><description>The search template and template query did not run the template through the script engine before searching for an inner template. This meant that parsing for the inner template failed because the template was not always valid JSON (if it contained mustache code) when it was parsed to find the inner template. This has been fixed and Tests added to check for the failing behaviour

Tests are from https://github.com/elastic/elasticsearch/pull/8393
</description><key id="85514318">11512</key><summary>Execute Scripting Engine before searching for inner templates in template query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T11:51:55Z</created><updated>2015-06-10T16:23:25Z</updated><resolved>2015-06-08T10:16:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-06-08T07:36:09Z" id="109893140">Except for the need to adapt this to the new repository layout LGTM
</comment><comment author="jpountz" created="2015-06-08T08:29:35Z" id="109906615">The change seems to be fixing a bug too? Maybe the PR title/description should reflect that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add safeguards to prevent simple user errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11511</link><project id="" key="" /><description>There are a number of places where a naive user can break Elasticsearch very easily.  We should add more (dynamically overridable) safeguards that prevent users from hurting themselves.

Note:
- We are adding high limits to start so that we don't suddenly disable things that users already do today, but so that sysadmins have tools that they can use to protect their clusters.  We can revisit the limits later on.
- All these settings should be prefixed by `policy.` to make them easier to document together and to understand their purpose.

Accepted limits:
- [x] #9311 Hard limit on `from`/`size` 
- [x] #12149 Global default value for search timeouts (Could be ridiculously high like an hour and it would still help)
- [x] #17386 Disable fielddata-loading on analyzed text fields by default (Adrien)
- [x] #17396 Limit the max number of shards to 1000 (Adrien)
- [x] #17133 Limit the size of all in-flight requests (Daniel)
- [x] #17357 Limit the number of fields that can be added to a mapping to 1000 
- [x] #17400 Add maximum mapping depth to 20 
- [ ] Add sane limits for thread size and queue size (Jim)
- [ ] Don't allow search requests greater than (eg) 10MB (Colin) 
- [x] #14983 Limit the number of `nested` fields per index to 50 (Yannick) 
- [x] #17522 Limit `window_size` in rescore API (@nik9000)
- [ ] #17558 Disable script access to `_source` fields by default
- [ ] #18739 Limit the number of shards that can be rerouted at the same time
- [ ] Hard limit on `from`/`size` in top hits (much smaller than a normal query)
- [x] #19694 Limit script compilation rate to avoid hard coding of params in scripts
- [ ] #20705 Max number of shards per node (enforced as total shards per cluster)
- [ ] #20760 Limit index creation rate
- [ ] #23268 Add upper limit for `scroll` expiry time

---

For discussion:
- [ ] Disable certain query types, eg wildcard, span etc?
- [ ] #14046 Limit on the number of buckets returned by aggs 
- [ ] #9310 Limit the size of the response (eg for very large doc bodies)
- [ ] ~~Kill slow scripts when search timeout has lapsed aka while(true) should not require a rolling restart to recover from~~ Don't run a script a second time when the first execution takes longer than 1 second
- [ ] ~~#6470 Disable searching on all indices by default~~ Handled by max number of shards

Any other ideas?
</description><key id="85501651">11511</key><summary>Add safeguards to prevent simple user errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Core</label><label>:Index APIs</label><label>:Mapping</label><label>:Search</label><label>enhancement</label><label>Meta</label><label>v6.0.0</label></labels><created>2015-06-05T10:30:11Z</created><updated>2017-06-28T10:17:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-05T10:43:12Z" id="109256324">&gt; Limit the max number of shards

I'm wondering if we should do it per index or per cluster. If we do it per index, then we might also want to have a max number of indices per cluster.

&gt; Limit the size of a bulk request

I guess it would also apply to multi-get and multi-search.
</comment><comment author="alexbrasetvik" created="2015-06-05T11:06:35Z" id="109259165">Some of this could go into a "sanity checker"-kind of plugin akin to the migration plugin that runs a bunch of tests as well.

That one could warn when e.g. minimum master nodes looks wrong, and when the number of shards/indexes/fields looks silly / approaches the above limits.
</comment><comment author="clintongormley" created="2015-06-05T13:58:56Z" id="109302311">@alexbrasetvik the requires the user to actually run the check.  Often poor sysadmins are at the mercy of their users.  What I'd like to do is to prevent users from blowing things up by mistake.
</comment><comment author="alexbrasetvik" created="2015-06-07T16:37:09Z" id="109770905">@clintongormley Agreed! I still think there's room for both, though such a tool should be another issue. 

For example, a high number of indexes with few documents and identical mappings can be a sign that the user is doing per-user index partitioning when he shouldn't. That will turn into a problem, even if the current values are far from hitting above mentioned limits. 
</comment><comment author="pickypg" created="2015-07-15T20:16:07Z" id="121733001">&gt; Any other ideas?
- Limit the max number of indices
  - It's effectively covered by limiting by shards, but touching too many indices may indicate more of a logical issue than the shard count (e.g., with daily indices, it's much easier to realize that sending a request to 5 indices represents five days rather than 25 shards with default counts).
- Limit the _concurrent_ request size
  - Request circuit breaker across all concurrent requests
</comment><comment author="dakrone" created="2015-07-16T18:49:54Z" id="122048095">&gt; Limit the concurrent request size 

This is already available with the thread pools and queue_sizes to limit the number of requests per-node and apply backpressure.

EDIT: I guess I am taking "size" as "count", is that what you mean?
</comment><comment author="pickypg" created="2015-07-17T20:57:40Z" id="122414160">@dakrone Size of an actual request. For instance, if one request comes in with an aggregation that uses `size: 0` at the same time as another, then maybe we should block the second one (or at least delay).
</comment><comment author="jpountz" created="2015-10-30T22:46:34Z" id="152668255">Another protection to add: check mapping depth #14370
</comment><comment author="ppf2" created="2015-11-02T18:41:05Z" id="153120179">Limit the max value that can be set for queue_size for our search, bulk, index, etc.. thread pools so users can't set them to unlimited, millions, etc..?
</comment><comment author="makeyang" created="2016-05-11T12:56:26Z" id="218450766">dose the size in term aggregation is considered by this issue?
</comment><comment author="clintongormley" created="2016-05-11T13:22:41Z" id="218457271">@makeyang it's covered by https://github.com/elastic/elasticsearch/issues/14046, which is under discussion
</comment><comment author="makeyang" created="2016-05-13T05:55:53Z" id="218958297">is it reasonable to add max_doc_number per index?
is it reasonable to add enable_all_for_search?
</comment><comment author="clintongormley" created="2016-05-13T17:11:07Z" id="219103243">&gt; is it reasonable to add max_doc_number per index?

Well, there's already a hard limit but what are you trying to achieve with this one?  And what is the user supposed to do instead of indexing into the same index?

&gt; is it reasonable to add enable_all_for_search?

What problem are you trying to prevent with disabling access to `_all`?  Why not just disable the `_all` field if you don't want it used?
</comment><comment author="makeyang" created="2016-05-16T02:41:21Z" id="219337362">@clintongormley 
1. some of users even put daily rolling log data into one index. so with max_doc_number parameter, I actually want to force users to think about put data into multi indices.
2. enable_all_for_search is not about _all field, it is 'http://localhost:9200/_all/_query?q=tag:wow', when i put one cluster for multi users, I really don't want users to search _all indices. 
</comment><comment author="clintongormley" created="2016-05-16T10:27:13Z" id="219395064">&gt;  some of users even put daily rolling log data into one index. so with max_doc_number parameter, I actually want to force users to think about put data into multi indices.

OK, we have a better solution for this that we're thinking about - basically an alias that will generate a new index when it reaches a specified limit (eg size, number of docs, time)

&gt; enable_all_for_search is not about _all field, it is 'http://localhost:9200/_all/_query?q=tag:wow', when i put one cluster for multi users, I really don't want users to search _all indices.

Querying all indices is not a problem per se.  Rather, it is the total number of shards, which is already handled by https://github.com/elastic/elasticsearch/pull/17396
</comment><comment author="makeyang" created="2016-05-16T10:38:02Z" id="219396786">@clintongormley  thanks a lot. that's all I need. 
btw: when will the better solution you are mentioned above will be formed a issue?
</comment><comment author="chenryn" created="2016-05-17T10:00:51Z" id="219673283">Would you consider howto use cgroup to control resource usage of search/index/percolator... threads?

elasticsearch need to run cross linux/windows...so, maybe there is a quick way: ES only need to give all thread a threadname, for example, a search thread named `search-thread-1` etc, then the linux users can get thread ids by grep threadname and then put tids into cgroup.
</comment><comment author="jonaf" created="2016-06-06T17:27:26Z" id="224028056">I'd like to put in a vote for an additional safeguard: some kind of protection on `Terms` queries that have hundreds or thousands of terms. I've seen many times where applications will produce `Terms` queries with hundreds or thousands of terms, and it craters Elasticsearch very easily. It'd be nice to have a default cap and truncate the query, like have a default terms limit (similar to default hits) that can be increased. Knowing that doing this is a problem early on can help application developers to architect their application to avoid needing terms queries that are so huge.
</comment><comment author="clintongormley" created="2016-06-07T14:57:10Z" id="224307459">@jonaf i like the idea. You want to open a separate issue where we can discuss it, and we can link it this this meta issue 
</comment><comment author="s1monw" created="2016-08-05T10:23:38Z" id="237816573">I think we should also limit the number of shards in an index. If somebody creates an index with 10k shards the node might go nuts immediately. I think we should limit this to 32 or maybe 128?
</comment><comment author="s1monw" created="2016-08-05T10:25:47Z" id="237816974">I also wonder if we should hard limit it and follow moors law and increase it every N years? :) lets start with 256 and force multi index?
</comment><comment author="ppf2" created="2016-08-05T16:52:53Z" id="237902971">&gt; I think we should also limit the number of shards in an index. If somebody creates an index with 10k shards the node might go nuts immediately. I think we should limit this to 32 or maybe 128?

Nice idea.  Similarly, for multitenant use cases that may have a ton of single sharded per-user indices, it can be nice to have a limit or warning when the # of shards per node becomes ridiculous.  Not sure what this limit will be based on, perhaps a combination of # of file descriptor, cores and heap.  But it will be nice to prevent users from having something like N # of shards per node, etc..
</comment><comment author="s1monw" created="2017-03-28T08:04:38Z" id="289694421">@clintongormley I think we missed one rather important aspect when it comes to soft-limits. Today the user can override those limits via dynamic properties which is ok most of the time but in the case of a cloud hosting infrastructure where the org that runs the infrastructure needs to have full control over these limits they should be able to disable the dynamic property or should disable setting these settings entirely?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log message to indicate that no `elasticsearch.yml` file was found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11510</link><project id="" key="" /><description>If the config file is not found, Elasticsearch starts without any exceptions and uses the default settings.  We should either log a message, or even throw an exception here.

Relates to #11485
</description><key id="85475899">11510</key><summary>Log message to indicate that no `elasticsearch.yml` file was found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Logging</label><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-05T08:56:34Z</created><updated>2015-08-13T12:26:03Z</updated><resolved>2015-08-13T12:26:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Show human readable Elasticsearch version that created index and date when index was created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11509</link><project id="" key="" /><description>By setting human parameter to true, it's now possible to see human readable versions of Elasticsearch that created and updated the index as well as the date when the index was created.

Closes #11484
</description><key id="85453199">11509</key><summary>Show human readable Elasticsearch version that created index and date when index was created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-05T06:43:22Z</created><updated>2015-06-17T23:05:25Z</updated><resolved>2015-06-17T23:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-17T15:02:34Z" id="112835236">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticsearchIllegalStateException when field's value is big integer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11508</link><project id="" key="" /><description>In mapping, the type of  field `name` is `string` . I put a document:

```
{
  "name":123456789123456789123456789123456789123456789123456789
}
```

It's OK. 

But when I search and highlight:

```
{
  "query":{
     "match_all":{}
 },
  "highlight":{
     "fields:{
        "name":{}
    }
  }
}
```

I get a excepton: `ElasticsearchIllegalStateException[No matching token for number_type [BIG_INTEGER]]` .

The call chain is `AbstractXContentParser#map()` -&gt; `AbstractXContentParser#readMap()` -&gt;  `AbstractXContentParser#readValue()` -&gt; `JsonXContentParser#numberType()` .
</description><key id="85417541">11508</key><summary>ElasticsearchIllegalStateException when field's value is big integer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thihy</reporter><labels><label>:Highlighting</label><label>:Mapping</label><label>bug</label><label>discuss</label></labels><created>2015-06-05T04:29:34Z</created><updated>2016-09-02T06:57:34Z</updated><resolved>2016-09-02T06:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eabovsky" created="2016-01-20T20:53:44Z" id="173355833">I am experiencing this issue, as well.
</comment><comment author="megastef" created="2016-01-26T13:59:24Z" id="175028677">+1 
curl -XPOST 'localhost:9200/test_number/doc/1' -d '{
 "name" : "test document",
 "test_int" : 129813493927432849126469276498264619863246932642169846316469
}'

{"error":{"root_cause":[{"type":"mapper_parsing_exception","reason":"failed to parse"}],"type":"mapper_parsing_exception","reason":"failed to parse","caused_by":{"type":"illegal_state_exception","reason":"No matching token for number_type [BIG_INTEGER]"}},"status":400}
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java</file></files><comments><comment>Source filtering should keep working when the source contains numbers greater than `Long.MAX_VALUE`. #20278</comment></comments></commit></commits></item><item><title>Change metadata file format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11507</link><project id="" key="" /><description /><key id="85323702">11507</key><summary>Change metadata file format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T23:18:17Z</created><updated>2015-06-10T20:45:49Z</updated><resolved>2015-06-10T15:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-08T21:43:18Z" id="110150076">LGTM
</comment><comment author="s1monw" created="2015-06-10T19:55:04Z" id="110892146">would it make sense to checksum the metadata while you are changing the format?
</comment><comment author="imotov" created="2015-06-10T19:55:56Z" id="110892355">@s1monw yes, it makes perfect sense. I will add that.
</comment><comment author="imotov" created="2015-06-10T20:45:49Z" id="110907382">@s1monw created #11589
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_update some time i get DocumentAlreadyExistsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11506</link><project id="" key="" /><description>I am doing _update on some doc using the url like
http://10.0.0.91:9200/alias_SOME_INDEX/SOME_TYPE/SOME_ID/_update
and my payload is like

{ 
"doc":{"baseName":"Microsoft Office"}, 
"upsert":{"baseName":"Microsoft Office"}
}

on some other thread i am doing PUT for the same document with same id.

i get document already exists]","status":409 
some time it works some time i get this error

so i suspect some thing to do with 2 threads doing similar thing causing this, but an "_update" call giving already exists kind of exception looks strange
</description><key id="85294903">11506</key><summary>_update some time i get DocumentAlreadyExistsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vedil</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label></labels><created>2015-06-04T21:51:08Z</created><updated>2015-10-07T15:16:19Z</updated><resolved>2015-10-07T15:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-05T08:53:06Z" id="109206256">I haven't tried to replicate this, but it sounds like it is trying to do an upsert, and not handling the conflict exception correctly.
</comment><comment author="vedil" created="2015-06-12T14:43:03Z" id="111515625">i have made a pull request for fixing this, do i have to do any thing more? is the pull request fine?
</comment><comment author="clintongormley" created="2015-06-12T17:13:54Z" id="111561111">thanks for the pr @vedil - i've marked it for review. somebody should get to it shortly
</comment><comment author="vedil" created="2015-07-09T01:11:13Z" id="119775651">messed up previous pull request, so created another one https://github.com/elastic/elasticsearch/pull/12137
</comment><comment author="jasontedor" created="2015-07-10T02:13:29Z" id="120202763">Can you provide a test case that replicates the `DocumentAlreadyExistsException`?

I agree that there is clearly potential for a race condition here and think that it's important that we get to the bottom of it. A test case that reproduces the exception would be helpful.
</comment><comment author="vedil" created="2015-07-14T18:28:06Z" id="121331165">tried creating a test case after removing my changes, and modifying my test a bit
 @Test
    public void testIndexNUpdateUpsert() {
            //update action goes to the primary, index op gets executed locally, then replicated
            String[] updateShardActions = new String[]{UpdateAction.NAME, IndexAction.NAME + "[r]"};
            interceptTransportActions(updateShardActions);

```
        String indexOrAlias = randomIndexOrAlias();

        String[] indexShardActions = new String[]{IndexAction.NAME, IndexAction.NAME + "[r]"};
        interceptTransportActions(indexShardActions);

        IndexRequest indexRequest = new IndexRequest(randomIndexOrAlias(), "type", "id").source("field", "value");
        IndexResponse indexResponse = internalCluster().clientNodeClient().index(indexRequest).actionGet();
        clearInterceptedActions();
        assertSameIndices(indexRequest, indexShardActions);
        assertThat(1L, equalTo(indexResponse.getVersion()));

        indexRequest = new IndexRequest(randomIndexOrAlias(), "type", "id").source("field", "value");
        indexResponse = internalCluster().clientNodeClient().index(indexRequest).actionGet();
        clearInterceptedActions();
        //assertSameIndices(indexRequest, indexShardActions);
        assertThat(2L, equalTo(indexResponse.getVersion()));

        UpdateRequest updateRequest = new UpdateRequest(indexOrAlias, "type", "id").upsert("field2", "value2").doc("field1", "value1");
        UpdateResponse updateResponse = internalCluster().clientNodeClient().update(updateRequest).actionGet();
        assertThat( updateResponse.getVersion(), greaterThan(indexResponse.getVersion()));

        clearInterceptedActions();
        System.out.println("updateRequest "+updateRequest +" updateShardActions = "+updateShardActions );
        assertSameIndicesOptionalRequests(updateRequest, updateShardActions);
}
```

now this is failing always by saying 1 is &lt; 2, is my assert supposed to succeed?
i am doing index, index, update and expecting a version &gt; 2
</comment><comment author="vedil" created="2015-07-14T18:29:00Z" id="121331430">i am running test case using these vm arguments in eclipse
-ea -Dtests.seed=806B4E52F9B20C5B -Dtests.assertion.disabled=false -Dtests.heap.size=512m -Dtests.locale=no_NO_NY -Dtests.timezone=America/Miquelon -Des.logger.level=DEBUG
</comment><comment author="vedil" created="2015-07-24T16:56:54Z" id="124578640">```
    @Test
    public void testIndexNUpdateUpsert() {
        //update action goes to the primary, index op gets executed locally, then replicated
        //String[] updateShardActions = new String[]{UpdateAction.NAME, IndexAction.NAME + "[r]"};
        //interceptTransportActions(updateShardActions);

        final  String indexOrAlias = randomIndexOrAlias();
        final int NUMBER_OF_THREADS = 10;
        final int UPDATE_EVERY = 2;
        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
        Thread[] threads = new Thread[NUMBER_OF_THREADS];
        for (int i = 0; i &lt; threads.length; i++) {
            threads[i] = new Thread() {
                @Override
                public void run() {
                    try {
                        for (long i = 0; i &lt; NUMBER_OF_THREADS; i++) {
                            if ((i % UPDATE_EVERY) == 0) {
                                UpdateRequest updateRequest = new UpdateRequest(indexOrAlias, "type", "id").upsert("field2", "value2").doc("field1", "value1");
                                UpdateResponse updateResponse = internalCluster().clientNodeClient().update(updateRequest).actionGet();
                                System.out.println("update response = "+updateResponse);
                            } else {
                                IndexRequest indexRequest = new IndexRequest(indexOrAlias, "type", "id").source("field", "value");
                                IndexResponse indexResponse =  internalCluster().clientNodeClient().index(indexRequest).actionGet();
                                System.out.println("index response = "+indexResponse);
                            }
                        }
                    } finally {
                        latch.countDown();
                    }
                }
            };
        }

        for (Thread thread : threads) {
            thread.start();
        }

        try {
            latch.await();
        } catch (InterruptedException e) {
            e.printStackTrace();
            throw new RuntimeException();
        }
}
```

using this test i am able to reproduce the "document already exists" also i see some exceptions whose message is like "version conflict, current [3], provided [1]" also
</comment><comment author="brwe" created="2015-07-28T13:21:01Z" id="125604266">I do not think this is an actual bug.

@vedil  I believe the unit test you put [here](https://github.com/elastic/elasticsearch/issues/11506#issuecomment-121331165) fails because you use a new `randomIndexOrAlias()` for each request and so the requests might not all got to the same index. I you use the same index each time the test will pass.

I agree that the `DocumentAlreadyExistsException` seems weird for the integration test but this is also expected I think. An update first retrieves the document via `get` and then issues an `index` request with the updated source. If a write sneaked in between `get` and issuing the `index` request we throw a `VersionConflictException` in case the document already existed before the update. However, in case the document did not exist when the `get` was executed we check that the document does still not exist when the `index` request is sent. If it does, we throw a `DocumentAlreadyExistsException`.
To circumvent this, you need to set the [retry on conflict parameter](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#_parameters_3) to a higher value.
Let me know if my explanation makes sense.

We could potentially throw a `VersionConfictException` instead of a `DocumentAlreadyExistsException` to have consistent exceptions for updates or have a dedicated `UpdateFailedException` that explains what happened  or just document this better. 
</comment><comment author="vedil" created="2015-07-28T14:13:35Z" id="125623153">regarding randomIndexOrAlias, i am calling it once and using it in all requests. so within a test it will use same index.
your other explanation makes sense, i feel VersionConfictException makes more sense.
</comment><comment author="brwe" created="2015-10-06T19:18:17Z" id="145970281">After https://github.com/elastic/elasticsearch/pull/13955 is in we will get `VersionConfictException`.
</comment><comment author="brwe" created="2015-10-07T15:16:19Z" id="146226827">#13955 was merged, closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Updated memory settings for Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11505</link><project id="" key="" /><description>Minor changes to make the memory settings section a little more OS agnostic since `mlockall` now applies to Windows.

Also, added some info on disabling the page file on Windows.
</description><key id="85263667">11505</key><summary>[DOCS] Updated memory settings for Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>docs</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T20:30:32Z</created><updated>2015-06-05T14:23:24Z</updated><resolved>2015-06-05T14:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2015-06-05T12:42:25Z" id="109282453">Thanks for the review @clintongormley !
</comment><comment author="clintongormley" created="2015-06-05T14:15:00Z" id="109306103">LGTM
</comment><comment author="gmarz" created="2015-06-05T14:23:24Z" id="109308483">Merged via https://github.com/elastic/elasticsearch/commit/9b230db095e0a012831d6a040f025526f56588f7
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide a list of all settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11504</link><project id="" key="" /><description>Provide a comprehensive complete list of **all** settings for each es version like https://gist.github.com/jprante/7dccffa7f0c0c7576fa9 does for 1.2
</description><key id="85258374">11504</key><summary>Provide a list of all settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/astefan/following{/other_user}', u'events_url': u'https://api.github.com/users/astefan/events{/privacy}', u'organizations_url': u'https://api.github.com/users/astefan/orgs', u'url': u'https://api.github.com/users/astefan', u'gists_url': u'https://api.github.com/users/astefan/gists{/gist_id}', u'html_url': u'https://github.com/astefan', u'subscriptions_url': u'https://api.github.com/users/astefan/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/893749?v=4', u'repos_url': u'https://api.github.com/users/astefan/repos', u'received_events_url': u'https://api.github.com/users/astefan/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/astefan/starred{/owner}{/repo}', u'site_admin': False, u'login': u'astefan', u'type': u'User', u'id': 893749, u'followers_url': u'https://api.github.com/users/astefan/followers'}</assignee><reporter username="">salyh</reporter><labels><label>:Settings</label><label>adoptme</label><label>docs</label></labels><created>2015-06-04T20:15:31Z</created><updated>2016-09-27T15:01:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-05T09:03:25Z" id="109210486">This would certainly be useful. One issue is that the way we manage settings makes it hard to have a centralized place with the list of all settings so this would require some refactoring...
</comment><comment author="clintongormley" created="2015-06-05T11:11:08Z" id="109259663">Related to #6732
</comment><comment author="clintongormley" created="2015-06-05T11:12:32Z" id="109259819">Also, I think that each setting should have an explanation next to it.  And we need to add a test to throw an exception if a new setting is not documented.
</comment><comment author="j0hnsmith" created="2015-06-17T08:40:08Z" id="112720412">Django has a good example https://docs.djangoproject.com/en/1.8/ref/settings/
</comment><comment author="clintongormley" created="2016-01-18T20:10:18Z" id="172640856">With #6732 on the verge of being finished (including the ability to return default settings), we're finally in a place where we can do this.
</comment><comment author="dakrone" created="2016-09-27T15:01:12Z" id="249891307">This is available with the `?include_defaultls` option, but since that doesn't include documentation for the settings, it might be good to add it to the documentation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Boolean settings parsing should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11503</link><project id="" key="" /><description>Spinoff from #11437.

Today, most boolean settings in Elasticsearch are parsed leniently
using `Booleans.parseBoolean`, which interprets `0`, `no`, `off` and `false`
as `false`, else `true` for any random strings like `NO` or `OFF` or `foobar`, etc.

I think this is dangerous: if the user has a typo for false, or maybe
thinks "disabled" would also mean false, they silently get true.

I think we should parse boolean settings strictly, maybe using this
awesome project: https://github.com/rmuir/booleanparser so that the
user gets a clear exception stating that the value is not recognized.

Unfortunately this is a biggish project ... I think we'd need to take
a similar approach to #11437, where for back compat if we see
un-parseable boolean settings from persisted places (cluster, index
settings), we dynamically map them to true and log a warning.
</description><key id="85256393">11503</key><summary>Boolean settings parsing should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-06-04T20:09:51Z</created><updated>2016-11-06T07:54:02Z</updated><resolved>2016-11-06T07:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-04T20:15:59Z" id="109035560">+1 to being strict. Could we limit to only true/false like booleanparser (i'm all for using it!), and do "backcompat" based on index created version?
</comment><comment author="clintongormley" created="2015-06-04T20:58:50Z" id="109048798">There are languages, such as Perl, which don't have boolean values.  This is the reason that boolean parsing in Elasticsearch is lenient.

I think it is OK to be lenient (as in have multiple ways of expressing true and false) but then to throw an exception if an unrecognised value is provided, eg:
- False: `false`, `0`, `"false"`, `"0"` 
- True: `true`, `"true"`, `1`, `"1"`

I believe that `no`, `yes`, `on`, `off`, etc are supported because of the YAML spec: http://yaml.org/type/bool.html
</comment><comment author="clintongormley" created="2015-06-04T20:59:31Z" id="109048952">To be clear, my comment applies to values passed through the REST API, as opposed to settings in a config file
</comment><comment author="rmuir" created="2015-06-05T06:54:02Z" id="109179727">Finally an issue worth discussing :)

I think we should have so-called "strict" boolean parsing, because we can assume what we are doing is actually important. When things aren't important e.g. `do you want another beer?`, then lenient parsing of booleans is just fine: you can default to `true` and nothing will go wrong!

So the context is critical: here we are talking about situations such as parameter configuration for a distributed system that might hold terabytes of data, and input parameters to APIs that might be connected to even larger systems doing god knows what.

This is just the assumption you must make with code: in general computers are used for important things these days, so that's why I think everyone should use boolean parsing that doesn't suck. 

Let's just make this clear and unambiguous to prevent problems: it's easy!

&gt; Today, most boolean settings in Elasticsearch are parsed leniently
&gt; using Booleans.parseBoolean, which interprets 0, no, off and false
&gt; as false, else true for any random strings like NO or OFF or foobar, etc.

Terrible

&gt; I think this is dangerous: if the user has a typo for false, or maybe
&gt; thinks "disabled" would also mean false, they silently get true.

Its dangerous for a ton of other reasons too, for example its inconsistent with java's equally crappy `Boolean.parseBoolean` which instead maps unknown values to `false`. This is just asking for bugs.

&gt; There are languages, such as Perl, which don't have boolean values. This is the reason that boolean parsing in Elasticsearch is lenient.

Perl has the worst boolean semantics on the planet:

```
The number 0, the strings '0' and '', the empty list "()", and "undef"
are all false in a boolean context. All other values are true.
Negation of a true value by "!" or "not" returns a special false
value. When evaluated as a string it is treated as '', but as a 
number, it is treated as 0.
```

Somehow they managed to actually make it worse than C. 

This doesn't mean we should map all non-zero numbers to `true`, or do other crazy things because of language X or Y or Z. Seriously, why can't we just supply simple unambiguous interfaces and let the Perl language client deal with making it "perl-like" on the client-side? That's its job.

Trying to do this stuff server-side just begs for complexity and bugs. Which numbers are acceptable? Maybe we should apply unicode normalization so that full-width numbers work? What about ideographic numbers? We can bring in ICU to parse chinese numbers. Some writing systems might not have a concept of zero, what to do there? 

Screw that. In most cases we expect these configuration values to be important, and the API to be used as an important interface for the application. Communication here should be clear, simple, and unambiguous. We can do this, and just transcend the language problem the same way we'd solve this in the physical world. What is the best way there? Something like this works surprisingly well:

```
Nod your head for `yes`, shake for `no`, anything else and I'll punch you in the face.
```

We can translate that directly into java code, by parsing "true" to `true` and "false" to `false`, with anything else as an exception. In Java, its important declare a checked exception like `ParseException` too, so that you inform the caller of the "rules", and so that the flow of their code is forced to deal with the exceptional case. I generally hate checked exceptions like anyone else, but when used sparingly as a tool to prevent bugs, they do have their place.

&gt; I believe that no, yes, on, off, etc are supported because of the YAML spec: http://yaml.org/type/bool.html

This doesn't mean we have to allow them. There are other things in the YAML spec we may not allow too. This isn't inconsistent, its the same as YAML only allowing certain whitespace characters, despite requiring a Unicode text encoding. They don't have to allow all Unicode whitespace just because Unicode defines whitespace in a certain way.

If we think the boolean settings being parsed are all unimportant, then by all means, stick with a crappy lenient parser. Otherwise we should really fix this everywhere.
</comment><comment author="uboness" created="2015-06-05T07:14:03Z" id="109184821">@rmuir I love your boolean parser, a piece of art... and love the copyrights on it too. I'm cool with stricter parsing, but only if we keep `42` to represent `maybe`!

+1 for `"true"` &amp; `"false"`
</comment><comment author="dadoonet" created="2015-06-05T09:06:41Z" id="109211728">Hahaha! I'd love to see in our code base:

``` java
if (str.equals("42")) {
  return randomBoolean();
}
```

Like an easter egg...

Totally +1 to either reject any unknown form and never fall back to `true`.
`true` and `false` makes sense to me.

In that case, may the upgrade plugin should try to detect "old settings" and warn the user to change their values? I guess it's hard to build that as this plugin does not read for example `elasticsearch.yml` file?
Also, how to know that users have been using in their data `something` which is translated to `true`?
So what would happen if someone tries to reindex the same doc again (eg. update it by script)?
</comment><comment author="saurajeet" created="2015-09-09T08:53:30Z" id="138836317">thanks @jpountz for taking time to find the right place for the issue i filed. 
+1 for "true" or "false" in stricter parsing.
</comment><comment author="rjernst" created="2015-09-09T08:55:58Z" id="138837493">I started a branch here:
https://github.com/rjernst/elasticsearch/tree/booleans_are_simple

But there are many tests that need to be fixed. Anyone feel free to pick it up and base future work on.
</comment><comment author="clintongormley" created="2016-11-06T07:54:02Z" id="258665921">Fixed in 5.0 with the great settings rewrite
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[1.3.7] shard fails to start IllegalArgumentException[No type mapped for [0]]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11502</link><project id="" key="" /><description>i can say that my mappings suck for this index, since every random payload that gets indexed from headers (indexing parsed emails) are set dynamically to "index": "no" instead of being ignored and not added to the mappings, but, at the same time, this is not something that should make 1/3 of shards fail after a power outage in the data center, where I had 2 replicas of each shard. Can some1 shed some light into this error and whether you want to examine the mapping with all the weird headers that people set on their emails?

``` java
[2015-06-04 12:53:08,633][WARN ][indices.cluster          ] [storage.01] [messages][9] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [messages][9] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:269)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: No type mapped for [0]
    at org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:224)
    at org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:34)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:241)
    ... 4 more
[2015-06-04 12:53:08,654][WARN ][cluster.action.shard     ] [storage.01] [messages][9] sending failed shard for [messages][9], node[Rus251Q6T5Ko5LhObmMzrA], [P], s[INITIALIZING], indexUUID [WgO6d6KnRQ6eNgw5CK7_Rw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[messages][9] failed to recover shard]; nested: IllegalArgumentException[No type mapped for [0]]; ]]
```
</description><key id="85252222">11502</key><summary>[1.3.7] shard fails to start IllegalArgumentException[No type mapped for [0]]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AVVS</reporter><labels /><created>2015-06-04T19:57:06Z</created><updated>2015-07-24T15:14:50Z</updated><resolved>2015-06-05T08:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-04T20:24:45Z" id="109037756">@AVVS it looks like your translog is corrupted, if you move the translog file away from the shard's directory you should be able to recover the shard.
</comment><comment author="clintongormley" created="2015-06-05T08:44:39Z" id="109205104">Just to add to this: we've done a lot of work recently to improve translog handling and to reduce the chances of corruption, eg #11011 and #11143.  Not only is corruption much less likely, but Elasticsearch is better able to detect and handle this situation if it occurs.

I don't think there is anything more to do on this ticket, so I'm going to close it.  Feel free to reopen if there is something that hasn't been covered.
</comment><comment author="nariman-haghighi" created="2015-07-24T15:14:50Z" id="124555333">Just ran into this exact error on a migration from 1.5.2 -&gt; 1.7.0. Deleted the translog file from the shard's directory and was able to recover. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minimize the usage of guava classes in interfaces, return types, arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11501</link><project id="" key="" /><description>Using these classes in interfaces forces downstream users to use shaded APIs.
We should minimize the usage of shaded APIs and keep them internal.
</description><key id="85242253">11501</key><summary>Minimize the usage of guava classes in interfaces, return types, arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T19:30:42Z</created><updated>2015-06-04T20:51:46Z</updated><resolved>2015-06-04T19:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-04T19:33:21Z" id="109020708">+1

In addition to guava being certifiably useless, if you mix in shading and bake this stuff into apis, its about as wrong as it gets. Thanks for fixing this!
</comment><comment author="rjernst" created="2015-06-04T19:38:23Z" id="109023166">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Spec isn't very readable nor is it sufficiently explicit to be useful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11500</link><project id="" key="" /><description>Contrast: https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/api/search.json#L160-L162

with (admittedly incomplete but not more so than your spec)

https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L536-L544

https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L601-L629

https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L823-L832

https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L1729-L1742

People are using Bloodhound in production.

I would suggest that a more complete spec is needed to assist client library authors, but I know how annoying maintaining documentation is. Given that, consider that Bloodhound (which has tests) can be tested against Elasticsearch to ensure spec correctness. Have you considered making an explicit spec via a client library in a language with explicit datatypes that was high enough level to make it usable for a casual reader?

Part of what exacerbates the situation with the spec is that your docs do _not_ show complete or working examples, they show fragments of the complete JSON document that would be required to work, the reader is expected to
1. Know this
2. Manually inject the fragment you're showing them into the rest of the structure which is not well explained or connected to the individual doc page they're on

The problems with 1 and 2 are compounded by the fact that it's _extremely_ easy to get the JSON structure wrong, then you are now faced with the prospect of making fiddly little changes to the JSON over and over until works.

Thus, [datatypes that take care of the JSON for you](https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L1745-L1758).

The _primary_ complaints I hear from people about Elasticsearch are the docs &amp; API. It makes what is otherwise a very useful product considerably more frustrating to use than it needs to be.
</description><key id="85240264">11500</key><summary>Spec isn't very readable nor is it sufficiently explicit to be useful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bitemyapp</reporter><labels /><created>2015-06-04T19:24:51Z</created><updated>2015-06-04T19:41:07Z</updated><resolved>2015-06-04T19:41:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T19:41:05Z" id="109024819">Closing as a duplicate of https://github.com/elastic/elasticsearch/issues/8965
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Slow indexing in upgrade from 1.3.6 to 1.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11499</link><project id="" key="" /><description>Hi all,

I upgraded from 1.3.6 to 1.5.1. Indexing is significantly slower, and there are 10x more IO reads occurring. After noticing this issue, I ran `/_upgrade` and turned off `refresh` (setting `refresh_interval: -1`). I also tried restarting the cluster. Despite this, I did not see improvement. The bulk indexing threads seem to be spending a lot of time in `InternalEngine.loadCurrentVersionFromIndex`.

My JVM version is 1.8.0_25.

This is the hot threads snippet I'm observing:

```
Hot threads at 2015-06-03T21:26:54.112Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

    1.6% (8.1ms out of 500ms) cpu usage by thread 'PauseDetector'
     10/10 snapshots sharing following 8 elements
       java.lang.Thread.sleep(Native Method)
       java.lang.Thread.sleep(Thread.java:340)
       java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
       com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly(Uninterruptibles.java:275)
       core.time.Clock.sleep(Clock.java:141)
       core.time.Clock.sleep(Clock.java:121)

    1.6% (7.8ms out of 500ms) cpu usage by thread '[flush][T#30]'
     10/10 snapshots sharing following 28 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:708)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:694)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:179)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:122)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:152)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:506)
       org.apache.lucene.index.BufferedUpdatesStream.applyTermDeletes(BufferedUpdatesStream.java:427)
       org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(BufferedUpdatesStream.java:286)
       org.apache.lucene.index.IndexWriter.applyAllDeletesAndUpdates(IndexWriter.java:3312)
       org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:3303)
       org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2989)
       org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3134)
       org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3101)
       org.elasticsearch.index.engine.InternalEngine.commitIndexWriter(InternalEngine.java:1214)
       org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:637)
       org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:593)
       org.elasticsearch.index.shard.IndexShard.flush(IndexShard.java:675)
       org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:203)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    0.8% (3.7ms out of 500ms) cpu usage by thread '[bulk][T#1]'
     10/10 snapshots sharing following 28 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:708)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:694)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:179)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:122)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:152)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:506)
       org.elasticsearch.common.lucene.uid.PerThreadIDAndVersionLookup.lookup(PerThreadIDAndVersionLookup.java:104)
       org.elasticsearch.common.lucene.uid.Versions.loadDocIdAndVersion(Versions.java:150)
       org.elasticsearch.common.lucene.uid.Versions.loadVersion(Versions.java:161)
       org.elasticsearch.index.engine.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1008)
       org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:404)
       org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
       org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:498)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:578)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:249)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:228)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```
</description><key id="85235024">11499</key><summary>Slow indexing in upgrade from 1.3.6 to 1.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">embark</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2015-06-04T19:10:39Z</created><updated>2016-01-18T20:02:16Z</updated><resolved>2016-01-18T20:02:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T19:42:16Z" id="109025625">@mikemccand any thoughts about this?
</comment><comment author="mikemccand" created="2015-06-04T21:02:17Z" id="109049574">Can you describe the docs you're indexing?  Do you pass your own IDs
or does ES generate them?  Do you ever replace docs or just index new ones?

How many segments are in the shard?  Do you use delete-by-query?

Any custom index settings?
</comment><comment author="embark" created="2015-06-04T21:31:40Z" id="109060148">Thanks for following up @mikemccand 

In response:
- The docs are JSON blobs.
- Yes, I pass in my own IDs.
- Mostly new docs are indexed, but replacement can happen.
- Running `GET` on `_segments`, each shard has roughly `num_committed_segments: 231` and `num_search_segments: 229`
- There are no deletes.
- Custom index settings include the following:

```
index:
  cache:
    field.type: soft
    field.max_size: 1g
    filter.max_size: 1g
  merge:
    policy:
      max_merge_at_once: 8
      max_merged_segment: 2g
      segments_per_tier: 24
  compound_format: false
  compound_on_flush: false
  shard.check_on_startup: false
  translog.flush_threshold_ops: 50000
  store.distributor: random

indices.memory.index_buffer_size: 10%
indices.fielddata.cache.size: 40%
index.store.type: niofs
```
</comment><comment author="mikemccand" created="2015-06-05T07:43:42Z" id="109192555">OK that's a rather high segment count, which then makes ID lookups more costly (it must visit every segment).

This is exacerbated by your high `segments_per_tier` setting and `max_merged_segment`; can you change that setting back to default?  And maybe put max_merged_segment back to its default (5g)?

Also, niofs gives worse performance for id-lookup heavy use cases; can you remove that setting (let ES use the default, which switches between niofs / mmap depending on which index file is being opened)?

Do you leave plenty of RAM (50% is recommended) to the OS for IO caching?
</comment><comment author="mikemccand" created="2015-06-05T07:52:48Z" id="109195304">Another question: what do your IDs look like, e.g. how many bytes are they typically?  Longer IDs make things slower, and which ID you use can have an impact on performance: http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html
</comment><comment author="embark" created="2015-06-08T06:24:24Z" id="109878635">In followup, @mikemccand, 

I let the all merge policies for `segments_per_tier`, `max_merged_segment`, and `max_merge_at_once`  go back to the defaults as well as `index.store.type`. There should be plenty of RAM...

I let the changes soak a little, but sadly performance is still slow, and disk read IO is actually slightly higher now. 

My ID's are about 13 bytes long, but I'm afraid that that the ID length isn't very changeable. The ID length was the same before the upgrade as well.
</comment><comment author="mikemccand" created="2015-06-08T09:29:17Z" id="109928143">Hmm that's odd that you see no improvement from those changes.

Can you confirm segments counts per shard came down to normal (~30-40) levels once you cut back to defaults for the merge policy?

13 byte IDs is not bad; I've seen much worse!

What IO system are you using?

Can you run the diagnostics plugin (https://github.com/elastic/elasticsearch-support-diagnostics) and post the results?
</comment><comment author="embark" created="2015-06-26T03:15:03Z" id="115484800">@mikemccand Sorry for the delay! And thank you for all the help so far. 

The segments counts per shard are roughly 80-90 now, so a bit higher than what you expect, but still a lot lower.

Not totally sure what you mean by IO system. Do you mean filesystem? (Edit: It occurs to me to tell you that I'm using spinny disks rather than SSD)

Here is the diagnostic run: https://gist.github.com/embark/54ae58bdf9755ddd90ce
</comment><comment author="mikemccand" created="2015-06-28T09:38:10Z" id="116244908">&gt; Here is the diagnostic run:

Thanks, I see lots of time spent throttling.  Can you disable or increase store merge IO throttling?  https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html#store-throttling

The default (20 MB/sec) is very low (in ES 2.0 this will auto-adapt).

&gt; The segments counts per shard are roughly 80-90 now, 

That's still too high, I'm not sure why.  Was this taken while indexing was still running?  Could be there were merges in flight.

&gt; I'm using spinny disks rather than SSD

OK, slow :)  Try setting `index.merge.scheduler.max_thread_count` to 1 which should net/net allow merges to complete faster.  Also see https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing for more ideas.

You should probably upgrade from 1.8.0_25.
</comment><comment author="embark" created="2015-06-30T18:34:23Z" id="117295355">&gt; That's still too high, I'm not sure why. Was this taken while indexing was still running? Could be there were merges in flight.

Yes, indexing is constantly happening, so it was taking while indexing was running. However, I turned it off for a little and still saw around 80-90 segments.

So far nothing helps, but I'll take a look at the document!
</comment><comment author="clintongormley" created="2016-01-18T20:02:15Z" id="172638147">Nothing more in the last 7 months - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Bug] 'ignore_malformed' applied to a malformed (boolean) numeric field does not work </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11498</link><project id="" key="" /><description>When I try to index a doc with a boolean field that's mapped as numeric, even with ìgnored_malformed`set to`true`, an exception is raised.

Repro steps:
##### Create a new index:

```
curl -XPUT localhost:9200/myindex
```

which yields `{ "acknowledged" : true }`
##### Create a mapping for a type in that index, with one integer field that has `ignore_malformed` set to true and coerce set to `true`

```
curl -XPUT localhost:9200/myindex/_mapping/mytype -d
'{
    "mytype": {
      "properties": {
        "foo": {
        "type": "integer",
        "ignore_malformed": true,
        "coerce": false
        }
      }
  }
}'
```

which yields `{ "acknowledged" : true }`
##### Index one document with a field whose value is false

```
curl -XPUT localhost:9200/myindex/mytype/one -d
'{
  a: 3,
  foo: false
}'
```

which yields an exception:

```
{
"error" : "MapperParsingException[failed to parse [foo]]; nested: JsonParseException[Current token (VALUE_FALSE) not numeric, can not use numeric value accessors at [Source: [B@3a98c595; line: 3, column: 13]]; ",
"status" : 400
}
```

According to `ignored_malformed: true`, even if false is not an integer, the doc should get indexed anyway.
</description><key id="85172122">11498</key><summary>[Bug] 'ignore_malformed' applied to a malformed (boolean) numeric field does not work </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">angeleg</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-06-04T16:30:13Z</created><updated>2017-06-20T11:51:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-16T23:28:01Z" id="112601597">This will not only fail with booleans, but with other non-numeric values as well, e.g.

```
{
  a: 3,
  foo: {
    value: []
  }
}
```

```
type: "json_parse_exception"
reason: "Current token (START_ARRAY) not numeric, can not use numeric value accessors at [Source: org.elasticsearch.common.io.stream.InputStreamStreamInput@2725adf1; line: 4, column: 13]"
```

Ignoring malformed values depends on the code throwing an `IllegalArgumentException` or `MapperParsingException` but here a `JsonParseException` is thrown. The solution would be to check the token type in `AbstractXContentParser.intValue()`, `shortValue()`, etc. after the String check, and if it is not numeric, do not even try parsing the value, throw an `IllegalArgumentException` immediately.

This touches the same area of code as #11513. Happy to take a stab if you all agree.
</comment><comment author="clintongormley" created="2015-12-05T20:25:31Z" id="162243371">Closing in favour of #11513
</comment><comment author="lwiskowski" created="2016-05-14T04:00:45Z" id="219199001">I changed AbstactXContentParser.java to address this issue. I will do a pull request with the updated code
</comment><comment author="clintongormley" created="2017-06-20T11:51:00Z" id="309729595">Related to https://github.com/elastic/elasticsearch/issues/12366</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shadow replica documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11497</link><project id="" key="" /><description>I think the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shadow-replicas.html) could use some clarification:
- Suggest that node.enable_custom_paths: true has some security implications (according to #8819)
- In the example it sets the data_path for the my_index index to /var/data/my_index. I found this a bit misleading, as it implies the path should contain the index name, but Elasticsearch will already include this in subdirectories, ie. would create /var/data/my_index/*/my_index in the example
</description><key id="85122306">11497</key><summary>Shadow replica documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Shadow Replicas</label><label>docs</label></labels><created>2015-06-04T14:27:15Z</created><updated>2016-02-21T20:12:57Z</updated><resolved>2016-02-21T20:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-08-10T23:12:46Z" id="129643721">Related to #12729 and #12776
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Add blurb about `data_path` not needing to include index name</comment></comments></commit></commits></item><item><title>Delete index api: reject when performed against filtered alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11496</link><project id="" key="" /><description>Delete index api currently ignores the filter in case it's performed against a filtered alias, resulting in deleting the whole index rather than just the subset of documents that match the filter. Rejecting the request is a better behaviour, as a delete index cannot effectively be performed. Note that in case of a delete index against multiple indices, the whole request fails. This is because the delete index doesn't currently support returning a detailed response, per index.

Closes #2318
</description><key id="85119369">11496</key><summary>Delete index api: reject when performed against filtered alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Aliases</label><label>breaking</label><label>enhancement</label></labels><created>2015-06-04T14:19:48Z</created><updated>2015-06-08T13:23:34Z</updated><resolved>2015-06-08T11:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-05T10:49:16Z" id="109257059">I updated this PR to only modify the delete index api (delete api stays untouched). This is ready for review, the only doubt is what we should do with delete against multiple indices where only one of them is a filtered alias, with this PR we simply fail the whole request, just because the delete index api doesn't currently support returning a detailed response with the result for each index.
</comment><comment author="clintongormley" created="2015-06-05T13:57:24Z" id="109301917">I think that we should delete the whole request if an alias is listed, and that wildcards should not expand to aliases in this end point, only indices.
</comment><comment author="javanna" created="2015-06-05T14:11:30Z" id="109305005">I see, the filter is not the only problem. routing associated with an alias can make you think that we will delete only a specific shard rather than the whole index, which we don't. It does make it simpler to reject any delete index against an alias, that said when it comes to resolving wildcards it gets trickier cause we don't have an option to expand to only concrete indices, which would require adding a new option to the kagillion existing indices options which is probably time to trim down and refactor instead...
</comment><comment author="bleskes" created="2015-06-08T11:51:47Z" id="109963241">+1 to reject DELETE against aliases in general (and not expending \* to aliases). It's ambiguous - do you delete the alias or do you delete the underlying index(es) + the alias?
</comment><comment author="javanna" created="2015-06-08T11:59:16Z" id="109965190">I am going to close this PR and move the discussion to the original issue (#2318), which I just labelled for discussion again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to monitor norms memory usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11495</link><project id="" key="" /><description>Been reading the Elasticsearch [reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html#norms), which warns me each field in each document with norms uses ~1 byte of memory. However I've been looking in the stats, but I can't see where this memory is accounted for so I can see if it is biting me. Is it included in fielddata? Is it always used, or only after I've run a query?
</description><key id="85081859">11495</key><summary>Ability to monitor norms memory usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels /><created>2015-06-04T12:44:06Z</created><updated>2015-06-04T15:32:15Z</updated><resolved>2015-06-04T13:01:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T13:01:41Z" id="108889234">Hi @jimmyjones2 

You can't get the norms memory usage all by itself, but the amount of memory that is used by Lucene for open segments (which includes norms, and things like the terms index) is available through the node stats or the indices stats under `segments.memory_in_bytes`.

As a side note, there is an open patch in Lucene to move norms fully on to disk, which will resolve the problem of heavy norms memory use.
</comment><comment author="rmuir" created="2015-06-04T15:07:57Z" id="108928235">&gt; However I've been looking in the stats, but I can't see where this memory is accounted for so I can see if it is biting me. Is it included in fielddata?

No, currently its included in the `segments` memory usage of the stats api:

```
"segments" : {
    "count" : 4,
    "memory_in_bytes" : 13160 &lt;-- there
```

But this statistic also includes other lucene datastructures. For 2.0, it is broken down more into categories like `norms_memory_in_bytes`, `terms_memory_in_bytes`, and so on:

```
"segments" : {
    "count" : 4,
    "memory_in_bytes" : 13160
    ...
    "norms_memory_in_bytes" : 384 &lt;-- there
```

Finally for 2.0 if you really want to know the gory details, pass `verbose=true` to the `_segments` api:

```
"ram_tree" : [
  ...
  , {
            "description" : "norms [Lucene50NormsProducer(fields=4,active=1)]",
            "size_in_bytes" : 112,
            "children" : [ {
              "description" : "field 'message' [table compressed]",
              "size_in_bytes" : 72,
              "children" : [ {
                "description" : "ordinals [Packed64SingleBlock2(bitsPerValue=2,size=5,blocks=1)]",
                "size_in_bytes" : 48
              } ]
            } ]
          }
```

&gt; Is it always used, or only after I've run a query?

Well you have to have norms for the field for them to be used. If you dont want them, then omit norms for fields where you dont care about scoring.

Today norms for a field are lazy-loaded on-demand, per-segment for the first query against the field. Additionally, background merges will cause norms for _all fields_ to be loaded at once. So you can't rely on this laziness really: omit norms if you don't need them. For 2.0 merges will not do that.
</comment><comment author="jimmyjones2" created="2015-06-04T15:32:15Z" id="108936422">@rmuir Thanks for the detailed response, that explains everything. I look forward to 2.0 that has already implemented exactly what I need.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get api: reject get requests against a filtered alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11494</link><project id="" key="" /><description>Documents that don't belong to the filtered alias might get returned as the filter cannot be taken into account when the document is retrieved from the transaction log, we simply reject the request given that we cannot answer it properly.
This change affects also apis that use the get api internally, like: multi_get, term_vector, multi_term_vector, explain and update. Percolator is not affected as it already knows how to execute alias filters against the lucene index in non real-time when needed.

I looked into supporting get against a filtered alias when the realtime flag is set to false, but it doesn't seem that straight-forward to be honest, as it would require executing a filter rather than a lookup in the terms enum for the proper uid. I am not sure we want to make the get api more complicated than it currently is, the right answer could just be to use the search api instead.

Closes #3861
</description><key id="85069692">11494</key><summary>Get api: reject get requests against a filtered alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:CRUD</label><label>breaking</label><label>enhancement</label></labels><created>2015-06-04T12:14:48Z</created><updated>2015-06-06T14:00:46Z</updated><resolved>2015-06-05T10:40:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T12:56:41Z" id="108884834">I don't think this should be mandatory - it breaks cases where it is just fine to do GET requests on filtered aliases.  But +1 for making it opt-in.
</comment><comment author="javanna" created="2015-06-04T13:37:18Z" id="108900890">I think it is never fine, because we return documents that don't belong to the alias, the problem being that the filter gets _always_ ignored. We provide the wrong answer while we should rather say "we cannot answer correctly". It would be good to allow for non real-time gets at some point though.
</comment><comment author="javanna" created="2015-06-05T10:40:17Z" id="109255972">This PR might not cover all the cases, think of reindexing a document using the index api against a filtered alias, we end up overriding it although it doesn't match the filter. There are probably other similar cases. We need to discuss again how we want to move forward. Will remark the original issue for discussion.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ClusterHealth shouldn't fail with "unexpected failure" if master steps down while waiting for events</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11493</link><project id="" key="" /><description>In order to wait for events of a certain priority to pass, TransportClusterHealthAction submits a cluster state update task. If the current master steps down while this task is in the queue, the task will fail causing the ClusterHealth to report an unexpected error.

We often use this request to ensure cluster stability in tests after disruption. However, depends on the nature of the failure it may happen (if we're unfortunate) that two master election rounds are needed. The above issues causes the get health request to fail after the first one. Instead we should try to wait for a new master to be elected (or the local node to be re-elected).

An example failure can be seen at http://build-us-00.elastic.co/job/es_core_master_suse/778/testReport/junit/org.elasticsearch.cluster/MinimumMasterNodesTests/multipleNodesShutdownNonMasterNodes/
</description><key id="85062948">11493</key><summary>ClusterHealth shouldn't fail with "unexpected failure" if master steps down while waiting for events</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T11:58:25Z</created><updated>2015-06-05T08:39:04Z</updated><resolved>2015-06-04T20:12:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-04T11:58:44Z" id="108859467">@s1monw we worked on this class together last time. Can you have a look?
</comment><comment author="s1monw" created="2015-06-04T15:07:10Z" id="108928044">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file></files><comments><comment>ClusterHealth shouldn't fail with "unexpected failure" if master steps down while waiting for events</comment></comments></commit></commits></item><item><title>Failure when adding the 'date' filter on logstash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11492</link><project id="" key="" /><description>Hey,

What i'm doing is loading TONS of events from S3 into ES (100s of millions) at once using logstash.

The problem is that when I add the filter:

```
  date {
    match =&gt; [ "server_time", "UNIX" ]
  }
```

Logstash fails with multiple warnings:

```
{:timestamp=&gt;"2015-06-03T19:59:30.118000+0000", :message=&gt;"retrying failed action with response code: 429", :level=&gt;:warn}
```

And then with errors:

```
{:timestamp=&gt;"2015-06-03T20:02:31.123900+0000", :message=&gt;"too many attempts at sending event. dropping: 2015-05-19T12:10:57.722Z %{host} ...}
```

When I remove the 'date' filter, it works great BUT I only get one index created (for today). I want to have multiple indices for the events' dates.

I found out that when I reduce the 'flush_size' of logstash things work fine but getting data into ES like this is extremely slow.

Another thing I found later is that when I set the following config to elasticsearch:

```
threadpool.search.type: fixed
threadpool.search.size: 20
threadpool.search.queue_size: 100

threadpool.bulk.type: fixed
threadpool.bulk.size: 60
threadpool.bulk.queue_size: 500

threadpool.index.type: fixed
threadpool.index.size: 20
threadpool.index.queue_size: 100

indices.memory.index_buffer_size: 30%
indices.memory.min_shard_index_buffer_size: 12mb
indices.memory.min_index_buffer_size: 96mb


indices.fielddata.cache.size: 15%
indices.fielddata.cache.expire: 6h
indices.cache.filter.size: 15%
indices.cache.filter.expire: 6h

index.refresh_interval: 30s
index.translog.flush_threshold_ops: 50000
```

I can raise the bulk size to 1000 and everything works but the machine gets stuck and things are REALLY slow.

Any help will be greatly appreciated ... :pray: 
</description><key id="84985981">11492</key><summary>Failure when adding the 'date' filter on logstash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">refaelos</reporter><labels /><created>2015-06-04T08:41:21Z</created><updated>2015-06-04T14:32:47Z</updated><resolved>2015-06-04T12:50:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T12:50:51Z" id="108883093">HI @refaelos 

I'm afraid I don't really understand what you're doing - other than mentioning the date filter, I have no idea what data you're sending to Elasticsearch, what your expected outcome is, or even what hardware you're running this on.  Really this feels like a logstash configuration issue, rather than a problem with Elasticsearch. 

I'd ask on the forums instead: https://discuss.elastic.co/c/logstash or https://discuss.elastic.co/c/elasticsearch

One thing for sure: you don't want to be setting the bulk threads this high - that's just going to put a lot of extra load on the system.  Setting thread pool config is a very expert option, you can easily break things if you set them incorrectly.
</comment><comment author="refaelos" created="2015-06-04T13:13:54Z" id="108893917">Thanks man!

I'll put the question on the forum and explain some more about the infrastructure and the data.
</comment><comment author="refaelos" created="2015-06-04T14:32:47Z" id="108916021">Posted it there: https://discuss.elastic.co/t/one-vs-multiple-indices-on-es-using-lss-date-filter

Will be great if you can help.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SameShardAllocationDecider should match on IP and hostname</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11491</link><project id="" key="" /><description>Updated logic whereby same nodes are determined. New logic requires both matching IPs _and_ matching hostnames.

Closes #11490
</description><key id="84980137">11491</key><summary>SameShardAllocationDecider should match on IP and hostname</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">umairmufti</reporter><labels><label>:Allocation</label><label>enhancement</label><label>feedback_needed</label><label>review</label></labels><created>2015-06-04T08:24:13Z</created><updated>2016-09-12T21:19:55Z</updated><resolved>2016-09-12T21:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="umairmufti" created="2015-06-04T16:25:58Z" id="108956947">I would argue that this is a bug fix rather than an enhancement.

To illustrate the point: we have a cluster with 5 nodes distributed among 5 hosts. The SameShardAllocationDecider incorrectly determines that each node is running on the same host because each node reports the same IP address. Each ES instance is running within its own network namespace with IP addresses assigned by Docker.
</comment><comment author="ywelsch" created="2016-03-11T14:56:10Z" id="195400059">@umairmufti sorry for getting so late back to you.

Requiring matching IPs **and** matching hostnames breaks other use cases. For example running multiple Elasticsearch instances on one host but binding each of them to a different network interface.

Maybe we can generalize `SameShardAllocationDecider` to make the conditions on which it checks "sameness" configurable (potentially allowing custom node attributes). What do you think?
</comment><comment author="dakrone" created="2016-04-06T21:00:59Z" id="206566012">ping @umairmufti, what do you think of @ywelsch's idea?
</comment><comment author="dakrone" created="2016-09-12T21:19:55Z" id="246497469">Closing as we didn't receive any feedback
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Bug] SameShardAllocationDecider does not correctly determine hosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11490</link><project id="" key="" /><description>The SameShardAllocationDecider attempts to prevent multiple instances of the same shard from being allocated to the same node. Nodes are identified by their IPs and hostnames. If _either_ the IPs match _or_ the hostnames match, the nodes are determined to be the same. 

The current logic is incorrect. At a minimum, _both_ hostnames _and_ IPs should match. Consider when ES is running inside of Docker and containers on different physical hosts receive the same IP address.
</description><key id="84972017">11490</key><summary>[Bug] SameShardAllocationDecider does not correctly determine hosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">umairmufti</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-04T08:01:18Z</created><updated>2017-05-05T17:04:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-05T09:12:36Z" id="109212971">If you are reporting this issue, I assume that you already set `cluster.routing.allocation.same_shard.host` to `true`, otherwise elasticsearch does not try to avoid allocating shards twice on the same node.

Then it's true that elasticsearch checks for either the same IP or host. If you want to have something better, I would recommend on setting attributes on each container and then using forced awareness to force elasticsearch to not allocate twice the same shard on containers that are hosted on the same machine. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#forced-awareness
</comment><comment author="clintongormley" created="2015-06-05T12:29:11Z" id="109279902">@jpountz Docker creates a private subnet, so two docker instances running on different hardware can indeed have the same IP address.  So the question for me is: would this change break the same_shard behaviour in other scenarios, eg with VMs?  I think it probably would as VMs on the same machine may well have different IPs.
</comment><comment author="umairmufti" created="2015-06-06T20:40:16Z" id="109646914">The proposed change is to bring the code in accordance with the documentation. Specifically, the documentation states that hostname _and_ IP address are checked to determine the host.

Moreover, the following cases exist:
1. Nodes running on the same host with different IP addresses and different hostnames
2. Nodes running on the same host with different IP addresses but same hostnames
3. Nodes running on the same host with same IP addresses but different hostnames
4. Nodes running on the same host with same IP addresses and same hostnames
5. Nodes running on the different hosts with different IP addresses and different hostnames
6. Nodes running on the different hosts with different IP addresses but same hostnames
7. Nodes running on the different hosts with same IP addresses but different hostnames
8. Nodes running on the different hosts with same IP addresses and same hostnames

Checking against only IP _or_ hostname produces a false negative in case 1; and, false positives in cases 6, 7, and 8.
Checking against both IP _and_ hostname produces a false negative in case 1; and, a false positive in case 8.

The cost of the false positives is to prevent shards from being relocated, thereby increasing the risk of data loss. In our case, it was exactly case 7 that caused all of our shards to end up on a single host as all 5 nodes were assigned the same IP (but different hostnames) by Docker.
</comment><comment author="umairmufti" created="2015-10-05T16:54:54Z" id="145597523">Any update on this issue?
</comment><comment author="dakrone" created="2015-12-18T17:55:26Z" id="165855260">&gt;  So the question for me is: would this change break the same_shard behaviour
&gt;  in other scenarios, eg with VMs? I think it probably would as VMs on the same
&gt;  machine may well have different IPs.

Wouldn't VMs have different host names though, so I don't think changing this
would break anything? (ie, VMs have different host names and different IPs, so a
user would still have to use allocation awareness for it)

@clintongormley I think we should make this change or at least make it configurable, do you have a preference?
</comment><comment author="ywelsch" created="2016-03-16T15:00:34Z" id="197370389">@dakrone Requiring matching IPs **and** matching hostnames breaks other use cases. For example running multiple Elasticsearch instances on one host but binding each of them to a different network interface.

Maybe we can generalize `SameShardAllocationDecider` to make the conditions on which it checks "sameness" configurable (potentially allowing custom node attributes). What do you think?
</comment><comment author="dakrone" created="2016-03-16T15:24:40Z" id="197381323">@ywelsch yeah, I think generalizing the conditions and potentially making them configurable (I don't think we should use custom node attributes, we already have an allocation decider for that) would be a good idea.
</comment><comment author="javanna" created="2017-05-05T16:05:14Z" id="299505923">@ywelsch @dakrone does anything need further discussion here? Or shall we label adoptme?</comment><comment author="ywelsch" created="2017-05-05T16:36:13Z" id="299513637">@javanna I've changed the labels</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix mvn `exec:exec` target to work.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11489</link><project id="" key="" /><description>Currently this will not work, as it doesn't configure the ES home.
We should fix this build target, or remove it if its not needed.
</description><key id="84927269">11489</key><summary>Fix mvn `exec:exec` target to work.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-06-04T05:43:28Z</created><updated>2015-08-10T21:58:04Z</updated><resolved>2015-08-10T21:58:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-04T05:56:10Z" id="108738127">Left a comment. Other than that it looks good to me.
</comment><comment author="dakrone" created="2015-06-08T19:54:18Z" id="110119951">LGTM to merge as-is, we can follow up to make it better/more-correct
</comment><comment author="jpountz" created="2015-07-08T17:25:43Z" id="119670338">@rmuir let's merge this PR?
</comment><comment author="rmuir" created="2015-07-08T17:27:58Z" id="119670800">I don't want to support this task at the moment. We have a hard enough time with supporting:
- bin/elasticsearch
- mvn test

Those are the only two ways we can currently support ES. I know there are a million other ways people _want_ to run it, but they are in a dreamland, because we have bigger issues like jar hell and plugins not working at all.

Once the basics are working, then we can look at the nice-to-haves, and consider their maintenance costs.
</comment><comment author="dakrone" created="2015-08-10T21:58:04Z" id="129626557">Closing this since we have `./run.sh`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[maven] reorganize the codebase (round 2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11488</link><project id="" key="" /><description>Continuation of #11470  (i pushed commits to my branch, and tested modifications)
</description><key id="84917635">11488</key><summary>[maven] reorganize the codebase (round 2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-06-04T05:12:14Z</created><updated>2015-06-05T11:11:09Z</updated><resolved>2015-06-05T11:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-04T15:10:12Z" id="108928750">LGTM

As an aside, I did run the migrate script and it looks like it's already found issues (awesome!):

```
[INFO] Reactor Summary:
[INFO] 
[INFO] Elasticsearch Build Resources ...................... SUCCESS [  0.409 s]
[INFO] Elasticsearch Parent POM ........................... SUCCESS [  1.543 s]
[INFO] Elasticsearch Core ................................. SUCCESS [ 13.067 s]
[INFO] Elasticsearch Plugin POM ........................... SUCCESS [ 10.420 s]
[INFO] Elasticsearch Japanese (kuromoji) Analysis plugin .. FAILURE [  6.381 s]
[INFO] Elasticsearch Smart Chinese Analysis plugin ........ SKIPPED
[INFO] Elasticsearch Stempel (Polish) Analysis plugin ..... SKIPPED
[INFO] Elasticsearch Phonetic Analysis plugin ............. SKIPPED
[INFO] Elasticsearch ICU Analysis plugin .................. SKIPPED
[INFO] Elasticsearch Mapper Attachment plugin ............. SKIPPED
[INFO] Elasticsearch Google Compute Engine cloud plugin ... SKIPPED
[INFO] Elasticsearch Azure cloud plugin ................... SKIPPED
[INFO] Elasticsearch AWS cloud plugin ..................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.166 s
[INFO] Finished at: 2015-06-04T09:07:45-06:00
[INFO] Final Memory: 67M/2216M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project elasticsearch-analysis-kuromoji: Compilation failure: Compilation failure:
[ERROR] /home/hinmanm/src/elasticsearch/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java:[28,39] error: cannot find symbol
[ERROR] symbol:   class ImmutableMap
[ERROR] location: package org.elasticsearch.common.collect
[ERROR] /home/hinmanm/src/elasticsearch/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/plugin/analysis/kuromoji/AnalysisKuromojiPlugin.java:[22,39] error: cannot find symbol
[ERROR] symbol:   class ImmutableList
[ERROR] location: package org.elasticsearch.common.collect
[ERROR] /home/hinmanm/src/elasticsearch/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java:[53,8] error: cannot find symbol
[ERROR] symbol:   class ImmutableMap
[ERROR] location: class JapaneseStopTokenFilterFactory
[ERROR] /home/hinmanm/src/elasticsearch/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/plugin/analysis/kuromoji/AnalysisKuromojiPlugin.java:[47,15] error: cannot find symbol
[ERROR] -&gt; [Help 1]
```

These can be fixed after this PR though.
</comment><comment author="rmuir" created="2015-06-04T15:13:22Z" id="108930168">@s1monw has been digging into this. we are concerned things are really working properly with shading.
</comment><comment author="s1monw" created="2015-06-04T20:49:04Z" id="109045360">rob and myself worked on this and fixed that build. We still have to work on preserving history a bit more but if you do fetch the branch and run

```
mvn clean test-compile -Delasticsearch.thirdparty.config=unshaded
```

it works as well as if you run 

```
mvn clean install -DskipTests
```

I guess this could help with the history http://gbayer.com/development/moving-files-from-one-git-repository-to-another-preserving-history/

the `-Delasticsearch.thirdparty.config=unshaded` is still needed as a temporary solution we have to fix that in the parent but can only do it shortly before of after we push this.
</comment><comment author="s1monw" created="2015-06-05T11:10:39Z" id="109259618">LGTM lets move here @rmuir ?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11488 from rmuir/pr/migration_script</comment></comments></commit></commits></item><item><title>Moving_avg model parser is too strict (should coerce numerics)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11487</link><project id="" key="" /><description>The settings parser for `moving_avg` models is too strict, and will throw an exception if it encounters a non-double (including coercible numerics).  Should be changed to accept any numeric.

E.g.

```
"movavg": {
  "moving_avg": {
    "buckets_path": "avg",
    "window": 12,
    "model": "holt_winters",
    "settings": {
      "alpha": 0.4992721,
      "beta": 0.1366293,
      "gamma": 1,
      "period": 4
    }
  }
}
```

```
{
  "error": {
    "root_cause": [
      {
        "type": "search_parse_exception",
        "reason": "Parameter [gamma] must be a double, type `Integer` provided instead"
      }
    ],
...
}
```
</description><key id="84876646">11487</key><summary>Moving_avg model parser is too strict (should coerce numerics)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T02:48:40Z</created><updated>2015-06-19T14:55:29Z</updated><resolved>2015-06-19T14:55:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Aggregations: moving_avg model parser should accept any numeric, not just doubles</comment></comments></commit></commits></item><item><title>Move in-progress snapshot and restore information from custom metadata to custom cluster state part</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11486</link><project id="" key="" /><description>Information about in-progress snapshot and restore processes is not really metadata and should be represented as a part of the cluster state similar to discovery nodes, routing table, and cluster blocks. Since in-progress snapshot and restore information is no longer part of metadata, this refactoring also enables us to handle cluster blocks in more consistent manner and allow creation of snapshots of a read-only cluster.

Closes #8102
</description><key id="84848107">11486</key><summary>Move in-progress snapshot and restore information from custom metadata to custom cluster state part</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-04T01:15:52Z</created><updated>2015-06-11T19:35:52Z</updated><resolved>2015-06-11T19:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-10T19:17:56Z" id="110883584">@dakrone I rebased and pushed the new version. Could you review when you have a chance?
</comment><comment author="dakrone" created="2015-06-10T20:20:50Z" id="110899557">Left a couple of really minor comments, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better validation of yaml </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11485</link><project id="" key="" /><description>Here is an example of an Elasticsearch yml.

```
action: 
  disable_delete_all_indices: "true"
bootstrap: 
  mlockall: "true"
cluster: 
  name: cluster_name
discovery: 
  zen: 
    ping: 
      - 
        multicast: 
          enabled: "false"
      - 
        unicast: 
          - host1
          - host2
          - host3
          - host4
          - host5
          - host6
          - host6
          - host8
gateway: 
  - 
    recover_after_time: 10m
  - 
    recover_after_nodes: 6
  - 
    expected_nodes: 8
http: 
  enabled: "true"
network: 
  host: "some_ip"
node: 
  name: some_name
processors: "24"
script: 
  disable_dynamic: "true"
threadpool: 
  bulk: 
    queue_size: "100"
```

It certainly doesn't look like a usual Elasticsearch yml file we can interpret, but it does pass yaml validation using http://www.yamllint.com/, http://yaml-online-parser.appspot.com/, etc..  And the user can probably tell from the ES log that it is not taking the entries because the cluster name will start up using the default elasticsearch.  But it will be nice to:
- Provide some indication in the log file that it is actually a yaml file that ES cannot parse or interpret
- And maybe additional details on why we cannot parse it.
</description><key id="84809707">11485</key><summary>Better validation of yaml </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>feedback_needed</label></labels><created>2015-06-03T23:05:10Z</created><updated>2015-06-05T08:57:15Z</updated><resolved>2015-06-05T08:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-04T12:35:09Z" id="108874410">I tried the above config and (except for the dummy host name etc) it worked just fine.  The cluster name was set correctly.  What makes you think that we can't parse it properly?
</comment><comment author="ppf2" created="2015-06-05T03:11:37Z" id="109150568">Ah!  I see what happened here (thx for trying :)).  I was testing various example ymls with different formatting, but also with different file names.  And didn't realize that if I just have 2 .yml files in the config directory, 1 for logging (logging.yml), and 1 for elasticsearch (except that the ES node yml is named something like not-working.yml), then it also will just load the defaults without warning.  

So you are right, the valid yml syntax within the file is actually working fine if I rename the example ymls back to "elasticsearch.yml".  It is just not loading it because the file itself is not called literally "elasticsearch.yml".   It will be nice though to write out a logging entry to log file indicating that no elasticsearch.yml is found and that it is going to start the node up with all default settings :)
</comment><comment author="clintongormley" created="2015-06-05T08:57:14Z" id="109207930">Closing in favour of #11510
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Show human readable Elasticsearch version that created index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11484</link><project id="" key="" /><description>Currently, you can see the Version ID of Elasticsearch that created an index by checking the index's settings:

```
$ curl -XGET localhost:9200/test/_settings?pretty\&amp;human
{
  "test" : {
    "settings" : {
      "index" : {
        "creation_date" : "1433353647548",
        ...
        "version" : {
          "created" : "1050299"
        },
        ...
      }
    }
  }
}
```

The value for `index.version.created` is not human readable. [It is exactly the value of `org.elasticsearch.Version.V_1_5_2_ID`](https://github.com/elastic/elasticsearch/blob/v1.5.2/src/main/java/org/elasticsearch/Version.java#L231). The value, `1050299`, is the same as `01050299`, [which maps to `XXYYZZAA` where](https://github.com/elastic/elasticsearch/blob/v1.5.2/src/main/java/org/elasticsearch/Version.java#L38):

```
// The logic for ID is (read from the right to avoid missing implied 0s):
// XXYYZZAA, where
// - XX is major version
// - YY is minor version
// - ZZ is revision,
// - AA is Beta/RC indicator
//   - values below 50 are beta builds
//   - values below 99 are RC builds
//   - exactly 99 indicates a release

XX=01
YY=05
ZZ=02
AA=99
```

It's great that we get a version, but it's not immediately readable. Moving into ES 2.0, when we enable doc values by default, it will be useful to be able to explain to users that indices created using prior to 2.0 will not have doc values by default, but those created afterward will have them. It will be helpful to have a way to see the version number akin to the `version.number` that appears when you hit the root endpoint:

```
$ curl -XGET localhost:9200/
{
  ...
  "version" : {
    "number" : "1.5.2",
    ...
    "lucene_version" : "4.10.4"
  },
  ...
}
```

... rather than having to check to see if it starts with a `2` _and_ it is 7 digits long (as opposed to 6 digits long, indicating `0.20`!).
</description><key id="84750889">11484</key><summary>Show human readable Elasticsearch version that created index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T20:11:51Z</created><updated>2015-06-17T23:05:25Z</updated><resolved>2015-06-17T23:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-06-03T20:15:32Z" id="108601836">:+1: 
</comment><comment author="colings86" created="2015-06-04T09:22:30Z" id="108802127">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/HumanReadableIndexSettingsTests.java</file></files><comments><comment>Show human readable Elasticsearch version that created index and date when index was created</comment></comments></commit></commits></item><item><title>Default to binding to loopback address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11483</link><project id="" key="" /><description>Binds to the address returned by `InetAddress.getLoopbackAddress()`.

Closes #11300
Closes #7077
</description><key id="84709654">11483</key><summary>Default to binding to loopback address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Network</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T18:36:03Z</created><updated>2015-06-04T20:20:46Z</updated><resolved>2015-06-04T16:32:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-03T18:36:33Z" id="108567378">cool +1
</comment><comment author="nik9000" created="2015-06-03T18:46:46Z" id="108571356">+1
</comment><comment author="drewr" created="2015-06-03T22:44:20Z" id="108634618">Tested :+1: 
</comment><comment author="dakrone" created="2015-06-03T23:15:27Z" id="108642721">Tested this on Linux, OSX, and Windows. All were able to form a cluster with the default settings using multicast.
</comment><comment author="imotov" created="2015-06-04T16:24:37Z" id="108956698">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for negative unix timestamps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11482</link><project id="" key="" /><description>This fixes an issue to allow for negative unix timestamps.
It also fixes the default date field mapper to support epochs.

Fixes #11478
Fixes #11692
</description><key id="84688226">11482</key><summary>Allow for negative unix timestamps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T17:50:00Z</created><updated>2015-08-13T13:55:23Z</updated><resolved>2015-06-22T09:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-03T17:50:42Z" id="108545293">One thing to discuss is, if it makes sense to check for the length of the epoch timestamps. Right now they are limited to 10/13 chars depending on millisecond resolution, which means the max date is 2286. Maybe we should just loosen this?
</comment><comment author="rmuir" created="2015-06-03T17:57:43Z" id="108547585">&gt; Right now they are limited to 10/13 chars depending on millisecond resolution, which means the max date is 2286. Maybe we should just loosen this?

If someone is using our 200 years from now, then they have larger problems.
</comment><comment author="spinscale" created="2015-06-03T18:25:29Z" id="108560691">I was more thinking about different worlds... like in games. however then one potentially should just use ISO8601 dates and not unix timestamps :)
</comment><comment author="nik9000" created="2015-06-03T18:30:26Z" id="108563626">&gt; just use ISO8601 dates

+1

Probably worth documenting the way it works if it isn't already but "just use ISO8601" is almost never bad advice. 
</comment><comment author="rmuir" created="2015-06-03T18:35:10Z" id="108566926">I actually disagree with that. ISO8601 is overly complex. 

RFC3339 is good enough and much simpler. Exotic use cases that don't work with it could be handled by a separate complicated ISO8601 plugin :)
</comment><comment author="spinscale" created="2015-06-03T20:22:42Z" id="108603775">agreed, lets replace iso8601 with rfc3339 in my sentence above... I think it's ok to keep the validity checks. Will update the docs as part of this PR as well
</comment><comment author="spinscale" created="2015-06-04T13:18:11Z" id="108895107">updated docs to reflect the ability to only go up/down to a certain year with a unix timestamp, also added bwc documentation
</comment><comment author="spinscale" created="2015-06-17T15:29:17Z" id="112846897">added an EpochPrinter here as well... @jpountz maybe you can have a look if that helps your case, I will check if I can optimize the code a bit more instead of reading out the fields piece by piece, maybe the timestamp already exist and I can just reuse it
</comment><comment author="jpountz" created="2015-06-22T08:14:23Z" id="114036095">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file></files><comments><comment>[TEST] Enable testcase since #11482 is merged</comment></comments></commit></commits></item><item><title>One single (global) way to register custom query parsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11481</link><project id="" key="" /><description>There are different ways to register custom query parsers through plugins, a couple of them work per index via index settings, which doesn't seem to be needed. There also three different ways to add a global custom query parser through either IndicesQueriesModule or IndicesQueriesRegistry. This commit consolidates the registration of custom query parsers via IndicesQueriesModule#addQuery(Class&lt;? extends QueryParser&gt;). The complexity of supporting parsers per index is not needed hence it got removed. Also the other ways of registering global custom parsers are dropped in favour of the one mentioned above by providing the class of the parser.

This PR breaks plugins that plug in their own custom queries. If the change looks good we can deprecate in 1.x the methods that we are removing here.
</description><key id="84681264">11481</key><summary>One single (global) way to register custom query parsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T17:37:19Z</created><updated>2015-06-10T16:22:09Z</updated><resolved>2015-06-08T10:33:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-04T11:14:06Z" id="108844814">@s1monw @kimchy can you have please have a look?
</comment><comment author="rmuir" created="2015-06-05T11:05:55Z" id="109259083">This looks great to me. Much simpler!
</comment><comment author="javanna" created="2015-06-05T13:07:36Z" id="109288650">thanks @rmuir for looking, I will push this soon if there are no objections (will just let the move to maven submodules settle a bit first)
</comment><comment author="spinscale" created="2015-06-08T09:10:54Z" id="109919977">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParserFactory.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/guice/IndexQueryParserModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/guice/MyJsonQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPlugin2Tests.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/PluginJsonQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java</file></files><comments><comment>Plugins: one single (global) way to register custom query parsers</comment></comments></commit></commits></item><item><title>IndicesStatsRequestBuilder is missing level parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11480</link><project id="" key="" /><description>The REST call for Indices stats provides a parameter for level, but the IndicesStatsRequestBuilder does not.

See also https://www.elastic.co/guide/en/elasticsearch/reference/1.5/indices-stats.html at the bottom. 

Quot: "The stats returned are aggregated on the index level, with primaries and total aggregations. In order to get back shard level stats, set the level parameter to shards"

and IndicesStatsRequestBuilder class https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java
</description><key id="84678924">11480</key><summary>IndicesStatsRequestBuilder is missing level parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcmoe</reporter><labels /><created>2015-06-03T17:33:06Z</created><updated>2015-06-04T09:22:31Z</updated><resolved>2015-06-04T09:22:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-04T09:22:30Z" id="108802124">The IndicesStatsResponse has methods (`getIndices()` and `getShards()`) which return the different levels objects so a parameter on the request in the Java API is not needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reduce shard inactivity timeout to 5m</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11479</link><project id="" key="" /><description>To better distribute the memory allocating to indexing, the IndexingMemoryController periodically checks the different shard for their last indexing activity. If no activity has happened for a while, the controller marks the shards as in active and allocated it's memory buffer budget (but a small minimal budget)  to other active shards.  The recently added synced flush feature (#11179, #11336) uses this inactivity trigger to attempt as  a trigger to attempt adding a sync id marker (which will speed up future recoveries).

We wait for 30m before declaring a shard inactive. However, these days the operation just requires a refresh and is light. We can be stricter (and 5m) increase the chance a synced flush will be triggered.
</description><key id="84663317">11479</key><summary>Reduce shard inactivity timeout to 5m</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T16:56:51Z</created><updated>2015-06-04T12:28:29Z</updated><resolved>2015-06-03T22:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-03T16:57:05Z" id="108523171">@s1monw can you have a look? 
</comment><comment author="s1monw" created="2015-06-03T18:04:17Z" id="108550229">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file></files><comments><comment>Reduce shard inactivity timeout to 5m</comment></comments></commit></commits></item><item><title>Date parsing: Ensure parsing negative unix timestamps works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11478</link><project id="" key="" /><description>A bug was introduced in the commit https://github.com/elastic/elasticsearch/commit/01e8eaf181148d1ae5239800b5d71bd38c0c2896 - negative unix timestamps are not parsed correctly.

Sample failure at http://build-us-00.elastic.co/job/es_core_master_metal/9694/

Relates #5328
</description><key id="84662768">11478</key><summary>Date parsing: Ensure parsing negative unix timestamps works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T16:55:36Z</created><updated>2015-06-22T09:58:54Z</updated><resolved>2015-06-22T09:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>Dates: Allow for negative unix timestamps</comment></comments></commit></commits></item><item><title>[doc] fix outdated java api examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11477</link><project id="" key="" /><description>Some API have been changed in master. Master documentation should reflect be updated:
- QueryBuilders.queryString is now QueryBuilders.queryStringQuery
- DateHistogram.Interval is now DateHistogramInterval
- Refactoring of buckets in aggs
- FilterBuilders has been replaced by QueryBuilders

Closes #9976.
</description><key id="84614328">11477</key><summary>[doc] fix outdated java api examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T15:11:46Z</created><updated>2015-07-02T12:40:19Z</updated><resolved>2015-06-16T07:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-04T13:59:02Z" id="108907697">LGTM
</comment><comment author="dpursehouse" created="2015-06-24T06:29:04Z" id="114745934">Note: There are still several references to `FilterBuilders`, which has been removed, in docs/java-api/query-dsl-filters.asciidoc
</comment><comment author="dadoonet" created="2015-06-24T12:53:47Z" id="114859598">Thank you @dpursehouse for raising it. I started revisiting the JAVA API documentation about this. Stay tuned :)
</comment><comment author="dadoonet" created="2015-07-01T20:38:53Z" id="117818515">@dpursehouse I pushed some changes in master branch for the documentation. Don't hesitate to raise other issues or sending PR if there are other things that should be fixed. Thanks!
</comment><comment author="dpursehouse" created="2015-07-02T12:40:19Z" id="118019419">Thanks @dadoonet.  If I find anything else I'll try to fix it myself.

BTW do you have any idea when there will be a release candidate for 2.0.0?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11477 from dadoonet/doc/update-java-api-master</comment></comments></commit></commits></item><item><title>errors in average aggregation of calculated field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11476</link><project id="" key="" /><description>We were playing around with scripted fields in kibana and took a certain field (page number) and just set the scripted field to the page number *2.  It seemed to work fine but then on close inspection we noticed that sometimes it is off.

Below shows the chart of the percentage - this should always be 66% but sometimes its not.

If we run the same aggregation using the sum instead of the average, it is correct.

I haven't tried this in pure elasticsearch but it doesn't seem it should be a problem in kibana.  It's just taking the results from ES and graphing them AFAIK.

![visualize_-_kibana_4](https://cloud.githubusercontent.com/assets/149253/7962177/b229d134-0a14-11e5-8fb5-0f684cc5b83e.png)

![visualize_-_kibana_4](https://cloud.githubusercontent.com/assets/149253/7962211/da79784c-0a14-11e5-89bd-56e9a2759441.png)
</description><key id="84590424">11476</key><summary>errors in average aggregation of calculated field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yehosef</reporter><labels><label>:Aggregations</label><label>bug</label><label>feedback_needed</label></labels><created>2015-06-03T14:22:10Z</created><updated>2015-06-14T15:27:56Z</updated><resolved>2015-06-14T14:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-05T09:19:29Z" id="109214654">hey there,

a couple of questions here:
- Does this also happen, when you dont run it on a scripted field?
- Can you share the queries, that kibana executes (kibana allows you to show the full query)?
- Most importantly, do you have the chance to provide us the data which causes this problem? Can you provide a snapshot if the data is not too sensitive or help us creating a dataset that reproduces the problem?
</comment><comment author="clintongormley" created="2015-06-05T11:21:35Z" id="109260991">Also wondering if some documents have multiple values in one field?
</comment><comment author="yehosef" created="2015-06-07T12:07:06Z" id="109745238">@spinscale 
1) - I don't know if it's happens on a non-scripted field - I don't have any fields that are always the same ratio of another field for comparison

2)  https://gist.github.com/yehosef/f96dc491bcd5ee9bf7d3 is the query
I also included the index template - perhaps it's related to doc_values.  

3) I'm not sure if I can share the data right now - if you contact me at my name at gmail, I can probably give you temp access to the marvel/kibana install. Because store is off for everything, you can't see the values but it might help in the debug.  If that doesn't help we can look at giving access to the data.

To give some background - this is a testing server to test load/capabilities for a server.  We have storing all fields to "off" and are using doc_values.  It's a single 16G machine with 8G heap - 220M rows, ~60G storage.
</comment><comment author="yehosef" created="2015-06-07T13:25:14Z" id="109756246">@clintongormley  - I'm pretty sure that it doesn't.  We're importing Mixpanel data and you don't have arrays in that data set.  We have found chunks of our import data that was truncated - but I think that would result in broken json and just wouldn't import those lines.
</comment><comment author="clintongormley" created="2015-06-12T13:54:28Z" id="111500581">@yehosef broken JSON can well end up with bad results.  Our JSON parser is too lenient (fixed in 2.0) so only part of the JSON document might have been indexed.  It'd be worth comparing doc counts for the buckets with funny averages with buckets with good averages.
</comment><comment author="yehosef" created="2015-06-13T19:01:11Z" id="111740687">It's hard for me to think that's the problem. There would have been one broken record per day (the last one) and even if it's broken how could that affect a computed field?  Whatever bad value there was would have been doubled.  And how would it only affect avg but not sum?  

But I'll investigated further and check with other data sources. 
</comment><comment author="clintongormley" created="2015-06-14T14:21:13Z" id="111830289">OK. I've indexed 100,000 documents with a timestamp and a number, and repeated your experiment.  The avg and sum lines are exactly equal. Then I added 10,000 documents with just a timestamp.  The sum line remains flat and the avg line shows differences.  So the difference that you are seeing is a result of your broken JSON, where some documents are missing values for this field (but the doc count is still taken into account when calculating the average).

This is not a bug.
</comment><comment author="yehosef" created="2015-06-14T15:27:56Z" id="111841185">thanks for looking into this - interesting.    
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use the smallest version rather than the default version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11475</link><project id="" key="" /><description>The minimum version comparison was always using the default version
sicne the comparison was flipped.

Closes #11474
</description><key id="84573583">11475</key><summary>Use the smallest version rather than the default version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Upgrade API</label><label>bug</label><label>v1.6.0</label></labels><created>2015-06-03T13:47:19Z</created><updated>2015-06-04T12:23:32Z</updated><resolved>2015-06-03T18:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-03T13:58:11Z" id="108433865">@rjernst @imotov can you review?
</comment><comment author="rmuir" created="2015-06-03T14:08:33Z" id="108441613">I added some questions. One bigger question is: I don't like the hoops we are jumping through here. I also don't like the hoops lucene jumps through just to deliver the correct exception. Maybe lucene should store the minimum version inside the segments_N when we commit? If its an empty commit, its still populated with that version. This might simplify lucene itself on reading commits and make stuff like this easier for everyone.
</comment><comment author="s1monw" created="2015-06-03T14:08:56Z" id="108441832">@rmuir I mixed something up you are right the test assertion didn't make sense
</comment><comment author="rjernst" created="2015-06-03T14:10:01Z" id="108442444">+1
</comment><comment author="rjernst" created="2015-06-03T14:10:52Z" id="108443065">Also +1 to storing min version in segments_N @rmuir 
</comment><comment author="mikemccand" created="2015-06-03T14:24:01Z" id="108451376">+1 to have Lucene track this in segments_N
</comment><comment author="s1monw" created="2015-06-03T14:28:08Z" id="108453813">I added more are asserts, I think the empty index is a different issue and should be solved elsewhere it's pretty tricky to get the version the segmetns_N file was written. I think we are good to go
</comment><comment author="imotov" created="2015-06-03T14:40:07Z" id="108461345">If this is an empty index, maybe we can just return `org.apache.lucene.util.Version.LUCENE_4_0_0` in 1.x and `org.apache.lucene.util.Version.LUCENE_5_0_0` in master since it doesn't have any segments anyway, so it's compatible with that, and if we will index any records afterwards, this is the version that we are going to get. Otherwise LGTM.
</comment><comment author="s1monw" created="2015-06-03T18:03:16Z" id="108549824">&gt; If this is an empty index, maybe we can just return org.apache.lucene.util.Version.LUCENE_4_0_0 in 1.x and org.apache.lucene.util.Version.LUCENE_5_0_0 in master since it doesn't have any segments anyway, so it's compatible with that, and if we will index any records afterwards, this is the version that we are going to get. Otherwise LGTM.

the problem is that if we try to open an empty index with a version taht doesn't support this segments_N format we get an exception we can't recover from so somehow we need to be able to upgrade it
</comment><comment author="rmuir" created="2015-06-03T18:32:20Z" id="108565401">+1 Lets spin off the empty index separately into another PR? We can leave a TODO for that. I still feel strongly it should be fixed, but this moves things forward.
</comment><comment author="imotov" created="2015-06-04T05:39:27Z" id="108733653">@s1monw we flush before running optimize and it overwrites the segments_N file even if there are no segments. So, shouldn't this make such index upgraded?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minimum compatible version being written as 3.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11474</link><project id="" key="" /><description>I've run the upgrade API on indices created in 0.20, and it sets the minimum compatible version to 3.6.0, which means that ES 2.0 fails to open them.
</description><key id="84557017">11474</key><summary>Minimum compatible version being written as 3.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Upgrade API</label><label>blocker</label><label>bug</label><label>v1.6.0</label></labels><created>2015-06-03T13:13:12Z</created><updated>2015-06-03T18:58:32Z</updated><resolved>2015-06-03T18:58:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeReallyOldIndexTest.java</file></files><comments><comment>Use the smallest version rather than the default version</comment></comments></commit></commits></item><item><title>Allow aggregations_binary to build and parse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11473</link><project id="" key="" /><description>Previously AggregationBuilder would wrap binary_aggregations in an aggregations object which would break parsing. This has been fixed so that for normally specified aggregations there are wrapped in an `aggregations` object, for binary aggregation which have the same XContentType as the builder it will use an `aggregations` field name and use the aggregationsBinary as the value (this will render the same as normal aggregations), and for binary aggregation with a different ContentType from the builder we use an `aggregations_binary` field name and add the aggregationsBinary as a binary value.

Additionally the logic in AggregationParsers needed to be changed as it previously did not parse `aggregations_binary` fields in sub-aggregations. A check has been added for the `aggregations_binary` field name and the binaryValue of this field is used to create a new parser and create the correct AggregatorFactories.

Close #11457
</description><key id="84550586">11473</key><summary>Allow aggregations_binary to build and parse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T12:58:03Z</created><updated>2015-06-07T17:44:49Z</updated><resolved>2015-06-04T11:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-04T08:28:15Z" id="108783754">LGTM

Maybe we should stop supporting this aggregations_binary thing. I assume it was added because facets had it too but if we are only finding about this bug now, it probably does not have enough usage to be worth supporting?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix possible BWC break after upgrading from pre 1.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11472</link><project id="" key="" /><description>This is happening because of #4074 when we required that the top-level "query" is present to delete-by-query requests, but prior to that we required that it is not present. So the translog has a DBQ without "query" and when we try to parse it we hit this exception.

This commit adds special handling for pre 1.0.0 indices if we hit parse exception, we
try to reparse without a top-level query object to be BWC compatible for these indices.

Closes #10262
</description><key id="84548027">11472</key><summary>Fix possible BWC break after upgrading from pre 1.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-06-03T12:52:20Z</created><updated>2015-06-03T18:38:08Z</updated><resolved>2015-06-03T18:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-03T12:58:33Z" id="108391353">LGTM, thanks @s1monw!  Nice test :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Added explanation of when to use the upgrade API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11471</link><project id="" key="" /><description>Closes #9779
</description><key id="84511088">11471</key><summary>Docs: Added explanation of when to use the upgrade API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T11:37:53Z</created><updated>2015-06-10T15:35:07Z</updated><resolved>2015-06-05T15:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-03T11:38:25Z" id="108323261">@rjernst @imotov could you review this please?
</comment><comment author="clintongormley" created="2015-06-05T15:53:11Z" id="109339137">thanks for the reviews @nik9000 and @imotov. I've fixed and pushed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11471 from clintongormley/upgrade_api_docs</comment></comments></commit></commits></item><item><title>[maven] reorganize the codebase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11470</link><project id="" key="" /><description>NOTE: this is a "different" kind of pull request. It is for reviewing @dadoonet shell script that we will execute against master, which will reorganize the codebase.

This commit adds a shell script which:
- move current elasticsearch core source in `/core`
- fetch `elasticsearch-parent` project in `/`
- fetch plugins in `/plugins`
- change `groupId` for all plugins to `org.elasticsearch.plugin` so versions won't conflict in maven central
- remove plugins/name/dev-tools dir which is not needed anymore
- remove plugins/name/.git dir
- remove plugins/name/LICENSE and plugins/name/CONTRIBUTING files
- clean `core/pom.xml` of useless settings that are inherited from parent project.
- `core/pom.xml` is adapted to change location of rest tests definition (`../`)
- change core name to `Elasticsearch Core`
- remove `plugins` dir from `.gitignore`

Plugins added:
- Analysis
  - analysis-kuromoji
  - analysis-smartcn
  - analysis-stempel
  - analysis-phonetic
  - analysis-icu
- Mapper
  - mapper-attachments
- Language
  - lang-python
  - lang-mvel
  - lang-javascript
- Cloud
  - cloud-gce
  - cloud-azure
  - cloud-aws

River plugins are ignored but might be added if we want to.

Todo:
- check and adapt our release tool. It now has to upload all submodules as well.
- adapt Jenkins jobs
</description><key id="84508141">11470</key><summary>[maven] reorganize the codebase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-06-03T11:31:02Z</created><updated>2015-06-10T09:35:11Z</updated><resolved>2015-06-04T05:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-03T11:39:48Z" id="108324418">May I say that it looks good to me! ;)

But I'd love other feedbacks! :p 
</comment><comment author="rmuir" created="2015-06-03T11:45:25Z" id="108329040">I gave this PR a more fitting title :)
</comment><comment author="dakrone" created="2015-06-03T15:30:00Z" id="108485159">Left some comments but overall this looks pretty good, exciting to get this in!
</comment><comment author="rmuir" created="2015-06-04T05:13:21Z" id="108727596">I applied the feedback here to #11488

Closing this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cant create XContentParser from source field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11469</link><project id="" key="" /><description>I'm currently create a CustomQuery, i need to parse _source field to get some specific field. But it some time throw ElasticsearchParseException

``` java
BytesArray bytes = new BytesArray(doc.getField("_source").binaryValue());
XContentParser parser = XContentFactory.xContent(bytes).createParser(bytes);
```

exception

``` java
org.elasticsearch.ElasticsearchParseException: Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@d94ffdcd
    at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:259)
```
</description><key id="84498688">11469</key><summary>Cant create XContentParser from source field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CaoManhDat</reporter><labels /><created>2015-06-03T11:10:40Z</created><updated>2015-06-03T23:48:36Z</updated><resolved>2015-06-03T11:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-03T11:53:52Z" id="108335818">Hi @CaoManhDat 

I suggest asking this question in the forum https://discuss.elastic.co/c/elasticsearch which is the place to ask questions.  We use github issue for bug reports and feature requests

thanks
</comment><comment author="CaoManhDat" created="2015-06-03T23:48:36Z" id="108648998">Hi @clintongormley 
Thanks for your suggestion. I already posted a topic about this issue in elasticsearch forum. But i think this kinda a issue of XContentFactory.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup DeleteByQuery code from IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11468</link><project id="" key="" /><description /><key id="84493958">11468</key><summary>Cleanup DeleteByQuery code from IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-06-03T10:57:14Z</created><updated>2015-06-03T18:05:35Z</updated><resolved>2015-06-03T18:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-03T10:57:34Z" id="108297239">@mikemccand quick review?
</comment><comment author="dakrone" created="2015-06-03T15:18:35Z" id="108479979">LGTM
</comment><comment author="mikemccand" created="2015-06-03T16:07:36Z" id="108502598">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file></files><comments><comment>Merge pull request #11468 from s1monw/cleanup_delete_by_query</comment></comments></commit></commits></item><item><title>TransportClient issues with nested calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11467</link><project id="" key="" /><description>This is a similar issue to #10766 but we've attached a test case that demonstrates it happening.

In our case we are running a search in ES and then caching the results of the search back into ES. As in the referenced case we are indexing the cached result in the ActionListener for the search results.

We've put together a very simple test case to replicate this. It's fine for a single run, but increasing the number of runs results in problems.

Under ES 1.4.5 with 10 parallel runs it seems to just hang altogether in most cases. Under ES 1.5.2 the test completes in about 40 secs but we have one or more "transport stopped" exceptions being thrown in the failure handler. In each case ES is running as a vanilla install on the same machine as the tests are running.

https://gist.github.com/frstie/75c04d85d93fff8a0738

We're not sure whether these results are expected - would be great to hear any suggestions or feedback!
</description><key id="84475367">11467</key><summary>TransportClient issues with nested calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frstie</reporter><labels><label>:Network</label></labels><created>2015-06-03T10:06:03Z</created><updated>2015-06-04T12:24:03Z</updated><resolved>2015-06-04T12:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-03T11:46:32Z" id="108329887">Related to #10766, #10704, #10644 

@spinscale any thoughts here?
</comment><comment author="spinscale" created="2015-06-03T14:09:46Z" id="108442296">@frstie can you retry running the same test, but ensure that the you do not use the calling thread for the listener by changing your search request builder call to

``` java
SearchRequestBuilder searchRequest = internalCluster().transportClient().prepareSearch().setListenerThreaded(true);
```
</comment><comment author="frstie" created="2015-06-03T14:25:48Z" id="108452503">Hi @spinscale  - adding that in sorts out the problem nicely. Still get a handful of "transport stopped" exceptions when ~ 50 concurrent requests but that's probably just a capacity issue.

Thinking about it this makes sense in our case, particularly since we are using vertx and want to make sure there is minimum blocking on the calling thread.

Are there any cases when we explicitly _wouldn't_ want to use setListenerThreaded() since it seems to me that it would be a sensible default for our use cases?
</comment><comment author="frstie" created="2015-06-03T14:29:28Z" id="108454625">I (and my colleague) also read it the wrong way around when reading the javadoc about setListenerThreaded() - I think we assumed it would be using a different thread by default and setting the property would have the opposite effect. 
</comment><comment author="spinscale" created="2015-06-03T14:53:48Z" id="108468178">Just a word of warning: Setting this property means, that the action listener is run in the `listener` threadpool. By default this threadpool is `fixed` and does not have a queue, see [threadpool docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html). The assumption is still that your task is kinda lightweight, so you shouldnt block in that threadpool either (which is simple in your example as you wait for the future instead of applying another listener).

Elasticsearch 2.0 will solve this by indeed changing the default for the transportclient to do the right thing, see https://github.com/elastic/elasticsearch/pull/10940
</comment><comment author="frstie" created="2015-06-03T18:59:48Z" id="108576688">Thanks. That all makes sense. Hopefully this will help other people with the same issue. Feel free to close the case if you like.
</comment><comment author="clintongormley" created="2015-06-04T12:24:02Z" id="108871092">thanks @frstie, closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can not make requests via netcat/nc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11466</link><project id="" key="" /><description>Although not particularly useful for many folk, this one had me scratching my head for a while, this does as expected:

```
# echo -ne 'GET / HTTP/1.0\r\n\r\n' | nc localhost 9200 | head -1
HTTP/1.0 200 OK
```

While this returns nothing:

```
# echo -ne 'GET /_search HTTP/1.0\r\n\r\n' | nc localhost 9200 | head -1
```

I believe this is down to Netty 3.x not supporting tcp half-closed sockets which is what is happening here (nectat closes the sender half of the socket as soon as it's done reading stdin).  Incidentally, it's strange that the first request works as expected, but I assume the response is quick enough that it is received before the sender half is shutdown.

By the looks of it Netty 4.0 has support for half-closed sockets, in the meanwhile a rather clumsy workaround is to introduce a small delay before netcat finishes with stdin:

```
(echo -ne 'GET /_search HTTP/1.0\r\n\r\n'; sleep 1) | nc localhost 9200 | head -1
HTTP/1.0 200 OK
```

Does this seem a reasonable explanation?
</description><key id="84374540">11466</key><summary>Can not make requests via netcat/nc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heathtechnical</reporter><labels><label>:Network</label></labels><created>2015-06-03T05:40:09Z</created><updated>2015-06-03T10:00:04Z</updated><resolved>2015-06-03T10:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-03T09:43:17Z" id="108273503">@spinscale any ideas?
</comment><comment author="spinscale" created="2015-06-03T09:54:03Z" id="108281379">Correct explanation. See the [netty 4.0 changes](http://netty.io/wiki/new-and-noteworthy-in-4.0.html#wiki-h3-23) and their mention of half-closed sockets. Not supported under netty 3.

A workaround might be to use the `-i` interval of netcat
</comment><comment author="clintongormley" created="2015-06-03T10:00:03Z" id="108283929">thanks @spinscale. I don't think there is anything we need to change here, so I'll close this issue.  @heathtechnical feel free to reopen if i've missed something
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix bug where moving_avg prediction keys are appended to previous prediction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11465</link><project id="" key="" /><description>Flow is now:
1. Iterate over buckets, generating movavg values if possible
2. If predictions are requested, a hashmap of keys is maintained for lookup later
3. Track the last valid key (e.g. last key that had data)
4. When a prediction is needed, we check in the map to see if the key is present.
   1. If the key is present, we create a new bucket with the old aggs + new prediction and overwrite the existing bucket
   2. If the key was not found, we simply generate a brand new bucket with the prediction
5. Predictions are now appended to the end of the "valid data", rather than the end of the requested range.  This makes more sense anyway, and works nicely with the gap policies.

The map is needed to avoid doing constant binary searches over the list of buckets.  We may need to add an `execution_mode` that allows a bin-search in the future, in case the user is executing massive moving avgs and would prefer to trade CPU for memory.

The solution feels a little janky...happy to entertain a different solution.  If this looks reasonable, I'll work up some more test cases (movavgs with different predictions, 3+ movavgs, etc)

Fixes #11454
</description><key id="84325075">11465</key><summary>Fix bug where moving_avg prediction keys are appended to previous prediction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-03T02:52:50Z</created><updated>2015-06-04T16:09:17Z</updated><resolved>2015-06-04T15:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-03T08:42:04Z" id="108248658">@polyfractal given that we know the keys are always in the correct order in the list, can we not just store the position of the last bucket which had a value? Then when we predict we just start at that position and iterate through the remaining buckets predicting values until we run out of existing buckets and have to start creating new ones.
</comment><comment author="polyfractal" created="2015-06-04T13:25:52Z" id="108898291">@colings86 Updated, cleaner now :)
</comment><comment author="colings86" created="2015-06-04T14:24:15Z" id="108913296">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file></files><comments><comment>Merge pull request #11465 from polyfractal/bugfix/movavg_double_predict</comment></comments></commit></commits></item><item><title>Search `preference` based on node specification </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11464</link><project id="" key="" /><description>Allow node selector api's with new preference
ONLY_NODE_SPEC ( selector apis like https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html)

https://github.com/elastic/elasticsearch/issues/5925  
</description><key id="84244748">11464</key><summary>Search `preference` based on node specification </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nirmalc</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T22:40:57Z</created><updated>2015-07-14T13:53:40Z</updated><resolved>2015-06-17T10:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-11T09:53:03Z" id="111071344">hey, thanks so much for this PR. I left some review comments but nothing major. Can you bring this PR up to date with master and fix my comments, I'd be happy to pull this in!!
</comment><comment author="nirmalc" created="2015-06-15T19:53:56Z" id="112187825">rebased with master + updates based on code review comments.
@s1monw  , I've made all changes except the name _only_node_spec to _only_node_attributes. Based on behavior , i think since it also supports node name , IP etc - just node_attributes can be confusing. I think _only_nodes is simple and covers it all . Please let me know if you still think _only_node_attributes is better choice, I will make the change.
</comment><comment author="s1monw" created="2015-06-16T15:51:23Z" id="112477328">I left some minor comments, I like `_only_nodes` @clintongormley WDYT?  I also need you to sign the CLA, can you go ahead and sign it?
</comment><comment author="nirmalc" created="2015-06-16T16:51:20Z" id="112495386">fixed javadoc per coments - Ive already signed CLA for my past contributions ; but so wierd - it asked to sign CLA again today - anyway now it looks alright. Thanks for your reviews !
</comment><comment author="s1monw" created="2015-06-16T18:15:07Z" id="112518552">@nirmalc yeah shows CLA signed to me now too... ok I will give @clintongormley some time to react and pull this in tomorrow.
</comment><comment author="clintongormley" created="2015-06-17T09:54:17Z" id="112741815">I'm ok with `_only_node_spec`. We refer to it in the docs as node spec, and it's a lot easier to spell :)
</comment><comment author="s1monw" created="2015-06-17T09:56:06Z" id="112742105">@clintongormley it's `_only_nodes`
</comment><comment author="clintongormley" created="2015-06-17T09:58:21Z" id="112742503">oops - didn't see that.  I'm good with `_only_nodes`
</comment><comment author="s1monw" created="2015-06-17T10:34:10Z" id="112749058">merged - thanks... I will backport in a bit
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchAllocationTestCase.java</file></files><comments><comment>Merge pull request #11464 from nirmalc/nodes-preference</comment></comments></commit></commits></item><item><title>Can't create repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11463</link><project id="" key="" /><description>when I try to create a new repository for backups doing:

```
sudo curl -XPUT 'http://localhost:9200/_snapshot/es_backup' -d '{ "type": "fs", "settings": { "location": "/home/user/ES/", "compress": true } }'
```

after (and also without it) creating the folder and giving it 777 permissions I get this error:

&gt; {"error":"RepositoryException[[es_backup] failed to create repository]; nested: CreationException[Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [/home/user/ES]\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)\n  while locating org.elasticsearch.repositories.fs.FsRepository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error]; nested: BlobStoreException[Failed to create directory at [/home/user/ES]]; ","status":500}

this is my cluster info:

&gt; curl -XGET 'http://localhost:9200'
&gt; {
&gt;   "status" : 200,
&gt;   "name" : "ES Hele test",
&gt;   "cluster_name" : "es_test",
&gt;   "version" : {
&gt;     "number" : "1.5.2",
&gt;     "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
&gt;     "build_timestamp" : "2015-04-27T09:21:06Z",
&gt;     "build_snapshot" : false,
&gt;     "lucene_version" : "4.10.4"
&gt;   },
&gt;   "tagline" : "You Know, for Search"
&gt; }
</description><key id="84226811">11463</key><summary>Can't create repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carlosvega</reporter><labels /><created>2015-06-02T21:47:03Z</created><updated>2015-06-03T02:24:39Z</updated><resolved>2015-06-03T02:24:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-03T02:10:17Z" id="108161774">@carlosvega That looks like a permission issue. What are permissions of /home/user? Can you do something like 

```
sudo -u elasticsearch touch /home/user/ES/test
```

where `elasticsearch` is the user name that you run elasticsearch under?
</comment><comment author="carlosvega" created="2015-06-03T02:18:01Z" id="108163139">Mmm it's weird, I created the folder in /backup_es and now I'm able to do it.
Thanks !
</comment><comment author="imotov" created="2015-06-03T02:24:38Z" id="108163917">The most likely reason for the original command to fail is that elasticsearch user wasn't able to list `/home/user` directory. I am going to close this issue since it doesn't look like a bug and we are trying to use github issue for bug reports and feature requests. If you have any additional questions about elasticsearch functionality or setup, please feel free to join us on https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rename TokenFilters to Transformers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11462</link><project id="" key="" /><description>Or anything else. This is a larger discussion (if there's a more appropriate place to have it, I'm happy to start it there). 

TokenFilters don't convey anything to me and they sound too similar to Tokenizers. And they don't filter search results (which is what I'm thinking of when I think filtering). 

From what I understand, TokenFilters are used to _transform_ tokens. So Transformer sounds like a better name. 
</description><key id="84185665">11462</key><summary>rename TokenFilters to Transformers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehulkar</reporter><labels /><created>2015-06-02T19:59:35Z</created><updated>2015-06-03T09:29:38Z</updated><resolved>2015-06-03T09:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-02T20:18:54Z" id="108085581">I dont think Transformers is a compelling enough name to go thru the process of renaming TokenFilters. They have had this name for over 15 years! This is definitely the first time i've ever heard anyone complain about the name. Despite growing up in the 1980's, "Transformers" makes me think of shitty movies.
</comment><comment author="clintongormley" created="2015-06-03T09:29:38Z" id="108265449">I've got to agree with @rmuir here.  Tokenizers convert strings into a stream of tokens.  Each token filter is then applied to the token stream in turn, adding, removing, or changing tokens in the stream.

There is an overlap between tokenizers and token filters (eg ngram tokenizer vs ngram token filter), so giving them completely different names would impede understanding instead of helping it. 

The token filters name also fits nicely with the character filters name (they filter characters before tokenizing).

Nobody understands the analysis process until they read through the documentation to understand what happens in each part of the process, so I'm comfortable with keeping the current names and just relying on the explanations that we provide.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch 1.5.2 error with java out of memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11461</link><project id="" key="" /><description>[2015-06-03 00:12:06,610][DEBUG][action.search.type       ] [log02] All shards failed for phase: [query]
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:714)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1357)
        at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:79)
        at org.elasticsearch.search.action.SearchServiceTransportAction.execute(SearchServiceTransportAction.java:551)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:228)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:83)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:176)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:158)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:62)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:52)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:100)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:98)
        at org.elasticsearch.client.FilterClient.execute(FilterClient.java:66)
        at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.execute(BaseRestHandler.java:92)
        at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:334)
        at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:81)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:53)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:225)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:170)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:329)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

The machine with 16 CPU, 32GB RAM, HEAP SIZE 16GB and got error when starting elasticsearch
</description><key id="84113806">11461</key><summary>elasticsearch 1.5.2 error with java out of memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jindov</reporter><labels><label>feedback_needed</label></labels><created>2015-06-02T17:15:22Z</created><updated>2015-06-03T09:38:55Z</updated><resolved>2015-06-03T09:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T17:21:08Z" id="108022350">Could you give us a hint as to what query you were running which triggered this OOM? eg do you have very high `from` or `size` parameters? Or are you requesting huge numbers of buckets in an aggregation?
</comment><comment author="imotov" created="2015-06-03T02:18:30Z" id="108163265">The error is `unable to create new native thread`. So thread pool stats would be useful as well.
</comment><comment author="jindov" created="2015-06-03T02:37:37Z" id="108166283">I started with mongo-river to connecting to mongodb, a number of document about:
[2015-06-03 09:27:28,934][INFO ][org.elasticsearch.river.mongodb.Slurper] Number of documents indexed in initial import of mydb.videos_ver2: 25805
i think it's not too much.
and the settings of ES bulk size: 

threadpool.bulk.queue_size: 10000
threadpool.bulk.type: fixed
threadpool.bulk.size: 6000
</comment><comment author="imotov" created="2015-06-03T04:00:27Z" id="108183144">@jindov indeed it's not too much in terms of number of records. The problem is that you are trying to index these records on 6000 threads, which is rather excessive. 
</comment><comment author="clintongormley" created="2015-06-03T09:38:54Z" id="108269474">This looks like a configuration issue.  The best place to discuss usage problems is in the forum: https://discuss.elastic.co/c/elasticsearch

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Conflation Aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11460</link><project id="" key="" /><description># Overview

GeoDistance search achieves the most basic spatial search use-case for "Find all points of interest within R units of  known location X". This is great when the starting location is already known (e.g., cell phone GPS, travelling location of interest) but doesn't work for conflation use-cases such as: "Find all CVS Pharmacies that are within 1 mile of a Walgreens pharmacy? Or all Home Depot's that are within 2 blocks of a Lowes?"  (After all, competition is essential to a free enterprise economy).  The current search toolbox (query, filter, aggregations, reducer, percolator) doesn't lend itself well to these types of queries without writing a bit of complex scripting.

This feature will add - what we'll temporarily call - a ConflationAggregator. The purpose of this aggregator is to achieve the above use case. A primary aggregator defines the primary list of buckets (e.g., Home Depots). A list of secondary &lt;filter, aggregator, query&gt; defines the operation or post-filter to perform using the documents in the result set of the primary.
# Conflation Aggregation Structure

Below is an initial cut at the colocation grammar. There can be one-to-many secondary filters to achieve multi-conflation queries, such as: "Find all Home Depots within 10 miles of a Lowes or Ace Hardware".  The design is intended to be flexible enough to avoid limiting this aggregation to geo only queries.

``` javascript
"aggs" : {
  "&lt;aggregation_name"&gt; : {
    "conflation" : {
      "primary" : {
        "&lt;filter&gt; | &lt;parent aggregation&gt;"
      },
      "secondary" : [
        "&lt;secondary_name&gt;" : {
          "&lt;filter&gt; | &lt;child aggregation&gt;"
        }
      ]
    } 
  }
}
```
# Example
## Query

This is an initial rough idea on how to use the conflation aggregator to achieve a complex geodistance query like:  "Find all Home Depots that are within 10 miles of a Lowes"

``` javascript
{
  "query": {"match_all": {}}, 
  "aggs": {
    "HomeDepots" : {
      "conflation" : {
        "primary" : {
          "filter" : {
            "bool" : {
              "must" : [
                {"term" : {"name" : "Home Depot"}},
                {"term" : {"businessType": "Home Improvement"}}
              ]
            }
          }
        },
        "secondary": [
          "Lowes": {
            "filter": {
              "and" : {
                "filters" : [
                  "term" : {"name" : "Lowes"},
                  "geo_distance": {
                    "field": "location",
                    "origin_field": "HomeDepots.location",
                    "ranges": [
                      {"from" : 0, "to" : 10}
                    ]
                  }
                ]
              } 
            }
          }
        ]
      }
    }
  }
}
```
## Result

``` javascript

"aggregations": {
  "HomeDepots": {
     "buckets": [
        {
           "key" : "Home Depot",
           "id" : 1034,
           "doc_count": 2
        },
        {
           "key" : "Home Depot",
           "id" : 3432,
           "doc_count": 2
        },
        {
           "key" : "Home Depot",
           "id" : 5644,
           "doc_count": 1
        },
        {
           "key" : "Home Depot",
           "id" : 8999,
           "doc_count": 1
        },
        {
           "key" : "Home Depot",
           "id" : 10232,
           "doc_count": 2
        }
     ]
  }
}
```

This issue is open for discussion around use-cases (non-geo), design, naming, etc.
</description><key id="84091961">11460</key><summary>Conflation Aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>discuss</label><label>feature</label><label>high hanging fruit</label></labels><created>2015-06-02T16:20:20Z</created><updated>2016-01-18T20:01:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T16:22:57Z" id="108005568">Hi @nknize 

This sounds exciting, but I don't see how it can work in a distributed environment in a scalable way?  Surely all nodes need to share all location data to get accurate results?
</comment><comment author="nknize" created="2015-06-02T16:30:15Z" id="108007116">@clintongormley performance is certainly the primary concern. In a way this is a multi (potentially exponential) query wrapped in aggregator clothing so it would need to be limited to small use cases. That may make the feature DOA? I'm not sure. That's why I'm super interested in soliciting feedback from everyone on ideas for providing easy-to-use tools for accomplishing these types of common use-cases.
</comment><comment author="markharwood" created="2015-06-02T16:39:43Z" id="108009278">Presumably in this "find an X near a Y" we are starting with the assumption that X and Y are both different elasticsearch documents?
As Clinton points out the initial problem is that the various X and Y docs may be scattered across different shards.
It feels like a two step op - 
1) query the X docs and use a geohashgrid agg or similar to build a geo-mask
2) query the Y docs and use the mask from 1 as a filter
</comment><comment author="clintongormley" created="2015-06-02T16:42:35Z" id="108010411">@nknize yeah - we avoid adding features that don't scale horizontally, eg see the discussion on https://github.com/elastic/elasticsearch/pull/3278 about the potential meltdown of cross-shard joins
</comment><comment author="nknize" created="2015-06-02T17:26:04Z" id="108023320">@markharwood That's basically the idea. The problem with using a grid is that there aren't overlaps.  i.e. There may be two Home Depots whose radius buffer overlap, and 1 Lowes falls within the criteria distance for both (i.e. falls in that venn overlap) So the mask would probably be something along the lines of a boolean 'OR' of geo_distance filters created from step 1.  Do we run the risk of cluster thrashing in this case?

@colings86 pointed to SignificantLongTerms as a possible reference implementation for something like this.
</comment><comment author="markharwood" created="2015-06-02T18:14:29Z" id="108039319">Sounds like 2 options for representation of the Home Depot coverage from step 1:
a) A full set of points from which you can later compute radius buffers (good for accuracy, bad if millions of points)
b) A form of multi-polygon that describes the union of the radius buffers (good if a lot of overlaps, bad for overall accuracy).

The multi-polygon in b) would not necessarily be a single-resolution grid but a set of geohashes or similar with variable resolution.

Not sure about the SignificantLongTerms connection?
</comment><comment author="colings86" created="2015-06-03T09:27:52Z" id="108264750">The reference to SignificantLongTerms was the ability to go back and query the index again in the combine phase on each shard but I hadn't thought about the fact that unlike significant terms (which relies on the assumption that the background stats on the shard are representative of the complete index) in this case you may not have all the information on the shard to be able to find all the hits.
</comment><comment author="clintongormley" created="2015-06-03T09:46:56Z" id="108276249">&gt; So the mask would probably be something along the lines of a boolean 'OR' of geo_distance filters created from step 1. Do we run the risk of cluster thrashing in this case?

If this could be represented as some constant-size data structure, regardless of how many documents/geo-points are involved, then it would work and be scalable.  If it requires recording every geo-point, then cluster thrashing would indeed ensue...
</comment><comment author="markharwood" created="2015-06-03T10:08:17Z" id="108285312">It started to feel like an image compression problem to me - accuracy and space required are based on:
1) The resolution (number of pixels) you want to spend describing a landscape and
2) The aggressiveness of the compression algorithms that bundle nearby similar items into blocks of the same colour
</comment><comment author="mikemccand" created="2015-06-03T21:31:19Z" id="108622040">This is sort of like a geo-spatial MoreLikeThis query...
</comment><comment author="xandyj" created="2015-06-08T13:48:51Z" id="110001790">You may already be planning this, but I'd like to note that it would be useful if this ConflationAggregator would be generalized so that it could support not only geospatial but also temporal correlation.  Also, it would be useful for it to support nesting.
</comment><comment author="nikonyrh" created="2015-06-08T14:42:22Z" id="110018881">I'm assuming that we want to find documents from a set A which are "nearby" a document from set B, these can be defined by any filters.

Spatiotemporal correlations could be implemented by dividing documents into buckets which have a "well defined order" and regular intervals (such as geohash and date_histogram). Implementation would is simpler if the query is executed on a pre-determined axis-aligned bounding box (2D or 3D) in this space.

First we'd query the list of non-empty buckets from each shard (and info whether they contain a document from set A, B or both), this results in two bitmaps / shard. The union of these bitmaps is formed, we'd iterate over each bucket which has a document from set A and check nearby buckets to check if they have a document from set B.

If such buckets are found then we need to launch a second query to retrieve those documents from set A. Actually each shard would have its own set of filters since we already know that which shards had documents at which buckets. I hope I managed to explain the main idea...

I'll be doing some simulations to estimate the expected amount of network traffic between nodes under different assumptions and compression methods. The good news is that response size from the first query is bounded by the number of buckets within the bounding box, not by the total number documents in each shard. Bitmaps could be replaced by actual document counts of each bucket but it would inflate data size considerably. I must admit I have zero experience in ES development, just on applied mathematics and programming.

It would be so cool to be able to run queries like "mobile phones which were near a crime scene within a few minutes in city A in last X days", naturally if we'd run this for the whole country and a long time span the number of buckets would grow significantly, also almost every bucket would have at least one mobile phone at any given time. That Home Depot example is a lot more graceful on computer resources and sets A and B are relatively sparse.
</comment><comment author="nikonyrh" created="2015-06-08T20:19:45Z" id="110125400">If the bounding box consists of 2048 x 2048 buckets (a 2D case) it would take 23*n bits for n non-empty buckets with naive encoding (1 bit to indicate if bucket is for set A or B, 2 \* 11 bits to encode its index i = x + 2048 \* y), about 400 \* n^0.7 bits with PNG compression and 114 \* n^0.77 bits with exponential-Golomb coded indexes (delta to previous index + 1 bit for set identification).

I used http://team358.org/files/frc_records/2010_Nighttime_PopDist_1.gif as a basis for random points sampling an example image is attached here, red channel forms the set A (212.5k buckets) and green channel forms set B (212.0k buckets), PNG is 548 KB and fill rate is 0.5 \* (212500 + 212000) / 2048^2 = about 5% for both channels. This corresponds to a grid size of approximately 1.31 miles or 2.1 kilometers across the whole USA which is about 2680 miles wide, assuming that the grid would fit it perfectly.

![setmap](https://cloud.githubusercontent.com/assets/1690501/8043379/5e700018-0e2e-11e5-9a56-18507e4f00ab.png)

For example with exponential-Golomb code and 10% fill rate / channel of this 2048 x 2048 map from 10 nodes would create about 114 \* (2048^2 \* 0.1 \* 2)^0.77 \* (10 - 1) / 8e6 = 4.7 megabytes of data to be sent to the coordinating node. This would be sufficient to determine that which buckets contain documents from set A which have a document from set B nearby. 20% fill rate would cause 8 MB and 35% fill rate 12.3 MB of traffic.

The resulting secondary query might grow large if set B is "dense" (meaning that most buckets have at least one document from set B). Results should hold for 3D case as well but are affected by document distribution within the grid.
</comment><comment author="markharwood" created="2015-06-10T09:57:57Z" id="110677915">Another use case to consider - using a geo demographic filter which is derived from another set.
e.g. the set of geohashes from this open UK house-sales data that allows us to get the median house price in a region to give an indication of affluence.
Deriving these geohashes is not simply an "all docs that match query" filter (like your HomeDepot example)  but an "all buckets that meet a threshold" post-filter for aggs e.g. all regions where 50th percentile house price is &gt;500k.

![query template](https://cloud.githubusercontent.com/assets/170925/8079749/9fea5530-0f5e-11e5-990e-dc5df3f5526d.jpg)
</comment><comment author="markharwood" created="2015-06-10T14:45:38Z" id="110780309">For the use case I outlined I thought I'd run through this on some real data as a practical exercise. A javascript client made the bridge manually from house sales index (0.5m houses) to crimes index (5m crimes) to do some analysis - screenshots here: https://twitter.com/elasticmark/status/608639164364550144 )

The steps were: 
1) Queried house price index, summarising by geohash and median price
2) Selected the geohash cells with high median house values (&gt; £750k). "Bucket reducers" may help here.
3) Parsed the geohashes into lat/lon bounding boxes for use in many `geo_bounding_box` filters wrapped in a `bool` `should` array. This was messy and - it clearly needs some compression to merge adjacent geohashes into more succinct filter expressions. 
4) Ran the geo-filtered query created in 3) on the crimes index with an agg on the `crime_type` field to see which crimes are correlated with high-value areas.
</comment><comment author="piotrantosik" created="2015-11-23T20:50:56Z" id="159060468">Hi,

Any plan to implement this feature in near future?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow null values in the bulk action/metadata line parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11459</link><project id="" key="" /><description>Closes #11458
</description><key id="84090161">11459</key><summary>Allow null values in the bulk action/metadata line parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T16:14:58Z</created><updated>2015-06-03T11:49:55Z</updated><resolved>2015-06-03T10:30:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-03T09:50:13Z" id="108278399">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>unable to index with logstash with current master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11458</link><project id="" key="" /><description>This was working for me as of a few weeks ago. but now I hit this:

Got error to send bulk of actions to elasticsearch server at localhost : [400] {"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"Malformed action/metadata line [1], expected a simple value for field [_id] but found [VALUE_NULL]"}],"type":"illegal_argument_exception","reason":"Malformed action/metadata line [1], expected a simple value for field [_id] but found [VALUE_NULL]"},"status":400} {:level=&gt;:error}

I'm using the simple config here: https://github.com/peterskim12/elk-index-size-tests
</description><key id="84065459">11458</key><summary>unable to index with logstash with current master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T15:09:32Z</created><updated>2015-06-03T10:30:20Z</updated><resolved>2015-06-03T10:30:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-02T15:10:29Z" id="107986549">isn't this a logstash issue in the first place? I mean null is not a valid id?
</comment><comment author="rmuir" created="2015-06-02T15:12:02Z" id="107986934">I don't know anything about the particulars, I just wanted to raise the issue. I'm using logstash 1.5 fwiw.
</comment><comment author="ph" created="2015-06-02T15:14:25Z" id="107987518">@rmuir When it was working you were running logstash 1.4.2?
</comment><comment author="rmuir" created="2015-06-02T15:15:56Z" id="107987884">When i started doing index size tests with master, I started with a 1.5 release candidate but upgraded to the official release. 

Since then i semi-regularly index that test dataset with logstash into es master, because its a little sample index for debugging, etc. (in this case, I am looking at our stats api so i wanted some data there)
</comment><comment author="ph" created="2015-06-02T15:18:27Z" id="107988594">@rmuir I'll have a look, lets keep this issue open, if needed I'll migrate it.
</comment><comment author="s1monw" created="2015-06-02T15:20:34Z" id="107989126">@ph this likely caused by https://github.com/elastic/elasticsearch/issues/10977 since we now barf if there is an invalid value
</comment><comment author="s1monw" created="2015-06-02T15:22:14Z" id="107989497">btw. this is also in `1.6` so we better check if we wanna keep it there since it seems to break logstash?
</comment><comment author="tlrx" created="2015-06-02T15:45:54Z" id="107995830">#11331 throws an exception if a "metadata" field (like `_id` , `_type` etc)  contains a null value. Let me know if this must be fixed on ES side.
</comment><comment author="colinsurprenant" created="2015-06-02T15:46:31Z" id="107995973">we should see if this is a regression on the logstash-output-elasticsearch plugin /cc @talevy .
I suggest we test with 1.4.2 to see if we can reproduce and this will tell us if this is a new behaviour introduced in the plugin. Note that there's been a handful of point releases of logstash-output-elasticsearch in the last weeks so it can also be a regression within the 1.5.0 release cycle.
</comment><comment author="clintongormley" created="2015-06-02T15:49:33Z" id="107996683">@tlrx i think that null values should be ignored here
</comment><comment author="ph" created="2015-06-02T16:00:30Z" id="108000670">@rmuir @s1monw I've tested with 1.5.0 and I can reproduce the bug, this also break logstash 1.4.2.
Even if it wasn't the intended behavior, this feel like a bit breaking for a 1.x release?
</comment><comment author="ph" created="2015-06-02T16:06:04Z" id="108001871">Easy fix on our side we can remove the `null` keys before doing the bulk request.
</comment><comment author="clintongormley" created="2015-06-02T16:07:31Z" id="108002169">@ph we should just fix this before 1.6, to ignore null values.
</comment><comment author="ph" created="2015-06-02T16:08:17Z" id="108002318">@clintongormley :+1: 
</comment><comment author="tlrx" created="2015-06-02T16:16:39Z" id="108004017">This will be fixed by #11459, sorry for the regression
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file></files><comments><comment>Bulk: allow null values in action/metadata line parameters</comment></comments></commit></commits></item><item><title>AggregationBuilder renders incorrect JSON when using binary or raw sub-aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11457</link><project id="" key="" /><description>Bug found from a forum post: https://discuss.elastic.co/t/problem-with-binary-sub-aggregations/1727

Using the below code to embed a sub-aggregation inside another aggregation:

``` java
import org.elasticsearch.common.xcontent.ToXContent;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentHelper;
import org.elasticsearch.common.xcontent.json.JsonXContent;
import org.elasticsearch.search.aggregations.AggregationBuilders;
import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
import java.io.IOException;

public class TestSubAggregation {

    public static void main(String[] args) throws IOException {
        // Build a simple term aggregation

        TermsBuilder termsBuilder = AggregationBuilders.terms("test").field("testfield");

        // Build a simple term sub aggregation
        TermsBuilder subTerm = AggregationBuilders.terms("subtest").field("subtestfield");

        // Add sub aggregation as an AggregationBuilder
        termsBuilder.subAggregation(subTerm);
        // It produces a correct output
        System.out.println(XContentHelper.toString(termsBuilder));

        // Reset term aggregation
        termsBuilder = AggregationBuilders.terms("test").field("testfield");

        // Create an XContentBuilder from sub aggregation
        XContentBuilder subTermContentBuilder = JsonXContent.contentBuilder().startObject();
        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
        subTermContentBuilder.endObject();

        // Add sub aggregation as a XContentBuilder (binary_aggregation)
        termsBuilder.subAggregation(subTermContentBuilder);

        // Produces an incorrect output (two aggregations levels instead of one)
        System.out.println(XContentHelper.toString(termsBuilder));

    }
}
```

The second sysout produces:

``` json
{
  "test" : {
    "terms" : {
      "field" : "testfield"
    },
    "aggregations" : {
      "aggregations":{"subtest":{"terms":{"field":"subtestfield"}}}
    }
  }
}
```

Which is invalid as `aggregations` object is specified twice.
</description><key id="84043520">11457</key><summary>AggregationBuilder renders incorrect JSON when using binary or raw sub-aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T14:15:07Z</created><updated>2015-06-04T11:59:24Z</updated><resolved>2015-06-04T11:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-03T11:47:40Z" id="108330780">Additionally to the issue with the builder, even when the builder is producing the correct output, the parser does not recognise the `aggregations_binary` field which is used when the XContentBuilder's contentType is different from the contentType of the sub-aggregation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java</file><file>src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryTests.java</file></files><comments><comment>Aggregations: Allow aggregation_binary to build and parse</comment></comments></commit></commits></item><item><title>heavy swap usage by elastic search even though plenty of ram is available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11456</link><project id="" key="" /><description>  "version" : {
    "number" : "1.4.2",
    "build_hash" : "927caff6f05403e936c20bf4529f144f0c89fd8c",
    "build_timestamp" : "2014-12-16T14:11:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"

The associated java processes consume quite a bit of swap even though plenty of ram is available:

![es1](https://cloud.githubusercontent.com/assets/10959745/7937345/d49f9fc6-093f-11e5-858b-7fddfb057ef6.PNG)

![es2](https://cloud.githubusercontent.com/assets/10959745/7937364/da223c06-093f-11e5-980a-94d3b7c2d5ee.PNG)

/etc/sysconfig/elasticsearch has the following settings:

ES_HOME=/usr/share/elasticsearch
ES_HEAP_SIZE=24g
MAX_OPEN_FILES=65535
MAX_MAP_COUNT=262144
LOG_DIR=/var/log/elasticsearch
DATA_DIR=/var/lib/elasticsearch
WORK_DIR=/tmp/elasticsearch
CONF_DIR=/etc/elasticsearch
CONF_FILE=/etc/elasticsearch/elasticsearch.yml
ES_USER=elasticsearch

/etc/sysconfig/elasticsearch2 - the second instance has identical settings.
</description><key id="84035898">11456</key><summary>heavy swap usage by elastic search even though plenty of ram is available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bauerstefan</reporter><labels /><created>2015-06-02T13:56:06Z</created><updated>2015-06-02T14:21:56Z</updated><resolved>2015-06-02T14:21:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T14:21:55Z" id="107971347">Hi @bauerstefan 

Please read this documentation about how to stop swapping: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory

Given that it looks like you are using one of the packages, you should also see the docs for setting MAX_LOCKED_MEMORY here: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cleanup and standardize settings placeholders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11455</link><project id="" key="" /><description>In #10918, a new placeholder was added for prompting settings. We already allow placeholders to read values from the environment/system properties. During the review, @uboness noted that it would be good to standardize the naming and cleanup the code that prompts the place holders. 

The new changes mentioned in the comments of the PR are:
- Consolidate code that handles replacing the placeholders to one place. Right now we have it in the `Settings` and the `InternalSettingsPreparer`. This logic is really internal and should not be exposed in the Settings.
- The system placeholders should follow the same convention and be in the form: `${env::foobar}`
- We can also introduce a new setting: `config.ignore_placeholders`:
  This will ignore all placeholder settings:

``` yaml
config.ignore_placeholders: true
```

will only ignore `env` placeholders:

``` yaml
config.ignore_placeholders: env
```

will only ignore `prompt` placeholders:

``` yaml
config.ignore_placeholders: prompt
```

will ignore both the `env` and `prompt` settings:

``` yaml
config.ignore_placeholders: env, prompt
```
</description><key id="84022229">11455</key><summary>cleanup and standardize settings placeholders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-06-02T13:29:16Z</created><updated>2016-01-18T20:00:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-03T09:34:01Z" id="108266212">Would `all` not be a more descriptive name for ignoring all placeholder settings. If I saw a setting which has a value of `true`, I would not think that it has any other options apart from `true` and `false` whereas seeing `all` would indicate to me that other values are available and there might be more granular values I can use.
</comment><comment author="clintongormley" created="2015-06-05T09:32:27Z" id="109217194">@jaymode Why do we need the `config.ignore_placeholders` setting?  If somebody has permission to start Elasticsearch or edit the config file, then they also have permission to change this setting?
</comment><comment author="jaymode" created="2015-06-05T12:31:35Z" id="109280663">The `config.ignore_placeholders` was a suggestion from @uboness, so he may be able to provide additional details about the suggestion of adding it. My opinion is that this may not really be needed, since it controls internal behavior.

For clarification and background, I think the overall idea here is standardization of the placeholder naming. Looking at the [replacePropertyPlaceholders](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/settings/Settings.java#L1248-L1299) method, we actually have two different ways of ignoring placeholders, the `shouldIgnoreMissing` method and then a check against completely ignored values passed in to the method, which is a bit of a mess (that I introduced). 

There is a reason for this mess. The prompt placeholders use the format `${prompt::text}`, which matches the format of other property placeholders. Where it differs is the use of `::` as a separator. The preexisting property placeholders use `:` to indicate there is a default value and `.` as a type of a prefix, such as `${env.foo}`. In order to support the `${prompt::text}` format, we cannot use the `shouldIgnoreMissing` method because the value would never be missing, it would always have a default value.

If we standardize the naming, then we can clean this up and at the same time remove the `replacePropertyPlaceholders` method from `Settings` since it is only useful when preparing the settings and doesn't need to be in that class.
</comment><comment author="clintongormley" created="2015-06-05T14:14:13Z" id="109305748">Wondering if it wouldn't be easier (and more bwc) to use this then:

```
${env.foo:default}
${prompt.text:default}
```
</comment><comment author="uboness" created="2015-06-05T14:19:44Z" id="109307363">@clintongormley perhaps, it'll be more aligned with what we have today
</comment><comment author="clintongormley" created="2015-08-16T11:12:58Z" id="131525797">@jaymode @uboness anything more to do here?
</comment><comment author="jaymode" created="2015-08-17T11:38:36Z" id="131791308">@clintongormley I think we still need to cleanup the code a little, (copied the first bullet from above):
- Consolidate code that handles replacing the placeholders to one place. Right now we have it in the Settings and the InternalSettingsPreparer. This logic is really internal and should not be exposed in the Settings

We also didn't decide on the `config.ignore_placeholders` setting. Maybe @uboness can add his thoughts around that setting?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsTests.java</file></files><comments><comment>make prompt placeholders consistent with existing placeholders</comment></comments></commit></commits></item><item><title>predicting two moving averages in the same aggregation produces incorrect keys for predicted buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11454</link><project id="" key="" /><description>In the example request below we have a date histogram with buckets for each month between 01 Jan 2001 (inclusive) and 01 Jan 2005 (not inclusive). In each bucket (month) we calculate the minimum distance, the derivative of that minimum (the speed) and then do a moving average of the distance and the moving average of the speed. Both moving averages predict the next 12 months of values.

The first moving average to execute (lets say the distance moving average) works fine and produces buckets for Jan 2005 to Dec 2005 (inclusive) as expected. The problem is that when the second moving average runs (the speed moving average), it appends its predictions onto the end of the buckets and predict for Jan 2006 to Dec 2006 (inclusive). This is because we assume predictions should always append buckets on the end of the histogram, rather than predicting for buckets starting at the next bucket after the last value for whatever metric we are predicting.

Example request:

``` json
{
  "query": {
    "range": {
      "date": {
        "gte": "2001-01-01",
        "lt": "2005-01-01"
      }
    }
  },
  "size": 0,
  "aggs": {
    "histo": {
      "date_histogram": {
        "field": "date",
        "interval": "month"
      },
      "aggs": {
            "dist": {
              "min": {
                "field": "distance"
              }
        },
        "speed": {
          "derivative": {
            "buckets_path": "distance"
          }
        },
        "speed-mov-avg": {
          "moving_avg": {
            "buckets_path": "speed",
            "predict": 12,
            "model" : "holt",
            "settings" : {
                "alpha" : 0.5,
                "beta": 0.5
            }
          }
        },
        "dist-mov-avg": {
          "moving_avg": {
            "buckets_path": "distance",
            "predict": 12,
            "model" : "holt",
            "settings" : {
                "alpha" : 0.5,
                "beta": 0.5
            }
          }
        }
      }
    }
  }
}
```
</description><key id="83989321">11454</key><summary>predicting two moving averages in the same aggregation produces incorrect keys for predicted buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T11:56:58Z</created><updated>2015-06-04T15:22:10Z</updated><resolved>2015-06-04T15:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file></files><comments><comment>Fix bug where predictions append to the previous prediction</comment></comments></commit></commits></item><item><title>Added epoch date formats to configure parsing of unix dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11453</link><project id="" key="" /><description>This PR adds the support the new date formats, namely `epoch_second` and `epoch_second_millis`. By adding those, we can remove the internal logic to try and parse everything as a unix timestamp first and only then use our regular date format.

This PR tries to remain backwards compatible by using `epoch_second_millis||dateOptionalTime` where before only `dateOptionalTime` was used in combination with the parsing.

Some BWC occured, ie, a date like `10000` is now always a year without any other configuration instead of a unix timestamp.

Also, in the current implementation, the `RangeQueryParser` allows to configure a timezone for queries using `from`/`to` unix timestamps, even though the timezone is ignored in the query, as a timestamp is always UTC.

Closes #5328
Relates #10971
</description><key id="83989316">11453</key><summary>Added epoch date formats to configure parsing of unix dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T11:56:55Z</created><updated>2015-06-06T17:59:34Z</updated><resolved>2015-06-03T16:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-02T14:53:03Z" id="107980722">Look good, left  a small naming comment. Also, worth checking how this fits into @rjernst 's mapping changes (I think he has a PR outstanding on that)
</comment><comment author="rjernst" created="2015-06-03T07:19:25Z" id="108225647">This is great! I left a couple comments. I also would prefer to just use `epoch_millis`.
</comment><comment author="spinscale" created="2015-06-03T09:43:27Z" id="108273643">updated the PR/replied to all of your comments. thx for the hints!
</comment><comment author="rjernst" created="2015-06-03T09:44:44Z" id="108274750">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>not only cast but also round OS stats from probes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11452</link><project id="" key="" /><description>values from Sigar should be rounded before being cast to `short`
</description><key id="83988037">11452</key><summary>not only cast but also round OS stats from probes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chaudum</reporter><labels><label>:Stats</label></labels><created>2015-06-02T11:53:36Z</created><updated>2015-07-08T16:03:28Z</updated><resolved>2015-07-08T16:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-08T16:03:23Z" id="119636084">@chaudum thanks for your contribution, but Sigar has been removed from codebase (see #11034  and #12010). I think we can close this pull request now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>3 Elasticsearch node cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11451</link><project id="" key="" /><description>Hi, we were having 2 nodes one with master and data and the other with data and no master, now we have added an additional 3rd node into the cluster with data and no master. So what happened now is that the indices are currently writing the data only to the newly added third node and the first 2 nodes are not have any shards located. 
I want the data/shards to be allocated to all the 3 nodes, is there any issue with the configs? Can you please assist. 
</description><key id="83885927">11451</key><summary>3 Elasticsearch node cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HSVK</reporter><labels /><created>2015-06-02T07:24:17Z</created><updated>2015-06-02T07:26:03Z</updated><resolved>2015-06-02T07:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-02T07:26:02Z" id="107837136">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sync up snapshot shard status on a master restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11450</link><project id="" key="" /><description>When a snapshot operation on a particular shard finishes, the data node where this shard resides sends an update shard status request to the master node to indicate that the operation on the shard is done. When the master node receives the command it queues cluster state update task and acknowledges the receipt of the command to the data node.

The update snapshot shard status tasks have relatively low priority, so during cluster instability they tend to get stuck at the end of the queue. If the master node gets restarted before processing these tasks the information about the shards can be lost and the new master assumes that they are still in process while the data node thinks that these shards are already done.

 This commit add a retry mechanism that checks compares cluster state of a newly elected master and the current state of snapshot shards and updates the cluster state on the master again if needed.

Closes #11314
</description><key id="83755045">11450</key><summary>Sync up snapshot shard status on a master restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-02T00:10:50Z</created><updated>2015-06-08T00:34:07Z</updated><resolved>2015-06-03T19:15:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-03T10:13:06Z" id="108286778">left a minor comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add script type and script name to error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11449</link><project id="" key="" /><description>Modified ScriptEngineService to pass in a CompiledScript object
to several methods with newly added name and type member variables.
This can in turn be used to give better scripting error messages
with the type of script used and the name of the script.

Required slight modifications to the caching mechanism.

Note that this does not enforce good behavior in that plugins will
have to write exceptions that also output the name of the script
in order to be effective.  There was no way to wrap the script
methods in a try/catch block properly further up the chain because
many have script-like objects passed back that can be run at a
later time.

fixes #6653
</description><key id="83744510">11449</key><summary>Add script type and script name to error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-01T23:33:59Z</created><updated>2016-02-01T21:03:58Z</updated><resolved>2015-07-11T01:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-06-03T00:19:52Z" id="108138029">Thanks to @rmuir @s1monw @javanna for the reviews -- and I'm ready for the next round.  I pushed a new commit with changes based on the prior comments, and I tried to add a bit more consistency to the modified error messages.
</comment><comment author="jdconrad" created="2015-06-04T17:06:31Z" id="108975414">@javanna Please take a look at the response I left you when you get a chance.
</comment><comment author="javanna" created="2015-06-05T08:28:48Z" id="109202808">thanks @jdconrad I replied, this looks good on my end besides the comments I left, maybe @uboness can have a look too?
</comment><comment author="javanna" created="2015-07-03T09:56:27Z" id="118303372">what's the status on this? @uboness did you have time to have a look?
</comment><comment author="jdconrad" created="2015-07-06T15:41:20Z" id="118900503">@javanna No update right now.  I have been caught up in other issues and need to come back to this.  I like @rjernst 's suggestion of making the CacheKey class into a simple string with all the necessary information to make the key unique as that's what's ultimately happening anyway.  What do you think?
</comment><comment author="javanna" created="2015-07-06T15:50:04Z" id="118905264">I think it would be nice if we could get this in, especially given that it's marked breaking. I wouldn't want this PR to be forgotten for 2.0.
</comment><comment author="jdconrad" created="2015-07-06T16:06:13Z" id="118910754">@javanna I will try to get it done before the end of the week then.
</comment><comment author="jdconrad" created="2015-07-10T22:29:20Z" id="120542063">@rjernst Oops, meant this PR :)
</comment><comment author="rjernst" created="2015-07-11T00:38:34Z" id="120556500">LGTM, I left a couple small suggestions.
</comment><comment author="jdconrad" created="2015-07-11T01:30:05Z" id="120560583">@rjernst Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/CompiledScript.java</file><file>core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java</file><file>core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptModesTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTest.java</file><file>plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTest.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java</file><file>plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTest.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java</file></files><comments><comment>Add script type and script name to error messages</comment></comments></commit></commits></item><item><title>Arbitrary meta field in queries?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11448</link><project id="" key="" /><description>Can we add an arbitrary field to ES queries? Basically I would like to add a client information field ( for example: `"client":"api"` or `"client":"ui"` ) to ES queries so I could find/grep them in slow log queries.
</description><key id="83695645">11448</key><summary>Arbitrary meta field in queries?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devfacet</reporter><labels /><created>2015-06-01T21:11:56Z</created><updated>2015-06-02T23:07:22Z</updated><resolved>2015-06-02T18:41:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T18:41:09Z" id="108049353">Kinda.  You can use stats groups.  Assign each family of queries to a particular group, which you can then grep for in the logs.  As an added benefit, the indices stats API will give you stats per group.

See https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html#stats-groups
</comment><comment author="devfacet" created="2015-06-02T23:07:22Z" id="108124972">Got it, thanks...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect search result using a top level field ES 1.3.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11447</link><project id="" key="" /><description>Given this mapping 

```
    PUT /test/_mapping/entity 
    {
        "properties" : {
            "message" : {
            "type" : "string", 
            "store" : false, 
            "index": "not_analyzed"
        },
        "user": {
            "type":"object"
        }
    },
    "dynamic_templates" : [
    {
        "message-lowercase": {
            "path_match": "(\\Quser.message.lc\\E$)",
            "match_pattern": "regex",
            "mapping": {
                "type" : "string", 
                "store" : true, 
                "index": "not_analyzed"
            }
        }
    },
    {
        "message-dynamic": {
            "path_match": "\\Qmessage\\E$",
            "match_pattern": "regex",
            "mapping": {
                "type" : "string", 
                "store" : false, 
                "index": "not_analyzed"
            }
        }
    }
    ]
    }
```

And If I Index following document -

```
PUT /test/entity/demo1 
{
    "message": "X",
    "user": {
        "message": "Y",
        "message.lc": "y"
    }
}
```

I do not expect to see any results if I search using message.lc

```
GET _search 
{
    "filter": {
        "term": {
           "message.lc": "y"
        } 
    }
}
```

But I do see demo1 in search results. I believe ES should return 0 hits for above search as message.lc does not have a value at root object level. Thoughts?
</description><key id="83675507">11447</key><summary>Incorrect search result using a top level field ES 1.3.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkelkarbv</reporter><labels /><created>2015-06-01T20:20:02Z</created><updated>2015-06-02T18:33:42Z</updated><resolved>2015-06-02T18:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T18:33:40Z" id="108046749">The `message.lc` is being resolved to `user.message.lc` because it looks up the "short name" of the field.  You shouldn't use periods in your field names.  We don't enforce it yet, but we will (see https://github.com/elastic/elasticsearch/issues/9059)

In 2.0, you won't get back any results for the above query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Always normalize root paths during resolution of paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11446</link><project id="" key="" /><description>Currently, when trying to determine if a location is within one of the configured repository
paths, we compare the root path against a normalized path but the root path is never
normalized so the check may incorrectly fail. This change normalizes the root path and
compares it to the other normalized path.

Relates to #11426
</description><key id="83649593">11446</key><summary>Always normalize root paths during resolution of paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-01T19:11:21Z</created><updated>2015-06-08T00:30:11Z</updated><resolved>2015-06-01T19:39:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-01T19:29:21Z" id="107678588">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate filter option in PhraseSuggester collate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11445</link><project id="" key="" /><description>Collate filter option is removed in v2.0.0, use collate query for
collation in PhraseSuggester.

see #11195
</description><key id="83616762">11445</key><summary>Deprecate filter option in PhraseSuggester collate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-06-01T17:36:29Z</created><updated>2015-06-16T04:18:22Z</updated><resolved>2015-06-02T16:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-06-01T17:36:58Z" id="107650060">@clintongormley quick review?
</comment><comment author="clintongormley" created="2015-06-02T16:23:49Z" id="108005750">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing doc_values field in DateMapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11444</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/guide/master/doc-values.html says that doc_values parameter is enabled for date fields, but there is not such property in DateMapping. Could you please help me figure it out? Thanks a lot!
</description><key id="83616505">11444</key><summary>Missing doc_values field in DateMapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crassirostris</reporter><labels /><created>2015-06-01T17:35:54Z</created><updated>2015-06-02T08:19:43Z</updated><resolved>2015-06-02T08:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="crassirostris" created="2015-06-02T08:19:43Z" id="107856349">Whooops, wrong repo, sorry, that's elasticsearch-net issue =)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add size or number of fields limit on dynamic mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11443</link><project id="" key="" /><description>Object mapping are dynamic by default. The mapping size get exploded when incoming document stream contains dynamic keys such as uuid string, time string etc. For example, {"fields":{"de305d54-75b4-431b-adb2-eb6b9e546014":{...}}}. It can kill the cluster very fast at high indexing rate. We need a way to limit mapping by size or by number of fields. When the limit is reached, the object mapping behavior changes from dynamic:true to dynamic:false.
</description><key id="83572128">11443</key><summary>Add size or number of fields limit on dynamic mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanjunh</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-01T15:49:02Z</created><updated>2016-03-29T17:40:55Z</updated><resolved>2016-03-29T17:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-02T18:20:24Z" id="108041827">Hi @yanjunh 

This feels like the wrong approach.  Why would we index field `foo-bar-123` and reject `foo-bar-124`?  Why index the first field at all, if you're just going to throw away the next field arbitrarily?

I'm afraid the proposed solution is not a solution at all. Instead, you need to change how you're indexing the data, or how you're adding fields.  
</comment><comment author="nik9000" created="2015-06-02T18:38:51Z" id="108048420">I think the idea is to catch mistakes and prevent the cluster from becoming unstable.
</comment><comment author="clintongormley" created="2015-06-02T18:42:10Z" id="108049692">@nik9000 sure, but this is still the wrong solution. we're not going to add a setting which arbitrarily rejects fields if X many fields still exist.
</comment><comment author="yanjunh" created="2015-06-02T18:49:05Z" id="108051916">@clintongormley We have been telling developers about the limitation but we can't prevent them from producing unfriendly data. Thought about detecting it on when data flow through Logstash but that's not a good place. What's the recommended way to guard against unbounded mapping growth?
</comment><comment author="nik9000" created="2015-06-02T18:53:22Z" id="108053709">&gt; What's the recommended way to guard against unbounded mapping growth?

Strong language.

Seriously though, for my use case I turn mapping to non-dynamic and only add fields "intentionally" but that's not always possible.

Also - I imagine this is something you could catch in code review and by giving all developers their own instance to test against. But I'm still for adding some extra defence against it to Elasticsearch.
</comment><comment author="yanjunh" created="2015-06-02T18:58:08Z" id="108056269">@nik9000 Forgive my wording
We have no control on what people log. So far I can only set alerts when problem happens. Even so it's often too late because the data are coming in so fast. 
</comment><comment author="clintongormley" created="2015-06-02T19:05:55Z" id="108058983">@yanjunh I'll reopen the issue for further discussion.
</comment><comment author="jpountz" created="2015-06-03T09:42:10Z" id="108272290">Going from `dynamic:true` to `dynamic:false` is not an option in my opinion. If you index the same set of documents twice, you would get different mappings depending on the order in which documents have been indexed.

I would be open to discuss switching from `dynamic:true` to `dynamic:strict` however, so that the cause of the issue would be propagated to the client which is indexing the document. I can see value in admins being able to protect against client-side bugs and/or enforce consumers of elasticsearch to follow some good practices.
</comment><comment author="nik9000" created="2015-06-03T11:38:58Z" id="108323720">&gt; I would be open to discuss switching from dynamic:true to dynamic:strict however, so that the cause of the issue would be propagated to the client which is indexing the document.

If dynamic:strict rejects the document then I'm +1 on that. So long as the setting is dynamically update-able then it shouldn't prevent people from doing this if they want - just get in their way and remind them its a bad idea.
</comment><comment author="yanjunh" created="2015-06-03T14:18:25Z" id="108447385">I agree with dynamic:strict. My original thought is to keep the good part of the document and just drop the bad field. Dropping whole document might be a better idea. That will force the data producers to structure their data in the right way. The data consumers can then effectively  search and aggregate on the right structured data. In either approach, being able to protect the search cluster from bad data can be very useful.
</comment><comment author="segalziv" created="2015-07-23T10:13:49Z" id="124045331">+1 for setting a limit on the number of (dynamic) fields, and moving to dynamic:false when that limit is reached. 

I'm also trying to protect elasticsearch from becoming unstable due to too many fields.
</comment><comment author="breml" created="2015-11-15T09:42:56Z" id="156794726">We use a custom analyzer to mark dynamically added fields, which allows us to monitor the count of added fields. More explanation in the following blog post:  https://breml.github.io/blog/2015/10/28/tagging-of-dynamically-added-fields-in-elasticsearch/
</comment><comment author="ppf2" created="2016-01-06T21:28:58Z" id="169468727">+1 on providing an option for the safeguard.  Another use case is key value logging and using the LS KV filter with dynamic mappings so that these fields are automatically extracted and generated.  Having a limit the user can set can help prevent unintentional explosion in mappings when someone decides to use UUIDs as keys, etc..
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java</file></files><comments><comment>Add limit to total number of fields in mapping. #17357</comment></comments></commit></commits></item><item><title>Search optimization - Highlighter causes many query rewrites</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11442</link><project id="" key="" /><description>A user reported very slow performance here https://github.com/elastic/elasticsearch/pull/10391#issuecomment-105466848

The issue is that the query being highlighted does not have its rewritten form cached and consequently is rewritten for every doc x field being highlighted.
</description><key id="83562665">11442</key><summary>Search optimization - Highlighter causes many query rewrites</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2015-06-01T15:31:09Z</created><updated>2015-11-09T08:26:15Z</updated><resolved>2015-07-15T13:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-06-01T16:57:55Z" id="107637236">A quick attempt to fix this was to cache the query when rewritten for search purposes and share it across all calls to the plain highlighter. This was a massive speed-up for the fuzzy queries in the example https://github.com/elastic/elasticsearch/pull/10391#issuecomment-105466848 but caused our unit tests to fail.
The WeightedSpanTermExtractor used for highlighting requires detailed knowledge of the query on a per-document basis and performs it's own detailed unpacking of the original un-rewritten query to achieve its goals. With my intended change, the unit tests showed that a PrefixQuery that is rewritten for search execution does not highlight correctly because as part of that translation it loses the information in the original query required by WeightedSpanTermExtractor.

It is possible there is an ugly trade-off to be made here between accuracy of highlighter results and the performance costs of rewrites. The solution may not be a simple one but I will dig deeper.
</comment><comment author="javanna" created="2015-06-02T08:50:33Z" id="107870669">Postings highlighter would be a great fit here as it doesn't require rewriting the query and pulls the automata out of the prefix query, I'd give it a try. The other highlighters are known to make the wrong trade-offs I'd say.
</comment><comment author="markharwood" created="2015-06-02T09:57:41Z" id="107903768">Postings Highlighter is demonstrably faster but requires a re-index for many users.
I was wondering about having a "fast-mode" for the plain highlighter that feeds it the re-written query used for search. It is significantly faster for the issue reported (100ms vs 30 seconds) and produces identical results for the fuzzy multi-match in question. What it loses is the ability to highlight certain other types of query (Prefix being one example). I'm not sure how many others there are at this stage.  
</comment><comment author="rmuir" created="2015-06-02T10:02:51Z" id="107904829">The other highlighters are definitively broken here.

doing any kind of rewrite at all, against the index, just to highlight a few docs, thats just wrong.
</comment><comment author="markharwood" created="2015-06-02T10:06:47Z" id="107905753">The plain highlighter continuously rewrites against a MemoryIndex from what I can see (but dives off to the main index to look up term doc freqs for weightings used when selecting "best" matches)
</comment><comment author="rmuir" created="2015-06-02T10:07:01Z" id="107905791">&gt; What it loses is the ability to highlight certain other types of query (Prefix being one example).

why not just use the same technique postings highlighter uses for all multitermqueries? IMO the only reasonable way to do it. It has the same runtime whether or not the query rewrites to 100 terms or 100 million terms.  Hint: it does not have anything to do with postings in the offsets! 
</comment><comment author="rmuir" created="2015-06-02T10:10:10Z" id="107906280">Why look up docfreqs? PH uses a "constant score" for multitermqueries. It really does not matter. someone highlighting wildcards does not care about ranking for the most part. Same reason they use  constant score execution in lucene for the actual query, too

and sorry, just for the record here. postings highlighter isnt any faster because there is some magic about postings in the offsets. maybe this has some nice locality, but IMO things are faster because it does not do dumb things.

doing dumb things to me includes:
- rewrite()ing against the entire index
- looking up docfreqs against the entire index
- making big datastructures in RAM
- using term vectors
</comment><comment author="rmuir" created="2015-06-02T10:15:10Z" id="107907016">The fast vector impl is the worst. it actually has a Terms/TermsEnum per-document, but rewrites against the whole index anyway... sorry, to me this stuff is hopeless
</comment><comment author="markharwood" created="2015-06-02T10:15:22Z" id="107907064">&gt; Why look up docfreqs?

I'm hazy on this but "plain" highlighter is all based around rewriting the query against a MemoryIndex containing the doc to be highlighted. The "getBestTextFragments" method is a summarizer and has to be aware of the quality of the words found in various fragments of what could be a long field so has to come up with an IDF value using the main index as a source of stats
</comment><comment author="rmuir" created="2015-06-02T10:24:11Z" id="107910294">Using docfreq is definitely broken. Its unrelated to anything at play here.

If you are ranking fragments (lets call them sentences for simplicity), then those sentences are your documents, and each document is your corpus. And you want the top-N sentences... so the measure you need is the number of sentences in the document that match that word.

This is not something thats easy to compute: because sentences usually arent split at index-time, but you can use approximations based on totalTermFreq instead (total # of occurrences in all sentences). This is the "freq" from the postings list in the postingshighlighter impl.

You don't need docfreq across the entire index: totally irrelevant.
</comment><comment author="markharwood" created="2015-06-02T10:51:57Z" id="107914852">It's a question of certainty and related to doc length - if I searched for `totalTermFreq measure` in your last comment I'd only have 4 sentence/documents in my comment/corpus and a docFreq of 1 for both `totalTermFreq` and `measure` but using the full index of all comments would give me more stats/certainty about the relative importance of those 2 terms.
A quality/performance trade-off that depends on the type of content you are dealing with.

It's certainly true to say that the summariser capabilities designed for large fields are fairly pointless to run in scenarios such as the original problem where the fields are very small.
</comment><comment author="rmuir" created="2015-06-02T11:06:34Z" id="107917009">The thing you are missing is, the corpus has "changed", you are just highlighting one document, with the goal to return the top-N sentences/snippets. the other documents at this point are not really comparable (and lots of people have data where docs vary wildly from each other).

Using the other corpus is not just bad, its also slow. So since the corpus is just one doc, use statistics for that one doc.

So yeah, I definitely think postingshighlighter ranks better too. At least it has the basic components for each sentence (term frequency, term importance, length normalization), and just tries to reuse stuff from IR we already know works and not do anything "sheisty" or "creative" like transfer statistics from other documents, that at best are "comparable corpora".

And the ranking is easy to tune to what you want (e.g. ignore length normalization completely, ignore term frequencies completely, or reasonable things in between, etc) with classic BM25 parameters. 
</comment><comment author="rmuir" created="2015-06-02T11:13:33Z" id="107917848">also, just being practical, i dont think highlighting selection for "foo fighters" for document A should ever change, because of what happens to other docs (sharding different, merging, adding other docs, whatever). This is confusing to the user.
</comment><comment author="markharwood" created="2015-06-02T11:32:52Z" id="107923429">I do get what you mean - I'm just sceptical about basing any statistical judgements based purely on the tiny amounts of evidence that are available in short docs.

&gt; This is confusing to the user.

Not if you consider we are fundamentally answering the question "why did this doc rank so high?". From a quality perspective, the selections should be as close as possible to representing "what the search engine saw as interesting" in each document. Somewhere in the querying process we have the IDF that was an ingredient in this ranking and so it would be useful if we could retain that. Using an alternative doc-local fragment-ranking system that is light on evidence/stats and deviates from the value judgements that place each doc in a certain position in the rankings is likely to be more misleading.
</comment><comment author="nik9000" created="2015-06-02T11:35:21Z" id="107923749">I have lots of opinions on this but I'm on mobile and tired.

Its worth pointing out that the Experimental highlighter pulls out the
automata for some queries (prefix, fuzzy, Wildcard) but falls back to
rewriting when it doesn't know what to do with the query. Its far from
perfect but it doesn't  put a dent in our performance. It might be a useful
"quick" fix for the mentioned problem because it doesn't need a reindex.
Just a restart to install it.

I totally filed an issue years ago about wildcard query being slow with the
plain highlighter.

Nik
On Jun 2, 2015 7:13 AM, "Robert Muir" notifications@github.com wrote:

&gt; also, just being practical, i dont think highlighting selection for "foo
&gt; fighters" for document A should ever change, because of what happens to
&gt; other docs (sharding different, merging, adding other docs, whatever). This
&gt; is confusing to the user.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11442#issuecomment-107917848
&gt; .
</comment><comment author="markharwood" created="2015-06-02T14:10:42Z" id="107968435">@nik9000 for the record, the experimental highlighter was fast on the example problem and produced equivalent results. I see it has a bundle of useful options to play with too. Very interesting!
</comment><comment author="markharwood" created="2015-06-15T14:29:22Z" id="112089762">Closing this - the rewrite operation is an integral part of the design of the plain highlighter (or at least the WeightedSpanTermExtractor that is used to implement "best fragment" decisions). While suitable for some use cases it is inappropriate for use cases where many small fields are being highlighted in large numbers of documents.
The Experimental highlighter plugin Nik linked to looks very promising.
</comment><comment author="jeantil" created="2015-11-06T13:51:28Z" id="154414643">@nik9000 just out of curiosity, is the "experimental" highlighter now considered `production ready` ? I am asking this because the postings highlighter doesn't play well with some character substitutions (https://github.com/elastic/elasticsearch/issues/11726) so we are left with fvh. I recently noticed some weird errors when indexing : 
`[498]: index [pattern_fr_1446775358072], type [pattern], id [AVDauiUumeu9P7Cdojxj], message [IllegalArgumentException[startOffset must be non-negative, and endOffset must be &gt;= startOffset, and offsets must not go backwards startOffset=6,endOffset=15,lastStartOffset=11 for field 'product_search']]`

I am not yet positively sure it is linked to the `fvh` highlighter interacting badly with some synonym substitutions but it is pretty high on my suspects list. If it were confirmed we would be left with no working options for our use case :(

-- edit -- the error above is in fact linked to some leftover configuration tu use the offsets indexing option from our attempts to use the postings higlighter, fvh does work for us. the tradeoffs of fvh are unclear to me though.
</comment><comment author="nik9000" created="2015-11-06T19:17:09Z" id="154505397">It's been in production at WMF, my old job for over a year now. It's
serving millions of diverse requests a day without any problem. But its not
an elastic supported plugin. I keep bringing it up because I wrote it. I'm
sorry to make things complicated!

I honestly don't know what the future holds for the plugin.
On Nov 6, 2015 8:51 AM, "Jean Helou" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 just out of curiosity, is the
&gt; "experimental" highlighter now considered production ready ?
&gt; 
&gt; thanks
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11442#issuecomment-154414643
&gt; .
</comment><comment author="jeantil" created="2015-11-09T08:26:15Z" id="154995403">@nik9000 thanks for your answer I'll have a look again :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs fix- added performance note about plain highlighter</comment></comments></commit></commits></item><item><title>Failed to parse date field with logstash.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11441</link><project id="" key="" /><description>Here is logstash rubydebug result:

```
{
     "@timestamp" =&gt; "2015-06-01T07:56:39.581Z",
        "message" =&gt; "2015-06-01 15:56:39,213 [vert.x-eventloop-thread-0] INFO  hawkeyes.rtds.monitor.MetricsVerticle  - Metrics of RTDS:\n{DB Worker={max=2674, min=1, total=2728, count=3, avg=909.3333333333}, Realtime Cacher={max=2349, min=1000, total=2349, count=1, avg=2349}}",
       "@version" =&gt; "1",
           "tags" =&gt; [
        [0] "multiline",
        [1] "rtds_log",
        [2] "vertx"
    ],
           "type" =&gt; "rtds",
           "host" =&gt; "hawkeyesTest",
           "path" =&gt; "/var/log/rtds/rtds.log",
           "date" =&gt; "2015-06-01 15:56:39,213",
    "thread_name" =&gt; "vert.x-eventloop-thread-0",
      "log_level" =&gt; "INFO",
       "verticle" =&gt; "hawkeyes.rtds.monitor.MetricsVerticle",
        "content" =&gt; "Metrics of RTDS:"
}
```

But elasticsearch raises an error:

```
[2015-06-01 15:56:29,906][DEBUG][action.bulk              ] [Watcher] [logstash-2015.06.01][2] failed to execute bulk item (index) index {[logstash-2015.06.01][rtds][AU2uHzNIK8Y8pKwZT0pi], source[{"@timestamp":"2015-06-01T15:56:24.560+08:00","message":"2015-06-01 15:56:24,213 [vert.x-eventloop-thread-0] INFO  hawkeyes.rtds.monitor.MetricsVerticle  - Metrics of RTDS:\n{DB Worker={max=2674, min=1, total=2728, count=3, avg=909.3333333333}, Realtime Cacher={max=2349, min=1000, total=2349, count=1, avg=2349}}","@version":"1","tags":["multiline","rtds_log","vertx"],"type":"rtds","host":"hawkeyesTest","path":"/var/log/rtds/rtds.log","date":"2015-06-01 15:56:24,213","thread_name":"vert.x-eventloop-thread-0","log_level":"INFO","verticle":"hawkeyes.rtds.monitor.MetricsVerticle","content":"Metrics of RTDS:"}]}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [date]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:416)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:709)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:500)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:542)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:491)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:410)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:432)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:149)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [2015-06-01 15:56:24,213], tried both date format [dateOptionalTime], and timestamp number with locale []
    at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:620)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:548)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:236)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:406)
    ... 12 more
Caused by: java.lang.IllegalArgumentException: Invalid format: "2015-06-01 15:56:24,213" is malformed at " 15:56:24,213"
    at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:754)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:614)
    ... 15 more
```

logstash version: 1.4.2
elasticsearch version: 1.4.5
</description><key id="83373179">11441</key><summary>Failed to parse date field with logstash.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abcfy2</reporter><labels /><created>2015-06-01T08:04:14Z</created><updated>2015-06-01T08:12:00Z</updated><resolved>2015-06-01T08:11:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-01T08:11:59Z" id="107355014">Your `date` field does not match date format defined in the mapping.

This is an usage error not a bug so I'm closing this "issue".

Please discuss it on discuss.elastic.co. We can definitely help there. I'd recommend discussing it in the logstash room as you have more likely to adapt your logstash configuration file.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail shard if search execution uncovers corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11440</link><project id="" key="" /><description>If, as part of the search execution, a corruption is uncovered, we should fail the shard
relates to #11419
</description><key id="83242150">11440</key><summary>Fail shard if search execution uncovers corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-01T00:38:02Z</created><updated>2015-06-02T19:03:00Z</updated><resolved>2015-06-02T17:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-01T01:03:55Z" id="107271722">+1. Its never ideal for it to go down in this way, but we should do the right thing when it happens. 
</comment><comment author="bleskes" created="2015-06-01T07:15:39Z" id="107336482">LGTM. Can't figure out why we weren't doing it until now :)
</comment><comment author="s1monw" created="2015-06-01T08:07:22Z" id="107352386">LGTM too
</comment><comment author="kimchy" created="2015-06-02T17:01:06Z" id="108015461">pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index Corrupted when upgrading from Java7u67 to Java7u80</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11439</link><project id="" key="" /><description>Hi guys,

I decided to open an issue here to see if this might be an issue that is affecting more people or just us. Everything is described here: [https://discuss.elastic.co/t/how-safe-is-java7-update-80-with-es-1-4-x/1380](https://discuss.elastic.co/t/how-safe-is-java7-update-80-with-es-1-4-x/1380) but I'll reproduce the text anyway for your convenience.

I'm planning on upgrading from Java7 u67 to u80, due to the fix for the upcoming leap second.

Today just tried shutting down all the nodes (3 in total, 2 with data, 1 just to route searches) and upgrading one of the data nodes to JRE7u80 (this was the only online node, just to be safe). It came up with index corrupted errors (note that everything was working flawlessly until the upgrade):

&gt; {"message":"[elastic2-west02] [index_2015_05_v1][3] sending failed shard for [index_2015_05_v1][3], node[Yx684lX3SC6uoD_yT0d5Tw], [P], s[INITIALIZING], indexUUID [UqUZW7iXTqez4M5nReZH1Q], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[index_2015_05_v1][3] failed to fetch index version after copying it over]; nested: CorruptIndexException[[index_2015_05_v1][3] Preexisting corrupted index [corrupted_UXeWPgF7Qn61i2_qi9gkJw] caused by: CorruptIndexException[Invalid fieldsStream maxPointer (file truncated?): maxPointer=102522365, length=40370176]\norg.apache.lucene.index.CorruptIndexException: Invalid fieldsStream maxPointer (file truncated?): maxPointer=102522365, length=40370176\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.(CompressingStoredFieldsReader.java:135)\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)\n\tat org.apache.lucene.index.SegmentCoreReaders.(SegmentCoreReaders.java:133)\n\tat org.apache.lucene.index.SegmentReader.(SegmentReader.java:108)\n\tat org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:145)\n\tat org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:239)\n\tat org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:104)\n\tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:422)\n\tat org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)\n\tat org.apache.lucene.search.SearcherManager.(SearcherManager.java:89)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1569)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:313)\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:710)\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:223)\n\tat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.lang.Thread.run(Unknown Source)\n]; ]]","@version":"1","@timestamp":"2015-05-31T11:57:31.887Z","type":"elasticsearch","host":"x.x.x.x:56107","path":"cluster.action.shard","priority":"WARN","logger_name":"cluster.action.shard","thread":"elasticsearch[elastic2-west02][generic][T#2]","class":"?","file":"?:?","method":"?"}

This is ES 1.4.4 (a couple of months ago we upgraded from 1.3.2 to 1.4.4 without issues) with JRE7u67 (the cluster has been running on this java version since the beginning). The node was upgraded to JRE7u80 and started up isolated from the cluster (all the nodes were down for this test), running on Amazon Linux 3.14.42-31.38.amzn1.x86_64

When we noticed the errors, we shut down the upgraded node and rollbacked the jre upgrade, renamed the data directory (just to have evidence and recreate the data dir from scratch) started the other nodes that were untouched and then the problematic one. Everything went back to JRE7u67, the shards were reallocated successfuly (from the copies in the other data node) and the cluster is now all green and happy... but, without the latest java update.

Anyone with any thoughts/comments?

Thanks!
</description><key id="83092704">11439</key><summary>Index Corrupted when upgrading from Java7u67 to Java7u80</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcelog</reporter><labels /><created>2015-05-31T13:45:35Z</created><updated>2015-06-14T20:18:20Z</updated><resolved>2015-06-02T11:21:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-31T21:43:45Z" id="107249492">I am not sure it is related to your JVM upgrade. 1.5 has a number of fixes for file truncation issues, I think that is the root problem, and you just got unlucky in a way that seems to correlate with 7u80.

By the way, if you are concerned about it, why not use 7u79 instead? Its significantly less scary
(http://www.oracle.com/technetwork/java/javase/2col/7u79-bugfixes-2494165.html) versus looking at the list of changes for 7u80 (http://www.oracle.com/technetwork/java/javase/2col/7u80-bugfixes-2494167.html). Basically just timezone and security fixes.
</comment><comment author="marcelog" created="2015-05-31T21:58:29Z" id="107251454">Thanks for your reply. We didn't notice that the leap second addition was available in 7u79, nice catch! 

In any case, if I understand correctly: you mean that the files are really corrupted in disk and we didn't notice during all these months? What can trigger this behavior? Sounds scary :)

But really, we didn't have any issues and we do a daily snapshot for all indices, daily queries for the whole month of data (1 index per month), and a lot of writes during each day. And not so long ago we restarted the cluster with the upgrade to 1.4.4 and some operating system updates (a few days later after that), and we never had any errors in the logs (we monitor the cluster with a marvel cluster but also we log the messages to a logstash installation that aggregate all the logs for us).

What would you guys recommend to handle this particular situation and avoid the risk of losing data with a java or ES upgrade? How can we be sure that our current index files are ok or not?

Thanks guys!
</comment><comment author="rmuir" created="2015-06-01T00:50:53Z" id="107270951">&gt; In any case, if I understand correctly: you mean that the files are really corrupted in disk and we didn't notice during all these months? What can trigger this behavior? Sounds scary :)

You can get an idea how things can happen by reviewing the 1.5 release notes (https://www.elastic.co/blog/elasticsearch-1-5-0-released). To me the most important change is that files are transferred in an atomic way on recovery (temp files, then renamed at the end). So if stuff goes wrong, you dont have the chance of a half-baked file sitting there. But there are really a ton of fixes to this kind of stuff there.

I honestly doubt you had corrupted data "all along". That is because the check in question was added in ES 1.2.x (https://issues.apache.org/jira/browse/LUCENE-5617), so the check was happening before you upgraded, too. So I think it likely happened before/after the upgrade? Things are getting shut down, Shards are shuffling around or whatever, stuff gets chatty, more chance for stuff to wrong or have an incomplete file with the old code.

&gt; What would you guys recommend to handle this particular situation and avoid the risk of losing data with a java or ES upgrade? How can we be sure that our current index files are ok or not?

My first recommendation is to use the latest elasticsearch version. Really, a lot of folks here have spent a lot of time on preventing these issues: this kind of stuff has matured a lot and will continue to get better in forthcoming releases. So IMO, if you are worried about safety of your data, prioritize it first by using the latest version, and maybe invite some additional risk of bugs or whatever in features, but its a good tradeoff.

As far as checking the index files: you can always explicitly do full checks with the lucene CheckIndex tool (see https://www.found.no/foundation/dive-into-elasticsearch-storage/#fixing-problematic-shards for some instructions, but please please don't ever use the badly-named _fix_ option). So if you want to manually do a check, that is your best option currently. Its also possible for ES to check on startup (`index.shard.check_on_startup`) but this is really slow and resource-hungry.

Going forward its really something you shouldnt need to do: files are fully checked continuously whenever they are "in motion" (e.g. network recovery, merge for indexes being changed, etc). We also have a fair amount of checks happening just on startup, like the one you hit here, which was extended to all files in the index in ES 1.4+. Finally coming in 2.0 there will also be a "reasonable" option `index.shard.check_on_startup=checksum` designed for situations like yours (https://github.com/elastic/elasticsearch/pull/9183), where maybe you want an explicit full check on startup to know your data is ok, after upgrades etc :)

Having an option for an immediate check... its been thrown around before: maybe its a good idea, maybe not. I do have concerns it could could be a trap like `optimize`, leading people to run it across the entire index after every operation or other stuff like that.
</comment><comment author="marcelog" created="2015-06-01T13:44:58Z" id="107483817">That is an amazing reply really, it clarified a lot and I highly appreciate it :) A few final questions then:

1.- Got it. We will try to upgrade to 1.5.2 (latest so far). But just to clarify  this case (and as a final conclusion): you think that this error might just happened while shutting down the node (and not before), and that there is a chance that repeating the process will yield a successful result?

2.- Just out of curiosity: [https://www.elastic.co/guide/en/elasticsearch/hadoop/current/requirements.html] still recommends 7u55. Do you recommend a jvm upgrade to 7u79 or sticking to 7u67 (in our case) is a safer bet? 

3.- Are snapshots stored in a way that can be trusted and safe from corruption? (hardware issues aside). I'm asking this to know if there might be a chance of snapshooting an already corrupted index (i.e: corrupted lucene files), and if there is a chance of losing data in scenarios like this, when using a snapshot to restore data that might have been snapshooted with older ES versions (i.e: older lucene versions).

Thanks again for your time and all the hard work!
</comment><comment author="rmuir" created="2015-06-01T14:03:11Z" id="107499921">&gt; 1.- Got it. We will try to upgrade to 1.5.2 (latest so far). But just to clarify this case (and as a final conclusion): you think that this error might just happened while shutting down the node (and not before), and that there is a chance that repeating the process will yield a successful result?

That is my best guess as to what happened, its the simplest explanation. As far as the chance of it happening, its not clear to me how likely it is, you know if you just got unlucky, but it can happen, which is why folks worked to fix it for 1.5. I definitely think if things are "not moving" when you shut down, you could have probably been just fine and not hit it, so you can at least get lucky.

&gt; 2.- Just out of curiosity: [https://www.elastic.co/guide/en/elasticsearch/hadoop/current/requirements.html] still recommends 7u55. Do you recommend a jvm upgrade to 7u79 or sticking to 7u67 (in our case) is a safer bet?

I don't know any hadoop-specifics of JDK versions (cc @costin). The recommendations page does say "or higher". I think the motivation there was just to avoid index corruption bugs that can happen in versions before 7u55, so 7u79 should be fine.

&gt; 3.- Are snapshots stored in a way that can be trusted and safe from corruption? (hardware issues aside). I'm asking this to know if there might be a chance of snapshooting an already corrupted index (i.e: corrupted lucene files), and if there is a chance of losing data in scenarios like this, when using a snapshot to restore data that might have been snapshooted with older ES versions (i.e: older lucene versions)

Its a good question. See https://github.com/elastic/elasticsearch/issues/5593 where this feature was added for elasticsearch 1.4
</comment><comment author="costin" created="2015-06-01T14:21:29Z" id="107514047">&gt; Just out of curiosity: [https://www.elastic.co/guide/en/elasticsearch/hadoop/current/requirements.html] still recommends 7u55

That's because the documentation belongs to the es-hadoop 2.0.2 and at the time, that was the latest safe JDK version. Hence why it also states 'or higher' - as time goes buy, new releases are published.
The beta documentation has a slightly different [wording](https://www.elastic.co/guide/en/elasticsearch/hadoop/2.1.Beta/requirements.html) pointing to the Elasticsearch [Matrix](https://www.elastic.co/subscriptions/matrix).

Note that most Hadoop distros use JDK 6 or old JDK 7 versions.

Hope this helps,
</comment><comment author="marcelog" created="2015-06-02T11:21:21Z" id="107918824">Thanks guys, will close this one then, and will try to schedule an update to the latest 1.5.x (1.4.4 has been working amazingly well for us, but due to this "episode" we'd like to  avoid any kind of possible "bad luck", as suggested) and only then bump up the java version to 7u79.

I really appreciate all the insights, and useful links to resources :)

Best!
</comment><comment author="marcelog" created="2015-06-14T11:55:02Z" id="111816134">Just wanted to report that we upgraded (not in a rolling upgrade, but shutting down the cluster) to ES 1.5.2 and Java 7u80 and everything went smoothly. Thanks again for your time and help.

Cheers.
</comment><comment author="clintongormley" created="2015-06-14T20:18:20Z" id="111872910">thanks for letting us know @marcelog 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delayed allocation on node leave</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11438</link><project id="" key="" /><description>Delay allocation of unassigned shards when node leaves for a specific period (defaults to 5m) to give it a chance to come back and not cause excessive allocations.

This new behavior, specifically with the default value, means that when a node leaves the cluster now, the shards assigned to it will only be allocated back to the rest of the cluster after the specified duration.

The number of delayed unassigned shards can be retrieved using the cluster health API.

The setting to control the duration is `cluster.routing.allocation.delay_unassigned_allocation.duration`, and its dynamically updatable using the cluster update settings API (only applicable to master nodes).

The reroute cluster command now also accepts an optional `delayed_duration` parameter, when set, it will override the duration for this reroute operation. This can be handy, for example, to set it to 0 and get the current delayed shards to be assigned.

The concept of a node key is also introduced, allowing to specify what represents a node in a "cross restart" manner. The potential values for the setting `cluster.routing.allocation.delay_unassigned_allocation.node_key` are `name` (node name), `id` (node id, note, randomly generated on node startup), `host_address` (the host ip address)
, `host_name` (the host name), `transport_addresss` (the transport address, ip + port of the node). Defaults to `transport_address`.

Closes https://github.com/elastic/elasticsearch/issues/7288
</description><key id="83012953">11438</key><summary>Delayed allocation on node leave</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2015-05-31T05:57:49Z</created><updated>2015-06-12T15:46:14Z</updated><resolved>2015-06-12T15:46:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-05-31T07:03:00Z" id="107135025">Minor comments and thoughts. I'm excited to see this get in!
</comment><comment author="clintongormley" created="2015-05-31T11:02:26Z" id="107156346">If a node dies unexpectedly, does the admin have a way to override the `duration`, or is it just a matter of resetting the `duration` temporarily?
</comment><comment author="kimchy" created="2015-05-31T18:26:44Z" id="107232268">pushed a change for the first review.

&gt; If a node dies unexpectedly, does the admin have a way to override the duration, or is it just a matter of resetting the duration temporarily?

Yea, the duration setting needs to be updated temporary to unblock it, we could have a flag in reroute as well to ignore it if we want.

I also want to raise the question of the default for duration. I went with 5 minutes, but I would love to get input into what the best default would be. It needs to cover the most common restart times of a node (+conceptually a vm)
</comment><comment author="kimchy" created="2015-06-01T00:21:21Z" id="107262098">@clintongormley I added the ability to set the `delayed_duration` in the cluster reroute command as well, so people can more easily set it without going through changing the setting value which is more permanent.
</comment><comment author="s1monw" created="2015-06-01T10:14:30Z" id="107388667">Thanks shay for opening this PR, this has been a pain for a long time. I looked at the implementation and there are a couple of things that concern me:
- having a time component directly attached to the shard allocation makes me wanna run away - I am really concerned about this producing very weird (maybe due to misconfiguration what have you) effects where people still see the problems we are trying to fix here. 
- binding a shard to something like a transport address, node ids, ports, $some_other_random_value is what we are trying to get rid of and not what we should add. We essentially have the right information already to do the right thing if a node comes back.
- adding such a large amount of state-full code to a class that I'd love to get rid of essentially is the wrong path forward IMO. GatewayAllocator owns too much already and I think we should rather make use of our already existing abstraction.

Based on this I'd like to propose a different way of preventing _ze allocation dance_ which might allow to separate out the time component as well. What I have in mind is something like a notion of _stickiness_ where a shard can only be allocated on a node that has the shards data locally AND a version of the shard greater or equal to the `ShardRouting` it used to have before it was moved to _UNASSIGNED_. Shards that are created due to replica expansion or due to index creation can always be allocated. 
Ideally we will be able to have a simple allocation decider that prevents us from allocating the shards. 

In a second step we can add some mechanism that removes the _stickiness_ from a shard after _time_, _number of nodes in the cluster_, _number of unallocated shards_, _user interaction_, _$yoursgoeshere_ 
</comment><comment author="kimchy" created="2015-06-12T15:46:13Z" id="111532379">we brainstormed about it a bit, and there is a way to make it simpler, basically keeping on the shard routing when and why a shard moved to unsaying, and use that to decide when to force allocate it (this only applies to replicas basically) when there is no copy found within the cluster. Will open subsequent pull request for the new logic
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Require units for time and byte-sized settings, take 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11437</link><project id="" key="" /><description>Here's a new PR to close #7616 and #10888, replacing #7633.

This includes the same changes from there, but also tries to upgrade byte size and time settings "live" by adding the default unit when it's missing.

I tried to gather all "known" index settings (see MetaDataIndexUpgradeService.java): I recorded all byte size or time settings named index.\* (is that too restrictive?) when running all tests.  I add the units in MetaDataIndexUpgraderService if the index was created before 2.0.

For cluster settings, I pulled all settings from ClusterDynamicSettingsModule ... is that right?  (See MetaData.java)  Or can other settings "become" cluster settings?  I add units when restoring a snapshot, and in MetaStateService.loadGlobalState ... I'm really not sure if these are the best places to "intercept", e.g. I now do it "always", not just for pre-2.0 cluster states.

I could very easily be missing settings (e.g. if there is a setting not tested by tests), so for "insurance" I also put the "kill switch" to (settings_require_units node level setting) so you can fully disable requiring units, but I didn't document it.

I fixed back-compat index generation to add some unit-less (one byte sized, one time) index and cluster settings, and confirmed on upgrade and on restoring the snapshot the default units are inserted and you get logged warnings like this:

```
  [2015-05-30 05:28:21,016][WARN ][cluster.metadata         ] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; now adding default units (b)
  [2015-05-30 05:28:21,016][WARN ][cluster.metadata         ] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; now adding default units (ms)
  [2015-05-30 05:28:21,016][DEBUG][cluster.service          ] [node_t0] processing [restore_snapshot[test_1]]: execute
  [2015-05-30 05:28:21,016][WARN ][cluster.metadata         ] [node_t0] byte-sized index setting [index.merge.policy.max_merged_segment] with value [5368709120] is missing units; now adding default units (b)
  [2015-05-30 05:28:21,017][WARN ][cluster.metadata         ] [node_t0] time index setting [index.gc_deletes] with value [60000] is missing units; now adding default units (ms)
```

I still have a few nocommits about other settings leniency ...
</description><key id="82949294">11437</key><summary>Require units for time and byte-sized settings, take 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-30T23:59:15Z</created><updated>2015-06-25T11:10:57Z</updated><resolved>2015-06-04T18:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-01T17:01:59Z" id="107639067">I left some comments, but i have some concerns of the whole approach. Is "parsing" input from users seriously not separable from "deserialization"? In other words, if there is a time setting today interpreted as milliseconds, and i provide "2h", what gets stored? what gets displayed back to users?

I would love for the parsing here to be transparent. If i provide "2h", i think we should serialize 7200 (or 7200ms is also fine), and display "7200ms". When parsing new input from the user, i dont understand why we should be "lenient" about that... why not just require it and end the confusion?

As far as "deserialization", however it currently looks today, we need to be able to read it. If someone put refresh_interval = 5 and its 5ms, there is no leniency really, it became 5ms the moment we returned "200 OK". So this is a different thing, its just backwards compatibility and something we gotta separately make sure works.
</comment><comment author="rmuir" created="2015-06-01T17:06:59Z" id="107641291">there is also an off-by-thousand bug in my comment. That is exactly why this pr is important :)
</comment><comment author="mikemccand" created="2015-06-03T09:14:08Z" id="108261624">I've folded in all feedback (thanks @rmuir!).

&gt; This method just made my head explode

I'm gonna have to open a follow-on for the boolean value leniency.  Unfortunately, Booleans.parseBooleanExact is only used in 3 places, but the lenient Booleans.parseBoolean is used in 100s of places...

&gt; Is "parsing" input from users seriously not separable from "deserialization"?

We don't really have a strong separation here...

We do check that the incoming settings "are valid", e.g. via cluster/indices settings update apis where we set the Validator for each setting in Cluster/IndexDynamicSettingsModule.java, but then add that exact user's string to the Settings object, and serialize these key/values as a simple Map&lt;String,String&gt; as the user originally sent it.  When we deserialize, we use Settings.Builder and add the strings values back using Settings.Builder.

But e.g. when loading node settings, which cannot (easily?) have a separation of parsing/serializing, it's not until a setting needs to be used, when some part of the code calls Settings.getAsTime/ByteSize, that we validate that it is in fact a valid time or byte-size.

I don't think I can fix all that here ... in this change (which is already huge enough) I'm just trying to find the places where we pull settings from serialized state and deserialize (MetaDataIndexUpgradeService.upgradeIndexMetaData for index settings, and MetaStateService.loadGlobalState and also in RestoreService.restoreSnapshot for cluster settings), and do the "upgrade known byte size/time settings" then.
</comment><comment author="s1monw" created="2015-06-03T09:49:26Z" id="108278037">left two smallish comments - Looks awesome mike!
</comment><comment author="mikemccand" created="2015-06-04T08:15:45Z" id="108779067">OK I folded in feedback from @s1monw (thanks!) and regen'd all back compat indices so they all have example "missing units" time and byte-size cluster and index settings.
</comment><comment author="s1monw" created="2015-06-04T10:22:23Z" id="108828474">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequest.java</file><file>src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>src/main/java/org/elasticsearch/common/Booleans.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/common/unit/ByteSizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>src/main/java/org/elasticsearch/common/unit/MemorySizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/MetaStateService.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/query/TimeoutParseElement.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/SimpleClusterStateTests.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java</file><file>src/test/java/org/elasticsearch/common/unit/ByteSizeValueTests.java</file><file>src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTest.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>src/test/java/org/elasticsearch/network/DirectBufferNetworkTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/SearchScrollTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/stresstest/get/GetStressTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java</file><file>src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file><file>src/test/java/org/elasticsearch/versioning/SimpleVersioningTests.java</file></files><comments><comment>Merge pull request #11437 from mikemccand/require_units</comment></comments></commit></commits></item><item><title>Fix `_parent.type` validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11436</link><project id="" key="" /><description>A `_parent` field can only point to a type that doesn't exist yet.
The validation that checks this relied on have all mappings in the
MapperService. The issue is that this check is performed on the
elected master node and it may not have the IndexService at all
to perform this check. In that case it creates a temporary IndexService and
MapperService to perform mapping validation, but only the mappings
that are part of the put index call are created, not the already existing mappings.
Because of that the `_parent` field validation can't be performed.

By changing the validation to rely on the cluster state's IndexMetaData instead
we can get around the issue with the IndexService/MapperService on the elected
master node. The IndexMetaData always holds the MappingMetaData instances
for all types.
</description><key id="82940141">11436</key><summary>Fix `_parent.type` validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-30T23:12:04Z</created><updated>2015-05-31T12:05:37Z</updated><resolved>2015-05-31T11:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-31T08:38:20Z" id="107145898">The change looks good. However, when I read:

&gt; but only the mappings that are part of the put index call are created, not the already existing mappings.

I'm wondering that this might be an issue for mapping validation in general, eg. if we want to enforce that different types on the same index have consistent field types. So maybe a better fix would be to load all mappings in this temporary index? cc @rjernst 
</comment><comment author="martijnvg" created="2015-05-31T09:19:51Z" id="107149229">@jpountz I thought about this too, but it never was an issue before because the there was no cross mapping/type validation. The `_parent` field is the first feature to do so. I didn't go for it because it adds an additional parsing cost that no other fields benefit from yet. 
</comment><comment author="jpountz" created="2015-05-31T09:22:29Z" id="107149300">I think we can merge your change as going back would be easy. Just wanted to make @rjernst aware of this. :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file></files><comments><comment>Merge pull request #11436 from martijnvg/parent-child/bug/_parent_field_validation</comment></comments></commit></commits></item><item><title>Reindexing losing data?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11435</link><project id="" key="" /><description>Hi,

I've been doing some playing around with ES for the purposes of introducing it into our organization for searching audit data. Part of my adventure is data modelling and playing with mappings.

I had a small index, called "audit_v1", which has 43,754 documents. I created a second index called "audit_v2" and did a scan &amp; scroll and bulk create the 43,754 documents 500 at a time into the new index. I've done this 5 times now, and every single time I'm seeing not only fewer number of records showing up in the audit_v2 index, but it's the same number ever single time. This is despite the _bulk api not reporting any errors in the response. From what I can tell all the documents _should_ be there, but apparently they aren't. 

Is this a legit bug, or is it possible I'm misunderstanding exactly what [index]/_count returns?

Sorry for the trivial question, but I can't find a clear answer online. I'm using v1.5.0.

Thanks
</description><key id="82912835">11435</key><summary>Reindexing losing data?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanbaldwin</reporter><labels><label>:Bulk</label><label>:Search</label><label>feedback_needed</label></labels><created>2015-05-30T21:04:39Z</created><updated>2015-06-03T09:35:34Z</updated><resolved>2015-06-02T23:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-31T11:30:44Z" id="107159408">Hi @ryanbaldwin 

Could you tell us more about your mappings, and exactly how you do the bulk and scan/scroll.  Also, in scan/scroll, could you check for shard failures, and check your logs to see if any exceptions are reported.

See https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107035786 for a similar issue.

Also, could you give us the output of `$JAVA_HOME/bin/java -version`
</comment><comment author="s1monw" created="2015-05-31T12:34:15Z" id="107171825">are you calling refresh before you do the count call? How many docs are you missing, how is the index created? Can you provide more infos?
</comment><comment author="ryanbaldwin" created="2015-05-31T15:28:15Z" id="107206106">Hey all. I'm not at home right now but will provide a detailed explanation when I'm able to. May be later tonight, or tomorrow at the latest. For now I'll go by memory while fat thumbing on my phone. 

High level answers: 
- don't know what version of Java I'm using off the top of my head. I'm using the elasticsearch docker repo, which I _think_ uses java 7.
- don't believe there were any shard failures. The bulk API didn't report any errors on any of the create responses. Though I haven't checked ES logs. Marvel is reporting all shards as good. 
- didn't call refresh (though that's a great suggestion) but I wasn't querying _count immediately afterwards either. My understanding is by default everything is refreshed about every 0-5 seconds. Even if I let the index sit for a bit (say, 10 minutes), count would still report fewer docs than the original index. 
- The number forever rested at the lower number. The original index had 44,754. When I migrated it with scan size of 200 (x 5 shards = 1000  docs per bulk post), the migrated index was 44,709. The odd thing is if I did this multiple times, using that same scan size, it was ALWAYS 44,709. I decided to drop the scan size to 100, thinking perhaps I was overloading or something, and migrated again. This time the count dropped to 43,665 (I think). Now that I'm looking at it, this seems like a pattern. It appears as though I'm missing about 1 document for every scan/bulk cycle. 
- the scan/bulk is done using a simple app I wrote in clojure. In a nutshell:
1. scan with the scan size of x on the old index, using match_all.
2. Using the scoll_id returned by 1, hit the scroll API /_search/scroll(?). I then create "create" request for each document returned by scan, interleaving the "create" with each doc I'm migrating. The "create" contains the target index, type, and existing ID for the respective doc. 

3, after the bulk call I repeat step 2 using the scroll id returned by the previous scroll call. Wash, rinse, repeat until hits on the call to scroll is 0.

Like I said, It seems as though I'm missing 1 doc for each scan/bulk cycle. Perhaps it's something in my script, but after specifying the scan size in the origian scan call, I don't rely on the number ever again. I simply iterate over every document. 

I can provide more details later, such as an excerpt of the bulk calls, etc. 

For now: thoughts?
- ryan.

On Sun, May 31, 2015 at 8:35 AM, Simon Willnauer notifications@github.com
wrote:

&gt; ## are you calling refresh before you do the count call? How many docs are you missing, how is the index created? Can you provide more infos?
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/11435#issuecomment-107171825
</comment><comment author="bleskes" created="2015-05-31T16:18:24Z" id="107215356">Thx Ryan for the details. Quick question - do you use parent &amp; child documents or custom routing?
</comment><comment author="ryanbaldwin" created="2015-05-31T16:47:00Z" id="107222791">Negative. Setup is pretty stock. Default 5 shards + 1 replica, and however they get routed is how they get routed. That said I AM using a dynamic mapping template on the target index, but it's mostly just setting 99% of the incoming string values to not_analyzed, since this is explicit audit data and not something that really requires full text search. 

As a side note, here's some possible useful information about the topology:

I have 2 ES servers, each in their own docker container, each configured identically, and each running on the same host. Each ES server has its own persistent logs/data volumes on the host (ie they are not sharing the same logs/data directories on the host). Sitting in front is an nginx doing simple round robin balancing between the two. The clojure app is doing everything through nginx, same with manual queries I run via Sense.

As far as I'm aware that topology with docker should roughly approximate (at a minimum) what 2 separate instances on two separate hosts in a network should look like. 
- ryan.

On Sun, May 31, 2015 at 12:19 PM, Boaz Leskes notifications@github.com
wrote:

&gt; ## Thx Ryan for the details. Quick question - do you use parent &amp; child documents or custom routing?
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/11435#issuecomment-107215356
</comment><comment author="ryanbaldwin" created="2015-05-31T16:47:45Z" id="107222817">Also - no parent child docs. Just 45k documents, each one an audit event, and each one completely independent.
- ryan.

On Sun, May 31, 2015 at 12:46 PM, ryan baldwin ryanbaldwin@gmail.com
wrote:

&gt; Negative. Setup is pretty stock. Default 5 shards + 1 replica, and however they get routed is how they get routed. That said I AM using a dynamic mapping template on the target index, but it's mostly just setting 99% of the incoming string values to not_analyzed, since this is explicit audit data and not something that really requires full text search. 
&gt; As a side note, here's some possible useful information about the topology:
&gt; I have 2 ES servers, each in their own docker container, each configured identically, and each running on the same host. Each ES server has its own persistent logs/data volumes on the host (ie they are not sharing the same logs/data directories on the host). Sitting in front is an nginx doing simple round robin balancing between the two. The clojure app is doing everything through nginx, same with manual queries I run via Sense.
&gt; As far as I'm aware that topology with docker should roughly approximate (at a minimum) what 2 separate instances on two separate hosts in a network should look like. 
&gt; - ryan.
&gt;   On Sun, May 31, 2015 at 12:19 PM, Boaz Leskes notifications@github.com
&gt;   wrote:
&gt;   &gt; Thx Ryan for the details. Quick question - do you use parent &amp; child documents or custom routing?
&gt;   &gt; ---
&gt;   &gt; Reply to this email directly or view it on GitHub:
&gt;   &gt; https://github.com/elastic/elasticsearch/issues/11435#issuecomment-107215356
</comment><comment author="clintongormley" created="2015-06-02T11:56:55Z" id="107927613">Hi @ryanbaldwin 

&gt; - The number forever rested at the lower number. The original index had 44,754. When I migrated it with scan size of 200 (x 5 shards = 1000  docs per bulk post), the migrated index was 44,709. The odd thing is if I did this multiple times, using that same scan size, it was ALWAYS 44,709. I decided to drop the scan size to 100, thinking perhaps I was overloading or something, and migrated again. This time the count dropped to 43,665 (I think). Now that I'm looking at it, this seems like a pattern. It appears as though I'm missing about 1 document for every scan/bulk cycle. 

This sounds a lot like a bug in your code, perhaps:
- not calling refresh (or waiting) on the destination index before retrieving the count
- an off by one error on every scroll request
- not collecting the last tranche of hits from scroll
- not performing the final bulk write

An easy way to test this would be to use a module known to work.  If you're familiar with Perl, you could install the Search::Elasticsearch module (see https://metacpan.org/pod/Search::Elasticsearch)  and run the following script (updating the index names for your local setup):

```
#! /usr/bin/env perl

use strict;
use warnings;
use Search::Elasticsearch;

my $src  = 'source_index';
my $dest = 'dest_index';
my $node = 'localhost:9200';

my $e = Search::Elasticsearch-&gt;new( nodes =&gt; $node );

$e-&gt;indices-&gt;delete(index =&gt; $dest, ignore =&gt; 404);

$e-&gt;bulk_helper( index =&gt; $dest, verbose =&gt; 1 )
  -&gt;reindex( source =&gt; { index =&gt; $src });

$e-&gt;indices-&gt;refresh( index =&gt; $dest );

print "\n\nNew index count: ". $e-&gt;count( index =&gt; $dest )-&gt;{count}."\n"
```
</comment><comment author="ryanbaldwin" created="2015-06-02T23:53:18Z" id="108134394">Ugh. Clinton. You are indeed correct. I made the classic _bulk error: I did not append a "\n" to the final document body. Hence the n*1 documents missing.

Very sorry, but thank you for your help.

Also - Clinton - I must congratulate you on the ElasticSearch: The Definitive Guide book. This is, by far, the best tech book I've read in over a decade. Extremely easy to understand, an excellent voice, and absolute gold on every page (including the "don't forget to put a \n after the last document when using the bulk api!", which I obviously, promptly, forgot). Huge kudos to you and Zach. 

Thanks for your help, and again, my apologies for the false alarm.
</comment><comment author="clintongormley" created="2015-06-03T09:35:34Z" id="108266903">kind words @ryanbaldwin - thank you :)  /cc @polyfractal 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Release search contexts after failed dfs or query phase for dfs queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11434</link><project id="" key="" /><description>When the dfs phase runs a SearchContext is created on each node that has a shard
for this query. When the query phase (or query and fetch phase) failed that SearchContext
was released only if the query was actually executed on the node. If for example
the query was rejected because the thread pool queue was full then the search context
was not released.
This commit adds a dedicated call for releasing the SearchContext in this case.

In addition, set the docIdsToLoad to null in case the fetch phase failed, otherwise
search contexts might not be released in releaseIrrelevantSearchContexts.

closes #11400
</description><key id="82846484">11434</key><summary>Release search contexts after failed dfs or query phase for dfs queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-30T15:54:27Z</created><updated>2015-06-08T00:26:20Z</updated><resolved>2015-06-01T11:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-30T20:13:27Z" id="107080616">this looks great! I left one minor comment on the test - LGTM otherwise
</comment><comment author="brwe" created="2015-05-31T14:08:42Z" id="107191082">thanks for the review! addressed all comments. want to take another look?
</comment><comment author="s1monw" created="2015-06-01T10:15:36Z" id="107389038">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/search/SearchWithRejectionsTests.java</file></files><comments><comment>Merge pull request #11434 from brwe/open-search-contexts</comment></comments></commit></commits></item><item><title>Better error messages when mlockall fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11433</link><project id="" key="" /><description>For example on OS X:

```
[2015-05-30 11:22:14,095][WARN ][bootstrap                ] Unable to lock JVM Memory: error=78,reason=Function not implemented. This can result in part of the JVM being swapped out
```

On Linux:

```
[2015-05-30 11:23:53,596][WARN ][bootstrap                ] Unable to lock JVM Memory: error=12,reason=Cannot allocate memory. This can result in part of the JVM being swapped out
[2015-05-30 11:23:53,597][WARN ][bootstrap                ] Increase RLIMIT_MEMLOCK (ulimit). soft limit:65536, hard limit:65536
```
</description><key id="82838985">11433</key><summary>Better error messages when mlockall fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-30T15:25:20Z</created><updated>2015-06-02T17:43:50Z</updated><resolved>2015-06-01T01:16:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-30T16:24:44Z" id="107059475">I tried to improve messages more for the Linux case, to give some instructions on how to fix it:

```
[2015-05-30 12:22:53,792][WARN ][bootstrap                ] Unable to lock JVM Memory: error=12,reason=Cannot allocate memory. This can result in part of the JVM being swapped out.
[2015-05-30 12:22:53,792][WARN ][bootstrap                ] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
[2015-05-30 12:22:53,793][WARN ][bootstrap                ] These can be adjusted by modifying /etc/security/limits.conf, for example: 
    # allow user 'esuser' mlockall
    esuser soft memlock unlimited
    esuser hard memlock unlimited
[2015-05-30 12:22:53,793][WARN ][bootstrap                ] If you are logged in interactively, you will have to re-login for the new limits to take effect.
```
</comment><comment author="s1monw" created="2015-05-30T20:10:13Z" id="107080158">cool stuff @rmuir this is a good improvement +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java</file><file>src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file></files><comments><comment>Merge pull request #11433 from rmuir/cleanup_mlockall</comment></comments></commit></commits></item><item><title>Explore parent/child self referential support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11432</link><project id="" key="" /><description>Now that PR #6511 has been merged, self referential parent/child support is explicitly disabled. Self referential parent/child was never explicitly supported. It wasn't documented and no tests existed for this parent/child use case. A long time ago it used to work, but since 0.90 self referential parent/child has not been working correctly. See #7357.

Right now we need to know what is a parent document and what is a child document. We do this via the `_type` field. The issue with self referential parent child is that we use the same type for both parent and child document and therefore we can't distinguish between parent and child document.

I think we can get around this issue by using the fact if a document has a `_parent` field instead of the `type` field for identification. I think this works okay for single level relationships (a parent type and a child type), but for multi level relationships (a parent type, child type and grandchild type) I don't think this works out as what is requested in #8100.
</description><key id="82751959">11432</key><summary>Explore parent/child self referential support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-05-30T08:40:21Z</created><updated>2017-03-13T15:29:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-05T09:58:11Z" id="109242455">We discussed this in FixItFriday.  This issue has only been brought up twice in two years, so it is not a widely requested feature.  The added complexity of supporting self-referential parent-child seems to outweigh the benefit.  

For now, let's leave this model unsupported.  We can always revisit this later on if needed.
</comment><comment author="NickCraver" created="2015-07-13T15:56:03Z" id="120977023">For what it's worth - we _want_ to use this at Stack Exchange. In our primary database Questions and Answers are the same type: `Post`, where the `PostTypeId` column differentiates.

Having self-referential types lets us store and search Posts the same way we do in the database. It dramatically simplifies the data model as well as allows us to do much more atomic indexing of the original content. For example when an answer changes we want to re-index _only_ the answer, not a large document - okay cool, parent/child in itself solves this case. 

However, when executing every search we'll have to search all fields like Post.Body twice, eating much more CPU on the ElasticSearch side. It's more of an efficiency problem for our 99% search use case than anything else here. We're of course very open to any solution that works, but as-is the disabling of self-ref nets us an approximate doubling of CPU usage on our primary user-facing ElasticSearch cluster.
</comment><comment author="clintongormley" created="2015-07-13T16:12:11Z" id="120983213">OK - I'll reopen this for future consideration.  Today this is difficult to do, hence the reluctance, so I can't promise any quick answer :)
</comment><comment author="martijnvg" created="2015-07-13T17:10:39Z" id="120995615">&gt; We're of course very open to any solution that works, but as-is the disabling of self-ref nets us an approximate doubling of CPU usage on our primary user-facing ElasticSearch cluster.

Just out of curiosity: is this based on a 1.x release or did you try out parent/child snapshot from master? The query execution should be improved significantly in master as has_child/has_parent is going to perform joins of document likely to be a match and it isn't going to try to join all children back to parents (or visa versa). 
</comment><comment author="deinspanjer" created="2015-10-12T23:22:28Z" id="147547468">I just wanted to toss another potential use case in here.
I'm working with some reddit submission and comment data right now.  Like many comment systems, comments can be replies to other comments.  While it is open for discussion whether you want the top level item to be of the same type as the comments, even if you are only dealing with the case of replies to other comments, this kind of self referential parent/child relationship is useful.  If you model it as a nested object instead, your queries are now all bound to the single top level object, and I'm not sure how many use cases might be restricted by that.
</comment><comment author="stephencelis" created="2015-10-26T19:20:24Z" id="151255376">Any chance we can at the very least get an error when this is attempted? Spent more time than I would have liked trying to set up an arbitrary tree data structure, confused when I kept getting empty results for queries that should have worked.
</comment><comment author="clintongormley" created="2015-10-29T19:20:36Z" id="152292762">@stephencelis In 2.0 you now get this: 

```
The [_parent.type] option can't point to the same type
```
</comment><comment author="lbornov2" created="2015-12-05T22:54:03Z" id="162255100">This is a pretty common use-case in relational databases (self-references - FK's that reference the same table).

Curious as to how ElasticSearch recommends handling that type of mapping..
</comment><comment author="pipesnipe" created="2016-01-08T15:16:35Z" id="170028771">+1 for alternative solution... We have huge structural data (folders in folders in folders). Theoretically infinite... 
How do you filter out results where a folder in its path has status hidden?

Now we have to do a seperate sql query to our database for this, and it's killing our performance
</comment><comment author="jconlon" created="2016-04-01T20:28:57Z" id="204557299">A self referential parent/child would be the solution for a use-case I have for a location service where locations are parents or children of other locations.   _Note: I would rather use geolocations for this, but in many indoor cases this is not available._
</comment><comment author="dularion" created="2016-07-05T19:56:33Z" id="230584932">+1, a use case for our project would be a tree structure of companies, having sub-companies in an endless structure. We would then search for an attribute recursively throughout children, and output the entire parent structure for the found element, so as to not loose the hierarchy structure even if the hit was nested deep. 
</comment><comment author="gerson721" created="2017-02-01T12:56:47Z" id="276650908">+1, our project has a tree structure of products. Think of a finished product which is made of components/ingredients which are products themselves. We want to also recursively search products (finished products) of a certain type which have some sort of a tick of approval from a governing body.</comment><comment author="blfrantz" created="2017-03-13T15:29:55Z" id="286142771">+1, our project also has object hierarchy as a core concept, with the possibility for self-referential relationships.  We designed our model on what we knew worked in SQL (our main DB), and are trying to copy our data into Elastic to leverage its awesome search capabilities.  It'd be nice to properly reflect this in Elasticsearch without having to resort to lots of nesting.  Mongo seems to be coming around to the fact that relational models do in fact make sense in a lot of cases (despite what they used to claim) and are adding more support for this kind of thing.  It'd be nice to see more attention to this in Elasticsearch as well.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for applying setting filters when displaying repository settings (Backport to 1.6)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11431</link><project id="" key="" /><description>Currently all settings that were specified during repository creation are displayed. This commit enables  plugins such as cloud-aws to filter out sensitive settings from the repository.

Closes #11265
</description><key id="82626213">11431</key><summary>Add support for applying setting filters when displaying repository settings (Backport to 1.6)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label></labels><created>2015-05-29T23:24:15Z</created><updated>2015-06-09T11:55:40Z</updated><resolved>2015-05-30T00:07:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-29T23:34:39Z" id="106958656">LGTM, left some super minor comments about docs
</comment><comment author="imotov" created="2015-05-30T00:07:33Z" id="106965232">Merged b76c1595f41b5144729d32dcc2dee1402940fce9
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rescore windowing not repeatable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11430</link><project id="" key="" /><description>I have a use case where I have a native custom scoring function that ignores the native ES score (vector space score) completely and replaces the native ES score with its own.  I am interested in returning both the native ES score and my custom score.  I haven't found a way to append 2 scores to the returned document so I went with doing a multi-search with 2 queries, the first using the native score and the second doing the same query as the first, but using a rescore that calls my custom scoring function.  I set the size:10 in both queries and the window_size:10 in the rescore section.

The bug (or confusing thing) I see is that I do not get back the same documents for both queries.  Since I set the size to 10 and window to 10 and use the exact same query in both the queries I expected to get the exact same documents back, just with my custom score instead of the native ES score.  Indeed the documentation describes the rescore as "executes a second query only on the Top-K results returned by the query and post_filter phases" which, to me at least, says it should only execute the custom scoring on the first 10 results in the original query.

So is this behavior expected?  Is there a way to append a second score to the document in a native custom_scoring function (which would be all I need but not fix this if it is a bug)?
</description><key id="82598772">11430</key><summary>Rescore windowing not repeatable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jostheim</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2015-05-29T21:42:31Z</created><updated>2015-05-31T10:52:48Z</updated><resolved>2015-05-31T04:18:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-30T08:27:25Z" id="107008766">Hi @jostheim 

Could you upload a small recreation of what you're doing and the results you're seeing. It'd be easier to investigate.

thanks
</comment><comment author="jostheim" created="2015-05-31T04:18:36Z" id="107124007">I solved my problem by using scriptfields, it is a better, faster more scaleable solution for me.  Though I have to say I think something is still not right with rescoring.
</comment><comment author="clintongormley" created="2015-05-31T10:52:48Z" id="107155973">@jostheim it would still be helpful to have more info about what wasn't working for you, so that we can investigate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>.prepareSearch("index").setTypes("mytype") does not set types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11429</link><project id="" key="" /><description>Hi,
When I print SearchRequestBuilder to see json the "types" : [ ]
Is this normal ?
</description><key id="82526785">11429</key><summary>.prepareSearch("index").setTypes("mytype") does not set types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rookie7799</reporter><labels><label>:Java API</label><label>bug</label></labels><created>2015-05-29T18:03:30Z</created><updated>2015-06-02T08:44:56Z</updated><resolved>2015-06-02T08:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T18:48:55Z" id="106901690">Hi @rookie7799 

Please can you ask questions like this on the forum instead https://discuss.elastic.co/c/elasticsearch

thanks
</comment><comment author="dadoonet" created="2015-05-29T20:32:13Z" id="106927850">@clintongormley Actually, it looks like a bug to me. 

``` java
SearchRequestBuilder builder = client().prepareSearch("index").setTypes("type");
System.out.println("builder = " + builder);
```

Gives

```
builder = { }
```

I think this change did that: https://github.com/elastic/elasticsearch/pull/9944/files#diff-8e501ed49a549876267b54efd2347077R170

I pinged @javanna about it. Let's see what he thinks about it but we might need to reopen this issue although it might be a duplicate of #9962.
</comment><comment author="clintongormley" created="2015-05-29T20:37:28Z" id="106928935">thanks for spotting @dadoonet - reopening
</comment><comment author="javanna" created="2015-05-30T06:49:27Z" id="106997612">The types do get set, but you don't see them as the output of the `toString`, simply because the toString only prints out the content of what would be the request body, the actual search request and not what would be in the url, like types, index etc. it is something that we'll have to get back to at some point, but kinda expected for now.
</comment><comment author="dadoonet" created="2015-05-30T07:22:21Z" id="107001728">Thanks @javanna. Should we close this issue? Is this going to be fixed with  https://github.com/elastic/elasticsearch/issues/9962?
</comment><comment author="javanna" created="2015-06-02T08:44:56Z" id="107866911">yes I'd close in favour of #9962, where we have to discuss what `toString` should do in general and follow the same convention everywhere.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make CompressedXContent.equals fast again.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11428</link><project id="" key="" /><description>We had to make CompressedXContent.equals decompress data to fix some
correctness issues which had the downside of making equals() slow. Now we store
a crc32 alongside compressed data which should help avoid decompress data in
most cases.

Close #11247
</description><key id="82504148">11428</key><summary>Make CompressedXContent.equals fast again.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T16:57:31Z</created><updated>2015-06-10T11:38:09Z</updated><resolved>2015-06-10T11:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-29T16:59:50Z" id="106870995">How long are the strings typically? for short strings something like murmur will be just as effective but faster. 
</comment><comment author="jpountz" created="2015-05-29T21:19:45Z" id="106938613">I think the main use-case is mappings (which are typically very repetitive and easily conpressible), which can have different sizes depending on the use-case... I believe some mappings could be several megabytes in their uncompressed form.

crc32 was convenient because it already has a streaming API that makes it easy to use in this context and it standardized
</comment><comment author="jpountz" created="2015-06-10T07:59:00Z" id="110642383">I rebased this pull request to the new modular structure. @rmuir do you think this change is good to go?
</comment><comment author="s1monw" created="2015-06-10T08:05:17Z" id="110643458">I left one comment on this LGTM in general
</comment><comment author="jpountz" created="2015-06-10T08:52:42Z" id="110659446">@s1monw I applied your suggestion.
</comment><comment author="s1monw" created="2015-06-10T10:17:19Z" id="110682067">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>Merge pull request #11428 from jpountz/fix/CompressedXConten_fast</comment></comments></commit></commits></item><item><title>Query Refactoring: ExistsQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11427</link><project id="" key="" /><description>Refactors ExistsQueryBuilder and Parser, splitting the parse() method into a parsing and a query building part. Also moving newFilter() uitlity method from parser to query builder.

Changes in the BaseQueryTestCase include introduction of randomized version to test disabled FieldNamesFieldMappers and also getting rid of the need for createEmptyBuilder() method by now using existing prototype constants.

Relates to #10217

PR goes agains query-refacoring feature branch.
</description><key id="82489018">11427</key><summary>Query Refactoring: ExistsQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-29T16:12:07Z</created><updated>2015-06-11T12:47:10Z</updated><resolved>2015-06-11T12:47:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-29T16:17:57Z" id="106861783">I'm still not 100% happy with the test for the query generation (toQuery) since constructing the expected Lucene query relies on using newFilter() helper method. Testing that helper method itself would require a lot of repetition of inner implementation logic in the creating of the expected query, so I choose not to go into that for now. Since it's a static method, maybe that would be better handled in a separate integration test?
</comment><comment author="cbuescher" created="2015-06-04T17:09:21Z" id="108977831">Extended test for the toQuery() part of the ExistsQueryBuilder. Setup now randomly disables FieldNamesFieldMapper so we cover more execution paths.
</comment><comment author="javanna" created="2015-06-05T14:56:35Z" id="109319729">looks good, left a few comments
</comment><comment author="cbuescher" created="2015-06-08T14:06:05Z" id="110007006">@javanna Thanks, adressed your last comments and rebased on current head of feature branch.
</comment><comment author="javanna" created="2015-06-09T15:52:00Z" id="110410905">I left a couple of minor comments, LGTM though, feel free to push once you addressed those
</comment><comment author="cbuescher" created="2015-06-10T10:52:48Z" id="110698757">Rebasing on current head of feature branch and addressing last round of comments. 
I needed to adapt the BaseQueryTest setup a bit to conform with current changes merged in from master concerning Mappings, especially FieldNameFieldMapper. See inline comment for explication.
</comment><comment author="javanna" created="2015-06-10T11:51:14Z" id="110715914">left two comments, LGTM though. We have to resolve upstream the problem around FieldNamesFieldMapper and we are good to go I'd say.
</comment><comment author="cbuescher" created="2015-06-10T12:53:21Z" id="110739590">Added randomization of the version in settings, will wait hearing back about possible upstream changes.
</comment><comment author="javanna" created="2015-06-11T12:44:45Z" id="111120619">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11427 from cbuescher/feature/query-refactoring-exists</comment></comments></commit><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java</file></files><comments><comment>Query Refactoring: ExistsQueryBuilder and Parser</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file></files><comments><comment>Refactors BoolQueryBuilder and Parser. Splits the parse(QueryParseContext ctx) method into a parsing</comment><comment>and a query building part, adding NamedWriteable implementation for serialization and hashCode(),</comment><comment>equals() for testing.</comment></comments></commit></commits></item><item><title>Fix check for locations in a repository path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11426</link><project id="" key="" /><description>Currently, when trying to determine if a location is within one of the configured repository
paths, we compare a canonical path against an absolute path. These are not always
equivalent and this check will fail even when the same directory is used. This changes
the logic to to follow that of the http server, where we use normalized absolute path
comparisons. A test has been added that failed with the old code and now passes with the
updated method.

This only targets the 1.x branch as the code for handling this is different on master.
</description><key id="82473435">11426</key><summary>Fix check for locations in a repository path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-29T15:30:21Z</created><updated>2015-05-30T10:57:58Z</updated><resolved>2015-05-29T17:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-29T15:30:31Z" id="106846479">@imotov can you review?
</comment><comment author="imotov" created="2015-05-29T15:47:01Z" id="106852997">@jaymode Nice catch! Thanks! Maybe we can make it to match master: 

``` java
    public static File resolve(File[] roots, String path) {
        for (File root : roots) {
            Path rootPath = root.toPath();
            Path normalizedPath = rootPath.resolve(path).normalize();
            if(normalizedPath.startsWith(rootPath)) {
                return normalizedPath.toFile();
            }
        }
        return null;
    }
```

What do you think?
</comment><comment author="jaymode" created="2015-05-29T15:57:57Z" id="106855958">@imotov makes sense! the only thing I did different than your example is normalize the root path, so we are comparing two normalized paths.
</comment><comment author="imotov" created="2015-05-29T15:59:54Z" id="106856566">LGTM then.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/PathUtils.java</file><file>src/test/java/org/elasticsearch/env/EnvironmentTests.java</file></files><comments><comment>always normalize root paths during resolution of paths</comment></comments></commit></commits></item><item><title>Handling throughput decrease after index is too big</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11425</link><project id="" key="" /><description>Hey,

I just spent entire day dealing with the fact that when index reached 20 million small documents indexing rate dropped to ~ 5% but Marvel Dashboard looked like the cluster wasn't used at all, all resource consumption down at a few percent.  This leads you to looking for a throughput problem in your application because ES looks "zero" bussy...

It would really need some sort of indicator cause this is a real pain to resolve
</description><key id="82472204">11425</key><summary>Handling throughput decrease after index is too big</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">l15k4</reporter><labels /><created>2015-05-29T15:27:30Z</created><updated>2015-11-23T14:21:59Z</updated><resolved>2015-05-29T18:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T18:37:52Z" id="106899278">Hi @l15k4 

You're really going to have to provide a lot more information than this, because 20M documents is tiny, Elasticsearch should handle this with ease.    I suggest you start by discussing your issue in the forum https://discuss.elastic.co/c/elasticsearch which is intended to help solve user issues.  If you find something that you think is a bug, please feel free to open a new ticket with the relevant information

thanks
</comment><comment author="l15k4" created="2015-05-31T10:43:25Z" id="107155434">Hi @clintongormley ,

The cluster has default configuration, running on 2 aws dual-core EC2 instances. Each ES node has 2GB RAM. I'm indexing in bulks (100 records / 2.5MB overall) concurrently from (1 to 4) threads and I have disabled `refresh_interval` for this index...

It's an index-only cluster, ES 1.5.1...

Is there any particular configuration setting that has the biggest impact on this issue? RAM doesn't seem to be an issue according to Marvel Overview. Thank you

I can think of these configs that might help : 

```
indices.memory.index_buffer_size: 40%
indices.store.throttle.type: none
index.translog.interval: 10s
index.gateway.local.sync: 10s
index.translog.flush_threshold_size: 1024mb
```

I'n not sure if I should also change `indices.recovery` settings...
</comment><comment author="clintongormley" created="2015-05-31T11:21:20Z" id="107157361">HI @l15k4 

I'd be interested to see:
- what's in the logs (esp looking for slow GCs)
- what your nodes info and stats say
- what hot threads say

And I'd investigate all of those before changing settings like the ones you suggest.
</comment><comment author="l15k4" created="2015-05-31T13:10:03Z" id="107174905">@clintongormley 

there are just these log lines : 

```
[2015-05-31 04:16:56,935][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:16:56,987][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:17:11,461][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:17:11,478][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][3] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:17:13,757][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:17:13,768][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:17:21,697][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:17:21,744][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:17:37,832][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:17:37,973][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:18:04,276][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:18:04,306][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:18:26,656][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:18:26,692][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:18:30,538][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][0] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:18:30,567][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][0] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:18:51,568][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:18:51,620][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:19:35,157][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:19:35,230][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
[2015-05-31 04:19:35,524][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
[2015-05-31 04:19:36,150][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
```

[node info](http://pastebin.com/raw.php?i=as6kFMcS)
[node stats](http://pastebin.com/raw.php?i=jpdxPyzV)
[hot threads](http://pastebin.com/raw.php?i=C04ABS4Y)

Btw it usually performs well, with ~ 80% CPU utilization on both nodes but after ~ 2-3 minutes it slows down to ~ 20% CPU utilization... RAM utilization remains the same ~ 65%
</comment><comment author="clintongormley" created="2015-06-02T17:48:16Z" id="108028980">It looks like your disks are slow, which is causing your merges to slow down.  You can either add more nodes (to spread the load across more disks) or get faster disks (eg SSD or provisioned IOPS)
</comment><comment author="l15k4" created="2015-06-02T20:30:20Z" id="108089651">@clintongormley  there is plenty of disk IO throughput available, really. The indexing is way faster if I shut one node down and use just one. Keeping the replica synced is the real bottleneck here imho.
</comment><comment author="l15k4" created="2015-06-03T10:26:57Z" id="108290860">In other words, I can index 8000 documents/second into small indices but when the index reaches 20-30 million records, ES cluster suddenly slows down to 200-300 docs/s **with very low RAM and CPU utilization** and if I split the documents into indices up to 20 million records it is indexing 8000 documents/second next 12 hours without problem... 
</comment><comment author="clintongormley" created="2015-06-03T11:49:34Z" id="108332227">@l15k4 Merging is a cost of indexing.  You have to pay it in order to continue indexing efficiently, but you start paying it only as your segments start getting bigger.  I suggest that you ask about your options in the forum: https://discuss.elastic.co/c/elasticsearch
</comment><comment author="bitonp" created="2015-08-29T13:38:23Z" id="135987909">2G Ram seems very low. Java requires half of that, so thats 1G gone... and Isuspect you are swapping the rest.
Add to that the fact that you have data and readers on teh same nodes, its probably gettiing a bit insane. we are on AWS and regularly index 140m-250m per day, using 32G machines. we have Kibana reading for 20+ graphs simultabeously... indexing queries off mysql databases. I suspect that arger machines will solve yoour issue.. but I could also be a couple of months late picking up on this.. sorry
</comment><comment author="l15k4" created="2015-10-21T17:50:29Z" id="149975789">@clintongormley I did, I'm disabling refresh interval, setting throttling to none and optimizing indices after bulk indexing, so that next bulk indexing starts on merged segments, see : 

15:05:25,915][INFO ][indices.store ] [es-884a70a1805147db95b8a28e776cd59a] updating indices.store.throttle.type from [merge] to [none]
15:05:25,966][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] updating refresh_interval from [1s] to [-1]
15:05:25,971][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] updating refresh_interval from [1s] to [-1]
15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] updating refresh_interval from [1s] to [-1]
15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] updating refresh_interval from [1s] to [-1]
15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] updating refresh_interval from [1s] to [-1]
15:05:26,731][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
15:05:26,807][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
15:05:26,865][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
15:05:26,922][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
15:05:26,979][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
15:05:27,018][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3
15:05:27,060][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
15:05:27,067][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
15:05:27,139][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
15:05:27,330][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
15:05:29,024][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3

@bitonp I meant 2GB for Heap ... anyway this isn't related to memory, there is always enough of memory.... GC is choking though, a lot of time is spent by GC young space
</comment><comment author="jsh2134" created="2015-11-23T14:21:59Z" id="158945699">@l15k4 having the same issue. We are trying to index 400M records and right around 100M records, throughput grinds to a halt, we are using larger nodes (m4.2xlarges) but once the throughput drops so does CPU and Load. Have you had any further success?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Repo location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11424</link><project id="" key="" /><description /><key id="82472185">11424</key><summary>Repo location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels /><created>2015-05-29T15:27:26Z</created><updated>2015-05-29T15:28:00Z</updated><resolved>2015-05-29T15:27:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Logging Provider Selection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11423</link><project id="" key="" /><description>Hi,

Elasticsearch tries to [determine which logging "backend"](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/logging/ESLoggerFactory.java#L33) is available on the classpath.

In my scenario I use the [slf4j-log4j-bridge](https://logging.apache.org/log4j/log4j-2.2/log4j-1.2-api/index.html). The comment at [L36](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/logging/ESLoggerFactory.java#L36) indicates that in this scenario a NoSuchMethod should be thrown and the Slf4jESLoggerFactory should be used (if slf4j is present).

But the slf4j-log4j-bridge accepts the setLevel(org.apache.log4j.Level). See [org.apache.log4j.Logger](https://git-wip-us.apache.org/repos/asf?p=logging-log4j2.git;a=blob;f=log4j-1.2-api/src/main/java/org/apache/log4j/Logger.java;h=2e643a17c1de79fb56a49710797f046e162de289;hb=HEAD#l26) and [org.apache.log4j.Category](https://git-wip-us.apache.org/repos/asf?p=logging-log4j2.git;a=blob;f=log4j-1.2-api/src/main/java/org/apache/log4j/Category.java;h=25408e79f7ecb61176333df6cbcd9aced3d448ee;hb=HEAD#l190).
</description><key id="82470230">11423</key><summary>Logging Provider Selection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">cleemansen</reporter><labels><label>:Logging</label></labels><created>2015-05-29T15:22:37Z</created><updated>2016-05-20T20:37:12Z</updated><resolved>2016-05-20T20:37:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-05T10:01:02Z" id="109245340">is there another method you know that we could use? Havent looked at slf4j for some time. Hints appreciated.
</comment><comment author="cleemansen" created="2015-06-23T07:18:00Z" id="114388019">No - I also don't know another method.
I'm not that java logging expert, but I looked at other 'common' frameworks and their selection process:

This is the most common order:
1. check weather log4j2 is present
2. check weather log4j1 is present
3. check weather slf4j is present
4. use jdk logging
- JBoss: [code](https://github.com/jboss-logging/jboss-logging/blob/master/src/main/java/org/jboss/logging/LoggerProviders.java), [dokumentation](http://docs.jboss.org/hibernate/orm/4.3/topical/html/logging/Logging.html)
- resteasy: [code](https://github.com/resteasy/Resteasy/blob/master/jaxrs/resteasy-jaxrs/src/main/java/org/jboss/resteasy/logging/Logger.java#L71)
- apache commons logging: [code](http://svn.apache.org/repos/asf/commons/proper/logging/trunk/src/main/java/org/apache/commons/logging/impl/LogFactoryImpl.java)

I hope this helps to make a decision!
</comment><comment author="jasontedor" created="2016-05-20T20:37:12Z" id="220712451">As of #16703, we only support log4j for the logging back end.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor core index/query time properties into FieldType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11422</link><project id="" key="" /><description>Mappers are currently used at both index and query time for deciding
how to "use" a field.  For #8871, we need the index wide view of
mappings to have a unified set of settings for each field of a given
name within the index.

This change moves all the current settings (and methods defining
query time behavior) into subclasses of FieldType. In a future
PR, this will allow storing the field type at the index level,
instead of mappers (which can still have settings that differ
per document type).

The change is quite large (I'm sorry). I could not see a way to
migrate to this in a more piecemeal way. I did leave out cutting
over callers of the query methods to using the field type, as
that can be done in a follow up.
</description><key id="82468337">11422</key><summary>Refactor core index/query time properties into FieldType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T15:17:17Z</created><updated>2015-06-08T08:57:19Z</updated><resolved>2015-06-03T10:10:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-29T15:41:40Z" id="106850592">I should clarify, this moves all settings that affects _query time behavior_. So if something affects _only_  indexing (eg coerce setting for numeric fields), it is left in the mapper.
</comment><comment author="clintongormley" created="2015-05-29T18:40:33Z" id="106899862">@jpountz, @imotov, @javanna your eyes would be greatly appreciated 
</comment><comment author="jpountz" created="2015-05-30T13:08:03Z" id="107039288">LGTM I like where this is going
</comment><comment author="s1monw" created="2015-05-30T20:34:55Z" id="107082139">it's a pretty big change and hard to review but I second @jpountz here this looks good and goes into the right direction. I left some minor comments regarding tests and assertions... otehr than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataType.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/GlobalOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/InternalGlobalOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexGeoPointFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexOrdinalsFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/IndexIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fieldvisitor/SingleFieldsVisitor.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxQuery.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java</file><file>src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/lookup/FieldLookup.java</file><file>src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/NoOrdinalsStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/search/child/ChildQuerySearchBwcTests.java</file><file>src/test/java/org/elasticsearch/search/child/ParentFieldLoadingBwcTest.java</file><file>src/test/java/org/elasticsearch/search/child/ParentFieldLoadingTest.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #11422 from rjernst/pr/fieldtype-mapper-split</comment></comments></commit></commits></item><item><title>Reduce cluster update reroutes with async fetch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11421</link><project id="" key="" /><description>When using async fetch, we can end up with cluster updates and reroutes based on teh number of shards. While not disastrous we can optimize it, since a single reroute is enough to apply to all the async fetch results that arrived during that time.
</description><key id="82467663">11421</key><summary>Reduce cluster update reroutes with async fetch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T15:15:15Z</created><updated>2015-05-29T21:40:37Z</updated><resolved>2015-05-29T21:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-29T17:25:15Z" id="106878317">left one comment about removing info from the message, other than that LGTM
</comment><comment author="kimchy" created="2015-05-29T20:23:55Z" id="106925601">@dakrone I added a comment back, tell me if it makes sense
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>GatewayAllocator: reset rerouting flag after error</comment></comments></commit></commits></item><item><title>Indexing geo_point Doesn't Support lon/lat Array Format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11420</link><project id="" key="" /><description>After creating a mapping for a `geo_point` field, attempting to index a document that specifies the point in GeoJSON-a-like `[lon,lat]` array format returns `ElasticsearchParseException[field must be either 'lat', 'lon' or 'geohash']`

This should be reproducible as follows, assuming an index called `geo`:

```
DELETE geo
PUT geo
PUT geo/_mapping/places
{
  "places": {
    "properties": {
      "centroid": {
        "type": "geo_point"
      },
      "bounds": {
        "type": "geo_shape"
      }
    }
  }
}
```

```
GET geo/places/_mapping
{
   "geo": {
      "mappings": {
         "places": {
            "properties": {
               "bounds": {
                  "type": "geo_shape"
               },
               "centroid": {
                  "type": "geo_point"
               }
            }
         }
      }
   }
}
```

```
PUT geo/places/44418
{
  "centroid": {
        "coordinates": [
            -0.12714,
            51.506321
        ]
    },
    "bounds": {
      "type": "polygon",
        "coordinates": [
            [
                [
                    -0.563,
                    51.261318
                ],
                [
                    -0.563,
                    51.686031
                ],
                [
                    0.28036,
                    51.686031
                ],
                [
                    0.28036,
                    51.261318
                ],
                [
                    -0.563,
                    51.261318
                ]
            ]
        ]
    }
}
```

... which returns

```
{
   "error": "MapperParsingException[failed to parse]; nested: ElasticsearchParseException[field must be either 'lat', 'lon' or 'geohash']; ",
   "status": 400
}
```

An interim work-around appears to be to change the mapping for the `centroid` field to be a `geo_shape` as well.

```
PUT geo/_mapping/places
{
  "places": {
    "properties": {
      "centroid": {
        "type": "geo_shape"
      },
      "bounds": {
        "type": "geo_shape"
      }
    }
  }
}
```

```
PUT geo/places/44418
{
  "centroid": {
        "type": "point",
        "coordinates": [
            -0.12714,
            51.506321
        ]
    },
    "bounds": {
      "type": "polygon",
        "coordinates": [
            [
                [
                    -0.563,
                    51.261318
                ],
                [
                    -0.563,
                    51.686031
                ],
                [
                    0.28036,
                    51.686031
                ],
                [
                    0.28036,
                    51.261318
                ],
                [
                    -0.563,
                    51.261318
                ]
            ]
        ]
    }
}
```

```
GET geo/places/44418
{
   "_index": "geo",
   "_type": "places",
   "_id": "44418",
   "_version": 1,
   "found": true,
   "_source": {
      "centroid": {
         "type": "point",
         "coordinates": [
            -0.12714,
            51.506321
         ]
      },
      "bounds": {
         "type": "polygon",
         "coordinates": [
            [
               [
                  -0.563,
                  51.261318
               ],
               [
                  -0.563,
                  51.686031
               ],
               [
                  0.28036,
                  51.686031
               ],
               [
                  0.28036,
                  51.261318
               ],
               [
                  -0.563,
                  51.261318
               ]
            ]
         ]
      }
   }
}
```

Unless I'm missing something, which is always a distinct possibility. This is all running on v1.5.2 on OS X Yosemite 10.10.3.
</description><key id="82463780">11420</key><summary>Indexing geo_point Doesn't Support lon/lat Array Format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vicchi</reporter><labels /><created>2015-05-29T15:01:25Z</created><updated>2017-05-19T22:41:04Z</updated><resolved>2015-05-29T18:34:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T18:34:10Z" id="106898593">Hi @vicchi 

You are indeed missing something :)  The geo-point doesn't accept the `coordinates` parameter.  Indexing it as follows works just fine:

```
PUT geo/places/44418
{
  "centroid": [
    -0.12714,
    51.506321
  ],
  "bounds": {
    "type": "polygon",
    "coordinates": [
      [
        [
          -0.563,
          51.261318
        ],
        [
          -0.563,
          51.686031
        ],
        [
          0.28036,
          51.686031
        ],
        [
          0.28036,
          51.261318
        ],
        [
          -0.563,
          51.261318
        ]
      ]
    ]
  }
}
```

See https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html#_input_structure for more
</comment><comment author="vicchi" created="2015-06-01T12:04:40Z" id="107416930">That totally works for me now. Thanks for the rapid reply.
</comment><comment author="lukas-gitl" created="2017-05-19T22:41:04Z" id="302827047">So we can't use GeoJson for points? This is stupid.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scroll scan query doesn't return all documents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11419</link><project id="" key="" /><description>Hi all, At the moment I am trying to scan and scroll my ES index to export all the data, but I noticed that not always all the documents are returned. In other words ES says I have 1.115.857 documents in my index, but my export contains 1.049.191 documents. Testing with an index with up to 900.000 document is working fine. Somebody that can explain this behavior?

ES is running on a CentOS machine with 6GB of RAM with a ES_HEAP_SIZE of 3GB.
</description><key id="82430323">11419</key><summary>Scroll scan query doesn't return all documents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">developmentstudio</reporter><labels><label>non-issue</label></labels><created>2015-05-29T13:35:23Z</created><updated>2015-08-18T15:56:25Z</updated><resolved>2015-06-01T10:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-29T13:40:34Z" id="106804842">&gt; In other words ES says I have 1.115.857 documents in my index, but my export contains 1.049.191 documents.

How are you measuring the number of documents in the index? Just `curl localhost:9200/index/_count`?
</comment><comment author="developmentstudio" created="2015-05-29T13:50:39Z" id="106807750">Hi @nik9000,

Starting the scroll/scan query with the query below return a count.

```
GET /20141016v3small/accesslog/_search?search_type=scan&amp;scroll=10m
```

Results in 1115857 documents.

```
{
   "_scroll_id": "c2Nhbjs1OzMxMjpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxNDpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxNTpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxMzpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxMTpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzE7dG90YWxfaGl0czoxMTE1ODU3Ow==",
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1115857,
      "max_score": 0,
      "hits": []
   }
}
```

I write every document id to an MySQL database table and simple check with 

```
SELECT COUNT(*) FROM table
```
</comment><comment author="clintongormley" created="2015-05-29T14:07:42Z" id="106816975">I wonder if you have documents with the same `_id` and different type names, which are getting overwritten when you index into mysql?  Or if you have used routing on some documents so that you have duplicate copies of the same document on different shards?

Instead of using mysql, why not just store these IDs in an in-memory hash and print out any clashes that you get?  You can maintain a simple count of all docs retrieved at the same time.
</comment><comment author="developmentstudio" created="2015-05-29T14:17:32Z" id="106821865">@clintongormley I am testing/prototyping some stuff where I, in the end, need a MySQL database to store some result sets for further analysis. But indeed checking the id's can be in-memory. I will check that.

What I do is just indexing via the bulk end-point in a fresh ES installation without changing the default configurations (except HEAP_SIZE). All the `_id` are generated by ES so I can assume all are unique? Correct me if I am wrong. 

```
http://127.0.0.1:9200/20141016v3small/accesslog/_bulk
```
</comment><comment author="clintongormley" created="2015-05-29T14:19:41Z" id="106823215">that should be the case, yes.  it's still worth doing the test i propose - it would be interesting to know if there is indeed an issue with generated IDs (or if perhaps inserts are being lost in mysql?).  Also, what version of ES are you using?
</comment><comment author="developmentstudio" created="2015-05-29T14:39:34Z" id="106830345">I will do some test on duplicate _id's later this evening and will let you know the results. I'm using ES version 1.5.1.
</comment><comment author="developmentstudio" created="2015-05-29T20:13:23Z" id="106923383">I've written a small PHP script to check the total amount of documents given by ES in the initial scroll response vs the document count to exactly see how much documents are not retrieved. See below.

``` php
&lt;?php
$query = '{
    "size": 1000,
    "filter": {
        "bool": {
            "must": []
        }
    },   
}';

$search = new ScrollSearch($query);

class ScrollSearch
{
    private $scrollId = null;
    private $totalDocuments = null;
    private $totalResultDocuments = 0;

    private $shards = array(
        'total' =&gt; 0,
        'successful' =&gt; 0,
        'failed' =&gt; 0
    );

    public function __construct($query) 
    {
        $this-&gt;initScrollScan($query);

        while ($this-&gt;scrollId != false) {
            $this-&gt;getDocuments();
        }

        $this-&gt;printInfo();
        echo "COMPLETED" . PHP_EOL;
    }

    private function initScrollScan()
    {
        $ch = curl_init("http://127.0.0.1:9200/20141016v3/accesslog/_search?scroll=10m&amp;search_type=scan");
        curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);
        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));
        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );
        $response = curl_exec($ch);
        $response = json_decode($response);

        $this-&gt;scrollId = $response-&gt;{'_scroll_id'};
        $this-&gt;totalDocuments = $response-&gt;{'hits'}-&gt;{'total'};

        curl_close($ch);
    }

    private function getDocuments()
    {
        $ch = curl_init("http://127.0.0.1:9200/_search/scroll?scroll=10m");
        curl_setopt( $ch, CURLOPT_POSTFIELDS, $this-&gt;scrollId);
        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));
        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );
        $json = curl_exec($ch);
        $response = json_decode(utf8_encode($json));
        $countDocuments = count($response-&gt;{'hits'}-&gt;{'hits'});
        $info = curl_getinfo($ch);

        $this-&gt;shards['total'] += $response-&gt;{'_shards'}-&gt;{'total'};
        $this-&gt;shards['successful'] += $response-&gt;{'_shards'}-&gt;{'successful'};
        $this-&gt;shards['failed'] += $response-&gt;{'_shards'}-&gt;{'failed'};

        if ($info['http_code'] != 200) {
            echo "Request goes wrong: " . $info['http_code'] . '. Retry.' .PHP_EOL;
            getDocuments();
        }

        if ($countDocuments &gt; 0) {
            $this-&gt;scrollId = $response-&gt;{'_scroll_id'};
            $this-&gt;totalResultDocuments += $countDocuments;
            $this-&gt;printInfo();
        } else {
            echo 'NO MORE RESULTS!' . PHP_EOL;
            $this-&gt;scrollId = false;
        }

        curl_close($ch);
    }

    private function printInfo() {
        echo ($this-&gt;totalResultDocuments - $this-&gt;totalDocuments) . " missing documents. " . $this-&gt;shards['total'] . " shards, " . $this-&gt;shards['failed'] . " failed and " . $this-&gt;shards['successful'] . " successful" . PHP_EOL;
    }

}
```

Index contains: 1.115.857 documents which resulted in:

``` bash
...
...
...
-57333 missing documents
-57323 missing documents
-57318 missing documents
NO MORE RESULTS!
-57318 missing documents
```

I did not check if their are duplicate id's, but just summed the amount of document return in each query.
</comment><comment author="clintongormley" created="2015-05-29T20:49:13Z" id="106930944">Hi @developmentstudio 

I put together a small test.  I indexed 1.115.857 docs with auto-generated IDs, then I retrieved them with scrolling and got back exactly 1.115.857 docs with 1.115.857 unique IDs.

I think you're going to need to give us more information - perhaps dig a bit deeper into what is and isn't being returned. Perhaps you need to check the `_shards` element of each response, to see whether you are having shard failures, perhaps because of long GCs.
</comment><comment author="nik9000" created="2015-05-29T21:02:02Z" id="106935722">I'm curious - does it always spit out the same number? What does _count
spit out?

Would you mind showing how you do the bulk indexing?

On Fri, May 29, 2015 at 4:13 PM, Kevin van Cleef notifications@github.com
wrote:

&gt; I've written a small PHP script to check the amount of document given by
&gt; ES in the initial scroll response vs the document count to exactly see how
&gt; much documents are not retrieved. See below.
&gt; 
&gt; &lt;?php$query = '{    "size": 1000,    "filter": {        "bool": {            "must": []        }    },   }';$search = new ScrollSearch($query);class ScrollSearch{    private $scrollId = null;    private $totalDocuments = null;    private $totalResultDocuments = 0;    public function __construct($query)     {        $this-&gt;initScrollScan($query);        while ($this-&gt;scrollId != false) {            $this-&gt;getDocuments();        }        $this-&gt;printMissingDocumentCount();        echo "COMPLETED" . PHP_EOL;    }    private function initScrollScan()    {        $ch = curl_init("http://127.0.0.1:9200/20141016v3/accesslog/_search?scroll=10m&amp;search_type=scan");        curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );        $response = curl_exec($ch);        $response = json_decode($response);        $this-&gt;scrollId = $response-&gt;{'_scroll_id'};        $this-&gt;totalDocuments = $response-&gt;{'hits'}-&gt;{'total'};        curl_close($ch);    }    private function getDocuments()    {        $ch = curl_init("http://127.0.0.1:9200/_search/scroll?scroll=10m");        curl_setopt( $ch, CURLOPT_POSTFIELDS, $this-&gt;scrollId);        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );        $json = curl_exec($ch);        $response = json_decode(utf8_encode($json));        $countDocuments = count($response-&gt;{'hits'}-&gt;{'hits'});        $info = curl_getinfo($ch);        if ($info['http_code'] != 200) {            echo "Request goes wrong: " . $info['http_code'] . '. Retry.' .PHP_EOL;            getDocuments();        }        if ($countDocuments &gt; 0) {            $this-&gt;scrollId = $response-&gt;{'_scroll_id'};            $this-&gt;totalResultDocuments += $countDocuments;            $this-&gt;printMissingDocumentCount();        } else {            echo 'NO MORE RESULTS!' . PHP_EOL;            $this-&gt;scrollId = false;        }        curl_close($ch);    }    private function printMissingDocumentCount() {        echo ($this-&gt;totalResultDocuments - $this-&gt;totalDocuments) . " missing documents" . PHP_EOL;    }}
&gt; 
&gt; Index contains: 1.115.857 documents which resulted in:
&gt; 
&gt; ...
&gt; ...
&gt; ...
&gt; -57333 missing documents
&gt; -57323 missing documents
&gt; -57318 missing documents
&gt; NO MORE RESULTS!
&gt; -57318 missing documents
&gt; 
&gt; I did not check if their are duplicate id's, but just summed the amount of
&gt; document return in each query.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106923383
&gt; .
</comment><comment author="developmentstudio" created="2015-05-30T12:32:38Z" id="107035786">@clintongormley Thanks for helping. I've changed the test code above so it also checks how many shards are successful and how many failed which resulted in `-57318 missing documents. 105857 shards, 1 failed and 105856 successful`. One missing shard doesn't explains all the missing the documents, right? Below the logfile after running the scroll query. This is related to the failing shard if I'm right.

```
[2015-05-30 13:48:53,034][DEBUG][action.search.type       ] [Robbie Robertson] [666] Failed to execute query phase
org.elasticsearch.search.fetch.FetchPhaseExecutionException: [20141016v3][0]: query[ConstantScore(cache(_type:accesslog))],from[165860],size[10]: Fetch Failed [Failed to fetch doc id [165863]]
    at org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:409)
    at org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:217)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:182)
    at org.elasticsearch.search.SearchService.executeScan(SearchService.java:261)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Corrupted: lengths mismatch: 17381 &gt; 17380 (resource=NIOFSIndexInput(path="/var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v3/0/index/_hn.fdt"))
    at org.apache.lucene.codecs.compressing.CompressionMode$4.decompress(CompressionMode.java:137)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:354)
    at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:335)
    at org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:407)
    ... 9 more

```

What do you mean with long GCs? 
How can I best approach diff the result set retrieved by the scroll-scan query and the documents in the index?

@nik9000 I've reindexed the dataset multiple times for testing. At the current index the missing amount of documents remains the same. (Missing: 57318) At the initial index where I started the post with I missed 66.666 documents. It looks like the amount of missing documents is constant for the created index. 

`GET /20141016v3/accesslog/_count` results in the same amount of documents namely 1.115.857, see below.

```
{
   "count": 1115857,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   }
}
```

The script used for indexing.

``` php
&lt;?php
$file_handle = fopen("access.json.log-20141016", "r");

$batch = array();
$max_number_of_batches = 130000;
while (!feof($file_handle)) {
    $line = fgets($file_handle);
    $line = preg_replace_callback("(\\\\x([0-9a-f]{2}))i", function($a) { return chr(hexdec($a[1])); }, $line);

    if (strlen($line) &gt; 0 ) {
        $batch[] = "{\"create\" : {}} \n" . $line;  
    }

    if (count($batch) == 10) {
        $result = bulkIndex($batch);

        $result = json_decode($result);
        if ($result-&gt;errors) {
            var_dump($result);
        }

        $batch = array();

        --$max_number_of_batches;
        echo $max_number_of_batches . PHP_EOL;
        if ($max_number_of_batches == 0) {
            break;
        }
    }
}
bulkIndex($batch);
fclose($file_handle);

function bulkIndex($batch) {
    $data = implode("\n", $batch) . "\n";

    $ch = curl_init("http://127.0.0.1:9200/20141016v3/accesslog/_bulk");
    curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);
    curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));
    curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );
    $result = curl_exec($ch);
    return $result;
    curl_close($ch);
}
```

Mapping used for the index.

```
PUT /20141016v3
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "default" : {
                    "type" : "keyword"
                }
            }
        }
    }
}
```

Example document structure.

``` json
{
    "request": {
        "timestamp": "2014-10-15T03:42:51+02:00",
        "method": "GET",
        "cookies": {},
        "url": {
            "host": "www.domein.nl",
            "uri": "/product/142855/product-name.html",
            "query": {}
        },
        "protocol": "HTTP/1.1",
        "headers": {
            "x-forwarded-host": "www.domein.nl",
            "x-forwarded-for": "111.11.11.1, 111.111.111.11, 111.111.111.111",
            "connection": "Keep-Alive",
            "accept": "*/*",
            "cache-control": "no-cache",
            "host": "www.domein.nl",
            "x-forwarded-server": "www.domein.nl",
            "sitespect": "1-1249",
            "pragma": "no-cache",
            "user-agent": "msnbot/2.0b (+http://search.msn.com/msnbot.htm)"
        },
        "remoteAddress": "111.11.11.1"
    },
    "response": {
        "status": 200,
        "bytesSent": 163552,
        "processingTime": 0.687,
        "headers": {
            "pragma": "no-cache",
            "expires": "Thu, 19 Nov 1981 08:52:00 GMT",
            "content-type": "text/html; charset=iso-8859-1",
            "set-cookie": "PHPSESSID=8llnj7b5vo4lrv1ehjeo09rre5; path=/",
            "x-frame-options": "SAMEORIGIN",
            "cache-control": "no-store, no-cache, must-revalidate, post-check=0, pre-check=0"
        }
    }
}
```
</comment><comment author="rmuir" created="2015-05-30T17:30:13Z" id="107068038">that exception looks suspicious. Is there any way we can get a copy of the shard for inspection? What is the output of $JAVA_HOME/bin/java -version ?
</comment><comment author="developmentstudio" created="2015-05-30T18:35:43Z" id="107074488">@rmuir I have to ask my security manager if sharing the dataset is allowed as it contains sensitive information.

```
$ java -version
openjdk version "1.8.0_45"
OpenJDK Runtime Environment (build 1.8.0_45-b13)
OpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)
```
</comment><comment author="clintongormley" created="2015-05-31T11:59:13Z" id="107162033">@developmentstudio one possibility is that you have a hardware issue.  Would it be possible to reindex your data on different disks and try these tests again?
</comment><comment author="clintongormley" created="2015-05-31T12:08:25Z" id="107163396">&gt; One missing shard doesn't explains all the missing the documents, right? Below the logfile after running the scroll query.

I bet what is happening is that, after the exception, no more results are being returned from that shard. But the main question is: what is the source of this corruption?
</comment><comment author="rmuir" created="2015-05-31T12:56:40Z" id="107173533">Can you send the output of checkindex against the shard?
</comment><comment author="developmentstudio" created="2015-05-31T13:00:29Z" id="107174044">@rmuir What do you mean exactly with output of checkindex?
</comment><comment author="rmuir" created="2015-05-31T13:19:08Z" id="107176964">Lucene has a commandline tool called CheckIndex which will do verification of the index and print out any problems. It can maybe tell us a little bit more about what happened: if you are ultimately able to share the index, it would be the first thing I would run. 

But if you do not mind, you can run it yourself. There are some instructions here that look good:

https://www.found.no/foundation/dive-into-elasticsearch-storage/#fixing-problematic-shards

But one note about those instructions, _PLEASE DO NOT_ pass the -fix option. We just want to run diagnostics here.
</comment><comment author="developmentstudio" created="2015-05-31T13:20:02Z" id="107177099">@clintongormley I am running ES in a Vagrant box with CentOS6.6, 2 cpu's and 6 GB of RAM of which 3 are for the HEAP.  Host is a MacBook Pro Retina which quad core i7 and 16GB RAM. Do you think there is something wrong in this setup which can explain the issue?

&gt; I bet what is happening is that, after the exception, no more results are being returned from that shard. But the main question is: what is the source of this corruption?

I think you are right. I've echo'ed the amount of shards returned in each response. After the first time the shard failed the total shards returned is lowered to 4 instead of 5.

```
-1079057 missing documents. 3680 shards, 0 failed and 3680 successful
Shards: 5
-1079007 missing documents. 3685 shards, 0 failed and 3685 successful
Shards: 5
-1078967 missing documents. 3690 shards, 1 failed and 3689 successful
Shards: 4
-1078927 missing documents. 3694 shards, 1 failed and 3693 successful
Shards: 4
-1078887 missing documents. 3698 shards, 1 failed and 3697 successful
```

Last night and this morning I have reindexed the data-set some more times. The amount of missing documents is different for each index. Which goes from 0 to above 200.000 missing documents, but the amount of missing documents resulted from the test stays the same for each index. This makes me think something goes wrong at the moment of indexing.
</comment><comment author="clintongormley" created="2015-05-31T13:35:37Z" id="107180813">&gt; Do you think there is something wrong in this setup which can explain the issue?

There's always a possibility that there is an actual hardware (eg disk) failure.  Just trying to exclude that.  
</comment><comment author="developmentstudio" created="2015-05-31T13:42:19Z" id="107182938">@rmuir @clintongormley See below the result of CheckIndex. Hardware is possibly the reason. I will check another machine to run these tests and let you know. (will be tomorrow) Thanks!

```
elasticsearch]$ java -cp lib/elasticsearch-*.jar:lib/*:lib/sigar/* -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex $SHARD_PATH

Opening index @ /var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v5/0/index

Segments file=segments_2 numSegments=19 version=4.10.4 format= userData={translog_id=1433018704706}
  1 of 19: name=_73 docCount=36477
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=75.554
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019039094}
    no deletions
    test: open reader.........OK
    test: check integrity.....FAILED
    WARNING: fixIndex() would remove reference to this segment; full exception:
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dc15b066 actual=cc0d3842 (resource=BufferedChecksumIndexInput(MMapIndexInput(path="/var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v5/0/index/_73.fdt")))
    at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:211)
    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.checkIntegrity(CompressingStoredFieldsReader.java:535)
    at org.apache.lucene.index.SegmentReader.checkIntegrity(SegmentReader.java:624)
    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:590)
    at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)

  2 of 19: name=_sp docCount=64676
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=176.853
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019758952}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [1736 fields]
    test: field norms.........OK [1699 fields]
    test: terms, freq, prox...OK [543408 terms; 5466327 terms/docs pairs; 1700019 tokens]
    test: stored fields.......OK [129352 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  3 of 19: name=_kn docCount=43922
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=103.51
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019490621}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [1832 fields]
    test: field norms.........OK [1762 fields]
    test: terms, freq, prox...OK [324060 terms; 3596507 terms/docs pairs; 1095198 tokens]
    test: stored fields.......OK [87844 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  4 of 19: name=_vh docCount=15790
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=57.05
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019852048}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [791 fields]
    test: field norms.........OK [776 fields]
    test: terms, freq, prox...OK [179732 terms; 1430534 terms/docs pairs; 462434 tokens]
    test: stored fields.......OK [31580 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  5 of 19: name=_yk docCount=14347
    version=4.10.4
    codec=Lucene410
    compound=false
    numFiles=12
    size (MB)=55.812
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019954193}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [743 fields]
    test: field norms.........OK [728 fields]
    test: terms, freq, prox...OK [169755 terms; 1333122 terms/docs pairs; 436246 tokens]
    test: stored fields.......OK [28694 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  6 of 19: name=_11c docCount=13144
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=50.831
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020047341}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [753 fields]
    test: field norms.........OK [739 fields]
    test: terms, freq, prox...OK [155725 terms; 1218242 terms/docs pairs; 398232 tokens]
    test: stored fields.......OK [26288 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  7 of 19: name=_13u docCount=13457
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=47.191
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020130429}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [747 fields]
    test: field norms.........OK [733 fields]
    test: terms, freq, prox...OK [149465 terms; 1207508 terms/docs pairs; 388684 tokens]
    test: stored fields.......OK [26914 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  8 of 19: name=_162 docCount=12123
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=31.108
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020204484}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [614 fields]
    test: field norms.........OK [599 fields]
    test: terms, freq, prox...OK [112993 terms; 994561 terms/docs pairs; 305514 tokens]
    test: stored fields.......OK [24246 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  9 of 19: name=_16c docCount=1636
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=2.946
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020213488}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [231 fields]
    test: field norms.........OK [221 fields]
    test: terms, freq, prox...OK [16298 terms; 123755 terms/docs pairs; 36203 tokens]
    test: stored fields.......OK [3272 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  10 of 19: name=_16m docCount=1540
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=2.533
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020222504}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [216 fields]
    test: field norms.........OK [205 fields]
    test: terms, freq, prox...OK [14237 terms; 116351 terms/docs pairs; 33909 tokens]
    test: stored fields.......OK [3080 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  11 of 19: name=_16w docCount=1713
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=2.678
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020231512}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [201 fields]
    test: field norms.........OK [191 fields]
    test: terms, freq, prox...OK [15570 terms; 127546 terms/docs pairs; 36869 tokens]
    test: stored fields.......OK [3426 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  12 of 19: name=_176 docCount=1681
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=2.608
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020241517}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [233 fields]
    test: field norms.........OK [222 fields]
    test: terms, freq, prox...OK [15259 terms; 124763 terms/docs pairs; 35983 tokens]
    test: stored fields.......OK [3362 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  13 of 19: name=_17g docCount=1695
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=2.617
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020250526}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [202 fields]
    test: field norms.........OK [188 fields]
    test: terms, freq, prox...OK [15204 terms; 126487 terms/docs pairs; 36605 tokens]
    test: stored fields.......OK [3390 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  14 of 19: name=_166 docCount=206
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.507
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020209005}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [111 fields]
    test: field norms.........OK [102 fields]
    test: terms, freq, prox...OK [2836 terms; 15893 terms/docs pairs; 4717 tokens]
    test: stored fields.......OK [412 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  15 of 19: name=_17f docCount=210
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.318
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020250756}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [94 fields]
    test: field norms.........OK [82 fields]
    test: terms, freq, prox...OK [2537 terms; 15574 terms/docs pairs; 4473 tokens]
    test: stored fields.......OK [420 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  16 of 19: name=_17h docCount=164
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.289
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020251771}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [91 fields]
    test: field norms.........OK [81 fields]
    test: terms, freq, prox...OK [1949 terms; 12846 terms/docs pairs; 3806 tokens]
    test: stored fields.......OK [328 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  17 of 19: name=_17i docCount=196
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.316
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020252790}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [99 fields]
    test: field norms.........OK [89 fields]
    test: terms, freq, prox...OK [2246 terms; 14785 terms/docs pairs; 4283 tokens]
    test: stored fields.......OK [392 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  18 of 19: name=_17j docCount=168
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.288
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020253810}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [98 fields]
    test: field norms.........OK [89 fields]
    test: terms, freq, prox...OK [2192 terms; 12480 terms/docs pairs; 3605 tokens]
    test: stored fields.......OK [336 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  19 of 19: name=_17k docCount=82
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.151
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020254838}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [76 fields]
    test: field norms.........OK [67 fields]
    test: terms, freq, prox...OK [1143 terms; 6104 terms/docs pairs; 1761 tokens]
    test: stored fields.......OK [164 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

WARNING: 1 broken segments (containing 36477 documents) detected
WARNING: would write new segments file, and 36477 documents would be lost, if -fix were specified
```
</comment><comment author="rmuir" created="2015-05-31T13:49:04Z" id="107186962">Thanks very much for running this, I am relieved it is not a lucene issue :)

In this case bits in the .fdt file are getting flipped somehow...
</comment><comment author="developmentstudio" created="2015-06-01T10:15:43Z" id="107389109">Just indexed and executed the tests twice on another machine without any problems. Looking for hardware failures on the machine used above with OS X's Disk Utility found some issues and I have repaired them via Disk Repair. After disk repair reindexed and executed the tests two more times, no problems occurred anymore. Thus so far, hardware failure was the reason for the issue. 

Many thanks to everybody for the help!
</comment><comment author="s1monw" created="2015-06-01T10:22:51Z" id="107390075">thanks for coming back with this information! I will close the issue for now feel free to come back if you have further issues.
</comment><comment author="lexand" created="2015-08-18T12:03:50Z" id="132187270">Salutations
have the same issue but cannot determine why

newly created index
insert 50 random documents (one text field per each doc) with autogenerated _id

Search without scroll

``` PHP
        $res = $client-&gt;search(
            [
                'index'  =&gt; $index,
                'type'   =&gt; $type,
                'size'   =&gt; 100,
                'body'   =&gt; []
            ]);
        print_r(['total'    =&gt; $res['hits']['total'],
                 'returned' =&gt; count($res['hits']['hits']),
                 '_shards'  =&gt; $res['_shards']]);
```

```
Array
(
    [total] =&gt; 50
    [returned] =&gt; 50
    [_shards] =&gt; Array
        (
            [total] =&gt; 3
            [successful] =&gt; 3
            [failed] =&gt; 0
        )
)
```

But search with scroll returns next

``` PHP
        $res = $client-&gt;search(
            [
                'index'  =&gt; $index,
                'type'   =&gt; $type,
                'scroll' =&gt; '1m',
                'size'   =&gt; 10, // per shards, so I expect 30 results maximum. Where are they ?
                'body'   =&gt; []
            ]);
                $scrollId = $res['_scroll_id'];
        print_r(...);

                $res = $client-&gt;scroll(['scroll_id' =&gt; $scrollId]);
        print_r(...);
                $res = $client-&gt;scroll(['scroll_id' =&gt; $scrollId]);
        print_r(...);
                $res = $client-&gt;scroll(['scroll_id' =&gt; $scrollId]);
        print_r(...);
```

```
Array
(
    [total] =&gt; 50
    [returned] =&gt; 10
    [_shards] =&gt; Array
        (
            [total] =&gt; 3
            [successful] =&gt; 3
            [failed] =&gt; 0
        )

)
scroll 1
Array
(
    [total] =&gt; 50
    [returned] =&gt; 10
    [_shards] =&gt; Array
        (
            [total] =&gt; 3
            [successful] =&gt; 3
            [failed] =&gt; 0
        )

)
scroll 2
Array
(
    [total] =&gt; 29
    [returned] =&gt; 10
    [_shards] =&gt; Array
        (
            [total] =&gt; 3
            [successful] =&gt; 2
            [failed] =&gt; 1
            [failures] =&gt; Array
                (
                    [0] =&gt; Array
                        (
                            [status] =&gt; 404
                            [reason] =&gt; SearchContextMissingException[No search context found for id [22]]
                        )

                )

        )

)
scroll 3
Array
(
    [total] =&gt; 2
    [returned] =&gt; 2
    [_shards] =&gt; Array
        (
            [total] =&gt; 3
            [successful] =&gt; 1
            [failed] =&gt; 2
            [failures] =&gt; Array
                (
                    [0] =&gt; Array
                        (
                            [status] =&gt; 404
                            [reason] =&gt; SearchContextMissingException[No search context found for id [22]]
                        )

                    [1] =&gt; Array
                        (
                            [status] =&gt; 404
                            [reason] =&gt; SearchContextMissingException[No search context found for id [23]]
                        )

                )

        )
)
```

And totally 32 documents are returned

```
Opening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/0/index/

Segments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k-fF4_530OBpQo, translog_id=1439890621685}
  1 of 1: name=_0 docCount=21
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.024
    diagnostics = {timestamp=1439890622956, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [7 fields]
    test: field norms.........OK [2 fields]
    test: terms, freq, prox...OK [168 terms; 2078 terms/docs pairs; 2222 tokens]
    test: stored fields.......OK [42 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

No problems were detected with this index.

Opening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/1/index/

Segments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k_fF4_530OBpQp, translog_id=1439890621705}
  1 of 1: name=_0 docCount=27
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.026
    diagnostics = {timestamp=1439890622962, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [7 fields]
    test: field norms.........OK [2 fields]
    test: terms, freq, prox...OK [168 terms; 2398 terms/docs pairs; 2530 tokens]
    test: stored fields.......OK [54 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

No problems were detected with this index.

Opening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/2/index/

Segments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k_fF4_530OBpQq, translog_id=1439890621710}
  1 of 1: name=_0 docCount=2
    version=4.10.4
    codec=Lucene410
    compound=true
    numFiles=3
    size (MB)=0.007
    diagnostics = {timestamp=1439890622865, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [7 fields]
    test: field norms.........OK [2 fields]
    test: terms, freq, prox...OK [143 terms; 254 terms/docs pairs; 278 tokens]
    test: stored fields.......OK [4 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

No problems were detected with this index.
```

ES Version

``` JSON
{
  "status" : 200,
  "name" : "Temugin",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.7.1",
    "build_hash" : "b88f43fc40b0bcd7f173a1f9ee2e97816de80b19",
    "build_timestamp" : "2015-07-29T09:54:16Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

Index settings

``` JSON
{
  "test" : {
    "aliases" : { },
    "mappings" : {
      "test" : {
        "properties" : {
          "text" : {
            "type" : "string"
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1439890619858",
        "uuid" : "0OgM42BBSVGjcd_z4mTKxw",
        "number_of_replicas" : "1",
        "number_of_shards" : "3",
        "version" : {
          "created" : "1070199"
        }
      }
    },
    "warmers" : { }
  }
}
```
</comment><comment author="lexand" created="2015-08-18T12:20:24Z" id="132189908">hmm

but when I requested scroll with  'scroll' =&gt; '1m'

``` PHP
$res = $client-&gt;scroll(['scroll' =&gt; '1m', 'scroll_id' =&gt; $scrollId]);
```

i got all 50 documents
</comment><comment author="clintongormley" created="2015-08-18T15:56:25Z" id="132257993">@lexand you are not using the new scroll ID from the previous scroll response.  Please ask questions like these on the forum instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Fail shard if search execution uncovers corruption</comment><comment>If, as part of the search execution, a corruption is uncovered, we should fail the shard</comment><comment>relates to #11419</comment></comments></commit></commits></item><item><title>org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11418</link><project id="" key="" /><description>I'm using version 1.9.4 , am getting the same error when i try to index.

I checked with cluster.name in elasticsearch.yml, which i'm providing the same. I checked in online i can't find any solution .. 
</description><key id="82426503">11418</key><summary>org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">premanandchandrasekar</reporter><labels /><created>2015-05-29T13:24:15Z</created><updated>2015-05-29T13:56:29Z</updated><resolved>2015-05-29T13:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-29T13:56:29Z" id="106811088">This version does not exist.
You should open a thread on https://discuss.elastic.co/c/elasticsearch so we can help you there. Also provide details about what you did.
This space is reserved for bug reports which is I'm almost sure not the case here.

Closing for now. Let me know if you think it's really an issue in the code base.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Synced flush backport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11417</link><project id="" key="" /><description>Was relatively unproblematic, only SyncedFlushService was a little hairy
</description><key id="82403141">11417</key><summary>Synced flush backport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>feature</label><label>v1.6.0</label></labels><created>2015-05-29T12:13:46Z</created><updated>2015-05-30T08:41:15Z</updated><resolved>2015-05-30T08:38:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-05-29T15:05:36Z" id="106840703">Thx @brwe for picking this up. Left some minor comments
</comment><comment author="spinscale" created="2015-05-29T15:10:26Z" id="106841735">did not spot any huge differences on my side, except the same question @bleskes had about removing the FLUSH executor...

apart from that LGTM
</comment><comment author="brwe" created="2015-05-29T16:09:00Z" id="106859573">ok, all done. I let the bwc tests run and found that there we also have to check if it is a bwc test or not when we call indexRandom(). pushed a fix for that.
</comment><comment author="brwe" created="2015-05-29T17:02:13Z" id="106871463">fixed the bwc test and added checks to make sure we actually have at least one current version node when we run the bwc tests. want to take another look?
</comment><comment author="bleskes" created="2015-05-29T19:34:58Z" id="106910728">LGTM. Left one little comment. Feel free to push.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consolidate shard level modules without logic into IndexShardModule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11416</link><project id="" key="" /><description>We have a lot of module classes that don't contain any actual logic,
only declarative bind actions. These classes are unnecessary and can
be consolidated into the already existings IndexShardModule
</description><key id="82399733">11416</key><summary>Consolidate shard level modules without logic into IndexShardModule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T12:02:44Z</created><updated>2015-06-08T12:50:26Z</updated><resolved>2015-05-29T12:17:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-29T12:15:13Z" id="106785763">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The restart node join cluster very slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11415</link><project id="" key="" /><description>I have 3 nodes es cluster.  Each node have 64G memory and  12 x 1T sata disk。The es version is 1.4.4

When I doing rolling restart, the restarted node join cluster very slow even if I disable the cluster.routing.allocation.enable.

From the log, it may take 6 minutes to join cluster after several retrying

So I open the debug log。

When I kill the A node then start it. I found:
A node discovered the master node, but just like waiting for something and blocked.

this is B node's log

``` log
[2015-05-29 17:43:32,019][DEBUG][cluster.service          ] [B] set local cluster state to version 3810
[2015-05-29 17:49:12,923][DEBUG][discovery.zen.publish    ] [B] node [C][r910JSFeSKmdAQGIsWQ0fA][test3][inet[/xxx.xxx.xxx.xxx:9300]] responded for cl
uster state [3810] (took longer than [30s])
```

After the C responded the state 3810,  then the A node join the cluster without any block。

Then I read the souce code and found the cluster may by block by the C node changeCluster state.
So I reproduce these stage and  use jstack to dump the process stack 。

``` log
elasticsearch[231.189][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007f00a9b42000 nid=0x2f508 runnable [0x00007ef5f0e37000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
    at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
    at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:376)
    at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.writeShardState(LocalGatewayShardsState.java:294)
    at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.clusterChanged(LocalGatewayShardsState.java:153)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:208)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:460)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.j
ava:184)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:154)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
```

The I use strace to attach the process
I see

``` log
[pid 193800] stat("/data1/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
[pid 193800] open("/data1/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state/state-63", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 2530
[pid 193800] stat("/data2/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
[pid 193800] open("/data2/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state/state-63", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 2530
[pid 193800] stat("/data3/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
[pid 193800] open("/data3/logsearch/data/elasticsearch/nodes/0/indices/xxx-2015.05.13/4/_state/state-63", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 2530
......
```

What happened in the cluster? How can I speed up the cluster restart?
</description><key id="82378097">11415</key><summary>The restart node join cluster very slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">foobarget</reporter><labels /><created>2015-05-29T10:50:05Z</created><updated>2015-06-01T06:26:38Z</updated><resolved>2015-05-29T13:59:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T13:59:08Z" id="106812356">Hi @foobarget 

The best place to ask about how to configure your cluster correctly is on the forum https://discuss.elastic.co/c/elasticsearch.  That said, I'm guessing that you have a lot of indices and it is taking time to process all of the new shard allocations.  There have been a number of improvements since 1.4, especially https://github.com/elastic/elasticsearch/pull/11179 and https://github.com/elastic/elasticsearch/pull/11336 and https://github.com/elastic/elasticsearch/pull/11262.

1.6 will be out shortly - I advise upgrading to it when it is out, and it should improve cluster restart times a lot.
</comment><comment author="foobarget" created="2015-06-01T06:25:54Z" id="107321237">thanks clintongormley.
Also the cluster have lots indexes, but most of them are read only. 
I post the question to the forum.
If the 1.6 is ready, I would have a try
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Validate parsed document does not have trailing garbage that is invalid json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11414</link><project id="" key="" /><description>See #2315
</description><key id="82367528">11414</key><summary>Validate parsed document does not have trailing garbage that is invalid json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T10:20:51Z</created><updated>2015-09-29T15:54:32Z</updated><resolved>2015-08-11T04:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T17:26:57Z" id="119670582">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Mappings: Validate parsed document does not have trailing garbage that is invalid json</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Revert "Merge pull request #11414 from rjernst/fix/2315"</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Merge pull request #11414 from rjernst/fix/2315</comment></comments></commit></commits></item><item><title>Internal error when indexing,[all instances of a given field name must have the same term vectors settings </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11413</link><project id="" key="" /><description>Hi, 

We have discovered this problem while trying to upgrade from es version 1.2.3 to the latest version. 1.5.2. The same problem occurs in the version 1.3.0 and any version after that one.

To reproduce the problem execute: 

curl -XPUT 'http://localhost:9200/test/' -d '
 {
   "mappings": {
     "test": {
       "_source": {
         "enabled": false
       },
       "properties": {
         "subject": {
           "type": "multi_field",
           "fields": {
             "subject": {
               "type": "string",
               "term_vector": "with_positions_offsets"
             },
             "subject_english_stemmer": {
               "type": "string",
               "term_vector": "with_positions_offsets"
             }
           }
         }
       }
     }
   }
 }'

The error appears when starting to index the field:
 curl -XPUT 'http://localhost:9200/test/test/1' -d '
 {
 "subject":"this is test subject",
  "subject.subject_english_stemmer": "this is test subject"
}'

{"error":"IllegalArgumentException[all instances of a given field name must have the same term vectors settings (storeTermVectors changed for field=\"subject.subject_english_stemmer\")]","status":500}

It seems that the problem has started when es was updated to lucence 4.9.0:
https://lucene.apache.org/core/4_9_0/changes/Changes.html#v4.9.0.bug_fixes

So what is the workaround in this case? We can not change the mapping because we have a lot of indices and data. 
</description><key id="82362387">11413</key><summary>Internal error when indexing,[all instances of a given field name must have the same term vectors settings </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blablatruc</reporter><labels /><created>2015-05-29T10:04:54Z</created><updated>2015-05-29T13:47:55Z</updated><resolved>2015-05-29T13:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T13:47:53Z" id="106806969">Hi @blablatruc 

i'm afraid that the only solution is to reindex your data.  Fields with the same name in different types in the same index are backed by the same Lucene field internally.  Having inconsistent data written to the same Lucene field can result in corruption.

In 2.0 we're getting a lot stricter about this limitation because too many people have suffered the consequences (see #8870).  I'm sorry that I can't give you an easier fix, but it is the correct way to progress.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Close lock even if we fail to obtain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11412</link><project id="" key="" /><description /><key id="82361186">11412</key><summary>Close lock even if we fail to obtain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T09:59:56Z</created><updated>2015-06-08T12:51:26Z</updated><resolved>2015-05-29T11:37:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-29T10:05:08Z" id="106763362">looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file></files><comments><comment>Merge pull request #11412 from s1monw/close_lock</comment></comments></commit></commits></item><item><title>query string does not take field analyzer into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11411</link><project id="" key="" /><description>I define a mapping with a simple text field, with two analyzers:
- index_analyzer : a ngram indexer, with stop words and whitespace tokenizer
- index_searcher : a analyzer, with the same elision, stop words, and same tokenizer, but without ngram

I index the french text **"part de reve"**
- "de" is a word that belongs to stop words
- the ngram indexes all ngram tokens from words "part" and "reve".

When I search (using query string) the string **"part de reve"** (using AND operator), the query returns nothing.
I have to search **"part reve"** (without "de") to retrieve my data ==&gt; the stop word is not removed from query.

Here is the whole example:
-    DELETE test
-    PUT test

``` javascript
{
  "analysis" : {
    "analyzer" : {
      "ngram_indexer" : {
        "tokenizer" : "whitespace",
        "char_filter" : ["my_mapping"],
        "filter" : [
          "stop_francais",
          "substring"
        ]
      },
      "ngram_searcher" : {
        "tokenizer" : "whitespace",
        "char_filter" : ["my_mapping"],
        "filter" : [
          "stop_francais"
        ]
      }
    },
    "filter" : {
      "stop_francais": {
        "stopwords": [
          "_french_", "a"
        ],
        "type": "stop" ,
        "remove_trailing": false
      },
      "substring" : {
        "type" : "nGram",
        "min_gram" : 1,
        "max_gram"  : 40
      }
    },
    "char_filter" : {
      "my_mapping" : {
        "type" : "mapping",
        "mappings" : ["&amp;=&gt;et", "+=&gt;plus", "-=&gt;\\u0020"]
      }
    }
  }
}
```
- PUT test/test/_mapping

``` javascript
{
  "test": {
    "properties": {
      "name": {
        "type": "string",
        "index": "analyzed",
        "search_analyzer": "ngram_searcher",
        "index_analyzer": "ngram_indexer"
      }
    }
  }
}
```
- PUT test/test/1

``` javascript
{
  "id": 1,
  "name" : "part de reve"
}
```
- GET test/test/_search

``` javascript
{
  "from": 0,
  "size": 10,
  "query": {
    "query_string": {
      "query": "part de reve",
      "default_field": "name",
      "default_operator": "and"
    }
  }
}
```

==&gt; it does not return the inserted record.
(use "part reve" as query string and it returns the inserted record)
</description><key id="82348626">11411</key><summary>query string does not take field analyzer into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MarneusCalgarXP</reporter><labels><label>:Query DSL</label></labels><created>2015-05-29T09:23:23Z</created><updated>2015-06-01T09:33:24Z</updated><resolved>2015-06-01T09:33:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T13:38:41Z" id="106804458">Hi @MarneusCalgarXP 

The problem is a result of the combination of the `query_string` query with the `remove_trailing` option in the stop filter.  Because the `query_string` query supports its own mini-language, it parses the query string by consuming each token separately.

The result is that `part`, `de`, and `reve` are each passed to the analyzer separately, so the stopwords filter sees `de` as the last token in the stream, and doesn't remove it because you've set `remove_trailing` to false. This is the same problem that exists when using multi-word synonyms with the query string query... it never recognises them as synonyms.

If you use the `match` query instead, then it will analyze your query string as you expect.

One thing that surprises me is that the `simple_query_string` behaves in the same way, including `de` in the list of tokens to search for.  As far as I know, this query is supposed to consume as much continuous text as possible.  @dakrone @jdconrad any ideas here?

(Btw, @MarneusCalgarXP:  you really don't want to use min/max ngrams 1..40 - your index will simply blow up.  This is not the same as using edge_ngrams. With ngrams you probably want a range more like 2..4 max.)
</comment><comment author="jdconrad" created="2015-05-29T16:30:27Z" id="106864699">simple_query_string gets passed into Lucene's SimpleQueryParser.  SimpleQueryParser will, by default, tokenize on whitespace.  Each token is then passed into the field's analyzer as @clintongormley mentioned.  Whitespace tokenization can be turned off in SimpleQueryParser, in which case the entirety of the string would be passed to the analyzer.  How the analyzer actually handles it depends on which analyzer is being used.  For this single case, I believe you would get the behavior you're looking for if whitespace tokenization is turned off. 
</comment><comment author="clintongormley" created="2015-05-29T18:42:34Z" id="106900604">thanks @jdconrad - i'm wondering if we should turn whitespace tokenization off by default?
</comment><comment author="jdconrad" created="2015-05-29T18:50:08Z" id="106901910">I don't know enough about the typical analyzers to know if that solution would work well.  Unexpected behavior may result from something like the following:
x | y
where "x " and " y" end up becoming terms that get passed in. 
</comment><comment author="clintongormley" created="2015-05-29T18:55:19Z" id="106902871">ah right, in the case of keyword tokenizers... i see what you mean.  

Pants!
</comment><comment author="MarneusCalgarXP" created="2015-06-01T09:33:23Z" id="107380679">OK, I tested, it seems to work with a match query.
Thank you all for explanations !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent changing the number of replicas on a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11410</link><project id="" key="" /><description>Setting the number of replicas on a closed index can leave the index
in an unopenable state since we might not be able to recover a quorum.
This commit simply prevents updating this setting on a closed index.

Closes #9566
</description><key id="82337981">11410</key><summary>Prevent changing the number of replicas on a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T08:50:28Z</created><updated>2015-06-01T13:34:11Z</updated><resolved>2015-05-29T09:22:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-29T08:57:34Z" id="106749502">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IdsQueryBuilder: Allow to add a list in addition to array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11409</link><project id="" key="" /><description>In case a developer gets a `List&lt;String&gt;` of ids from another data source,
it does not make any sense, to convert it to an array first,
and then internally in `IdsQueryBuilder` a list is created a out of this again.

Closes #5089
</description><key id="82318007">11409</key><summary>IdsQueryBuilder: Allow to add a list in addition to array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-29T07:52:33Z</created><updated>2015-06-12T13:38:59Z</updated><resolved>2015-06-09T08:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-29T09:43:05Z" id="106759676">the only thing I don't like here is that we add two methods to do the same, but I see why we do it (consistency with existing methods I guess)... LGTM besides that
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Close ShardFilterCache after Store is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11408</link><project id="" key="" /><description>The ShardFilterCache relies on the fact that it's closed once the last reader on the shard is closed. This is only guaranteed once the Store and all its references are closed. This commit moves the closing into the internal callback mechanism we use for deleting shard data etc. to close the cache once we have all searchers released.
</description><key id="82316815">11408</key><summary>Close ShardFilterCache after Store is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2015-05-29T07:49:39Z</created><updated>2015-05-29T09:49:37Z</updated><resolved>2015-05-29T09:13:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-29T08:43:36Z" id="106746561">LGTM, just left one question
</comment><comment author="s1monw" created="2015-05-29T08:55:57Z" id="106748831">@jpountz pushed a new commit
</comment><comment author="jpountz" created="2015-05-29T08:56:20Z" id="106748932">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix stream version check in ShardActiveRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11407</link><project id="" key="" /><description>This change was introduced in 1.6
</description><key id="82180464">11407</key><summary>Fix stream version check in ShardActiveRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Store</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-29T00:05:54Z</created><updated>2015-06-09T11:56:49Z</updated><resolved>2015-05-29T06:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-29T02:53:34Z" id="106665366">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggester: Fix loss of precision of completion weights</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11406</link><project id="" key="" /><description>Currently suggester scores are treated as floats, which may lead to loss of precision
when using when using int weights that are greater than 2^23 in Completion Suggester.

This change treats the int weights as doubles rather than floats to ensure no loss in precision

closes #11392
</description><key id="82153487">11406</key><summary>Completion Suggester: Fix loss of precision of completion weights</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feedback_needed</label></labels><created>2015-05-28T22:33:34Z</created><updated>2016-04-07T21:10:56Z</updated><resolved>2016-04-07T20:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-10T11:00:06Z" id="194791777">@areek is this still relevant or can it be closed?
</comment><comment author="areek" created="2016-03-10T18:17:56Z" id="194985589">@clintongormley this is still relevant, I will update the PR once `feature-suggest-refactoring` is merged
</comment><comment author="dakrone" created="2016-04-06T21:02:18Z" id="206566459">@areek I think the suggest refactoring is merged (correct me if I'm wrong), so can this be merged now?
</comment><comment author="areek" created="2016-04-07T21:10:56Z" id="207089959">I have opened https://github.com/elastic/elasticsearch/pull/17603 and updated the change to master, unfortunately we need lucene level change to ensure we don't loss precision. I will open a lucene issue regarding this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parse date string '0000-00-00 00:00:00' correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11405</link><project id="" key="" /><description>When adding documents to ES using results from MySQL, attempting to parse '0000-00-00 00:00:00'  as a date results in the following error: 

```
index: /&lt;index name&gt;/1 caused MapperParsingException[failed to parse [created_at]]; nested: JsonParseException[Current token (VALUE_FALSE) not numeric, can not use numeric value accessors
 at [Source: [B@5fa33d; line: 1, column: 81]]; 
```

This appears to be a bug.
</description><key id="82096658">11405</key><summary>Parse date string '0000-00-00 00:00:00' correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhtimmins</reporter><labels /><created>2015-05-28T19:46:58Z</created><updated>2015-05-28T19:59:25Z</updated><resolved>2015-05-28T19:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow plugins to specify dependent plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11404</link><project id="" key="" /><description>Example use case: Installing the watcher plugin should automatically install the license plugin as well. At the very least, trying to install the watcher plugin should check if the license plugin is not installed and abort installation with an actionable error message.

Currently this check is performed at node startup but it would be even better UX if we could fail earlier.
</description><key id="82076059">11404</key><summary>Allow plugins to specify dependent plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ycombinator</reporter><labels><label>:Plugins</label><label>feature</label></labels><created>2015-05-28T18:42:46Z</created><updated>2015-10-02T13:30:21Z</updated><resolved>2015-10-02T13:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-28T18:49:06Z" id="106564218">IIRC @tlrx has something pending about it. The plugin manager has been refactored in another PR which needs to be pulled in.
</comment><comment author="clintongormley" created="2015-10-02T13:30:21Z" id="145020462">Closing - see https://github.com/elastic/elasticsearch/issues/13823#issuecomment-143901597
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fall back to reading SegmentInfos from Store if reading from commit fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11403</link><project id="" key="" /><description>In the event that reading from the latest commit fails, we should fall
back to reading from the `Store` using the traditional
`Directory.listAll()`

Related to #11361
</description><key id="82037175">11403</key><summary>Fall back to reading SegmentInfos from Store if reading from commit fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Store</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T17:19:57Z</created><updated>2015-06-08T12:52:51Z</updated><resolved>2015-05-28T22:37:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-28T18:01:42Z" id="106534524">Related CI failure: http://build-us-00.elastic.co/job/es_core_master_centos/4928/consoleFull
</comment><comment author="s1monw" created="2015-05-28T20:29:50Z" id="106586834">left one comment otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem occured with same name of cluster in different machines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11402</link><project id="" key="" /><description>Hello,

I have a weird issue today and it gives me lots of problem. I had two cluster different but unfortunatelly I gave them the same names by mistake; 

lets say;
test_cluster
- both cluster have had totaly different machines ( it was working well its been one month)
- today I added a machine in one of the cluster and I saw that all the machines were in the same cluster physically (it was not the case before adding the machine)
- I have chacked the unicast list; there were no machine in common

So someone can explain me how I had this problem and how I can seperate them ? 

Thank you 
</description><key id="82031079">11402</key><summary>Problem occured with same name of cluster in different machines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehmetozerr</reporter><labels /><created>2015-05-28T17:04:27Z</created><updated>2015-05-28T19:40:41Z</updated><resolved>2015-05-28T19:40:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T19:40:41Z" id="106575733">@mehmetozerr you should give them different cluster names and disable multicast (this is how the two clusters discovered each other)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Client Node Request/Fetch Circuit Breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11401</link><project id="" key="" /><description>Currently, it's possible to overload a client node during the fetch process by requesting extreme amounts of data. In particular, this is possible to observe by repeating the problem in #11070 (e.g., fetching too many things in an aggregation).
</description><key id="82023035">11401</key><summary>Client Node Request/Fetch Circuit Breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Circuit Breakers</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-05-28T16:45:20Z</created><updated>2017-03-06T18:13:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-09-08T09:08:05Z" id="245538537">This is at least partially solved by:
- #17133 - limiting the size of inflight requests
- #17396 - limiting the number of shards in a request to 1000
- #9311 - hard limit on `from`/`size`
- #19394 - circuit break on aggregation buckets

It would still be possible to overwhelm a client node by requesting too many enormous documents as hits, but the chances are greatly reduced
</comment><comment author="clintongormley" created="2017-01-23T16:18:35Z" id="274534066">@metacret rather open a pull request where people can review it properly</comment><comment author="metacret" created="2017-01-23T17:39:06Z" id="274560444">@clintongormley it was edited on v1.7.5 branch which is the version we're using. This is the urgent hot fix to prevent our tribe nodes from being crashed by long GC pause or OOM error. I just wanted to ask you to verify how feasibly my idea will work before setting up the official PR because making PR will need huge amount of work including writing all unit tests.

My summarized idea is TransportResponseHandler should keep recording how many bytes it has processed and if that number is over the threshold, MessageChannelHandler should throw an exception before response deserialization to prevent memory allocation. It was quite tricky how to record this data because TransportResponseHandler is allocated on the fly for every shard request. So I made SearchServiceListener to keep how many bytes are processed and that information is being propagated up to TransportResponseHandler.</comment><comment author="metacret" created="2017-01-23T17:45:36Z" id="274562309">I couldn't find easy solution to Install the circuit breaker in handleResponse because it was very hard for me find out how to release everything.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SearchContext not released when search request is rejected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11400</link><project id="" key="" /><description>It seams that SearchContexts are not released in case the search requests fails due to a rejection (thread pool queue full). I have a test that fails each time here: https://github.com/brwe/elasticsearch/blob/open-search-contexts/src/test/java/org/elasticsearch/search/SearchWithRejectionsTests.java
</description><key id="82022274">11400</key><summary>SearchContext not released when search request is rejected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T16:44:01Z</created><updated>2015-06-01T11:15:51Z</updated><resolved>2015-06-01T11:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-29T12:04:47Z" id="106783650">I think we should fix this for 1.6 if we can though
</comment><comment author="brwe" created="2015-05-30T15:55:17Z" id="107056752">This only happens with dfs queries. I made a pull request to fix it here: https://github.com/elastic/elasticsearch/pull/11434
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/search/SearchWithRejectionsTests.java</file></files><comments><comment>search: release search contexts after failed dfs or query phase for dfs queries</comment></comments></commit></commits></item><item><title>Export hostname as environment variable for plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11399</link><project id="" key="" /><description>In #9474, we exported the hostname in the bin/elasticsearch scripts so that it
could be used as a variable in the elasticsearch.yml file but did not do the same
for plugin manager. When using the hostname variable in elasticsearch.yml and
trying to use the plugin manager, initialization will fail because the property could
not be resolved. This change will allow the hostname to be resolved in the same
manner as the service scripts.

Closes #10902
</description><key id="82009510">11399</key><summary>Export hostname as environment variable for plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T16:18:19Z</created><updated>2015-05-28T19:41:01Z</updated><resolved>2015-05-28T17:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-28T16:18:34Z" id="106457162">@tlrx can you please review?
</comment><comment author="tlrx" created="2015-05-28T16:24:08Z" id="106462478">LGTM, thanks for taking care of this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix the quotes in the explain message for a script score function without parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11398</link><project id="" key="" /><description>…hout parameters
</description><key id="81980690">11398</key><summary>Fix the quotes in the explain message for a script score function without parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">ankon</reporter><labels><label>:Search</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-05-28T15:22:32Z</created><updated>2015-10-15T17:58:27Z</updated><resolved>2015-10-15T17:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T19:34:21Z" id="106574581">@brwe could you review please?
</comment><comment author="ankon" created="2015-10-08T12:40:57Z" id="146525833">Anything blocking here? 
</comment><comment author="nik9000" created="2015-10-08T12:44:55Z" id="146526951">LGTM
</comment><comment author="nik9000" created="2015-10-08T13:13:17Z" id="146540495">@ankon, could you rebase this against master? I'm sorry its so long.
</comment><comment author="ankon" created="2015-10-13T09:00:23Z" id="147653559">@nik9000 sure, can do :)
</comment><comment author="nik9000" created="2015-10-15T17:56:17Z" id="148472587">OK! All tests pass and I'm merging and backporting all the way to 2.x and 2.1 branches.
</comment><comment author="nik9000" created="2015-10-15T17:58:27Z" id="148473133">And done! Thanks @ankon!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file></files><comments><comment>Merge pull request #11398 from Collaborne/bugs/explain-function-script-quotes</comment></comments></commit></commits></item><item><title>Serialization: Remove old version checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11397</link><project id="" key="" /><description>As the 2.x release does not need to be backwards compatible in terms of
serialization, we can remove a fair share of the serialization checks.

TODO: Unsure about the `MetaData` and the `Priority` version check removal. Just want to make sure, we dont persist any of those (snapshot/restore maybe?) - maybe someone can clarify.
</description><key id="81975637">11397</key><summary>Serialization: Remove old version checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T15:12:54Z</created><updated>2015-05-29T13:15:24Z</updated><resolved>2015-05-29T06:46:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-28T15:18:56Z" id="106399449">@spinscale even if we do persist them I have no idea how we would know that these are written by an old version. S/R uses XContent to persist stuff so I think it's all fine
</comment><comment author="s1monw" created="2015-05-28T15:18:59Z" id="106399487">LGTM
</comment><comment author="dakrone" created="2015-05-28T15:26:44Z" id="106408585">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Acquire index writer lock before renaming translog file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11396</link><project id="" key="" /><description>in 1.x we still rename the translog file before recovery. Yet,
essentially the translog is guarded by the IW lock but we perform
these operations before we start the engine. This can cause trouble
in the shared FS case where we do full restarts and the lock owning
node has already dropped off the cluster (another taking over the primary)
but has not yet flushed it's translog. If we wait for the IW lock we guarantee
that the translog has been flushed.
This fixes CI failures like  http://build-us-00.elastic.co/job/es_g1gc_1x_metal/9894/
</description><key id="81971525">11396</key><summary>Acquire index writer lock before renaming translog file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-28T15:04:26Z</created><updated>2015-05-28T19:33:21Z</updated><resolved>2015-05-28T15:16:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-28T15:06:22Z" id="106386597">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Illegal state log entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11395</link><project id="" key="" /><description>I'm seeing this log message for random indices several times a day in my cluster:

```
[2015-05-28 15:19:49,092][WARN ][index.shard              ] [mynode] [index-foo][2] suspect illegal state: trying to move shard from primary mode to replica mode
```

Also, maybe unrelated, the cluster is relocating shards all the time, never stopping.

Other than that all seems normal in the cluster.
</description><key id="81956331">11395</key><summary>Illegal state log entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2015-05-28T14:34:36Z</created><updated>2015-06-12T16:17:38Z</updated><resolved>2015-06-12T16:17:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-28T15:05:41Z" id="106385860">what version of es are you running? can we have more info about your logs?
</comment><comment author="faxm0dem" created="2015-05-29T06:31:25Z" id="106707571">1.5.2 on EL6 other close log entries only show a few unrelated (other index) parse failures
</comment><comment author="bleskes" created="2015-05-29T07:25:34Z" id="106722337">Is there anything interesting in the master node logs? any idea why shards will be continuously relocated?

can you also check that all nodes report the same node via `GET _cat/master` ?
</comment><comment author="faxm0dem" created="2015-05-29T11:33:47Z" id="106779608">- all 9 nodes see the same master
- nothing exciting in the master's logfile mainly:
  - `update_mapping [...] (dynamic)`
  - `creating index`
  - `now throttling indexing`
- I tried reindexing the indices that seem to have the most `suspect illegal state` messages. Let's see what happens
</comment><comment author="faxm0dem" created="2015-06-11T11:02:16Z" id="111085148">### Update

I do not have any `suspect illegal state` messages anymore since reindexing all indices that used to trigger those in the past.
I guess you can close this if you like, but my feeling is this issue will pop up again, not necessarily on my account.
</comment><comment author="clintongormley" created="2015-06-12T16:17:37Z" id="111541071">thanks for letting us know @faxm0dem - closing for now
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better support for async indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11394</link><project id="" key="" /><description>I am indexing from a message stream where I can get bursts of about 1m messages in about 10sec.
Elastic (embedded, 1 node, 5 shards at the moment) wont keep up with this.

To maximise throughput, I would like to be able to index these asynchronously (ie, call IndexRequestBuilder.execute() with no .actionGet() ).
If I do, however, within a few seconds, the indexing threadpool queue gets overrun and I get EsRejectedExecutionException errors.

What I would like to be able to do is specify on the request that the call blocks until it is queued rather than giving me an EsRejectedExecutionException - ie slowing the client down rather than rejecting the requests.

At the moment, I run the indexing requests through a ThreadPoolExecutor with a fixed capacity queue and a CallerRunsPolicy - so that when the queue reaches capacity. It would be nice if I didnt need this additional threadpool, and instead I could lean directly on Elastic's threadpool.

(BTW, I can optimise further by batching index requests - but I doubt it would eliminate the problem - and also its not completely trivial because I frequently end up with multiple updates to the same document in the one batch - which then fails)

I am presently using the Java API, but I guess this is applicable to all interfaces.
</description><key id="81932087">11394</key><summary>Better support for async indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2015-05-28T13:43:39Z</created><updated>2016-01-18T19:55:15Z</updated><resolved>2016-01-18T19:55:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T19:28:25Z" id="106573408">hi @nickminutello 

(not a java dev but...)

What you're asking for is back pressure, which is best implemented by indexing synchronously with multiple threads (it sounds like what you're doing today).  In fact, we're deprecating async indexing because it complicates other features that we're introducing (https://github.com/elastic/elasticsearch/pull/10642)

If Elasticsearch has to queue these requests up, it ends up consuming memory. That's the last thing you want to do on your server.  I think what you're doing already is correct.

(repeat: not a java dev and apologies if i'm missing something)
</comment><comment author="nik9000" created="2015-05-28T19:53:41Z" id="106578857">&gt; In fact, we're deprecating async indexing because it complicates other features that we're introducing (#10642)

I just skimmed it but that looks like async replication not async indexing on the client, right?

&gt; What you're asking for is back pressure

@nickminutello didn't ask for backpressure, but it'd be useful in this situation. If the inserting application could signal to the queueing application that it needs to slow down and maybe discard some data points, that'd be back pressure. Its really useful in the kind of real time systems I don't work on any more!

I think this is the crux of the request:

&gt; the call blocks until it is queued rather than giving me an EsRejectedExecutionException

Which makes sense to me. java.util.concurrent doesn't have a caller blocks policy, annoyingly. So that is a pain. But it'd still be nice if the java api blocked instead of barfed on these. But you'd need to support some different exception signatures which is, I think, why Java doesn't do it by default.
</comment><comment author="nickminutello" created="2015-06-01T15:25:13Z" id="107581346">@nik9000 I presumed the caller blocking was a form of back-pressure.
But yes, some way of pushing back and making the async indexing client wait.

It doesnt have to even block, per se, it could just execute the request synchronously - which has the desired effect of pushing back on the client.

@clintongormley I understand what you mean by memory consumption - blocking the caller implies keeping the request in memory, however that memory consumption would be proportional to the number of indexing client threads, so in most cases, that will be naturally bounded.

It just seems a bit of a shame to have another level of threadpool on the client to achieve asynchronicity - when elastic natively handles 90% of the problem - async indexing, with bounded queue, etc but only 1 policy for dealing with queue over-capacity.
If the indexing client code is quite simple (which in my case it is) then adding thread-pooling, etc is a big delta in complexity.
</comment><comment author="clintongormley" created="2016-01-18T19:55:15Z" id="172634578">We've removed async indexing in order to be able to support synced flushes, so I'm going to close this ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster Health: Add wait time for pending task and recovery percentage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11393</link><project id="" key="" /><description>In order to get a quick overview using by simply checking the cluster state
and its corresponding cat API, the following two attributes have been added
to the cluster health response:
- pending_task_time_in_queue, the time value of the first task of the
  queue and how long it has been waiting
- recovery percent: The percentage of the number of shards that are in
  initializing state

This makes the cluster health API handy to check, when a fully restarted
cluster is back up and running.

In addition a small serialization fix has been added, which removes version
checks for the this branch in the ClusterHealthResponse.

Closes #10805
</description><key id="81924832">11393</key><summary>Cluster Health: Add wait time for pending task and recovery percentage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Stats</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T13:25:59Z</created><updated>2015-06-22T13:07:52Z</updated><resolved>2015-06-22T13:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-28T13:28:19Z" id="106309060">biggest question here is, if the `shardRouting.initialized()` call is sufficient as an information, that this shard is being recovered.. if not this cannot be implemented as part of the cluster health being a master only operation, as we need to get shard information and the current `RecoveryState`
</comment><comment author="spinscale" created="2015-06-01T08:55:15Z" id="107372090">@clintongormley care to take a look, if this matches your expectation, even though no new cat API has been added?
</comment><comment author="clintongormley" created="2015-06-02T17:50:54Z" id="108029732">LGTM
</comment><comment author="bleskes" created="2015-06-03T06:58:07Z" id="108217470">Hey @spinscale, left some comment about naming and implementation. I think we comments also address the safe use of `shardRouting.initialized()` - I think we should make it a master only API, no reaching out to the individual nodes.  If we want to have an aggregation of the `_recovery` info, we should add it as a header there imho.
</comment><comment author="spinscale" created="2015-06-03T12:54:59Z" id="108388962">@bleskes so, you are proposing two important changes here, which I want to understand before applying them
1. `pending_task_waiting` should become the longest waiting time in the task list. This means, that when the cluster continouosly accepts new pending tasks with higher priority we do not see a change here. However, as one can also see the number of pending tasks and those change, this might be ok. What worries my about the approach of scanning all tasks, is that the speed of the cluster health response is dependent on the number of tasks in the queue. Slowing down this API doesnt sound like a good idea.
2. `recovery_percent` should become `active_percent`. Sounds good. The current metric is pretty useless, as the number of concurrent recoveries per node (2 by default iirc) and the number of total shards is going to be static. Something I hadnt thought about yet, is how unassigned shards come into play here. But I dont think it is a big problem that the active percentage is 50%, when you have a single node and one replica for your indices.
</comment><comment author="bleskes" created="2015-06-08T13:24:33Z" id="109994586">&gt; What worries my about the approach of scanning all tasks, is that the speed of the cluster health response is dependent on the number of tasks in the queue.

That was my concern as well when thinking about this but I decided it's worth it. I think it's more power full this way. It's only an array scan so we're still talking super fast, even if it's 10K ops.

&gt; But I dont think it is a big problem that the active percentage is 50%, when you have a single node and one replica for your indices.

Yeah, I think it communicates clearly how bad is "YELLOW"
</comment><comment author="spinscale" created="2015-06-15T14:43:26Z" id="112093882">upgraded the PR with all the naming refactoring and scanning all the tasks (also introduced a `HasCreationDate` interface to make it simpler to read the creation date).
</comment><comment author="bleskes" created="2015-06-16T14:09:38Z" id="112445232">I like this! one thing I would love seeing is getting rid of org.elasticsearch.cluster.service.InternalClusterService.TimedPrioritizedRunnable - the time aspect should be folded into the executor..
</comment><comment author="spinscale" created="2015-06-17T12:44:54Z" id="112785153">@bleskes thx for the comments, folded them in, also removed the `TimedPrioritizedRunnable`
</comment><comment author="bleskes" created="2015-06-19T08:39:24Z" id="113433409">Went through it and it looks good! I mis a non-cat rest test. If you agreed with the comments, feel free to push this  - no need for another round.

I also wonder if we should bite the bullet and add active_primary_shards_percent (since we have  active_primary_shards )
</comment><comment author="spinscale" created="2015-06-22T12:05:30Z" id="114080380">I'll add another test and check the renaming as well. will push upon running tests. Thanks for reviewing!
</comment><comment author="spinscale" created="2015-06-22T13:07:51Z" id="114096157">closed by https://github.com/elastic/elasticsearch/commit/88f8d58c8bec33494139a10c921afd55d3895d7c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Suggest weights are silently bucketed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11392</link><project id="" key="" /><description>The positive integer `weight` provided to the `completion` type to allow custom results ranking seems to be converted to an approximate score, apparently using some bucket scheme.

The resulting scores are unevenly distributed which further skews the picture: 

```
weight 2144409647 =&gt; score 2144409600
weight 2144397647 =&gt; score 2144397700
```

The ordering of suggestions depends first on the score, second on alphabetical sorting. Hence, if two different weights are conflated into a single score, then alphabetical sorting takes over which is often not what you want.

The correct behavior is to use the assigned weight _as is_.

Note that a workaround seems to be to manually space out weights values by some constant, e.g. 1000.
</description><key id="81919571">11392</key><summary>Suggest weights are silently bucketed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">malthe</reporter><labels><label>:Suggesters</label></labels><created>2015-05-28T13:09:38Z</created><updated>2015-05-29T06:58:54Z</updated><resolved>2015-05-29T06:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T19:20:43Z" id="106571146">what happens in the new suggester @areek?
</comment><comment author="areek" created="2015-05-28T22:01:48Z" id="106613029">Hi @malthe,
Thanks for reporting this.
Internally, the integer `weight` is returned as a `float`. In Java, a `float` can represent values &lt;= `2^23` without losing precision. Both `2144409647` and `2144397647` are &gt; `2^23`, hence the loss in precision. You should not see the loss of precision if you use `weight` &lt;= `2^23`.
One way to avoid this precision loss is to return `weight` as a `double` instead, I will open an issue for this.

@clintongormley this is not related to completion suggester, but how the scores are handled after.
</comment><comment author="malthe" created="2015-05-29T06:58:53Z" id="106713432">@areek ah that's what's happening. Thanks for clarifying – _and fixing_.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST Test blacklist should not use PathMatcher</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11391</link><project id="" key="" /><description>Currently we use `PathMatcher` to perform matching on REST test names against a blacklist. This is an abuse of the the `PathMatcher` class since it is intended to be used with a `Path`, which is intended for file based operations as the javadocs state:

&gt; An object that may be used to locate a file in a file system.

While it may work right now, there are always special cases at play like #11389, which makes this brittle. We should think about another way to do the matching for blacklists. At the same time, there is another issue which is that we specify the blacklist as a comma separated list, but we also have tests with a comma in their name so this is problematic. A workaround exists, which is to use a glob pattern but this can blacklist more tests than necessary.
</description><key id="81900389">11391</key><summary>REST Test blacklist should not use PathMatcher</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>low hanging fruit</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-05-28T12:10:44Z</created><updated>2015-11-21T09:32:41Z</updated><resolved>2015-11-19T14:30:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-21T16:52:59Z" id="149960203">Seems like there is not much to discuss here, thanks Jay for raising it, it's a good point. I originally used the PathMatcher because I thought it was ok, didn't see all those issues coming to be honest and I didn't want to write my own path matcher at the time. Marking adoptme.
</comment><comment author="danielmitterdorfer" created="2015-11-03T13:03:14Z" id="153346275">I had a look at it and I see basically these options:
1. We can implement our own globbing from scratch
2. We can use a copy of `com.google.common.jimfs.GlobToRegex` (licensed under ASL 2.0) and base our implementation on that, which would then match internally using a regex.
3. We can change the format of the property `tests.rest.blacklist` and allow only regexes

In case 1, I'm not sure whether we really need the whole set of globbing options or a subset is sufficient, I frown upon copying (case 2) and case 3 is obviously breaking and a tad more quirky to use. What do you think @javanna?
</comment><comment author="danielmitterdorfer" created="2015-11-04T08:25:40Z" id="153633083">Just a quick update: After some discussion with different developers, we've decided to check with @jaymode when he's back again. I'll assign myself for now.
</comment><comment author="jaymode" created="2015-11-16T13:29:33Z" id="157027078">thanks for taking this on. I think just supporting `*` would be enough as I think that's all we use now and hopefully we can also add a way to escape a `,` in the test names so we don't have to skip more tests than necessary
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test-framework/src/main/java/org/elasticsearch/test/rest/BlacklistedPathPatternMatcher.java</file><file>test-framework/src/main/java/org/elasticsearch/test/rest/ESRestTestCase.java</file><file>test-framework/src/test/java/org/elasticsearch/test/rest/BlacklistedPathPatternMatcherTests.java</file></files><comments><comment>Use a specific matcher implementation for REST test blacklists</comment></comments></commit></commits></item><item><title>Consolidate directory lock obtain code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11390</link><project id="" key="" /><description>The Directory#makeLock API is trappy and can easily lead to unexpected
lock release if native locks are used. see LUCENE-6507 for details.
This commit consolidates the lock lock into one place and only returns
the lock instance if we actually acquired it.
</description><key id="81898842">11390</key><summary>Consolidate directory lock obtain code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T12:06:04Z</created><updated>2015-05-29T15:53:46Z</updated><resolved>2015-05-28T14:06:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-28T14:02:43Z" id="106326979">looks good. it simplifies callers usage of locks. but this wont' avoid any problems from LUCENE-6507, its just a good cleanup.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test: filter out colons in test section names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11389</link><project id="" key="" /><description>On Windows, colons ':' are illegal in file names and since we use a Path to
check if the test is blacklisted, tests with a colon in the test section name
will fail. This change simply removes the colon from the name when matching
against the blacklist.
</description><key id="81874799">11389</key><summary>Test: filter out colons in test section names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-28T10:53:14Z</created><updated>2015-05-28T12:15:35Z</updated><resolved>2015-05-28T12:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-28T11:03:54Z" id="106276518">looks ok to me. but we should open a followup issue... i dont quite understand why we are formulating paths from things that aren't really paths, seems like pathmatcher is being abused here.
</comment><comment author="jaymode" created="2015-05-28T12:11:25Z" id="106292431">Agreed. I opened up #11391 as the followup issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cat fielddata endpoint does funky things if field names are [id, ip, host, node, total]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11388</link><project id="" key="" /><description>To recreate, start with a freshly started cluster (1.5.2) with nothing stored in fielddata.

```
GET /_cat/fielddata?v

  id                     host                      ip         node  total 
  oXbVJr3_QNqRhTSkKB1WSw Olivers-MacBook-Pro.local 10.4.1.106 Crown    0b 
```

Now create a document that contains fields with the same name as the 'meta' fields that appear in the output to fielddata.

```
POST /lul/wat/1
{
  "id": "hah",
  "host": "yeah",
  "ip": 1,
  "node": "lulwat",
  "total": 5
}
```

Then search with aggregations against those fields, to load them into fielddata.

```
POST /lul/wat/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "one": {
      "terms": {
        "field": "ip"
      }
    },
    "two": {
      "terms": {
        "field": "node"
      }
    }
  }
}
```

Now check the _cat/fielddata endpoint again.

```
GET /_cat/fielddata?v

  id                     host                      ip         node     total  node       ip
  Bt6uoLNcS0mlVSdY1xGgSA Olivers-MacBook-Pro.local 10.4.1.106 Crown    8.9kb  Crown      10.4.1.106
```

Note that `node` and `ip` are both repeated, but we can't actually see the usage for those fields - instead we get the metadata again. For the sake of completeness...

```
POST /lul/wat/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "one": {
      "terms": {
        "field": "ip"
      }
    },
    "two": {
      "terms": {
        "field": "node"
      }
    },
    "three": {
      "terms": {
        "field": "host"
      }
    },
    "four": {
      "terms": {
        "field": "id"
      }
    },
    "five": {
      "terms": {
        "field": "total"
      }
    }
  }
}
```

```
GET /_cat/fielddata?v

  id                     host                      ip         node   total node   total ip         host                      id
  oXbVJr3_QNqRhTSkKB1WSw Olivers-MacBook-Pro.local 10.4.1.106 Crown 18.4kb Crown 18.4kb 10.4.1.106 Olivers-MacBook-Pro.local oXbVJr3_QNqRhTSkKB1WSw
```

Note that locally I can recreate the field duplication, but in our production environment (1.4.0) we see even stranger things:

```
  id                     host                           ip             node          total content.source name.raw delivery.id timestamp id                      _type sections    _id created delivery.channel content.id campaign published content.sections content.topics ip             referrer topics content.created source title.raw       p property.type _parent property.id 
  NcxVMomeTC6TOTEVH9lt3Q [REDACTED]                     [REDACTED]     prd-elastic-c 2.7gb           16mb       0b     805.9kb   172.3mb NcxVMomeTC6TOTEVH9lt3Q 32.8mb    9.9mb 52.2mb  34.4mb          491.3kb    116.7mb   82.5mb       3mb          113.5mb          1.5gb [REDACTED]     110.8mb 29.9mb            31mb 10.6mb     111kb 400.7mb        26.8mb 741.5mb       1.3mb 
                 152.6kb [REDACTED]                     0b             prd-elastic-a 1.5gb          6.4mb       0b     645.1kb    86.7mb                152.6kb 17.8mb    4.7mb 36.1mb    24mb          458.4kb     79.1mb   48.3mb     1.9mb           75.1mb        859.9mb          0b    84.3mb 12.7mb              0b  4.4mb   836.9kb 197.5mb          11mb 493.9mb     861.5kb 
  CqC9hO3tT2O_j2gMPrc2Uw [REDACTED]                     [REDACTED]     prd-elastic-f 3.1gb         23.1mb       0b     865.4kb   160.3mb CqC9hO3tT2O_j2gMPrc2Uw 18.1mb    8.5mb 49.3mb  35.2mb          559.1kb    135.2mb   80.2mb       3mb          128.7mb          1.9gb [REDACTED]     124.1mb 22.8mb          35.3mb  8.1mb   577.3kb 368.8mb        22.8mb 671.8mb       1.2mb 
                   5.6kb [REDACTED]                     0b             prd-elastic-d   3gb         18.7mb       0b       1.2mb   159.9mb                  5.6kb 12.5mb    9.2mb 51.3mb    38mb          769.9kb    134.1mb     82mb     3.6mb          132.4mb          1.7gb          0b    141mb 25.7mb          58.8mb  9.5mb   399.7kb 408.7mb          23mb 709.8mb       1.3mb 
  WkXcjwj8T6yQ1X2Pb_M2ow [REDACTED]                     [REDACTED]     prd-elastic-b 1.9gb         15.8mb       0b       1.1mb    87.3mb WkXcjwj8T6yQ1X2Pb_M2ow     0b    5.1mb 34.5mb  35.9mb          593.9kb     86.5mb   60.7mb     1.9mb           79.1mb          1.1gb [REDACTED]     87.7mb 19.2mb            31mb  7.2mb   110.6kb 202.9mb        20.3mb 505.8mb       1.2mb 
                 116.6kb [REDACTED]                     38.2mb         prd-elastic-e 3.5gb         25.9mb   21.6kb         1mb   204.5mb                116.6kb     0b   11.1mb 57.2mb    34mb          724.2kb    161.6mb   90.7mb     3.6mb          152.8mb          2.1gb      38.2mb    150mb 31.5mb          29.9mb 10.2mb   615.9kb 368.8mb        32.8mb 764.5mb       1.7mb 
```

`ip` and `id` are both repeated, but in the output from _some_ nodes the id and ip fields display the fielddata size for the fields, and _some_ of the nodes display the metadata as I guess they should. I haven't been able to reproduce this locally (it might have been fixed since 1.4.0?), nor have I haven't yet been able to find any pattern to which nodes display fielddata sizes and which display the metadata.

Thanks!
</description><key id="81868598">11388</key><summary>cat fielddata endpoint does funky things if field names are [id, ip, host, node, total]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">36degrees</reporter><labels><label>:CAT API</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-05-28T10:34:55Z</created><updated>2015-06-18T17:00:48Z</updated><resolved>2015-06-18T17:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T19:08:04Z" id="106568531">Hi @36degrees 

Thanks for reporting the issue and giving such a good reproduction.
</comment><comment author="szroland" created="2015-06-17T00:03:53Z" id="112607473">The code uses maps internally and practically depends on unique column names. A simple workaround would be to recognize conflicting field names, and maybe prefix them with an underscore. E.g. `ip` column would show the ip address of the node, and `_ip` would show the ip fieldata size.

However, this becomes redundant if #10249 is implemented. That issue is about changing this API not to use dynamic columns, but include one row per field and a total row.

Which way should this go?
</comment><comment author="36degrees" created="2015-06-17T08:10:38Z" id="112710086">FWIW, I like the proposal outlined in #10249, it makes a lot of sense to me.

But, failing that, I'd 'expect' the meta fields to be the ones prefixed with underscores (and always, not just when conflicting), and 'my' column names to remain untouched.

But then there's nothing to stop me from indexing something with a field name of `_ip` (as far as I know?) and then all that's been achieved is making the conflict less likely? And prefixing the meta fields with underscores would be a breaking change?
</comment><comment author="szroland" created="2015-06-17T14:43:19Z" id="112827749">Yes, that's why I suggested prefixing the conflicting field name. No matter how we name the internal fields, you could create a field with the same name and have this same problem. We could also prefix all dynamic fields in the table and be consistent (maybe having an alias if that is not conflicting), but that would be more "breaking" a change.

Then again, if #10249 is the right direction, I could simply look into implementing that instead. 
</comment><comment author="clintongormley" created="2015-06-18T16:57:31Z" id="113219692">I'm +1 on going with #10249
</comment><comment author="clintongormley" created="2015-06-18T17:00:47Z" id="113221166">In fact, I think we should close this in favour of #10249
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SSO Plugin for ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11387</link><project id="" key="" /><description>Hi I'm a Apache Fediz Developer and I would like to build an authentication plugin for elasticsearch which would make it possible to use SSO based on WS-Federation. Until now I could not find any documentation on how to extend the shield plugin, with custom authentication styles. I could only find information about the realm concept with basic authentication. I would like integrate my solution with the features from the shield plugin. Will this be possible, or do I need to reinvent the wheel to make it work?
</description><key id="81857318">11387</key><summary>SSO Plugin for ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">janbernhardt</reporter><labels /><created>2015-05-28T10:03:50Z</created><updated>2015-05-28T20:28:32Z</updated><resolved>2015-05-28T20:28:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-05-28T20:28:31Z" id="106586562">Currently it's not possible to extend Shield. That said, Shield realm infrastructure was built with that in mind. We're working on exposing the appropriate extension points in Shield such that you'll be able to write and plug in you own realm implementation. This option will be available in the one of the next few releases (targeted to this year).

As a side note, if you have more questions around this topic or Shield in general, please post them on our dedicated shield forum at https://discuss.elastic.co/c/shield (elasticsearch repo is not the best place for it... so I'll be closing this issue)

thx!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>search by polygon can not get all result, can someone tell me why?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11386</link><project id="" key="" /><description>HI all, I am sorry ,but I can not open https://discuss.elastic.co/,so I have to ask the problem here。when I search by polygon I can not get all result, I just get a part of result. can sb tell me why？

when I debug the code, I find that when my geoPoint is [38.90340748200646, 121.52986586093903], but 
filterBuilder is {
  "geo_polygon" : {
    "location" : {
      "points" : [ [ 121.52991950511932, 38.90357029204773 ], [ 121.52986586093903, 38.90340748200646 ] ]
    }
  }
}
</description><key id="81852151">11386</key><summary>search by polygon can not get all result, can someone tell me why?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmqwudi</reporter><labels /><created>2015-05-28T09:49:03Z</created><updated>2015-05-28T18:27:53Z</updated><resolved>2015-05-28T18:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-28T18:27:53Z" id="106551188">Hi @lmqwudi 

Why can't you open  https://discuss.elastic.co/ ? It works for me (including in incognito mode).

Either way, this list isn't for questions about how to use the API. Happy to help you access the forum if there is a problem.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Catch UnsatisfiedLinkError on JNA load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11385</link><project id="" key="" /><description>This catches UnsatifiedLinkError when attempting to load the JNA Native
class, in cases where there are error loading the native libraries that JNA
needs to function.
</description><key id="81584725">11385</key><summary>Catch UnsatisfiedLinkError on JNA load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T19:16:18Z</created><updated>2015-06-07T10:12:43Z</updated><resolved>2015-05-27T19:34:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-27T19:18:31Z" id="106040104">Can we also pass 'e' to logger messages? Otherwise looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better exception if array passed to `term` query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11384</link><project id="" key="" /><description>Added a check that explicitly throws an exception in case of an array, and a small unit test to verify it. 

By the way, the particular test case would not throw an exception before the fix, but would actually only evaluate the last value of the array, so the behavior was indeed erratic in case of an array.

Closes #11246.
</description><key id="81563839">11384</key><summary>Better exception if array passed to `term` query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">spyk</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T18:20:50Z</created><updated>2015-06-06T14:02:38Z</updated><resolved>2015-05-29T11:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-28T14:50:27Z" id="106371542">Hi @spyk thanks for your PR, would you mind signing our [CLA](http://www.elastic.co/contributor-agreement) so we can merge it in please?
</comment><comment author="spyk" created="2015-05-28T15:05:40Z" id="106385846">Yep, signed yesterday just after the PR, do I need to do anything else to update the status?
</comment><comment author="clintongormley" created="2015-05-28T15:09:11Z" id="106389524">Yes - I see the signature, it uses a different email address.  thanks @spyk 
</comment><comment author="javanna" created="2015-05-29T11:41:31Z" id="106780484">Thanks @spyk merged! Also marking as breaking as people who might be sending an array will get errors back now.
</comment><comment author="clintongormley" created="2015-06-06T14:02:38Z" id="109581583">I think this is a bug fix rather than a breaking enhancement - relabelled as such.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: throw an exception if array passed to `term` query.</comment></comments></commit></commits></item><item><title>Ignore 3x segment upgrade if unneeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11383</link><project id="" key="" /><description>If the index was created with 0.90.x or later, skip the check for 3x
segments, since Lucene 4.x was used for those versions.

This helps in particular with sharedfs recoveries, where the check for
upgrading segments can cause FS race conditions.

(Note, this is for 1.x only)

Slightly related to #11361
</description><key id="81541733">11383</key><summary>Ignore 3x segment upgrade if unneeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Engine</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-27T17:24:32Z</created><updated>2015-06-01T22:34:27Z</updated><resolved>2015-05-28T16:24:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-28T15:44:53Z" id="106427296">@s1monw pushed another commit making this encapsulated into the InternalEngine
</comment><comment author="s1monw" created="2015-05-28T15:58:19Z" id="106442066">left one comment otherwise LGTM
</comment><comment author="imotov" created="2015-05-28T23:14:35Z" id="106624588">@s1monw Are we saying that v1.6 will not be compatible with indices created before v0.19?
</comment><comment author="s1monw" created="2015-05-29T05:25:35Z" id="106692570">@imotov see https://github.com/elastic/elasticsearch/blob/1.x/src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java#L472 we are upgrading them
</comment><comment author="imotov" created="2015-05-29T05:40:45Z" id="106696596">@s1monw I see. Thanks! It looks like this is going to be a problem only in master, which cannot open these files anyway. It might be a good idea to improve the error message though. I will take a look.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typed parameters in IndexRequestBuilder and CreateIndexRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11382</link><project id="" key="" /><description>IndexRequestBuilder#setSource as well as CreateIndexRequestBuilder#setSettings and CreateIndexRequestBuilder#setSouce will not work with `Map&lt;String, String&gt;` argument although the API looks like it should. This PR fixes the problem introducing correct wildcard parameters and adds tests.

Closes #10825
</description><key id="81533079">11382</key><summary>Fix typed parameters in IndexRequestBuilder and CreateIndexRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T17:01:21Z</created><updated>2015-05-29T14:03:08Z</updated><resolved>2015-05-29T12:56:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-28T14:45:57Z" id="106370216">left one comment around testing, looks good besides that
</comment><comment author="cbuescher" created="2015-05-29T11:16:10Z" id="106777277">I changed the tests to use an existing NoOpClient to make it a proper unit test case. To facilitate that I moved the formerly private test client to its own class alongside the original test it came from. This way I can use it in my own test setup.
</comment><comment author="javanna" created="2015-05-29T11:23:29Z" id="106778154">looks great thanks a lot @cbuescher 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java</file><file>src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java</file><file>src/test/java/org/elasticsearch/rest/HeadersAndContextCopyClientTests.java</file><file>src/test/java/org/elasticsearch/rest/NoOpClient.java</file></files><comments><comment>Merge pull request #11382 from cbuescher/fix/10825</comment></comments></commit></commits></item><item><title>Remove unused code.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11381</link><project id="" key="" /><description /><key id="81519815">11381</key><summary>Remove unused code.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T16:28:53Z</created><updated>2015-06-08T08:51:20Z</updated><resolved>2015-05-29T08:11:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-27T16:52:43Z" id="105991496">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/common/collect/BoundedTreeSet.java</file><file>src/main/java/org/elasticsearch/common/collect/IdentityHashSet.java</file><file>src/main/java/org/elasticsearch/common/collect/ImmutableOpenLongMap.java</file><file>src/main/java/org/elasticsearch/common/collect/Iterators2.java</file><file>src/main/java/org/elasticsearch/common/lucene/HashedBytesRef.java</file><file>src/main/java/org/elasticsearch/common/lucene/store/ThreadSafeInputStreamIndexInput.java</file><file>src/main/java/org/elasticsearch/common/unit/Percent.java</file><file>src/test/java/org/elasticsearch/common/collect/Iterators2Tests.java</file></files><comments><comment>Merge pull request #11381 from jpountz/fix/remove_unused_code</comment></comments></commit></commits></item><item><title>Sibling Pipeline Aggregations can now be nested in SingleBucketAggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11380</link><project id="" key="" /><description>Previously this would throw a ClassCastException as we explicitly cast the parent aggregation to a InternalMultiBucketAggregation

Closes #11379
</description><key id="81511015">11380</key><summary>Sibling Pipeline Aggregations can now be nested in SingleBucketAggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T16:00:50Z</created><updated>2015-05-27T16:37:14Z</updated><resolved>2015-05-27T16:36:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-27T16:04:49Z" id="105977591">LGTM
</comment><comment author="colings86" created="2015-05-27T16:36:20Z" id="105987035">Pushed to master in https://github.com/elastic/elasticsearch/commit/95faa35853bff06d14c9071120988c0cba9aae84
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sibling Pipeline Aggregations only work if nested in multi-bucket aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11379</link><project id="" key="" /><description>Sibling Pipeline Aggregations don't work if their parent aggregation is a SingleBucketAggregation. This is because [1] explicitly casts the parent aggregation to a MultiBucketAggregation. We should check the type of the parent aggregation and process it as now if its a MultiBucketAggregation or process it as a SingleBucketAggregation if not.

[1] https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/pipeline/SiblingPipelineAggregator.java#L49
</description><key id="81490408">11379</key><summary>Sibling Pipeline Aggregations only work if nested in multi-bucket aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T15:17:26Z</created><updated>2015-05-27T16:35:52Z</updated><resolved>2015-05-27T16:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/InternalSingleBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/SiblingPipelineAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketTests.java</file></files><comments><comment>Aggregations: Sibling Pipeline Aggregations can now be nested in SingleBucketAggregations</comment></comments></commit></commits></item><item><title>Make JNA optional for tests and move classes to bootstrap package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11378</link><project id="" key="" /><description>Today, JNA is a optional dependency in the build but when running tests or running
with mlockall set to true, JNA must be on the classpath for Windows systems since
we always try to load JNA classes when using mlockall.

The old Natives class was renamed to JNANatives, and a new Natives class is
introduced without any direct imports on JNA classes. The Natives class checks to
see if JNA classes are available at startup. If the classes are available the Natives
class will delegate to the JNANatives class. If the classes are not available the
Natives class will not use the JNANatives class, which results in no additional attempts
to load JNA classes.

Additionally, all of the JNA classes were moved to the bootstrap package and made
package private as this is the only place they should be called from.

Closes #11360
</description><key id="81485348">11378</key><summary>Make JNA optional for tests and move classes to bootstrap package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T15:06:51Z</created><updated>2015-06-07T18:15:00Z</updated><resolved>2015-05-27T17:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-27T15:13:29Z" id="105951484">This looks great: it makes this stuff way more contained. thanks for the cleanup. 
</comment><comment author="gmarz" created="2015-05-27T16:03:21Z" id="105976959">Nice @jaymode !  LGTM.

I tested to make sure mlockall still works properly on Windows with these changes, and it looks good.

Can/should this be back ported into 1.x?
</comment><comment author="jaymode" created="2015-05-27T16:18:42Z" id="105982920">@gmarz thanks for checking it out! 

I think we should backport to 1.x since we can have the same issue if `bootstrap.mlockall` is set to true in the configuration file.
</comment><comment author="gmarz" created="2015-05-27T16:44:14Z" id="105989051">@jaymode Yea, I agree.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deduplicate field names returned by simpleMatchToFullName &amp; simpleMatchToIndexNames in FieldMappersLookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11377</link><project id="" key="" /><description>Relates to #10916
</description><key id="81481171">11377</key><summary>Deduplicate field names returned by simpleMatchToFullName &amp; simpleMatchToIndexNames in FieldMappersLookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T14:54:53Z</created><updated>2015-06-07T10:13:03Z</updated><resolved>2015-05-28T07:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-27T14:56:48Z" id="105944360">Makes sense.
</comment><comment author="rjernst" created="2015-05-27T15:11:46Z" id="105950620">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Internal: deduplicate field names returned by `simpleMatchToFullName` &amp; `simpleMatchToIndexNames` in FieldMappersLookup</comment></comments></commit></commits></item><item><title>Aggregations: combining children, nested and range filter - filter not working properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11376</link><project id="" key="" /><description>Hello,

I am experiencing strange behavior when using aggregations combining `children`, `nested`, and `filter` aggs (with a `range` filter).

It looks like `children` aggregation is somehow affecting the `range` filter - it either returns count of all nested docs of a child document or none.

&lt;pre&gt;
curl -XPUT 'localhost:9200/nestingtest' -d '{
  "mappings": {
    "teams": {
      properties: {
        name: {
          type: "string",
          index: "not_analyzed",
          doc_values: true
        }
      }
    },
    "tasks": {
        _parent: { type: "teams" },
      properties: {
        task_id: {
          type: "long",
          doc_values: true
        },
        started_at: {
          type: "object",
          properties: {
            raw: {
              type: "date",
              doc_values: true
            },
            year: {
              type: "short",
              doc_values: true
            },
            month: {
              type: "short",
              doc_values: true
            },
            day: {
              type: "short",
              doc_values: true
            },
            hour: {
              type: "short",
              doc_values: true
            },
            minute: {
              type: "short",
              doc_values: true
            },
            wday: {
              type: "short",
              doc_values: true
            }
          }
        },
        events: {
          type: "nested",
          properties: {
            id: {
              type: "long",
              doc_values: true
            },
            happened_at: {
              type: "object",
              properties: {
                raw: {
                  type: "date",
                  doc_values: true
                },
                year: {
                  type: "short",
                  doc_values: true
                },
                month: {
                  type: "short",
                  doc_values: true
                },
                day: {
                  type: "short",
                  doc_values: true
                },
                hour: {
                  type: "short",
                  doc_values: true
                },
                minute: {
                  type: "short",
                  doc_values: true
                },
                wday: {
                  type: "short",
                  doc_values: true
                }
              }
            }
          }
        }
      }
    }
  }
}'
&lt;/pre&gt;


Data:

&lt;pre&gt;
curl -XPOST 'localhost:9200/nestingtest/teams/1' -d '{ name: "team-A"}'

curl -XPOST 'localhost:9200/nestingtest/tasks/1?parent=1' -d '
    {
      _parent: 1,
      task_id: 1,
      started_at: {
        raw: "2015-05-27T13:05",
        year: 2015,
        month: 5,
        day: 27,
        hour: 13,
        minute: 5,
        dow: 3
      },
      events: [
        {
          id: 1,
          happened_at: {
            raw: "2015-05-27T13:10",
            year: 2015,
            month: 5,
            day: 27,
            hour: 13,
            minute: 10,
            dow: 3
          }
        }
      ]
    }
'

curl -XPOST 'localhost:9200/nestingtest/tasks/2?parent=1' -d '
    {
      _parent: 1,
      task_id: 2,
      started_at: {
        raw: "2015-05-27T16:05",
        year: 2015,
        month: 5,
        day: 27,
        hour: 16,
        minute: 5,
        dow: 3
      },
      events: [
        {
          id: 21,
          happened_at: {
            raw: "2015-05-27T16:10",
            year: 2015,
            month: 5,
            day: 27,
            hour: 16,
            minute: 10,
            dow: 3
          }
        },
        {
          id: 22,
          happened_at: {
            raw: "2015-05-27T17:10",
            year: 2015,
            month: 5,
            day: 27,
            hour: 17,
            minute: 10,
            dow: 3
          }
        }
      ]
    }
'   
&lt;/pre&gt;


Queries:
This one, querying for parent documents is the one which behaves strangely

&lt;pre&gt;
curl -XPOST 'localhost:9200/nestingtest/teams/_search?pretty' -d '
    {
      aggs: {
        "to-tasks": {
          children: {
            type: "tasks"
          },
          aggs: {
            "to-events": {
              nested: {
                path: "tasks.events"
              },
              aggs: {
                filtered: {
                  filter: {
                    range: {
                      "events.happened_at.hour": {
                        gt: 15
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }

'
&lt;/pre&gt;
   

It returns: 

&lt;pre&gt;
 "aggregations" : {
    "to-tasks" : {
      "doc_count" : 2,
      "to-events" : {
        "doc_count" : 3,
        "filtered" : {
          "doc_count" : 3
        }
      }
    }
&lt;/pre&gt;


Where I would expect `filtered &gt; doc_count` set to `2`.

If I change `events.happened_at.hour` to `tasks.events.happened_at.hour` it returns an empty bucket.

Just for comparison, this one queries the same data, but directly into children type (i.e., not using `children` aggregation) and is working as expected (returning 2 docs in the bucket). However, I would prefer to query for parent type, as I want to combine other criteria into query.

&lt;pre&gt;
curl -XPOST 'localhost:9200/nestingtest/tasks/_search?pretty' -d '
     {
      aggs: {
        "to-events": {
          nested: {
            path: "events"
          },
          aggs: {
            filtered: {
              filter: {
                range: {
                  "events.happened_at.hour": {
                    gt: 15
                  }
                }
              }
            }
          }
        }
      }
    }
'
&lt;/pre&gt;

# 

&lt;pre&gt;
"version" : {
    "number" : "1.5.0",
    "build_hash" : "544816042d40151d3ce4ba4f95399d7860dc2e92",
    "build_timestamp" : "2015-03-23T14:30:58Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  }
&lt;/pre&gt;
</description><key id="81470569">11376</key><summary>Aggregations: combining children, nested and range filter - filter not working properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">crutch</reporter><labels><label>:Aggregations</label><label>:Parent/Child</label><label>bug</label></labels><created>2015-05-27T14:28:39Z</created><updated>2015-06-29T07:50:02Z</updated><resolved>2015-06-26T15:08:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-05-27T15:58:31Z" id="105974552">@crutch I suspect that you're experiencing a bug that has been fixed via PR #10263.

Several bugs in the `children` agg have been fixed since 1.5.0, can you try to run with the latest 1.5 release?
</comment><comment author="crutch" created="2015-05-27T16:37:48Z" id="105987425">@martijnvg I am sorry to say, but I was able to reproduce the same weird behavior under 1.5.2.
</comment><comment author="vhyza" created="2015-05-28T11:18:31Z" id="106282777">I am experiencing similar behaviour on elasticsearch 1.5.2 with `children -&gt; nested -&gt; filter` aggregation with combination of `term filter` on `numeric` field.

Aggregation looks like this:

``` json
{
  "aggs" : {
    "variants" : {
      "children" : {
        "type" : "variant"
      },
      "aggs" : {
        "parameters" : {
          "nested" : {
            "path" : "variant.parameters"
          },
          "aggs" : {
            "specifications" : {
              "filters" : {
                "filters" : {
                  "flavour" : {
                    "term" : {
                      "parameters.parameter_id": 1000
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size" : 0
}
```

response

``` json
{
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2682,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "variants": {
         "doc_count": 2682,
         "parameters": {
            "doc_count": 62924,
            "specifications": {
               "buckets": {
                  "flavour": {
                     "doc_count": 0
                  }
               }
            }
         }
      }
   }
}
```

When I reindexed `parameter_id` as string, everything works as expected (with the same query as above).

``` json
{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2682,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "variants": {
         "doc_count": 2682,
         "parameters": {
            "doc_count": 62924,
            "specifications": {
               "buckets": {
                  "flavour": {
                     "doc_count": 2438
                  }
               }
            }
         }
      }
   }
}
```

When I changed `term` filter to `match` query filter

``` json
{
  "aggs" : {
    "variants" : {
      "children" : {
        "type" : "variant"
      },
      "aggs" : {
        "parameters" : {
          "nested" : {
            "path" : "variant.parameters"
          },
          "aggs" : {
            "specifications" : {
              "filters" : {
                "filters" : {
                  "flavour" : {
                    "query" : {
                      "match" : {
                      "parameters.parameter_id": 1000
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size" : 0
}
```

It works

``` json
{
   "took": 11,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2682,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "variants": {
         "doc_count": 2682,
         "parameters": {
            "doc_count": 62924,
            "specifications": {
               "buckets": {
                  "flavour": {
                     "doc_count": 2438
                  }
               }
            }
         }
      }
   }
}
```

With master (95faa35853bff06d14c9071120988c0cba9aae84) build It seems nested aggregation is not working.

Query

``` json
{
  "aggs" : {
    "variants" : {
      "children" : {
        "type" : "variant"
      },
      "aggs" : {
        "parameters" : {
          "nested" : {
            "path" : "variant.parameters"
          },
          "aggs" : {
            "specifications" : {
              "filters" : {
                "filters" : {
                  "flavour" : {
                    "query" : {
                      "match" : {
                      "parameters.parameter_id": 1000
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size" : 0
}
```

returns

``` json
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2682,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "variants": {
         "doc_count": 2682,
         "parameters": {
            "doc_count": 0 // here should be 62924
         }
      }
   }
}
```

Note that doc_count of parameters aggregations returns 0 instead of 62924 as elasticsearch 1.5.2 did.

When I remove `children` aggregation everything works as expected on both 1.5.2 and master (95faa35853bff06d14c9071120988c0cba9aae84) (with both `long` and `string` types)

``` json
{
  "aggs": {
    "parameters": {
      "nested": {
        "path": "parameters"
      },
      "aggs": {
        "specifications": {
          "filters": {
            "filters": {
              "flavour": {
                "query": {
                  "match": {
                    "parameters.parameter_id": 1000
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
```

returns

``` json
{
   "took": 7,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2682,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "parameters": {
         "doc_count": 62924,
         "specifications": {
            "buckets": {
               "flavour": {
                  "doc_count": 2438
               }
            }
         }
      }
   }
}
```
</comment><comment author="martijnvg" created="2015-06-22T13:17:16Z" id="114099217">@crutch The issue in the first search request that you have shared is that the type defined in the url is letting the wrapped `nested` aggregator work incorrectly. If you omit the type from the url then the response is as expected:

``` bash
curl -XPOST "http://localhost:9200/nestingtest/_search?pretty" -d'
{
  "size" : 0,
  "aggs": {
    "to-tasks": {
      "children": {
        "type": "tasks"
      },
      "aggs": {
        "to-events": {
          "nested": {
            "path": "events"
          },
          "aggs": {
            "filtered": {
              "filter": {
                "range": {
                  "events.happened_at.hour": {
                    "gt": 15
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}'
```

@vhyza Does removing the type from the url help in your case too?
</comment><comment author="vhyza" created="2015-06-22T14:08:57Z" id="114117101">@martijnvg Thanks! Yes, when I removed type from url, I got same result as with `match` query filter.
</comment><comment author="martijnvg" created="2015-06-22T18:46:23Z" id="114216025">The `nested` aggregator doesn't work, because the mapping can't be resolved. This is because the search is scoped to the `teams` type and the `tasks.events` object field doesn't exist in that type. This behaviour kind of is understandable, but hard to figure out, because the `nested` aggregator is ignored and just results an empty result.

I've been thinking about this and the best way is just make this kinds of requests fail, so it is clear that the type needs to be removed from the request url. I think we can get this behaviour in when #11806 gets in. The plan is there to by default let `nested` aggregation fail if no nested field can be resolved.
</comment><comment author="vhyza" created="2015-06-23T11:06:24Z" id="114445861">@martijnvg so, it is not a bug? I don't fully understand why (in my case) `match` query filter works and `term` filter does not (or why `parameter_id` indexed as `string` works even with `term` filter)
</comment><comment author="martijnvg" created="2015-06-23T12:43:19Z" id="114481871">@vhyza No, it is a bug. The bug here is that ES should fail the search request and tell that it couldn't resolve a nested field (or another field), instead of ignoring that and returning no results.

I'm not sure too, because I don't have the mapping and data you used, but this can be explained too by the fact that the mapping can't be resolved because the parent type is used in the url. In the case for term filter it assumes that the field is a string. This causes zero matches because the field is indexed as a number. Strings and numbers are stored differently and filters/queries need to be analysed different in ES to actual be able to match properly. 
</comment><comment author="crutch" created="2015-06-23T16:13:40Z" id="114560054">@martijvng thank you for the quick solution for this.

However, I do not really understand why ES should fail on a completely
valid and correct query. From my point of view, I prefer to scope my query
to a particular type, just to be sure that I only get results of that type.
And I always thought that this type scope is related only to top-level
document. If I decide to jump to another document type via `has_child` and
then use a `nested` filter inside, I do not expect that type scope declared
in the url to be that sticky and to force ES to declare that it was unable
to resolve a field here.  I would expect ES to know that the scope has
changed for this part of a query.

I understand that this might be difficult to achieve with current
implementation and I am OK with your workaround (thank you again for it).
But when I as a human can read a query and its interpretation is
unambiguous, then it should be clear for ES as well.
On Jun 23, 2015 2:44 PM, "Martijn van Groningen" notifications@github.com
wrote:

&gt; @vhyza https://github.com/vhyza No, it is a bug. The bug here is that
&gt; ES should fail the search request and tell that it couldn't resolve a
&gt; nested field (or another field), instead of ignoring that and returning no
&gt; results.
&gt; 
&gt; I'm not sure too, because I don't have the mapping and data you used, but
&gt; this can be explained too by the fact that the mapping can't be resolved
&gt; because the parent type is used in the url. In the case for term filter it
&gt; assumes that the field is a string. This causes zero matches because the
&gt; field is indexed as a number. Strings and numbers are stored differently
&gt; and filters/queries need to be analysed different in ES to actual be able
&gt; to match properly.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11376#issuecomment-114481871
&gt; .
</comment><comment author="martijnvg" created="2015-06-23T16:24:06Z" id="114562440">@crutch I see your point and maybe ES shouldn't scope the internal field lookup in the mapping to the type defined in the url. I think the reason this exist, is that today one can define fields in different types in the same index that have the same name, but have different settings and field types. By scoping by type one has control of what actual field is used instead of let ES guess.

Actually this mapping characteristic is going to change and #11812 is going to prohibit having fields with same name but with different settings / field types. If that gets in then the type scoping that happens in the _search api during parsing can be removed and the search request that you initially shared should then work.
</comment><comment author="martijnvg" created="2015-06-26T14:44:24Z" id="115712087">@crutch @vhyza The underlying issue (field lookup taking the type in the url into account) has been fixed in master. This got fixed when #11812 got merged in. So your search request will also work with the type specified. 

This issue will not be fixed in 1.x, so for all current released ES versions this workaround needs to be used. When the first 2.0 release gets out the work around will not longer be needed.
</comment><comment author="clintongormley" created="2015-06-26T15:08:22Z" id="115721737">thanks @martijnvg - closing
</comment><comment author="vhyza" created="2015-06-27T08:52:23Z" id="115987575">thanks @martijnvg!
</comment><comment author="crutch" created="2015-06-29T07:50:02Z" id="116507317">thanks @martijnvg, looking forward to 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed Moving Average prediction to calculate the correct keys</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11375</link><project id="" key="" /><description>The Moving average predict code generated incorrect keys if the key for the first bucket of the histogram was &lt; 0. This fix makes the moving average use the rounding class from the histogram to generate the keys for the new buckets.

Closes #11369
</description><key id="81459214">11375</key><summary>Fixed Moving Average prediction to calculate the correct keys</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T13:59:44Z</created><updated>2015-06-07T17:45:05Z</updated><resolved>2015-05-27T14:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-05-27T14:24:27Z" id="105930138">@colings86 LGTM.  So much cleaner!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix HTML response during redirection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11374</link><project id="" key="" /><description>Fixes issue #11370 by fixing response HTML
</description><key id="81448687">11374</key><summary>Fix HTML response during redirection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">datval</reporter><labels><label>:Plugins</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-05-27T13:34:18Z</created><updated>2015-11-20T09:27:00Z</updated><resolved>2015-11-03T14:02:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-27T16:06:35Z" id="105978529">Hi @datval 

Thanks for the PR.  Please could you remove the extra comma that has been added, and sign the CLA so that we can merge this in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="datval" created="2015-05-27T16:44:56Z" id="105989235">Thanks @clintongormley, I just signed the CLA, I guess it needs some time to reflect here.
</comment><comment author="clintongormley" created="2015-05-28T12:53:56Z" id="106300589">@datval did you sign the corporate CLA?  I ask because those have to be checked manually.  I don't see you in the personal CLA list.
</comment><comment author="datval" created="2015-05-28T13:22:08Z" id="106307198">@clintongormley Yep, I signed it, but as "An Individual contributing under an existing company contributor agreement".
</comment><comment author="clintongormley" created="2015-05-28T14:14:03Z" id="106339011">@datval thanks - I'll need to look it up manually
</comment><comment author="datval" created="2015-05-28T14:16:14Z" id="106340957">@clintongormley Do you want reference number?
</comment><comment author="clintongormley" created="2015-05-28T17:36:16Z" id="106513723">@datval got it, thanks!
</comment><comment author="bleskes" created="2015-11-03T12:54:14Z" id="153342582">LGTM. Will merge soon :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/http/HttpServer.java</file></files><comments><comment>Fix HTML response during redirection</comment></comments></commit></commits></item><item><title>deb package postrm script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11373</link><project id="" key="" /><description>Hi,

i have /var/lib/elasticsearch on separate filesystem, i found that in your postrm scripts you are removing this directory, apparently if it is mountpoint and filesystem is mounted, script won't be able to remove it, thus removing will fail

elasticsearch version: 1.5.2
</description><key id="81438676">11373</key><summary>deb package postrm script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">p53</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T13:13:12Z</created><updated>2015-07-01T09:08:44Z</updated><resolved>2015-07-01T09:08:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-27T13:28:40Z" id="105908319">@p53 thanks for reporting. Note that this repository is removed only if it's empty. Did the uninstall fail because of this?
</comment><comment author="p53" created="2015-05-28T06:31:12Z" id="106194258">Hi i removed all things from /var/lib/elasticsearch and yes now package was removed. But those things which were in /var/lib/elasticsearch were there by default, so it seems unconsistent, to install package by one operation and to uninstall you need beside uninstall operation also remove files which were not created by me, but by package, manually. I know that this is probably because it wants to be safe, but maybe if it would have some index of default files/folders to check. Thanks anyway for your response
</comment><comment author="tlrx" created="2015-05-28T06:51:26Z" id="106198880">@p53 thanks for your response. The folder is created by the package installation but it remains empty until elasticsearch is started. Once elasticsearch is started we should not removed this folder because important data can reside in it.

I'll update the script to not fail when deleting this folder. In worst case an empty folder won't be deleted.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Postrm script should not fail</comment></comments></commit></commits></item><item><title>Mapping: Serialize transform script using correct key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11372</link><project id="" key="" /><description>Even if we use "script_file" or "script_id", key for script is always serizlied as "script".
Fix it to serialize script key using the correct key.
</description><key id="81437736">11372</key><summary>Mapping: Serialize transform script using correct key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels /><created>2015-05-27T13:11:34Z</created><updated>2015-05-27T14:31:36Z</updated><resolved>2015-05-27T14:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="masaruh" created="2015-05-27T14:00:35Z" id="105922403">Won't be needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bulk updates with groovy script leads to long gc pauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11371</link><project id="" key="" /><description>We are doing bulk updates for hundreds of thousands of documents. We are using a groovy script to avoid the default merging of defunct data in one field. After 5-10 thousand documents, taking only a minute or so, we see garbage collection on the old generation. It's very predictable. The old generation gc repeats while we are loading updates each hour (about 10 secs of old gen gc time per minute). After about a day or so (a few million documents), the stop the world pauses become untenable - 20-40 seconds of old gen gc per minute.

When we discovered that we were using a script for the update, we removed it and throughput immediately improved 10x. The old gen gc virtually disappeared.

OpenJDK 1.7.0_65, elasticsearch-1.4.2-1
5 data nodes using the standard gc params 
"-Xms7g -Xmx7g -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC"
bulk batch size is 100 documents. Using 10 or 20 concurrent threads.

We are looking for alternative ways to get the same semantics without a script. In the meantime, I thought I would share our experience. If nothing else, it would be great to see a note about gc in the scripting section of the docs and a note about scripting in the gc section of the docs. Let me know if you'd like any more information. 
</description><key id="81434532">11371</key><summary>bulk updates with groovy script leads to long gc pauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adler</reporter><labels><label>:Scripting</label><label>discuss</label><label>docs</label><label>feedback_needed</label></labels><created>2015-05-27T12:58:49Z</created><updated>2017-03-31T13:52:43Z</updated><resolved>2017-03-31T13:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adler" created="2015-05-27T21:14:24Z" id="106080481">It would be nice if there was a way to avoid merging some parts of the document on partial updates without having to resort to using groovy. We are going to fetch the existing fields to the client and then update all defunct fields we find to have a value of 0. This extra round trip allows us to avoid the unwanted semantics of default merging and the performance problems caused by gc pausing.
</comment><comment author="dakrone" created="2016-09-27T14:28:05Z" id="249881443">This should be helped in 5.0 with the move to Painless. Can you try it again and see if it improves?
</comment><comment author="colings86" created="2017-03-31T13:52:43Z" id="290717914">No further feedback. Please reopen if you still see this issue in 5.x</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Redirect response, during missing trailing /, missing escaped "</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11370</link><project id="" key="" /><description>Elasticsearch redirects site plugin requests when trailing '/' is missing. It's using Header + meta tag to do job. Meta tag is missing escaped ", Jetty plugin loses Location header so redirection fails.
One solution is to fix Jetty plugin, other is to fix returned response.
</description><key id="81433480">11370</key><summary>Redirect response, during missing trailing /, missing escaped "</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">datval</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2015-05-27T12:53:24Z</created><updated>2015-11-20T09:26:52Z</updated><resolved>2015-11-20T09:26:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-20T09:26:52Z" id="158335057">Closed by #11374
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect keys for predicted buckets in Moving Average Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11369</link><project id="" key="" /><description>My query:

``` javascript
GET weatherdata/_search
{
  "size": 0,
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "element": {
              "value": "tmax"
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "maxTMax": {
      "max": {
        "field": "value",
        "script": "_value / 10",
        "format": "###.##"
      }
    },
    "maxAvgTMaxMonth": {
      "max_bucket": {
        "buckets_path": "tMaxHisto&gt;avgTMax",
        "format": "###.##"
      }
    },
    "minAvgTMaxMonth": {
      "min_bucket": {
        "buckets_path": "tMaxHisto&gt;avgTMax",
        "format": "###.##"
      }
    },
    "tMaxHisto": {
      "date_histogram": {
        "field": "date",
        "interval": "quarter"
      },
      "aggs": {
        "avgTMax": {
          "avg": {
            "field": "value",
            "script": "_value / 10",
            "format": "###.##"
          }
        },
        "movavg": {
          "moving_avg": {
            "buckets_path": "avgTMax",
            "model": "holt",
            "window": 12,
            "gap_policy": "skip",
            "predict": 12,
            "settings": {
              "alpha": 0.8
            }
          }
        }
      }
    }
  }
}
```

Response snippet (the first three bucket are existing buckets, i.e. not predictions. The following 12 buckets are predictions but the bucket keys seem to start at 1986 and decrease by 20 years each time):

``` json
            {
               "key_as_string": "2005-07-01T00:00:00.000Z",
               "key": 1120176000000,
               "doc_count": 11,
               "avgTMax": {
                  "value": 28.8,
                  "value_as_string": "28.8"
               },
               "movavg": {
                  "value": 29.15195798473475
               }
            },
            {
               "key_as_string": "2005-10-01T00:00:00.000Z",
               "key": 1128124800000,
               "doc_count": 11,
               "avgTMax": {
                  "value": 29.172727272727272,
                  "value_as_string": "29.17"
               },
               "movavg": {
                  "value": 29.024912216300212
               }
            },
            {
               "key_as_string": "2006-01-01T00:00:00.000Z",
               "key": 1136073600000,
               "doc_count": 4,
               "avgTMax": {
                  "value": 29.525,
                  "value_as_string": "29.52"
               },
               "movavg": {
                  "value": 29.34023027099398
               }
            },
            {
               "key_as_string": "1986-01-01T00:00:00.000Z",
               "key": 504921600000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.34023027099398,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1966-01-01T00:00:00.000Z",
               "key": -126230400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.28650429104976,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1946-01-01T00:00:00.000Z",
               "key": -757382400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.232778311105537,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1926-01-01T00:00:00.000Z",
               "key": -1388534400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.179052331161316,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1906-01-01T00:00:00.000Z",
               "key": -2019686400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.125326351217094,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1885-12-31T00:00:00.000Z",
               "key": -2650838400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.071600371272872,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1865-12-31T00:00:00.000Z",
               "key": -3281990400000,
               "doc_count": 0,
               "movavg": {
                  "value": 29.01787439132865,
                  "value_as_string": "1970-01-01T00:00:00.029Z"
               }
            },
            {
               "key_as_string": "1845-12-31T00:00:00.000Z",
               "key": -3913142400000,
               "doc_count": 0,
               "movavg": {
                  "value": 28.96414841138443,
                  "value_as_string": "1970-01-01T00:00:00.028Z"
               }
            },
            {
               "key_as_string": "1825-12-31T00:00:00.000Z",
               "key": -4544294400000,
               "doc_count": 0,
               "movavg": {
                  "value": 28.910422431440207,
                  "value_as_string": "1970-01-01T00:00:00.028Z"
               }
            },
            {
               "key_as_string": "1805-12-31T00:00:00.000Z",
               "key": -5175446400000,
               "doc_count": 0,
               "movavg": {
                  "value": 28.856696451495985,
                  "value_as_string": "1970-01-01T00:00:00.028Z"
               }
            },
            {
               "key_as_string": "1785-12-30T00:00:00.000Z",
               "key": -5806598400000,
               "doc_count": 0,
               "movavg": {
                  "value": 28.802970471551763,
                  "value_as_string": "1970-01-01T00:00:00.028Z"
               }
            },
            {
               "key_as_string": "1765-12-30T00:00:00.000Z",
               "key": -6437750400000,
               "doc_count": 0,
               "movavg": {
                  "value": 28.74924449160754,
                  "value_as_string": "1970-01-01T00:00:00.028Z"
               }
            }
```
</description><key id="81422635">11369</key><summary>Incorrect keys for predicted buckets in Moving Average Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T12:09:14Z</created><updated>2015-05-27T14:27:19Z</updated><resolved>2015-05-27T14:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-27T12:32:18Z" id="105890259">The problem is with [1], as this calculation of the interval does not work if `key &lt; 0` since the first bucket will calculate the interval as a negative value which will always be less than the actual interval. Also, I'm not sure this method works for non-fixed intervals like months, where the interval will change depending on what the key is

[1] https://github.com/elastic/elasticsearch/blob/35deb7efea9552528780082143db469afbcd3812/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java#L137-145
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file></files><comments><comment>Aggregations: Fixed Moving Average prediction to calculate the correct keys</comment></comments></commit></commits></item><item><title>`fielddata_fields` query string parameter was ignored.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11368</link><project id="" key="" /><description>The RestSearchAction did not parse the fielddata_fields parameter. Added test case and missing parser code.

Closes #11025
</description><key id="81418868">11368</key><summary>`fielddata_fields` query string parameter was ignored.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:REST</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T11:55:24Z</created><updated>2015-06-08T00:19:31Z</updated><resolved>2015-05-27T16:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-27T11:57:58Z" id="105884642">LGTM
</comment><comment author="markharwood" created="2015-05-27T16:50:10Z" id="105990650">Thanks, @javanna 
Pushed to 1.5, 1.x and master in https://github.com/elastic/elasticsearch/commit/f2d18984ba11aaca2389d9270d44b38c2dd7e30a https://github.com/elastic/elasticsearch/commit/05bd2505fce54b50d2be29135f708d899099eea2 and https://github.com/elastic/elasticsearch/commit/57653df7133d02fa09b67343232f33c096100e16 respectively.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add wildcard support for header names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11367</link><project id="" key="" /><description>This adds wildcard support (simple regexes) for specifying header names.
Aliases are supported as well.

Closes #10811
</description><key id="81382355">11367</key><summary>Add wildcard support for header names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:CAT API</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T10:08:38Z</created><updated>2015-06-06T17:59:02Z</updated><resolved>2015-05-27T14:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-27T13:21:34Z" id="105905519">left a few comments, looks good though
</comment><comment author="javanna" created="2015-05-27T13:47:48Z" id="105917019">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add alias with 'tags' like 'xxx-xxx' not working?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11366</link><project id="" key="" /><description>Hi experts,

My elasticsearch version is 1.3.7. 
When i add a new alias for an index, with following parameters:
"aliases":{
"aaaa_alias":{
"filter":{
"term":{
"tags":["aaa","bbb","ccc-ddd"]}}}}}

when i use kibana to query string 'tags:"&lt;each tag above&gt;"', it could show results with tags 'aaa', 'bbb', but no 'ccc-ddd' found.

I'm not sure if newer version could resolve this issue...

Could we have any restrictions for writing a tag?

Thanks in advance. :)
</description><key id="81364965">11366</key><summary>add alias with 'tags' like 'xxx-xxx' not working?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peteryj</reporter><labels /><created>2015-05-27T09:22:42Z</created><updated>2015-05-27T11:05:48Z</updated><resolved>2015-05-27T11:05:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-27T11:05:47Z" id="105871880">Hi @peteryj 

Please ask questions like these in the forum https://discuss.elastic.co/c/elasticsearch - this issues list is for bug fixes and feature requests. Hint: your `tags` field needs to be `not_analyzed`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>InternalDateHistogram.Bucket type is not visible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11365</link><project id="" key="" /><description>I process a response which includes datehistogram ,

so I made List&lt;InternalDateHistogram.Bucket&gt; but, this type is not visible,

I think this happens because  'org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram' -&gt; inner class Bucket is defined as a static class (not a public);

What should I do?

Can I modify class file in elasticsearch-1.4.1.jar?
</description><key id="81359490">11365</key><summary>InternalDateHistogram.Bucket type is not visible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sim1st</reporter><labels /><created>2015-05-27T09:07:04Z</created><updated>2015-05-28T00:05:21Z</updated><resolved>2015-05-27T11:11:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-27T10:44:06Z" id="105862471">@sim1st Why do you want to access InternalDateHistogram.Bucket? This is an internal class and not part of the Java API. DateHistogram.Bucket is the public interface you need to use to access the buckets of a date histogram [1]. You should use that.

[1] https://github.com/elastic/elasticsearch/blob/1.4/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogram.java
</comment><comment author="sim1st" created="2015-05-28T00:05:21Z" id="106116335">Thank you very much!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wildcard field names in highlighting should only return fields that can be highlighted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11364</link><project id="" key="" /><description>When we highlight on fields using wildcards then fields might match that cannot
be highlighted by the specified highlighter. The whole search request then
failed. Instead, check that the field can be highlighted and ignore the field
if it can't.

In addition ignore the exception thrown by plain highlighter if a field conatins
terms larger than 32766. 

closes #9881
</description><key id="81311193">11364</key><summary>Wildcard field names in highlighting should only return fields that can be highlighted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Highlighting</label><label>breaking</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T06:52:58Z</created><updated>2015-05-27T15:11:51Z</updated><resolved>2015-05-27T15:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-27T07:08:42Z" id="105788269">I left some comments ;)
</comment><comment author="brwe" created="2015-05-27T10:39:56Z" id="105861680">@javanna thanks a lot for the review! addressed all comments. want to have another look?
</comment><comment author="javanna" created="2015-05-27T11:49:31Z" id="105881725">looks great, left a few more comments, I think this should marked as breaking as it breaks plugins that plug in custom highlighters, maybe @nik9000 would like to have a look too?
</comment><comment author="nik9000" created="2015-05-27T12:39:08Z" id="105892098">&gt; looks great, left a few more comments, I think this should marked as breaking as it breaks plugins that plug in custom highlighters, maybe @nik9000 would like to have a look too?

Just did.

I think its fine. I agree with your point about a sorted list being easier to read. I also think its worth moving any concerns about highlighter plugins registering themselves to the list to another issue. At this point I don't think that is really an important feature.
</comment><comment author="brwe" created="2015-05-27T12:56:08Z" id="105896222">ok, changed that now to use a list. want to take another look?
</comment><comment author="brwe" created="2015-05-27T13:34:32Z" id="105910510">all done. want to take another look?
</comment><comment author="javanna" created="2015-05-27T13:52:43Z" id="105919477">done another review, few more comments :)
</comment><comment author="brwe" created="2015-05-27T14:19:29Z" id="105928508">pushed another commit
</comment><comment author="javanna" created="2015-05-27T14:28:33Z" id="105931436">LGTM besides the two very minor comments I left, thanks for taking care of this @brwe !
</comment><comment author="nik9000" created="2015-05-27T14:30:50Z" id="105932084">&gt; LGTM besides the two very minor comments I left, thanks for taking care of this @brwe !

I left another distinct minor comment but yeah, its great! This'll make highlighting so much less blowy-upy.
</comment><comment author="brwe" created="2015-05-27T14:52:25Z" id="105943112">addressed all comments
</comment><comment author="javanna" created="2015-05-27T14:56:47Z" id="105944357">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/Highlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/CustomHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Merge pull request #11364 from brwe/highlighter-wildcard</comment></comments></commit></commits></item><item><title>Restart recovery upon mapping changes during translog replay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11363</link><project id="" key="" /><description>In rare occasion, the translog replay phase of recovery may require mapping changes on the target shard. This can happen where indexing on the primary introduces new mappings while the recovery is in phase1. If the source node processes the new mapping from the master, allowing the indexing to proceed, before the target node does and the recovery moves to the phase 2 (translog replay) before as well, the translog operations arriving on the target node may miss the mapping changes. Since this is extremely rare, we opt for a simple fix and simply restart the recovery. Note that in the case the file copy phase will likely be very short as the files are already in sync.

Restarting recoveries in such a late phase means we may need to copy segment_N files and/or files that were quickly merged away on the target again. This annoys the write-once protection in our testing infra. To work around it I have introduces a counter in the termpoary file name prefix used by the recovery code.

***\* THERE IS STILL AN ONGOING ISSUE ***: Lucene will try to write the same segment_N file (which was cleaned by the recovery code) twice triggering test failures.

Due ot this issue we have decided to change approach and use a cluster observer to retry operations once the mapping have arrived (or any other change)

 Closes #11281
</description><key id="81307461">11363</key><summary>Restart recovery upon mapping changes during translog replay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-27T06:42:11Z</created><updated>2015-06-04T20:30:51Z</updated><resolved>2015-06-04T20:27:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-28T09:39:39Z" id="106249633">@bleskes I looked at this and I think we should not try to restart the recovery in this `hyper corner case` I think we should just fail the shard, fail the recovery and start fresh. This makes the entire code less complicated and more strict. It's I think we should not design for all these corner cases and rather start fresh?
</comment><comment author="bleskes" created="2015-06-04T13:30:13Z" id="108899510">@s1monw I pushed an update based on our discussion ... no more DelayRecoveryException
</comment><comment author="s1monw" created="2015-06-04T19:27:41Z" id="109019661">LGTM, one question if we elect a new master will this somehow get notified and we retry?
</comment><comment author="bleskes" created="2015-06-04T20:00:22Z" id="109032355">I'm not sure I follow the question exactly, but if the recovery code is wait on the observer it will retry on any change in the cluster state, master related or not.
</comment><comment author="s1monw" created="2015-06-04T20:04:34Z" id="109033171">@bleskes I got confused... nevermind
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file></files><comments><comment>Recovery: fix recovered translog ops stat counting when retrying a batch</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Recovery: restart recovery upon mapping changes during translog replay</comment></comments></commit></commits></item><item><title>configuration with include directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11362</link><project id="" key="" /><description>Hi,
I think it could be usefull to have an include dir like apache or nginx have for the configuration.
Why?
Simply because it's very usefull and easy to configure (add/replace a file instead of editing), easier to keep the default configuration or .... and adding specific configuration.

Thanks
</description><key id="81132425">11362</key><summary>configuration with include directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Grummfy</reporter><labels><label>:Settings</label></labels><created>2015-05-26T20:54:18Z</created><updated>2016-06-09T17:09:06Z</updated><resolved>2015-06-26T09:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jtharpla" created="2015-05-27T23:49:25Z" id="106112710">This would be a big help for using configuration management with multiple plugins
</comment><comment author="Grummfy" created="2015-05-28T07:07:24Z" id="106203256">Yes or if you have some small configuration file this allows you to assemble them for each server. Since apache do that, there is less conflict in upgrade but also easier maintenance (and this applies to every package that are cut in short part). 
</comment><comment author="clintongormley" created="2015-05-29T09:30:02Z" id="106757471">I think we should do this.  The syntax needs to be YAML compatible, so I'd suggest something like this:

Include `config/my_conf.yaml`:

```
include: my_conf.yaml
```

Include `config/my_conf_1.yaml` and `config/my_conf_2.yaml`

```
include: [ my_conf_1.yaml, my_conf_2.yaml ]
```

Absolute paths can be specified as well
</comment><comment author="rmuir" created="2015-05-29T09:33:38Z" id="106758243">Why do we need it? Configuration is already crazy-overengineered today. Do we need to be able to re-read all these include'd configuration files at a later time after bootstrap ever (this is the case for the linked logging PR at the least)

If anything, I think we should simplify this stuff, to have even less features than it has today, and be simpler.
</comment><comment author="dadoonet" created="2015-05-29T09:36:44Z" id="106758761">@rmuir I'm not sure if it's really needed but one of the use case I can think about is when it comes to S3 credentials. May be extracting this part from `elasticsearch.yml` and have different settings for DEV, QA, PROD might make sense? 

Not a big deal to manage though.
</comment><comment author="rmuir" created="2015-05-29T09:39:20Z" id="106759129">Rather than add a lot of complicated functionality, or hear about specific implementations, i would rather hear about what the actual missing use cases are, and we can think about what the best idea is.

And ideally whatever PR is doing that is removing more lines of code than it is adding, or it is suspect!
</comment><comment author="Grummfy" created="2015-05-29T11:46:41Z" id="106781289">In my idea is more adding something similar to what exist in other known and successful product by adding an include dir so you can have 
/etc/elasticsearch/main_config_file.yml (or the name you want)
/etc/elasticsearch/conf.d/someotherconfig.yml
/etc/elasticsearch/conf.d/an-extra-config.yml

the *.yml  in conf.d will be loaded after the main_config_file (that have a key like config_dir : [ conf.d/, ... ]

The idea is not to add complexity but to reflect what other product have added that permit a lot of more flexibility. When you need to deal with an nginx server nobody want a simple file with all the configuration. Why we should have the same on elastic? What's the problem with subfile : it's not a so big deal to load and it's a lot easier to maintain.
</comment><comment author="dadoonet" created="2015-05-29T11:59:27Z" id="106782992">I think that there is not so much thing to configure in Elasticsearch as you are often configure things using API.

My 2 cents on this.
</comment><comment author="clintongormley" created="2015-06-26T09:15:24Z" id="115596352">We've had a long discussion about this in our FixItFriday session.  Our feeling is that we're trying to remove complication from the code base.  Having one way to do things just makes it simpler to understand and debug.  So for now, we're going to close this PR.  We may revisit this in the future if there is popular demand and the change makes life easier for the majority of users.

thanks anyway
</comment><comment author="serac" created="2016-06-09T17:08:02Z" id="224961709">@rmuir, here's a use case: Shield credentials. Our LDAP directory requires a bind_password and I don't want it sitting in cleartext in our configuration-controlled elasticsearch.yml file. I would like to include a file that contains credentials exclusively that is either:
1. External to our configuration management
2. Encrypted (e.g. ansible vault) and configuration controlled

Some things that could work but are not ideal:
1. Encrypting elasticsearch.yml makes it harder to manage everything else in the file.
2. Injecting credentials via -Des.shield.authc.realms.ldap1.bind_password is not ideal since it leaks the password in ps output.

Externalizing credentials in a separate file is the best solution.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Read segment info from latest commit whenever possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11361</link><project id="" key="" /><description>Instead of listing the directory to file the latest segments_N file, we
should re-use the generation/filename from the last commit. This allows
us to avoid potential race conditions on the filesystem as well as
reduce the number of directory listings performed.
</description><key id="81130834">11361</key><summary>Read segment info from latest commit whenever possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Store</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T20:50:10Z</created><updated>2015-06-08T12:53:18Z</updated><resolved>2015-05-26T23:46:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-26T20:50:27Z" id="105660778">@mikemccand can you take a look at this?
</comment><comment author="mikemccand" created="2015-05-26T22:57:24Z" id="105691884">LGTM, thanks @dakrone!
</comment><comment author="dakrone" created="2015-05-27T00:12:15Z" id="105701851">I don't think I'm going to backport this, the Lucene API is different and I think I will work on an alternative to avoid upgrading Lucene 3.x segments when not needed, which is when the race condition usually occurs.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file></files><comments><comment>Fall back to reading SegmentInfos from Store if reading from commit</comment><comment>fails</comment></comments></commit></commits></item><item><title>JNA is not optional when testing on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11360</link><project id="" key="" /><description>The JNA library is listed as optional in the pom file but when running tests in another project without the library, we still try to load it and loading fails with a `ClassNotFoundException`:

```
   &gt; Throwable #1: java.lang.NoClassDefFoundError: com/sun/jna/win32/StdCallLibrary$StdCallCallback
   &gt;    at org.elasticsearch.common.jna.Natives.tryVirtualLock(Natives.java:83)
   &gt;    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93)
   &gt;    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:52)
   &gt;    at org.elasticsearch.test.ElasticsearchTestCase.&lt;clinit&gt;(ElasticsearchTestCase.java:97)
   &gt;    at java.lang.Class.forName0(Native Method)
   &gt;    at java.lang.Class.forName(Class.java:274)
   &gt; Caused by: java.lang.ClassNotFoundException: com.sun.jna.win32.StdCallLibrary$StdCallCallback
   &gt;    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
   &gt;    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
   &gt;    at java.security.AccessController.doPrivileged(Native Method)
   &gt;    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
   &gt;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
   &gt;    ... 7 more
```

which then cascades into:

```
   &gt; Throwable #1: java.lang.NoClassDefFoundError: org.elasticsearch.test.ElasticsearchTestCase
   &gt;    at java.lang.Class.getDeclaredMethods0(Native Method)
   &gt;    at java.lang.Class.privateGetDeclaredMethods(Class.java:2615)
   &gt;    at java.lang.Class.getDeclaredMethods(Class.java:1860)
   &gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:215)
   &gt;    at com.carrotsearch.randomizedtesting.ClassModel$3.members(ClassModel.java:212)
   &gt;    at com.carrotsearch.randomizedtesting.ClassModel$ModelBuilder.build(ClassModel.java:85)
   &gt;    at com.carrotsearch.randomizedtesting.ClassModel.methodsModel(ClassModel.java:224)
   &gt;    at com.carrotsearch.randomizedtesting.ClassModel.&lt;init&gt;(ClassModel.java:207)
   &gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
```

This seems to be provoked only on Windows by the `Natives#tryVirtualLock` method currently.
</description><key id="81098833">11360</key><summary>JNA is not optional when testing on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T19:20:08Z</created><updated>2015-05-27T17:14:26Z</updated><resolved>2015-05-27T17:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/ConsoleCtrlHandler.java</file><file>src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java</file><file>src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java</file><file>src/main/java/org/elasticsearch/bootstrap/JNANatives.java</file><file>src/main/java/org/elasticsearch/bootstrap/Natives.java</file><file>src/main/java/org/elasticsearch/monitor/process/ProcessInfo.java</file><file>src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/bootstrap/JNANativesTests.java</file></files><comments><comment>make JNA optional for tests and move classes to bootstrap package</comment></comments></commit></commits></item><item><title>ResourceWatcher: Rename settings to prevent watcher clash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11359</link><project id="" key="" /><description>The ResourceWatcher used settings prefixed `watcher.`, which
potentially could clash with the watcher plugin.

In order to prevent confusion, the settings have been renamed to
`resource.reload` prefixes.

This also uses the deprecation logging infrastructure introduced
in #11033 to log deprecated settings and their alternative at
startup.

Closes #11175
</description><key id="81041229">11359</key><summary>ResourceWatcher: Rename settings to prevent watcher clash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T16:36:07Z</created><updated>2015-06-12T13:39:26Z</updated><resolved>2015-06-09T08:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-08T08:23:02Z" id="109903633">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Queries with `size:0` break aggregations that need scores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11358</link><project id="" key="" /><description>Aggregations like Sampler and TopHits that require access to scores did not work if the query has size param set to zero. The assumption was that the Lucene query scoring logic was not required in these cases.
Added a Junit test to demonstrate the issue and a fix to test if any aggregations require scores to execute - if so a normal search is performed instead of making a count.

Closes #11119
</description><key id="81041077">11358</key><summary>Queries with `size:0` break aggregations that need scores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T16:35:45Z</created><updated>2015-06-08T08:43:32Z</updated><resolved>2015-05-28T13:50:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-26T16:39:55Z" id="105597336">@markharwood left one comment but it looks good
</comment><comment author="jpountz" created="2015-05-26T21:20:06Z" id="105668257">Thanks Mark for opening this pull request, I understand why we have this bug now: we only execute "secondary" collectors in the low-level `ContextIndexSearcher.search(List&lt;LeafReaderContext&gt;)`, where the weight is already created. While your patch fixes the problem we are having with aggs, I think it is a bit fragile in the sense that it would not work for other collectors that we might want to plug in that need scores too. In my opinion we should instead build a collector up-front and pass it to `ContextIndexSearcher.search(Query, Collector)` so that Lucene can decide whether or not the weight should be created with scoring support?
</comment><comment author="markharwood" created="2015-05-27T08:47:16Z" id="105827445">@jpountz There is an existing collector-registration mechanism in ContextIndexSearcher. AggregationPhase adds an entry to the "queryCollectors" map and I assume future collectors would register in a similar fashion. With this in mind I shifted the test for needing scores over to ContextIndexSearcher.
</comment><comment author="jpountz" created="2015-05-27T12:19:51Z" id="105887726">Sorry this still looks like a hack to me: the right fix would be to create the multi collector before the weight and pass collector.needsScores to Query.createWeight. I agree this is a larger change given how ContextIndexSearcher and SearchContext interact today but I don't think we can avoid it.
</comment><comment author="markharwood" created="2015-05-27T13:32:58Z" id="105909519">&gt; the right fix would be to create the multi collector before the weight 

Right. I see that MinimumScoreCollector is another example of a query setting that is broken by the size=0 condition.

By the time we get to the ContextIndexSearcher.search(List&lt;LeafReaderContext&gt;, Weight, Collector) call we already have a Weight created then add up to 5 levels of collector wrappers for various things like timeouts and minScoreCollector. The objective then is to shift all of these wrapper additions out to a section of code before we call Lucene's IndexSearcher and it creates Weights
</comment><comment author="markharwood" created="2015-05-28T09:39:47Z" id="106249661">Shifted the ContextIndexSearcher's collector-wrapping logic from search(List&lt;LeafReaderContext&gt;, Weight, Collector) method to override of IndexSearcher.search(Query, collector) method. As a result all collectors are considered as part of the "Collector.needsScores()" check before Weights are created.
The QueryPhase no longer uses searcher.count() when size=0 - it calls searcher.search() with a TotalHitCountCollector
</comment><comment author="jpountz" created="2015-05-28T10:08:10Z" id="106260003">Thanks @markharwood I think this is much better now!

&gt; The QueryPhase no longer uses searcher.count() when size=0 - it calls searcher.search() with a TotalHitCountCollector

This makes me curious, does it fail when using count() directly?
</comment><comment author="markharwood" created="2015-05-28T10:26:22Z" id="106264508">&gt; This makes me curious, does it fail when using count() directly?

No - you are right. This was a relic from some earlier changes when I thought it required a more complex change. I've removed it and so all we have now is a change moving to earlier creation of Collector wrappers
</comment><comment author="jpountz" created="2015-05-28T12:16:44Z" id="106293145">LGTM
</comment><comment author="markharwood" created="2015-05-28T13:50:14Z" id="106318656">Pushed in https://github.com/elastic/elasticsearch/commit/283b0931ff7de13b1e35ad8646bec1dba63b8112
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Match query with wildcard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11357</link><project id="" key="" /><description>There is `match`, `match_phrase` and `match_phrase_prefix` query. Why there is no `match_prefix` query? I need something like `match_phrase_prefix` but ignoring token position (ie no phrase search). 
</description><key id="81013887">11357</key><summary>Match query with wildcard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spajak</reporter><labels /><created>2015-05-26T15:28:44Z</created><updated>2015-05-27T11:25:05Z</updated><resolved>2015-05-27T10:08:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-27T10:08:14Z" id="105851994">Hi @spajak 

The `query_string` and `simple_query_string` queries have support for wildcards, and the `wildcard` `prefix`, and `regexp` term-level queries allow for wildcards or pattern matching.  However, using wildcards at query time is not best practice.  These can produce very heavy queries indeed.  A better performing solution is to use ngrams or edge ngrams at index time.  

For this reason, we prefer not to make wildcards a "first-class" citizen in the `match` family. (The `match_phrase_prefix` is also a heavy query, combining phrase and wildcards, but it was added early on before we had functionality like the completion suggester).

I recommend reading about Partial Matching in the Definitive Guide for an explanation of the available options: https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-matching.html
</comment><comment author="spajak" created="2015-05-27T10:30:29Z" id="105856769">Hi @clintongormley 

Thank You for the answer. ngrams is not an option for my data, as its mix of numbers, (money) nominals, codes, names, short descriptions. And the results must be very accurate. In a better world it should be separated into individual fields.
With the help of analyze api, I've constructed what I need using `term` and `prefix` queries. Its complicated, but gives me what I need.
</comment><comment author="clintongormley" created="2015-05-27T10:35:44Z" id="105860413">@spajak if you're using prefix queries, then edge-ngrams may be the right solution for you
</comment><comment author="spajak" created="2015-05-27T10:56:01Z" id="105867790">@clintongormley ngrams would give me a lot of false positives. 

I've decided to use `*` for explicit prefix search. 
So for example I have a query like `50 zł ande*`. `ande*` is extracted and passed to `prefix` query without `*`. The rest of the string is passed to the analyzer. Analyzed tokens are then used to feed the `term` queries. All queries are then merged with `bool: must`.

If `match` query supports wildcards, that would be perfect solution for me.
</comment><comment author="clintongormley" created="2015-05-27T11:10:26Z" id="105873241">@spajak i refer to EDGE-ngrams, not ngrams.  So instead of using the prefix query you could use the match query on a field indexed as edge-ngrams, ie `a`, `an`, `and`, `ande`,...
</comment><comment author="spajak" created="2015-05-27T11:25:05Z" id="105875775">@clintongormley I will give it a try. Thank You for help
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When ES Master leader node get restarted, all sharding slice back to "unassigned" status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11356</link><project id="" key="" /><description>I'm using ES 1.5.2 with 3 master node + 3 data node via discovery.zen.ping.unicast mode.
I encounter issue is when restart master coordinator node's ES service, first report MasterNotDiscover, then when check _cluster/health URL, all sharding is back to unassigned.
unassigned_shards: 20

And it takes long time to mark shading to "Started" status when checking "_cat/shards" URL even I enhancement recovery concurrency.
 "cluster.routing.allocation.node_initial_primaries_recoveries":20,
 "cluster.routing.allocation.node_concurrent_recoveries":4,
 "indices.recovery.concurrent_streams":10,
 "indices.recovery.concurrent_small_file_streams":10,
 "indices.recovery.max_bytes_per_sec":"100mb"

Also enable gateway policy.
gateway:
  expected_nodes: 5
  recover_after_time: 10m

Is it normal behavior to rebuild index slice eachtime master leader node get restarted?
</description><key id="81012710">11356</key><summary>When ES Master leader node get restarted, all sharding slice back to "unassigned" status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shanling2004</reporter><labels /><created>2015-05-26T15:26:11Z</created><updated>2015-05-27T09:42:00Z</updated><resolved>2015-05-27T09:41:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-27T09:41:59Z" id="105841101">HI @shanling2004 

No it is not normal for shards to become unassigned when a master is restarted.  It sounds like you have a misconfiguration somewhere.  I suggest asking your question in the forum https://discuss.elastic.co/c/elasticsearch and providing your full config for both master and data nodes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: add utility methods to serialize/deserialize inner queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11355</link><project id="" key="" /><description>`QueryBuilder`s can be serialized being them `Writeable`. Given that we support different types of queries, which can be nested into one another, we also have to make sure  that we read and write the name of the query before the actual query fields so that we know which instance of query we have to create when de-serializing. That is why serializing a query builder will be done by calling `QueryBuilderSerializer#write`, which will serialize the name of the query before the query itself, and reading only by calling `QueryBuilderSerializer#read`, which will lookup the query in the parsers registry, retrieve its corresponding empty builder from it (`getPrototypeBuilder`) and call `readFrom` against it, which will return a new query builder containing all the de-serialized fields. Compound queries will then have to use these methods to read and write their inner queries.

The registration of custom queries via plugin doesn't change, only the parser needs to be registered still, but it has now to implement the `getPrototypeBuilder` method too introduced with #11344, which returns an empy builder instance that can be used to de-serialize the query.

The way this was done is probably not the most elegant one, through a static injection and static `read` and `write` methods. Let's discuss if there are better ways to do it.
</description><key id="81000360">11355</key><summary>Query refactoring: add utility methods to serialize/deserialize inner queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-26T14:58:09Z</created><updated>2015-06-09T11:04:29Z</updated><resolved>2015-06-09T11:04:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-26T14:59:33Z" id="105553674">@cbuescher @s1monw could you please both have a look at this?
</comment><comment author="javanna" created="2015-06-09T11:04:27Z" id="110319092">Superseded by #11553 according to review comments.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[maven] fix maven warnings when running assembly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11354</link><project id="" key="" /><description>- Ignore `dependency-reduced-pom.xml` even if used in submodules
- Fix maven assembly warning about using root dir …

It's a bad practice in Maven to define `/` as the output dir.
It's better to leave it empty.

See also http://stackoverflow.com/questions/28500401/maven-assembly-plugin-warning-the-assembly-descriptor-contains-a-filesystem-roo

@rmuir Wanna review it?
</description><key id="80988429">11354</key><summary>[maven] fix maven warnings when running assembly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T14:30:37Z</created><updated>2015-06-10T09:34:09Z</updated><resolved>2015-05-27T10:01:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-26T17:41:55Z" id="105615035">looks good! thanks for cleaning this up
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Installing plugins in logstash 1.5.0 ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11353</link><project id="" key="" /><description>Hello,

I'm trying to install a custom plugin into my logstash 1.5.0.
According to the documentation i should be able to do so with this command: bin/plugin install logstash-filter-cidr

However when I try it I get this:

root@elky:/usr/local/logstash-1.5.0# bin/plugin install logstash-filter-cidr

/usrlocal/logstash-1.5.0/vendor/jruby/lib/ruby/shared/rubygems/specification.rb:102 warning: Unrecognized time zone: localtime
io/console not supported; tty will not be manipulated
Validating logstash-filter-cidr
Plugin logstash-filter-cidr does not exist
ERROR: Installation aborted, verification failed for logstash-filter-cidr

We are behind a proxy and I set this environment to get through it:
http_proxy=http://www:8080

Can anyone help me or point me in the right direction so I can get this plugin installed?

Thanks

Regards,
Johan Gregoire
</description><key id="80977340">11353</key><summary>Installing plugins in logstash 1.5.0 ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JohanGregoire</reporter><labels /><created>2015-05-26T14:06:38Z</created><updated>2015-05-26T14:10:20Z</updated><resolved>2015-05-26T14:08:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T14:08:47Z" id="105536719">Hi @JohanGregoire 

Please could you open this issue on the logstash issues list instead? https://github.com/elastic/logstash/issues

thanks
</comment><comment author="JohanGregoire" created="2015-05-26T14:10:20Z" id="105537029">Hello,

I saw it too late that this was for elasticsearch indeed. I've opened it over at logstash.
My apologies.

Regards,
Johan Gregoire
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>autosense in documentation should output curl syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11352</link><project id="" key="" /><description>I see you are switching from hardcoded curl syntax in the documentation to an `autosense` macro. The "open in sense" links have a bit of a smell of the nasty parts of open core but what really bugs me is that the documentation is no longer copy-and-paste-able into a console. It takes away a lot from the "easy to start" nature of Elasticsearch and feels like its trying to lock you in to a non-free tool in the process.

Even if sense were open source it'd be weird to tell people that they have to file bugs with curl but give them documentation in sense.
</description><key id="80957093">11352</key><summary>autosense in documentation should output curl syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-05-26T13:14:26Z</created><updated>2016-01-18T19:41:29Z</updated><resolved>2016-01-18T19:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-05-26T13:16:11Z" id="105521773">I believe the plan is to use the more terse syntax of sense for display and offer both copy as curl  and open in sense functionality via buttons/link. @clintongormley will surely have something to say ...
</comment><comment author="clintongormley" created="2015-05-26T14:07:09Z" id="105536368">As @bleskes says, Sense examples are much easier to read than the curl clutter, and sense (and marvel) is free for dev use.  It also means that people can open examples directly in a console where they can run them, as opposed to having to copy to a text file and paste one by one.

Plus there are a number of things on the go:
- sense will be able to translate multiple curl statements to sense syntax on paste
- sense will be able to copy multiple statements as curl
- sense will be able to run multiple curl statements in sequence

Once we have the second point implemented, we can look at adding a copy-as-curl button to the code blocks on the website.

&gt; Even if sense were open source it'd be weird to tell people that they have to file bugs with curl but give them documentation in sense.

I much prefer bugs filed in sense syntax.  It's the major reason that I want the multiple-curl-to-sense functionality implemented :)
</comment><comment author="nik9000" created="2015-05-26T16:43:43Z" id="105598216">&gt; sense will be able to copy multiple statements as curl

I'm sold if you replace "sense" in this sentence with "the autosense macro documentation macro". A button that copies the example to your clipboard as curl would be useful.

&gt; I much prefer bugs filed in sense syntax. It's the major reason that I want the multiple-curl-to-sense functionality implemented :)

The bugs I write often want the expressive power (lol) of bash. For loops and things.
</comment><comment author="clintongormley" created="2016-01-18T19:41:29Z" id="172631921">Given Sense is now open source, closing this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] make filter_path a default parameter in java rest runner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11351</link><project id="" key="" /><description>The query string `filter_path` parameter should be supported by all apis, we shouldn't need to declare it in the rest spec. Removed its declaration and made sure that the java runner still works, as it validates the params used in tests based on the ones declared in the rest spec.

I wonder how we can better test that this param is really supported across the board, as up until now we test it only in search and nodes stats.
</description><key id="80956054">11351</key><summary>[TEST] make filter_path a default parameter in java rest runner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T13:11:06Z</created><updated>2015-05-26T13:40:29Z</updated><resolved>2015-05-26T13:40:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-26T13:19:32Z" id="105522446">left one small comment, apart from that LGTM
</comment><comment author="tlrx" created="2015-05-26T13:20:29Z" id="105522629">LGTM and tests are OK. Thanks for fixing this :)
</comment><comment author="javanna" created="2015-05-26T13:25:20Z" id="105523617">pushed a new commit @spinscale ;)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/client/RestClient.java</file></files><comments><comment>[TEST] make filter_path a default parameter in java rest runner</comment></comments></commit></commits></item><item><title>Can we have easier access to _ttl &amp; _timestamp values?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11350</link><project id="" key="" /><description>If I am not mistaken, after enabling _ttl &amp; _timestamp, one must also then add a request for the _ttl &amp; _timestamp fields (and _source, otherwise, it disappears) to (every) search request thereafter in order to get the values - if we want to display them.

Is there a downside to having them included with the other hit fields (_id, _index, _type, _score) automatically? 

It seems to me to be the most intuitive behaviour.
At least thats what I (and my pair) were expecting - and we had to go on a bit of a tour of stackexchange to find out why it wasnt behaving like that what hoops we had to jump through
</description><key id="80932591">11350</key><summary>Can we have easier access to _ttl &amp; _timestamp values?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels /><created>2015-05-26T12:06:35Z</created><updated>2015-05-26T13:21:15Z</updated><resolved>2015-05-26T12:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T12:21:52Z" id="105504838">hi @nickminutello 

Agreed - in fact we have another issue open for it: https://github.com/elastic/elasticsearch/issues/8068

&gt; Is there a downside to having them included with the other hit fields (_id, _index, _type, _score) automatically?

There used to be, a long time ago.  Now there is almost no overhead

Closing as a duplicate of #8068
</comment><comment author="nickminutello" created="2015-05-26T13:21:15Z" id="105522798">Awesome. And quick response! Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify Transport*OperationAction names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11349</link><project id="" key="" /><description>As a follow up to #11332, this commit simplifies more class names by remove the superfluous Operation:

TransportBroadcastOperationAction -&gt; TransportBroadcastAction
TransportMasterNodeOperationAction -&gt; TransportMasterNodeAction
TransportMasterNodeReadOperationAction -&gt; TransportMasterNodeReadAction
TransportShardSingleOperationAction -&gt; TransportSingleShardAction
</description><key id="80923270">11349</key><summary>Simplify Transport*OperationAction names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T11:37:05Z</created><updated>2015-05-27T07:38:05Z</updated><resolved>2015-05-27T06:36:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-26T13:46:22Z" id="105529276">LGTM
</comment><comment author="bleskes" created="2015-05-27T07:38:05Z" id="105801661">Thx @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodeHotThreads.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/ShardRecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/CountResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/ExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/exists/ExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/exists/ShardExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainRequest.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardResponse.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java</file><file>src/main/java/org/elasticsearch/action/suggest/ShardSuggestResponse.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestResponse.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastResponse.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardResponse.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeReadOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeReadRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeReadAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/ClusterInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/NodesOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/action/admin/HotThreadsTest.java</file><file>src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Simplify Transport*OperationAction names</comment></comments></commit></commits></item><item><title>Update project.name in bin/elasticsearch script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11348</link><project id="" key="" /><description> Commit 60519911b4e50bcc958c924a768dca2ae618101b changed the project.name, preventing elasticsearch to start.
</description><key id="80921153">11348</key><summary>Update project.name in bin/elasticsearch script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T11:30:54Z</created><updated>2015-06-08T15:24:22Z</updated><resolved>2015-05-26T13:29:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-26T11:31:28Z" id="105494384">@dadoonet can you have a look please?
</comment><comment author="spinscale" created="2015-05-26T13:10:03Z" id="105520546">alternatively we could just use the `artifactId` from the `pom.xml`, which is not going to change very soon...

otherwise LGTM, lets get this fixed ASAP
</comment><comment author="tlrx" created="2015-05-26T13:29:40Z" id="105524543">Merged with `project.artifactId`, thanks @spinscale 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't write recoveryType twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11347</link><project id="" key="" /><description>This caused scary warnings in the logs
Message not fully read (request) for [157] and action [internal:index/shard/recovery/start_recovery], resetting.

closes #11335

Must have happened while backporting #11179
</description><key id="80905320">11347</key><summary>Don't write recoveryType twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-26T10:41:16Z</created><updated>2015-05-29T18:19:17Z</updated><resolved>2015-05-26T13:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-05-26T11:40:24Z" id="105495952">LGTM. I wonder if we can catch this kind of thing in our BWC tests.. (unconsumed left over).
</comment><comment author="brwe" created="2015-05-26T13:47:19Z" id="105529612">@bleskes it is not really a backwards compat problem, it was for same versions as well. Would be great to always check that but I have no smart idea how.
I found that I do not have to just read something and catch the EOF but instead `assertThat(in.read(), equalTo(-1));` Still looks good?
</comment><comment author="bleskes" created="2015-05-26T13:51:17Z" id="105531580">yeah looks good. We should probably look at the place that logs the warning message and make it assert  that everything is consumed  (with some escape root for actions where this is expected - I remember discussing using it as a temporary solution in a mixed version cluster). /cc @spinscale 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query timeouts can terminate query execution prematurely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11346</link><project id="" key="" /><description>Using 1.5.2, we see the following in about 1% of executed queries:

![screen shot 2015-05-26 at 12 46 49 pm](https://cloud.githubusercontent.com/assets/212252/7809823/bb366492-03a5-11e5-9f50-342974b7e416.png)

Where metric is the `took` from the search response, timedOut is `timed_out` and queryTimeout is the timeout value as requested from Elasticsearch. As you can see, in over 90% of cases Elasticsearch will bail out way too early in the query execution. Only about 10% of queries have the queryTimeout value closely bounding the actual time it took the query to run.

This seems like a bug in the code making the decision to return prematurely. In many cases it seems like the query returned in full but the timed_out value was still set incorrectly.

I should also mention we are using `_primary_first` in this search request as our search preference.
</description><key id="80898029">11346</key><summary>Query timeouts can terminate query execution prematurely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Search</label></labels><created>2015-05-26T10:20:44Z</created><updated>2016-09-06T07:36:39Z</updated><resolved>2016-08-31T16:21:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T12:12:13Z" id="105502174">@markharwood could you investigate this please?
</comment><comment author="synhershko" created="2016-08-31T08:16:17Z" id="243691954">Hey, has there been any progress on this? ideally we'd want to trust ES in timing out queries, but given these finding we just can't and do crazy things client-side instead.
</comment><comment author="markharwood" created="2016-08-31T16:21:45Z" id="243818771">The timer has a default resolution which can be configured using the setting `thread_pool.estimated_time_interval`. The error margins for measuring elapsed time are therefore subject to +/- errors of this value. The default setting is 200ms so the results are as expected for the given request.
</comment><comment author="synhershko" created="2016-09-06T07:36:39Z" id="244872647">Ok thanks @markharwood , we will run tests again with this in mind.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactors CommonTermsQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11345</link><project id="" key="" /><description>Refactors CommonTermsQuery analogous to TermQueryBuilder. Still left to do are
the tests to compare between builder and actual Lucene query.

Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="80897166">11345</key><summary>Refactors CommonTermsQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-26T10:19:02Z</created><updated>2015-06-23T20:32:45Z</updated><resolved>2015-06-23T20:32:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-26T10:42:13Z" id="105484974">I did a first round of review, left some comments
</comment><comment author="javanna" created="2015-06-04T08:31:10Z" id="108784448">thanks @alexksikes I left some comments
</comment><comment author="javanna" created="2015-06-09T17:15:58Z" id="110437635">I left some more comments, I'd love also for @cbuescher to have a look once last comments are addressed
</comment><comment author="javanna" created="2015-06-18T16:19:56Z" id="113207069">this is very close, I left a few comments
</comment><comment author="javanna" created="2015-06-19T08:03:05Z" id="113422148">I did a last round of review, you can rebase, it would be good to have #11730 fixed upstream though before merging this I think.
</comment><comment author="javanna" created="2015-06-22T15:15:07Z" id="114148899">left a couple of minor comments, this is very close
</comment><comment author="javanna" created="2015-06-23T09:49:03Z" id="114423853">LGTM besides the few cosmetic comments I left
</comment><comment author="javanna" created="2015-06-23T15:53:58Z" id="114555287">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Adding getBuilderPrototype() method to all QueryParsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11344</link><project id="" key="" /><description>Currently there is a registry for all QueryParsers accessible via the IndicesQueriesModule. For deserializing nested queries e.g. for the BoolQueryBuilder (https://github.com/elastic/elasticsearch/pull/11121) we need to look up query builders by their name to be able to deserialize using a prototype builder of the concrete class. This PR adds a getBuilderPrototype() method to each query parser so we can re-use the parser registry to get the corresponding builder using the query name. This might change in the future when we decide to register also builders or merger parsers and builders in a later stage of the refactoring. (see https://github.com/elastic/elasticsearch/pull/11121#discussion_r30214415) for past discussion. 
</description><key id="80873106">11344</key><summary>Query Refactoring: Adding getBuilderPrototype() method to all QueryParsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-05-26T09:11:44Z</created><updated>2015-09-02T19:19:53Z</updated><resolved>2015-05-26T14:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-26T10:28:01Z" id="105482433">looks good, left a few comments
</comment><comment author="cbuescher" created="2015-05-26T13:48:07Z" id="105529888">@javanna had another round, moved the constants to the builders and reverted most of the package private constructors, only introduced private constructors where needed.
</comment><comment author="javanna" created="2015-05-26T13:55:33Z" id="105533611">LGTM thanks @cbuescher 
</comment><comment author="cbuescher" created="2015-05-26T14:03:30Z" id="105535387">Thanks, rebased and will merge then.
</comment><comment author="javanna" created="2015-05-26T14:34:59Z" id="105545983">I am marking this as breaking, as it breaks plugins that introduce custom queries by requiring their `QueryParser` implementation to implement the new `getPrototypeBuilder` method. The new method is supposed to return an empty builder instance corresponding to the parser. The builder will be used aas intermediate query object and serialized over the wire for communication between the nodes.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/guice/MyJsonQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/PluginJsonQueryParser.java</file></files><comments><comment>Merge pull request #11344 from cbuescher/feature/query-refactoring-linkParsers</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/guice/MyJsonQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/PluginJsonQueryParser.java</file></files><comments><comment>Query refactoring: Adding getBuilderPrototype() method to all QueryParsers</comment></comments></commit></commits></item><item><title>URI Search fails with parse error when query string contains slash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11343</link><project id="" key="" /><description>I get an error message when including a slash in query string.

The query is of the following form:
http://localhost:9200/twitter/tweet/_search?q=user:/kimchy

Here's the response from ES:

```
HTTP/1.1 400 Bad Request
Content-Type: application/json; charset=UTF-8
Content-Length: 763

{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[BvVKBbulQdinJXUAXqBoaQ][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"user:/kimchy\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [user:/kimchy]]; nested: ParseException[Cannot parse 'user:/kimchy': Lexical error at line 1, column 33.  Encountered: &lt;EOF&gt; after : \"/kimchy\"]; nested: TokenMgrError[Lexical error at line 1, column 33.  Encountered: &lt;EOF&gt; after : \"/kimchy\"]; }]","status":400}
```

This occurs with ES v1.4.4.
</description><key id="80855612">11343</key><summary>URI Search fails with parse error when query string contains slash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marko-asplund</reporter><labels /><created>2015-05-26T08:23:55Z</created><updated>2016-10-19T20:24:50Z</updated><resolved>2015-05-26T08:37:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T08:37:44Z" id="105445737">Yes - it is part of the query string syntax: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_regular_expressions
</comment><comment author="marko-asplund" created="2015-05-26T09:19:19Z" id="105460133">IMO, the documentation ambiguous about this. The document just says

&gt; Regular expression patterns can be embedded in the query string by wrapping them in forward-slashes ("/")

But I don't think this statement implies that
- by including `/` characters the query gets automatically interpreted as a regexp or
- you can't use `/` characters in the query without being escaped

If this is considered a feature, I think the documentation should be changed to unambiguously state the syntax.
</comment><comment author="clintongormley" created="2015-05-26T09:23:48Z" id="105461697">@marko-asplund The documentation provides a list of reserved characters, which includes `/`.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_reserved_characters

It says:

&gt; If you need to use any of the characters which function as operators in your query itself (and not as operators), then you should escape them with a leading backslash.

I think that is pretty unambiguous but, if you disagree, please send a PR to improve the wording.
</comment><comment author="marko-asplund" created="2015-05-26T10:35:55Z" id="105483493">@clintongormley Thanks for the pointer. Yes, the list of reserved characters looks pretty unambiguous on this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't truncate TopDocs after rescoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11342</link><project id="" key="" /><description>I'm not sure why we had this fragment from #7707 ... it is over-trimming `TopDocs` such that you get `size-from` results instead of `size`, which is wrong when `from != 0`.

Closes #11277 
</description><key id="80850114">11342</key><summary>Don't truncate TopDocs after rescoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Search</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-26T08:08:54Z</created><updated>2015-05-26T09:34:12Z</updated><resolved>2015-05-26T08:50:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-26T08:35:48Z" id="105445436">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/rescore/RescorePhase.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file></files><comments><comment>Don't truncate TopDocs after rescoring</comment></comments></commit></commits></item><item><title>script_id not working on update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11341</link><project id="" key="" /><description>Hey,
Trying to use a groovy script to increase a counter.
I saw there is a security issue, which disables dynamic scripting by default.
So I created a script like it's mentioned here(https://www.elastic.co/blog/running-groovy-scripts-without-dynamic-scripting#_how_to_continue_to_use_scripting)

With the following http request:

```
POST http://localhost:9200/index/type/123/_update
{
    "script_id" : "update_counters"   
}
```

And I'm getting nested: ScriptException[dynamic scripting for [groovy] disabled].
Is it not supported for Update API?

Thanks,
Ben
</description><key id="80849075">11341</key><summary>script_id not working on update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ben-av</reporter><labels /><created>2015-05-26T08:06:10Z</created><updated>2015-05-26T08:41:57Z</updated><resolved>2015-05-26T08:10:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T08:10:07Z" id="105436521">Hi @ben-av 

Indexed Groovy scripts are also disabled by default because anybody with access to the API could create one.  You either need to change settings to allow indexed scripts (and lock down your APIs to ensure that nobody else can create an indexed script) or use files to add your scripts to `config/scripts/`
</comment><comment author="ben-av" created="2015-05-26T08:13:38Z" id="105437122">Hi Clinton,
Forgot to mention it is a script under config/scripts.
How do I send it in the request?(thought script_id searches there as well) 
</comment><comment author="clintongormley" created="2015-05-26T08:36:33Z" id="105445556">@ben-av depends on the version you are using... the script parameters are in the process of being standardised across the codebase.

Assuming your script is called `config/scripts/test.groovy`, then:
- In 1.4, do: `{ "script": "test" }`
- In 1.5, do: `{ "script": { "file": "test" }}`
</comment><comment author="ben-av" created="2015-05-26T08:41:57Z" id="105446499">Awesome, thanks @clintongormley . 
Might be good updating https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html, it's really confusing at the moment.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception occured when accessing elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11340</link><project id="" key="" /><description>While accessing a dataset thats been working for a week or so, this morning this occurred.
Error: Request to Elasticsearch failed: {"error":"ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef]; "}
KbnError@http://elastic.sec.lan:5601/index.js?_b=6004:39225:30
RequestFailure@http://elastic.sec.lan:5601/index.js?_b=6004:39262:18
http://elastic.sec.lan:5601/index.js?_b=6004:42993:56
try@http://elastic.sec.lan:5601/index.js?_b=6004:46233:31
http://elastic.sec.lan:5601/index.js?_b=6004:46211:27
map@[native code]
map@http://elastic.sec.lan:5601/index.js?_b=6004:46210:33
callResponseHandlers@http://elastic.sec.lan:5601/index.js?_b=6004:42965:25
http://elastic.sec.lan:5601/index.js?_b=6004:43083:36
wrappedCallback@http://elastic.sec.lan:5601/index.js?_b=6004:20888:81
wrappedCallback@http://elastic.sec.lan:5601/index.js?_b=6004:20888:81
http://elastic.sec.lan:5601/index.js?_b=6004:20974:34
$eval@http://elastic.sec.lan:5601/index.js?_b=6004:22017:28
$digest@http://elastic.sec.lan:5601/index.js?_b=6004:21829:36
$apply@http://elastic.sec.lan:5601/index.js?_b=6004:22121:31
done@http://elastic.sec.lan:5601/index.js?_b=6004:17656:51
completeRequest@http://elastic.sec.lan:5601/index.js?_b=6004:17870:15
onreadystatechange@http://elastic.sec.lan:5601/index.js?_b=6004:17809:26
</description><key id="80834802">11340</key><summary>Exception occured when accessing elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aimdev</reporter><labels /><created>2015-05-26T07:24:08Z</created><updated>2017-02-24T03:12:10Z</updated><resolved>2015-05-26T07:58:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-26T07:25:11Z" id="105424836">What version of ES?
</comment><comment author="aimdev" created="2015-05-26T07:34:41Z" id="105427196">Hi
Version: 1.5.2, Build: 62ff986/2015-04-27T09:21:06Z, JVM: 1.8.0_45
Aimee
</comment><comment author="clintongormley" created="2015-05-26T07:58:04Z" id="105433439">Hi @aimdev 

It sounds like you have run into a known mapping bug. Either:
- you have two fields with the same name with different mappings in different types in the same index (#8871), or
- you are using dynamic mapping and two shards have ended up mapping the same field as different data types (https://github.com/elastic/elasticsearch/issues/8688).  when one shard gets moved to a different node it applies the data type as specified on the master, which can clash with the data type stored on disk

Both of these issues will be fixed in 2.0 (see #8870).  For now, the only solution for now is to (fix your mappings in the first case, then) to reindex your data.
</comment><comment author="nidgupta" created="2017-02-24T03:12:10Z" id="282192169">I also faced the same issue of dynamic mapping ended up with different data types on 2 shards. we are not yet on 2.0, so as work around I snapshotted and restored the index, all good now. </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>feature/querystringquery-nestedqueries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11339</link><project id="" key="" /><description>Implements #11322, both the basic proposal and the features listed under 'other considerations.' Includes a new test and added assertions to another test.

`mvn test Dtests.slow=true` passes all tests.

This is my first Java commit so please be kind :)
</description><key id="80788791">11339</key><summary>feature/querystringquery-nestedqueries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuespetre</reporter><labels><label>:Query DSL</label><label>feature</label><label>review</label></labels><created>2015-05-26T04:44:27Z</created><updated>2015-10-19T10:35:06Z</updated><resolved>2015-10-19T10:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tuespetre" created="2015-05-26T04:50:22Z" id="105392624">I have signed the CLA but it is not appearing here yet. Just a heads up
</comment><comment author="clintongormley" created="2015-05-26T07:43:58Z" id="105429187">Hi @tuespetre 

Thanks for the PR.  I marked your issue as "discuss" yesterday because I'm not at all sure that this is something we want to do.  Personally I don't like the idea of further overloading the already fragile query string syntax with more syntax, but I'd like to hear what others think first.

We're in the process of wrapping up a few big features, so it'll probably take a while to get any feedback on this.  Hang in there and we'll come back to you in due course :)

thanks
</comment><comment author="tuespetre" created="2015-05-26T13:00:12Z" id="105518173">Thanks @clintongormley 

And if there is anything particular you are thinking of with the query string syntax as fragile I would be glad to help, of course if that means the syntax itself is what's fragile and not the code supporting it, there's not much anyone could do :tongue: 
</comment><comment author="clintongormley" created="2015-05-26T13:56:04Z" id="105533712">@tuespetre yeah, that's exactly what i mean. it's a strict syntax, so you really have to understand it to use it properly.  It's definitely not for exposing to your average user.  The `simple_query_string` takes a more lenient approach (but wouldn't be a suitable candidate for your patch as you can't specify field names).
</comment><comment author="tuespetre" created="2015-05-26T14:26:43Z" id="105542928">I fully understand what you mean. I wrote a gist here the other day (https://gist.github.com/tuespetre/f6951bb665c79abbb7c8) that helps to create user interfaces that build query string queries in the fashion of Github's issue tracker, precisely because we can't expect all of our employees/users to memorize the syntax. But for those who are not intimidated, it's a very powerful tool, and it's so much less boilerplate in the backend to be able to just pass a query string to Elasticsearch. 

We have use cases that require matching on nested documents ("accounts with a nested item containing this code and that ip address", for instance) so I did feel personally motivated to do this; plus it was very fun to try out Java! I am tweaking the gist this morning to support the 'nested query string' syntax so we can just break it down in the back end and pass it to Elasticsearch in the appropriate way to accomplish the same thing with minimal cruft. So we won't necessarily be missing anything if this doesn't become incorporated. 

I am just grateful for your consideration. Best of luck with your work and in the meantime if there is anything else I might be able to contribute, feel free to point me there.
</comment><comment author="cbuescher" created="2015-10-19T10:35:06Z" id="149178852">@tuespetre this PR looks like the issue it refers to is closed already. Closing this now, feel free to reopen if I was mistaken.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding JAVA_HOME to documents and env config file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11338</link><project id="" key="" /><description>JAVA_HOME can optionally be configured in the system config file used by deb/rpm packages.

Closes #11291
</description><key id="80672229">11338</key><summary>Adding JAVA_HOME to documents and env config file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">jzinner</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2015-05-25T21:22:47Z</created><updated>2016-10-18T08:52:28Z</updated><resolved>2016-04-06T21:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-26T07:35:24Z" id="105427391">@tlrx please could you review?
</comment><comment author="dakrone" created="2016-04-06T21:06:42Z" id="206568130">LGTM, will merge.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping should throw when field names with dots are specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11337</link><project id="" key="" /><description>This was previously reported by @brwe on https://github.com/elastic/elasticsearch/issues/5972 but closed in favour of this comment https://github.com/elastic/elasticsearch/issues/6736#issuecomment-48451854 on #6736. That issue is now closed however so opening this again as a separate ticket so it won't be buried in the field name/path resolution refactor.

Consider the following mapping:

``` json
DELETE index123
PUT index123
{
  "mappings": {
    "people": {
      "properties": {
        "o.x.y": {
          "type": "string",
          "analyzer": "standard"
        },
        "o.x.z": {
          "type": "string",
          "boost": 2
        }
      }
    }
  }
}

POST index123/people
{
  "o" : {
    "x" : {
      "y" : "this is analyzed",
      "z" : "2015-05-25T22:13:04+02:00"
    }
  }
}
```

When we do a `GET index123/_mapping` we get the following mapping for people:

``` json
{
  "people": {
    "properties": {
      "o": {
        "properties": {
          "x": {
            "properties": {
              "y": {
                "type": "string"
              },
              "z": {
                "type": "date",
                "format": "dateOptionalTime"
              }
            }
          }
        }
      },
      "o.x.y": {
        "type": "string",
        "analyzer": "standard"
      },
      "o.x.z": {
        "type": "string",
        "boost": 2
      }
    }
  }
}
```

The string mapping for `o.x.z` is ignored and it becomes a `date`. 

Interestingly if we had mapped it as an `integer` e.g

``` json
"o.x.z": {
    "type": "integer",
    "ignore_malformed": false
}
```

and indexed the following document

``` json
{
  "o" : {
    "x" : {
      "y" : "this is analyzed",
      "z" : "asdasda"
    }
  }
}
```

we DO get a `NumberFormatException[For input string: \"asdasda\"]`. Something is out of sync here. 

Regardless of the field name/path resolution fixes it would be very handy if we'd get a mapping exception whenever we try to map a field with dots in it to prevent ambiguous mappings. 
</description><key id="80660027">11337</key><summary>Mapping should throw when field names with dots are specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-05-25T20:38:18Z</created><updated>2015-08-26T19:50:03Z</updated><resolved>2015-08-26T19:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-26T09:25:57Z" id="105462035">Is this issue encompassed by #9059?
</comment><comment author="Mpdreamz" created="2015-05-26T11:28:49Z" id="105494050">yes and no that ticket makes no mention of validating field names during mapping.

Terminology wise i think there is a distinction between field paths and field names. #9059 makes no clear distinction between the two and what to do when an api expect field names but gets a field path. 

NEST models these separately for instance, mapping taking PropertyName's and e.g the query dsl taking PropertyPath's.
</comment><comment author="jpountz" created="2015-08-26T19:50:03Z" id="135151715">Fixed via https://github.com/elastic/elasticsearch/pull/12068
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move index sealing terminology to synced flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11336</link><project id="" key="" /><description>#10032 introduced the notion of sealing an index by marking it with a special read only marker, allowing for a couple of optimization to happen. The most important one was to speed up recoveries of shards where we know nothing has changed since they were online by skipping the file based sync phase. During the implementation we came up with a light notion which achieves the same recovery benefits but without the read only aspects which we dubbed synced flush. The fact that it was light weight and didn't put the index in read only mode, allowed us to do it automatically in the background which has great advantage. However we also felt the need to allow users to manually trigger this operation.

 The implementation at #11179 added the sync flush internal logic and the manual (rest) rest API. The name of the API was modeled after the sealing terminology which may end up being confusing. This commit changes the API name to match the internal synced flush naming, namely `{index}/_flush/synced'.

  On top of that it contains a couple other changes:
- Remove all java client API. This feature is not supposed to be called programtically by applications but rather by admins.
- Improve rest responses making structure similar to other (flush) API
- Change IndexShard#getOperationsCount to exclude the internal +1 on open shard . it's confusing to get 1 while there are actually no ongoing operations
- Some minor other clean ups

Closes #11251
</description><key id="80641876">11336</key><summary>Move index sealing terminology to synced flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>feature</label><label>release highlight</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-25T19:34:23Z</created><updated>2015-05-29T18:01:22Z</updated><resolved>2015-05-29T08:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-05-25T19:34:41Z" id="105295254">@brwe @clintongormley can you please review?
</comment><comment author="clintongormley" created="2015-05-25T19:48:26Z" id="105299084">Docs look great! (two tiny changes)
</comment><comment author="nik9000" created="2015-05-26T12:55:38Z" id="105516726">I don't mind the "seal" name - I just stopped thinking of it as a [hermetic seal](https://en.wiktionary.org/wiki/hermetic_seal) and started thinking of it as a [wax seal](https://en.wikipedia.org/wiki/Seal_%28emblem%29) on an envelope. You break it when you stuff more documents in it. I forget the other half of the metaphor about having to break the seal to read the documents.
</comment><comment author="nik9000" created="2015-05-26T13:38:48Z" id="105526262">&gt; I don't mind the "seal" name - I just stopped thinking of it as a hermetic seal and started thinking of it as a wax seal on an envelope. You break it when you stuff more documents in it. I forget the other half of the metaphor about having to break the seal to read the documents.

I take it back - after reviewing the documentation this way makes more sense to me. No fun metaphor though.
</comment><comment author="bleskes" created="2015-05-27T07:36:19Z" id="105800796">@brwe @clintongormley @nik9000 thx for all the feedback. I pushed a new commit. Also assumed will end up with a 409 for #11251 ..
</comment><comment author="clintongormley" created="2015-05-27T10:34:07Z" id="105859794">Minor doc comments, but looking good!
</comment><comment author="brwe" created="2015-05-27T11:30:36Z" id="105878417">this is tagged for 2.0 but should it not also go in 1.6?
</comment><comment author="nik9000" created="2015-05-27T12:39:58Z" id="105892415">&gt; this is tagged for 2.0 but should it not also go in 1.6?

Yeah, I was just about to ask that.
</comment><comment author="bleskes" created="2015-05-27T12:40:57Z" id="105892714">pushed another update. I was planning the back port PR (which is likely to happen) with 1.6., but can mark this one as well... 
</comment><comment author="brwe" created="2015-05-28T09:34:44Z" id="106247945">LGTM but not sure if that counts as a go
</comment><comment author="s1monw" created="2015-05-28T09:55:27Z" id="106253571">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file></files><comments><comment>Reduce shard inactivity timeout to 5m</comment></comments></commit><commit><files /><comments><comment>[doc] remove reference to seal, was removed in #11336</comment></comments></commit></commits></item><item><title>Serialization error for action [internal:index/shard/recovery/start_recovery]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11335</link><project id="" key="" /><description>If I start a node of version 1.5.0 and another built from 1.x, and create an index, I get several messages like this:

```
[2015-05-25 21:17:57,012][WARN ][transport.netty          ] [Donald Ritter] Message not fully read (request) for [482] and action [internal:index/shard/recovery/start_recovery], resetting
[2015-05-25 21:17:57,020][WARN ][transport.netty          ] [Donald Ritter] Message not fully read (request) for [483] and action [internal:index/shard/recovery/start_recovery], resetting
[2015-05-25 21:17:57,519][WARN ][transport.netty          ] [Donald Ritter] Message not fully read (request) for [485] and action [internal:index/shard/recovery/start_recovery], resetting
[2015-05-25 21:17:57,524][WARN ][transport.netty          ] [Donald Ritter] Message not fully read (request) for [486] and action [internal:index/shard/recovery/start_recovery], resetting
[2015-05-25 21:17:57,591][WARN ][transport.netty          ] [Donald Ritter] Message not fully read (request) for [488] and action [internal:index/shard/recovery/start_recovery], resetting
```

Could this be to do with the index sealing PR? /cc @brwe ?
</description><key id="80637696">11335</key><summary>Serialization error for action [internal:index/shard/recovery/start_recovery]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-25T19:20:44Z</created><updated>2015-05-26T13:54:03Z</updated><resolved>2015-05-26T13:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-05-26T13:54:02Z" id="105533226">fixed in #11347
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Standardise wildcard matching </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11334</link><project id="" key="" /><description>With the new `path_filter` parameter (https://github.com/elastic/elasticsearch/pull/10980) we support wildcards with `*` matching a single "segment" of the path  and `**` matching multiple segments.

Currently we use slightly different simple wildcards in a number of other places, such as when looking up field names.  Index wildcards also support a leading `-` to exclude matches.

I'd like to propose that we standardise the syntax across all APIs that use wildcard matching, as follows:
- `*` matches zero or more characters up to but not including a period `.`
- `**` matches all zero or more characters including a period
- multiple patterns can be separated by a comma `,`
- any pattern can be preceded by a `+` to include matches (the default if not specified) or a `-` to exclude matches
- it should be possible to escape any of these characters: `*`, `+`, `-`, `,`, `\` with a preceding `\`

Relates to #11294
</description><key id="80600673">11334</key><summary>Standardise wildcard matching </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2015-05-25T17:05:32Z</created><updated>2016-01-18T19:39:58Z</updated><resolved>2016-01-18T19:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-25T17:18:41Z" id="105272229">Highlighting has an issue about excluding fields from \* that this applies to. I don't have the number handy. 
</comment><comment author="clintongormley" created="2015-05-25T17:23:22Z" id="105272629">thanks @nik9000 - it is https://github.com/elastic/elasticsearch/issues/9881
</comment><comment author="abibell" created="2015-05-26T00:09:35Z" id="105339192">If we used Regex would that prevent us from hand crafting the parsing? How much of performance hit is that going to be? We could execute regex only when we detect the presence of regex.
</comment><comment author="clintongormley" created="2015-05-26T07:39:58Z" id="105428054">You'd have to have some marker to indicate that it is a regex, otherwise `foo*` would be ambiguous. The obvious marker would be to wrap the pattern in `/`, but not all users of wildcard matching could support regexes.  For instance the streaming path filtering is highly optimised because it only considers segments. If we were to change that to regexes it would have a big performance overhead.
</comment><comment author="clintongormley" created="2015-06-12T09:48:41Z" id="111431819">Discussing this some more: changing the index pattern matching would be problematic, eg you'd have to escape the . in `.marvel` etc.  Probably not worth making this change everywhere.
</comment><comment author="clintongormley" created="2016-01-18T19:39:58Z" id="172631645">Won't fix
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor upgrade API to use transport and write minimum compatible version that the index was upgraded to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11333</link><project id="" key="" /><description>In #11072 we are adding a check that will prevent opening of old indices. However, this check doesn't take into consideration the fact that indices can be made compatible with the current version through upgrade API. In order to make compatibility check aware of the upgrade, the upgrade API should write a new setting `index.version.minimum_compatible` that will indicate the minimum compatible version of lucene this index is compatible with and `index.version.upgraded` that will indicate the version of elasticsearch that performed the upgrade.

Closes #11095
</description><key id="80588460">11333</key><summary>Refactor upgrade API to use transport and write minimum compatible version that the index was upgraded to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Upgrade API</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-25T16:29:54Z</created><updated>2015-05-30T11:42:06Z</updated><resolved>2015-05-28T15:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-25T16:32:48Z" id="105263358">@rjernst could you review it please?
</comment><comment author="rjernst" created="2015-05-26T10:02:42Z" id="105475580">@imotov I left some comments. It is unfortunate how much boiler plate is needed for a transport api...
</comment><comment author="imotov" created="2015-05-26T16:00:58Z" id="105577153">@rjernst I pushed the changes. The upgrade command contains two phases - upgrade followed by settings update, so it added some additional complexity and code. 
</comment><comment author="rjernst" created="2015-05-27T07:19:06Z" id="105792830">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename TransportShardReplicationOperationAction to TransportReplicationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11332</link><project id="" key="" /><description>TransportShardReplicationOperationAction is a mouthful and is the only thing we mean when we say replication.  This commit also changes some related friends.
</description><key id="80543044">11332</key><summary>Rename TransportShardReplicationOperationAction to TransportReplicationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-25T13:49:08Z</created><updated>2015-05-25T19:49:08Z</updated><resolved>2015-05-25T19:49:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-25T14:08:04Z" id="105240018">@bleskes :), LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodeHotThreads.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/ShardRecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/CountResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/ExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/exists/ExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/exists/ShardExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainRequest.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardResponse.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java</file><file>src/main/java/org/elasticsearch/action/suggest/ShardSuggestResponse.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestResponse.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastResponse.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardResponse.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeReadOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeReadRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeReadAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/ClusterInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/NodesOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportSingleShardAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/action/admin/HotThreadsTest.java</file><file>src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Simplify Transport*OperationAction names</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequest.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file></files><comments><comment>Merge pull request #11332 from bleskes/replication_action</comment></comments></commit></commits></item><item><title>Throw exception if unrecognized parameter in bulk action/metadata line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11331</link><project id="" key="" /><description>It now throws an exception when an action/metadataline of a bulk request contains an unknown parameter or object like:

``` json
{"index": {"_index": "test", "_type": "doc", "_id": 1, "_foo": "bar"}}
```

``` json
{"index": {"_index": "test", "_type": "doc", "_id": 2, "_unkown": ["foo", "bar"]}}
```

Closes #10977
</description><key id="80531945">11331</key><summary>Throw exception if unrecognized parameter in bulk action/metadata line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-25T13:08:31Z</created><updated>2015-06-02T15:21:33Z</updated><resolved>2015-05-27T16:11:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-05-25T14:03:11Z" id="105239303">awesome - extra bonus points for the line indication. Left some minor comments.
</comment><comment author="tlrx" created="2015-05-26T08:12:26Z" id="105436809">@bleskes thanks for your review! I updated the code following your comments.
</comment><comment author="bleskes" created="2015-05-26T12:16:23Z" id="105502901">LGTM
</comment><comment author="s1monw" created="2015-06-02T15:21:33Z" id="107989341">this is likely breaking logstash see https://github.com/elastic/elasticsearch/issues/11458
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MySQL LIKE syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11330</link><project id="" key="" /><description>Clearly, nobody cares, but there is an error here in the poor man MySQL full text search:

https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-matching.html

```
    WHERE text LIKE "*quick*"
      AND text LIKE "*brown*"
      AND text LIKE "*fox*" 
```

It's not `*` but `%` in MySQL.
</description><key id="80469693">11330</key><summary>MySQL LIKE syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JulienPalard</reporter><labels /><created>2015-05-25T09:38:59Z</created><updated>2016-01-18T19:39:39Z</updated><resolved>2016-01-18T19:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-25T12:44:41Z" id="105226937">https://github.com/elastic/elasticsearch-definitive-guide/pull/360
</comment><comment author="JulienPalard" created="2015-05-25T13:51:03Z" id="105237694">TIL the doc is on github. ^-^
</comment><comment author="nik9000" created="2015-05-25T15:20:07Z" id="105250974">Some of the docs are in the main  elasticsearch project but the ones in the
"guide" are in that other repository. Those are the source of the paper
book.
On May 25, 2015 9:51 AM, "Julien" notifications@github.com wrote:

&gt; TIL the doc is on github. ^-^
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11330#issuecomment-105237694
&gt; .
</comment><comment author="clintongormley" created="2016-01-18T19:39:39Z" id="172631562">Fixed by https://github.com/elastic/elasticsearch-definitive-guide/pull/360
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation typo in precision_step for byte number type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11329</link><project id="" key="" /><description>`precision_step` paragraph of [Number](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html#number) section of Core Types says that number of generated terms for `byte` number type is 2147483647. This looks certainly wrong unless I completely misunderstood byte number type.
</description><key id="80468940">11329</key><summary>Documentation typo in precision_step for byte number type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haizaar</reporter><labels /><created>2015-05-25T09:36:26Z</created><updated>2015-05-26T07:36:03Z</updated><resolved>2015-05-25T18:12:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T18:12:16Z" id="105283914">@haizaar it says it _influences_  the number of terms produced :)  That rather arcane setting actually means that it creates only 1 term for bytes. Hopefully, with https://issues.apache.org/jira/browse/LUCENE-5879, we won't ever need to set that value in the future.
</comment><comment author="haizaar" created="2015-05-25T23:00:02Z" id="105332072">I understand... but why not to change 2147483647 to 1 in the current docs?
</comment><comment author="clintongormley" created="2015-05-26T07:36:03Z" id="105427486">@haizaar Because MAXINT is the actual setting, not 1.  It results in 1 byte being stored.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for Surround Query Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11328</link><project id="" key="" /><description>Currently Elasticsearch supports Lucene's `classic` and `simple` query parsers via its `query_string` and `simple_query_string` queries.

I'd like to make use of Lucene's [surround query parser](https://lucene.apache.org/core/4_1_0/queryparser/org/apache/lucene/queryparser/surround/parser/QueryParser.html) and I'm willing to implement it.

Before I start anything:
1. Is that something other people would like to have? I haven't found any relevant issue in the tracker.
2. Is there any up-to-date documentation on how to properly implement a new query parser? My plan was to try to mimic what has been done in the two previous ones.
</description><key id="80459488">11328</key><summary>Support for Surround Query Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicbaz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>feature</label></labels><created>2015-05-25T09:07:02Z</created><updated>2017-04-19T09:07:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2016-06-22T05:20:46Z" id="227646293">This parser seems to have some advantages over the existing parserswhen dealing with proximity (edit distance is not intuitive to all users).  Are there plans to implement this?
</comment><comment author="bairongdong1" created="2016-12-22T05:10:18Z" id="268720093">I also need that in my company's product.</comment><comment author="clintongormley" created="2016-12-23T10:27:41Z" id="268969682">I'd be OK with this being added as a plugin.  Same goes for the SpanQueryParser in https://issues.apache.org/jira/browse/LUCENE-5205</comment><comment author="bairongdong1" created="2016-12-27T13:18:05Z" id="269324698">I had written a test plugin for this, just implement the basic function, just to pass my test, I am a newcomer to ES and lucene. [test parser](https://github.com/bairongdong1/BayParser)</comment><comment author="nicbaz" created="2017-01-25T19:08:28Z" id="275202318">@clintongormley we're currently working to implement LUCENE-5205 in 2.x, 1.x as well as 5.x after that.

1) we're mimicking org/apache/lucene/queryparser/classic/MapperQueryParser but maybe the logic could be reimplemented to work with both classic and SpanQueryParser ?
2) should we aim for this be added as a core plugin ? or should we maintain it as a third-party ?</comment><comment author="clintongormley" created="2017-01-26T10:45:13Z" id="275358980">@nicbaz Why not submit it as a WIP PR as a core plugin, and we can make a decision once we see the code?</comment><comment author="gcampbell-epiq" created="2017-03-14T22:35:54Z" id="286583062">I would love to get support for the SpanQueryParser built into the ES query API. Since Lucene has several query parsers which are not yet supported by the ES query API, why not prove a generalized ES query string API that accepts as an option, which parser to use? 

Other query parsers should also be supported by highlighting. 

This would be a huge value-add in the legal space where users regularly deal in _surprisingly_ complex full text searches, especially involving recursive span queries (i.e. nested proximity queries) (e.g. [[john doe]~3 ["car or auto*" "accident or wreck"]~3]~5 -representing a search for mention of john doe and a car accident within a close proximity)</comment><comment author="speedplane" created="2017-03-30T07:07:29Z" id="290321753">@gcampbell-epiq I agree this would be nice, we ended up implementing something that does exactly that ([see example here](https://www.docketalarm.com/search/documents/?q=%22personal+injury%22+w%2F10+boat)). However, if this will largely be implemented for the legal space, it would make sense to use the query language that's largely been standardized across legal search engines rather than the Lucene style of brackets and tildas.</comment><comment author="carlholmboe" created="2017-04-19T09:06:04Z" id="295176130">I am curious to know the status on support for SpanQueryParser in ES.
My company, Retriever, is doing media monitoring on behalf of our customers in the Nordic countries by means of a large number of predefined queries. 
These can be very complex as they are set up by our professional staff.
We are currently using Fast ESP Alerting and plan to migrate to ES Percolator.
Many of the queries contains extensive use of NEAR and ONEAR  (Ordered NEAR), such as
“term* ONEAR/20 ‘some phrase’”, “(term1 or term2) NEAR (term*3)”.
Will the plugin support Percolator queries?
Is there a possibility to test the SpanQueryParser plugin somehow for this purpose? 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error when renaming logs on UNC path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11327</link><project id="" key="" /><description>I have configured elastic search with log path pointing to UNC path.
I Keep on receiving below error

log4j:ERROR Failed to rename [//elasticmaster1/logs/cluster1.log] to [[//elasticmaster1/logs/cluster1.log.2015-05-21] .

I even confirmed with my network admin if in case there is some permission related issue for the log path but this is not the case.

we are using virtual environment with operating system WINDOWS SERVER 2008 R2 STANDARD

Am I missing anything to configure?

One more wired thing noticed when above error is received  I see increase in memory.
For example before receiving the error, memory consumption was 11.9 GB by master and after receiving the error it increased to 12.1 GB.
</description><key id="80368499">11327</key><summary>Error when renaming logs on UNC path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels><label>:Logging</label></labels><created>2015-05-25T03:27:40Z</created><updated>2015-05-26T08:05:14Z</updated><resolved>2015-05-26T07:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-25T03:29:30Z" id="105113515">It definitely sounds like a permissions error, are you 100% sure that the user you are running ES as has can rename.

Why are you logging to a UNC path anyway?
</comment><comment author="dobariya" created="2015-05-25T03:34:22Z" id="105113811">yes i evened tried renaming the logs but it throws error saying this file is in use by other process which makes sense its used by ES it self.
When i shut down ES then am able to rename the logs manually on the UNC path.

We are storing logs on UNC path because we want all logs logged centrally on one path 
</comment><comment author="clintongormley" created="2015-05-25T18:08:31Z" id="105283387">@dobariya are you sure that each of your nodes is using a different file name for their logs?
</comment><comment author="dobariya" created="2015-05-26T07:35:24Z" id="105427388">No all nodes are using same log path.
Yes this is the issue i can say.The first node started will lock the log file and others will not able to write.
</comment><comment author="clintongormley" created="2015-05-26T07:58:36Z" id="105433519">OK great - so renaming the log file should fix that. thanks for letting us know
</comment><comment author="dobariya" created="2015-05-26T08:02:01Z" id="105434174">can i say this will get fix in next version or do i have to set up anything else
</comment><comment author="clintongormley" created="2015-05-26T08:05:14Z" id="105435213">@dobariya this is not a bug - you just need to configure your nodes to use different log file names
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>why muti threads cant share the one  transportclient </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11326</link><project id="" key="" /><description>why muti threads cant share the one  transportclient. Bulk insert error .
</description><key id="80316377">11326</key><summary>why muti threads cant share the one  transportclient </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoyokeen</reporter><labels /><created>2015-05-25T00:19:08Z</created><updated>2015-05-25T00:20:00Z</updated><resolved>2015-05-25T00:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-25T00:19:59Z" id="105081108">Join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>why muti threads </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11325</link><project id="" key="" /><description /><key id="80316079">11325</key><summary>why muti threads </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoyokeen</reporter><labels /><created>2015-05-25T00:17:34Z</created><updated>2015-05-25T00:20:28Z</updated><resolved>2015-05-25T00:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-25T00:20:27Z" id="105081129">See #11326 

Also you can edit issues after you create them, there is no need to recreate a new one if you make a mistake.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bulk insert 20w records </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11324</link><project id="" key="" /><description>bulk insert 20w records, need  one hour ,howto advance it in 10min s ?
</description><key id="80314156">11324</key><summary>bulk insert 20w records </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoyokeen</reporter><labels /><created>2015-05-25T00:09:52Z</created><updated>2015-05-25T00:10:37Z</updated><resolved>2015-05-25T00:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-25T00:10:35Z" id="105080760">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11323</link><project id="" key="" /><description /><key id="80313530">11323</key><summary>bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoyokeen</reporter><labels /><created>2015-05-25T00:07:27Z</created><updated>2015-05-25T00:10:45Z</updated><resolved>2015-05-25T00:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-25T00:10:45Z" id="105080766">See #11324
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for nested query syntax within query string query DSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11322</link><project id="" key="" /><description>I understand that issue #9611 was closed regarding this:

&gt; Nested fields need to be queries with nested queries/filters, because multiple documents can match and you need to be able to specify how these multiple scores should be reduced to a single score.
&gt; &amp;mdash; -- @clintongormley 
# Proposal

I propose that, when a field name within a query string query is parsed, and it does not match a field mapping, an attempt should be made to match the field name to a nested object mapper. If the attempt is successful, the query text for that field name should then be parsed as a query string query using the same settings as the root level query string query. The resulting query from that parsing will in turn be used to create a ToParentBlockJoinQuery (a nested query) that uses the same default scoring mode that would be applied when manually submitting a nested query ("avg".) 
# The syntax

The acceptable syntax for a nested query within a query string query is similar to this:

`nestedPath:"&lt;query string query&gt;"`

This means that any constructs you would use in a query string query are valid:

`children:"children.first:peggy"`
`children:"children.first:\"peggy\""`
`children:"children.first:(peggy ruby)"`
`children:"children.first:peggy AND children.last:sue"`
`children:"children.first:pegyg~ +children.last:su?"`

Note that the nested query MUST be surrounded with quotes. I wanted it to be parentheses instead but unfortunately the Lucene QueryParser class does not recognize the field names the way I wanted it to (`children:(children.first:peggy)` would come out as a TermQuery on `children.first`, the `children` field name would be discarded.)
# Other considerations
- Support for specifying scoring modes within the query string query settings based on nested object paths is a possibility.
- Support for inner hits may also be a possibility, in a similar fashion to scoring modes.

Support for nested queries in query strings at all would be an enhancement, but these options could provide additional enhancements. Example of how they may look:

```
{
    "query_string" : {
        "query" : "children:\"children.first:peggy\"",
        "nested": [
            {
                "path": "children",
                "score_mode": "max",
                "inner_hits": {
                    &lt;inner_hits_options&gt;
                }
            }
        ]
    }
}
```
# Pull request

For the basic functionality, I have already made the necessary modifications (three changed files, one changed test file to add a test with several assertions) on the 'master' branch of my local clone of the repository. I would like to submit a pull request; please advise as to how you would like that to be done (if I need to rebase onto another branch, etc.)
</description><key id="80244282">11322</key><summary>Support for nested query syntax within query string query DSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuespetre</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-05-24T19:14:39Z</created><updated>2017-07-20T13:34:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tuespetre" created="2015-07-24T12:58:45Z" id="124511935">I've since worked around this in other ways (simple regex to parse out nested field expressions on my end and submit them properly to ES); It was fun to mess around with this but I fully support axing it now. It would just be more complexity to maintain; perhaps the query string documentation could hint at some kind of better solution for developers that may look for this functionality.
</comment><comment author="clintongormley" created="2015-07-27T11:12:32Z" id="125168379">thanks @tuespetre 
</comment><comment author="radenui" created="2016-01-22T15:44:56Z" id="173956731">Hi @tuespetre ,

I'm very interested in the workaround you used. Did you manage to make it work with kibana ?
Thanks !
</comment><comment author="tuespetre" created="2016-01-22T16:16:13Z" id="173965565">@radenui 

I wrote the following drop-in helper class (written in C#, but should be easily portable to other languages): https://gist.github.com/tuespetre/f6951bb665c79abbb7c8

You basically use the class to create new URIs by performing some function against the existing query string (remove this filter, replace that filter, add this filter, etc.) When you specifically need to allow users to perform a 'proper' nested query, you can just use the helper to extract the filters on the nested properties out and build up a separate query string, which you would then submit as a nested query string query in your request to Elasticsearch.

I'm using it to offer both 'customer service representative friendly' interfaces (where the query string built up by the 'friendly' controls is stored in a hidden input) and 'technical user friendly' interfaces (where the query string is spit out into a visible text box that you can also type in, a-la GitHub Issues.)
</comment><comment author="rmm5t" created="2016-05-10T18:30:41Z" id="218248185">I actually quite like this proposal. Is it something that would be considered by the elasticsearch team or is this something that's not likely to ever be a feature? I'd love if the query string syntax allowed for nested query combinations. 

&gt; I wanted it to be parentheses instead 

Agreed. I think this syntax would be much better served by parentheses instead of quotations. 
</comment><comment author="jsangari-ssat" created="2016-05-24T12:58:51Z" id="221260467">Hi,

Is the syntax recommended here for the query_string supported in ES, I am using Version 2.2 and am having hard time getting it to work
</comment><comment author="alexgarel" created="2016-09-19T13:45:07Z" id="247997016">Hello I also think this should be supported. query_string remains a nice helper, and being able to use nested objects whit it would be great.
</comment><comment author="tuespetre" created="2016-09-19T14:05:08Z" id="248002413">@alexgarel and all everyone:

I think it would be more beneficial to keep something this niche and complex out of the core elasticsearch, and offer your own query DSL 'layer' that can be translated into a 'proper' ES query on the backend. By brushing up on regular expressions (or even parsing!) a little bit you can put together some pretty cool UX affordances specific to your application.
</comment><comment author="rmm5t" created="2016-09-19T14:12:54Z" id="248004642">&gt; ...keep something this niche and complex out of the core elasticsearch...
&gt; 
&gt; ...By brushing up on regular expressions (or even parsing!) a little bit...

So, is it niche and _complex_ or is it as simple as adding a few items to the elasticsearch grammar? 

Personally, I agree that you can add a custom syntax on top (with regexes or otherwise), but I also would like this discussion to remain open, because I think having a conversation about making the query string syntax more robust isn't necessarily a bad thing. Having every elasticsearch application implement yet another hack on top of the query string syntax to accomplish this isn't necessarily a great use of global man-hours. 

I'm mostly interested to better understand if the elasticsearch team is interested in a Pull Request for this feature. So far,  we don't have an answer to that question.
</comment><comment author="alexgarel" created="2016-09-19T14:21:43Z" id="248007084">@tuespetre
Ok I understand, it's the way we have chosen but not fully implemented yet. If someone needs it, we have a (GPL)  [lucene query parser in python](https://github.com/jurismarches/luqum/)
</comment><comment author="tuespetre" created="2016-09-19T14:37:51Z" id="248011842">@rmm5t I had submitted a PR (https://github.com/elastic/elasticsearch/issues/11339) but as @clintongormley points out it's just a fragile thing to have in the core application, and as I found out when working the PR initially, it can't really be done with a pleasant syntax -- it comes out feeling very verbose and awkward, especially being unable to hijack the parenthesis for it. With a small handful of regular expressions I was able to implement a much nicer syntax specific to the particular needs of our application without feeling like I had to 'settle' for something subpar.
</comment><comment author="rmm5t" created="2016-09-19T19:16:53Z" id="248093863">&gt; I had submitted a PR (#11339) but as @clintongormley points out it's just a fragile thing to have in the core application, and as I found out when working the PR initially, it can't really be done with a pleasant syntax

@tuespetre  Interesting point. That PR was tagged for discussion (which, respectfully, never really happened amongst the elasticsearch team, aside from @clintongormley willingness to comment and chime in). Then, it was closed, solely because you closed this particular issue after building a workaround -- not because a discussion really happened. 

I agree with your first assessment that the double-quoted syntax isn't ideal. I understand there are problems with the clearer parentheses syntax, but I suspect those can probably be overcome.

If the core query string syntax and implementation are _"fragile,"_ maybe that's something that should be addressed and potentially refactored as well. To be clear, I'm not trying to make light of this; I'm sure a refactor would be a tricky endeavor.

**Overall, I'd really just like to see an ability to narrow a query string search to one particular embedded object.  I'd like to see a syntax that looked like this:**

`children:(gender:male AND age:&gt;=18 AND age:&lt;=25)`

Otherwise, there's no way to use the query string syntax and (in this particular US-centric example) find parents who have children who should be signed up for the US Selective Service System. 
</comment><comment author="traut" created="2017-07-20T10:41:01Z" id="316665996">can we resurrect this issue please?</comment><comment author="clintongormley" created="2017-07-20T10:47:27Z" id="316667255">Yeah, I think we need to think more about whether to expose this.  Opening for more discussion</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Absorb ImmutableSettings into Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11321</link><project id="" key="" /><description>Spinoff from #7633:

I don't think Settings should be an interface separated from the implementation?

We have only ImmutableSettings today, I think it should be the only (final) one.
</description><key id="80211211">11321</key><summary>Absorb ImmutableSettings into Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-24T16:59:59Z</created><updated>2015-06-06T19:08:32Z</updated><resolved>2015-05-25T17:13:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-24T17:27:47Z" id="105041056">+1
</comment><comment author="s1monw" created="2015-05-24T20:06:38Z" id="105058295">+1
</comment><comment author="rmuir" created="2015-05-25T12:31:32Z" id="105225326">+1, you cannot make sense of this api without reading the src code!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/TransportSealIndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>src/main/java/org/elasticsearch/client/support/Headers.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java</file><file>src/main/java/org/elasticsearch/common/cli/CliTool.java</file><file>src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>src/main/java/org/elasticsearch/common/compress/lzf/LZFCompressor.java</file><file>src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/network/MulticastChannel.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/common/settings/SettingsFilter.java</file><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/main/java/org/elasticsearch/index/IndexService.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>src/main/java/org/elasticsearch/index/analysis/ClassicTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/FrenchStemTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltCharFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenizerFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/deletionpolicy/KeepLastNDeletionPolicy.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataType.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/IndexIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>src/main/java/org/elasticsearch/index/store/IndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/StoreModule.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogService.java</file><file>src/main/java/org/elasticsearch/indices/IndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>src/main/java/org/elasticsearch/indices/IndicesWarmer.java</file><file>src/main/java/org/elasticsearch/indices/SyncedFlushService.java</file><file>src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file><file>src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java</file><file>src/main/java/org/elasticsearch/indices/breaker/NoneCircuitBreakerService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>src/main/java/org/elasticsearch/monitor/MonitorModule.java</file><file>src/main/java/org/elasticsearch/monitor/MonitorService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/JmxFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/SigarFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/node/NodeBuilder.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java</file><file>src/main/java/org/elasticsearch/plugins/AbstractPlugin.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryModule.java</file><file>src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>src/main/java/org/elasticsearch/rest/RestController.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/verify/RestVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java</file><file>src/main/java/org/elasticsearch/river/RiverModule.java</file><file>src/main/java/org/elasticsearch/river/RiversManager.java</file><file>src/main/java/org/elasticsearch/river/RiversService.java</file><file>src/main/java/org/elasticsearch/river/cluster/RiverClusterService.java</file><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPoolModule.java</file><file>src/main/java/org/elasticsearch/transport/TransportService.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>src/test/java/org/elasticsearch/VersionTests.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/RejectionActionTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/repositories/RepositoryBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequestTests.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkProcessorClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkProcessorTests.java</file><file>src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java</file><file>src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java</file><file>src/test/java/org/elasticsearch/action/support/replication/ShardReplicationOperationTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/MultiTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java</file><file>src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java</file><file>src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java</file><file>src/test/java/org/elasticsearch/blocks/SimpleBlocksTests.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/ClusterStateBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/StaticIndexBackwardCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/node/NodeClientTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterInfoServiceTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>Absorb ImmutableSettings into Settings</comment></comments></commit></commits></item><item><title>[Docs] Fix minor spelling errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11320</link><project id="" key="" /><description>I have signed the CLA.
</description><key id="80144937">11320</key><summary>[Docs] Fix minor spelling errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexwlchan</reporter><labels /><created>2015-05-24T12:15:00Z</created><updated>2015-05-25T17:58:37Z</updated><resolved>2015-05-25T17:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexwlchan" created="2015-05-24T12:25:26Z" id="105009803">I thought I had signed it? Did I miss a step?
</comment><comment author="clintongormley" created="2015-05-25T17:58:32Z" id="105280250">No worries @alexwlchan - just a slight delay.  Merged thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[Docs] Fix minor spelling errors</comment></comments></commit></commits></item><item><title>[maven] fix paths for final artifact</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11319</link><project id="" key="" /><description>We need to define an absolute path (based on `${project.basedir}`) instead of using related paths.

@rmuir Wanna review it?
</description><key id="79897560">11319</key><summary>[maven] fix paths for final artifact</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-05-23T16:55:45Z</created><updated>2015-06-10T09:35:39Z</updated><resolved>2015-05-26T09:42:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-23T17:10:00Z" id="104926285">looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES Cluster on Openstack keeps going down on large indexing jobs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11318</link><project id="" key="" /><description>I am trying to index 17k (~20KB JSON) docs into ES, these are heavily nested docs.  It seems to be doing fine, then the cluster will always go down and the index job with fail.  Is there any special way I should configure the ES nodes so that it can handle the indexing?  Once the cluster goes 'red' I have to 'Soft Reboot' all the nodes and after a short while the cluster is back to being 'green' and the files that it indexed before it went down are available and queries are VERY fast (from cli, not Kibana, Kibana is VERY slow)

Is there anything I can do?  I am using elasticsearch-py to index, should I be using Logstash instead?  Do I need to put a cache or river in front of ES to trickle it in with data assurance?

Please help.

---

1 - m1.large load balancer node
3 - m1.large master nodes
20 - m1.large data nodes
- multicast discovery
- 10 shards
- 1 repilca set
- 1 index_name
- 3 doc_types
- 10G network between nodes
</description><key id="79868857">11318</key><summary>ES Cluster on Openstack keeps going down on large indexing jobs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sandywater</reporter><labels /><created>2015-05-23T15:00:18Z</created><updated>2015-05-24T01:54:04Z</updated><resolved>2015-05-24T01:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GlenRSmith" created="2015-05-23T18:11:12Z" id="104933818">Are you bulk indexing?
</comment><comment author="sandywater" created="2015-05-24T00:29:05Z" id="104960832">Yes, but only in ~15-25 doc chunks.  Should I up that amount?

Update: I blew away my ES cluster and just replaced it with a single node.  With 2VCPU, 16GB RAM and set the HEAP to 15g.  This happily indexed my JSON, with only a few connection time outs.  Now I am really confused.
</comment><comment author="GlenRSmith" created="2015-05-24T01:10:23Z" id="104961899">You gave 15G to heap out of 16G? Are you configuring your AWS data nodes similarly? You should have your heap set to about half your available RAM.
m1.large is pretty small for the data nodes, memory-wise.
https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html
Try to use at least m3.xlarge.
The size of the bulk requests is just something you'll have to experiment with. The sweet spot is going to depend on the complexity of your docs and their analysis (along with the various resource capacities/configs). Maybe try some smaller sample sets under "time" to see whether, for example, 100 bulk calls with 10 docs each, 50 with 20, 20 with 50, or 10 with 100 work out best.

In any case, I think soon enough you'll get the job basically performing, and further optimization discussion can be pursued via the forums (https://discuss.elastic.co/), IRC (#elasticsearch), or stackoverflow.

Good luck!
</comment><comment author="markwalkom" created="2015-05-24T01:54:03Z" id="104966785">As @GlenRSmith mentioned - Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting: keep track of the original query only in HighlighterContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11317</link><project id="" key="" /><description>We used to keep track of the rewritten query in the highlighter context to support custom rewriting done by our own postings highlighter fork. Now that we rely on lucene implementation, no rewrite happens, we can simply keep track of the original query and simplify code around it.
</description><key id="79791306">11317</key><summary>Highlighting: keep track of the original query only in HighlighterContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-23T09:20:38Z</created><updated>2015-05-26T11:24:27Z</updated><resolved>2015-05-26T11:24:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-23T22:20:12Z" id="104951116">LGTM, thanks for cleaning things up!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file></files><comments><comment>Highlighting: keep track of the original query only in HighlighterContext</comment></comments></commit></commits></item><item><title>Update search.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11316</link><project id="" key="" /><description>I think the request should use the POST method. The GET method works fine with the request, but including data as the argument to the `-d` option leads the user to think of the POST method. It's also easier to use the query with tools like Postman if the method is POST.

I wouldn't say this is a high priority issue by any means.
</description><key id="79742963">11316</key><summary>Update search.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gkb</reporter><labels /><created>2015-05-23T05:52:09Z</created><updated>2015-05-23T19:52:10Z</updated><resolved>2015-05-23T19:52:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gkb" created="2015-05-23T06:01:27Z" id="104850026">I signed the CLA.
</comment><comment author="javanna" created="2015-05-23T10:23:42Z" id="104879253">I am not sure this should get merged. We support both GET and POST, although some clients/proxies don't support GET with body (which is why we also support POST). Not sure I want to start a long debate around this... but I believe GET is ideal when it comes to retrieving data, POST is just there for people that can't use GET, so I wouldn't replace GET with POST in the docs.
</comment><comment author="gkb" created="2015-05-23T19:52:07Z" id="104939597">@javanna Thanks for the explanation. I agree with you that it's not worth spending too much time on this. I'm closing this out.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove build duplication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11315</link><project id="" key="" /><description>We've refactored the following logic into elasticsearch-parent, and its now being used by all plugins:
- third party dependency versions
- test configuration
- compiler configuration (with minimum java version in one place)
- build numbering / jar manifest metadata
- forbidden-apis checks / definitions
- license header checks / definitions
- "dev" profile for skipping checks
- ide configuration tasks and other minutae

There is more to do, like factoring out coverage analysis, PMD/findbugs and other static analysis, but this is a step I think we should make for overall consistency of builds.
</description><key id="79709805">11315</key><summary>remove build duplication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-05-23T03:33:53Z</created><updated>2015-05-23T11:14:02Z</updated><resolved>2015-05-23T11:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-23T06:32:54Z" id="104853566">LGTM thanks for moving this to the parent and hooking it up
</comment><comment author="dadoonet" created="2015-05-23T07:47:00Z" id="104861790">I think it closes my own tentative there #9735.
</comment><comment author="rmuir" created="2015-05-23T11:13:58Z" id="104883173">time to turn jenkins red :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ExternalNode.java</file><file>src/test/java/org/elasticsearch/test/disruption/LongGCDisruption.java</file></files><comments><comment>Merge pull request #11315 from rmuir/build_cleanup</comment></comments></commit></commits></item><item><title>Snapshot/Restore: restart of a master node during snapshot can lead to hanging snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11314</link><project id="" key="" /><description>When a snapshot operation on a particular shard finishes, the data node where this shard resides sends an update shard status request to the master node to indicate that the operation on the shard is done. When the master node receives the command it queues cluster state update task and acknowledges the receipt of the command to the data node. 

The update snapshot shard status tasks have relatively low priority, so during cluster instability they tend to get stuck at the end of the queue. If the master node gets restarted before processing these tasks the information about the shards can be lost and the new master assumes that they are still in process while the data node thinks that these shards are already done. 

This might be the root cause of #9924 and #10564
</description><key id="79673268">11314</key><summary>Snapshot/Restore: restart of a master node during snapshot can lead to hanging snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-05-23T00:50:05Z</created><updated>2015-06-07T20:26:31Z</updated><resolved>2015-06-03T19:15:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cxxr" created="2015-06-07T20:26:31Z" id="109797431">This has happened to us a few times in production. We've added "check to make sure a snapshot is not running" during any maintenance we perform to our ES cluster.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: sync up snapshot shard status on a master restart</comment></comments></commit></commits></item><item><title>Display low disk watermark to be consistent with documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11313</link><project id="" key="" /><description>This PR is an attempted fix for Issue #10588. I have added altered log entries, so that messages involving disk usage percentages display used disk percentages, rather than free disk percentages.

Closes #10588 
</description><key id="79625816">11313</key><summary>Display low disk watermark to be consistent with documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mkis-</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T21:54:31Z</created><updated>2015-06-01T22:18:33Z</updated><resolved>2015-06-01T22:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T12:29:37Z" id="105225179">@dakrone please could you review
</comment><comment author="dakrone" created="2015-05-28T17:57:39Z" id="106531043">@mkis- I left one comment, other than that this looks good to me, let me know when that is fixed and I will merge it in!
</comment><comment author="mkis-" created="2015-05-30T14:18:53Z" id="107044164">@dakrone Thank you for your feedback. I changed the phrasing of messages around it for consistency.
</comment><comment author="dakrone" created="2015-06-01T16:34:30Z" id="107628962">@mkis- left one more comment, looks like a missed `100.0 -`, can you fix and squash these to a single commit and I'll merge?
</comment><comment author="mkis-" created="2015-06-01T19:02:16Z" id="107672241">@dakrone Made the change and squashed the commits.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make some sec mgr / bootup classes package private and final.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11312</link><project id="" key="" /><description>These don't need to be accessible except by bootstrap.
</description><key id="79603789">11312</key><summary>Make some sec mgr / bootup classes package private and final.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T20:49:35Z</created><updated>2015-06-07T10:13:13Z</updated><resolved>2015-05-22T20:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-22T20:50:17Z" id="104763982">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>src/main/java/org/elasticsearch/bootstrap/JVMCheck.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file></files><comments><comment>Merge pull request #11312 from rmuir/pkg_priv</comment></comments></commit></commits></item><item><title>Upgrade to lucene-5.2.0-snapshot-1681214</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11311</link><project id="" key="" /><description>This just pulls in https://issues.apache.org/jira/browse/LUCENE-6497
</description><key id="79595326">11311</key><summary>Upgrade to lucene-5.2.0-snapshot-1681214</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2015-05-22T20:22:06Z</created><updated>2015-05-23T00:27:00Z</updated><resolved>2015-05-22T20:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-22T20:39:40Z" id="104762238">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11311 from rjernst/lucene/r1681214</comment></comments></commit></commits></item><item><title>Support timeouts in _msearch API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11310</link><project id="" key="" /><description>The `_msearch` API currently doesn't allow you to specify a timeout parameter in the header of each request.

This is problematic given that long running queries can easily swamp an ES node.

Recommend that we add timeout to the API which is specified per request in milliseconds for consistency with the timeout parameter for the `search` API:

```
{"timeout": 100}
{"query": ...}
{"timeout": 50}
{"query": ...}
...
```

Happy to contribute here but not sure where to start.

Thoughts?
</description><key id="79577254">11310</key><summary>Support timeouts in _msearch API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msukmanowsky</reporter><labels><label>:Search</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-05-22T19:24:15Z</created><updated>2017-04-26T09:37:58Z</updated><resolved>2017-04-26T09:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="msukmanowsky" created="2015-05-22T19:36:00Z" id="104749900">I suppose if https://www.elastic.co/guide/en/elasticsearch/guide/current/empty-search.html#_timeout is accurate

&gt; Use the time-out because it is important to your SLA, not because you want to abort the execution of long-running queries.

Then this is irrelevant considering that the queries themselves will keep executing and swamping the server...
</comment><comment author="clintongormley" created="2015-05-25T17:37:55Z" id="105276307">@markharwood is this comment still (or was it ever) accurate?  
</comment><comment author="markharwood" created="2015-05-26T08:43:23Z" id="105447039">The parameter referenced in https://www.elastic.co/guide/en/elasticsearch/guide/current/empty-search.html#_timeout overrides any setting in the query JSON body.

The timeout settings do curtail execution of queries on shards _but only when it gets the opportunity to do so._ 
Most of the heavy loops that are part of the query execution (e.g. collecting matching documents) have a timer check which should halt execution but there is always the potential for some other untimed loops (e.g.while loading FieldData) to overrun. We deliberately chose not to put a timeout into FieldData loading because that activity is a cost shared by many queries and so it doesn't make sense to shut it down early.
</comment><comment author="msukmanowsky" created="2015-05-26T13:36:30Z" id="105525811">In that case, sounds like adding support in msearch does make sense (and that the docs maybe need updating).
</comment><comment author="colings86" created="2017-04-26T08:49:55Z" id="297299980">@clintongormley @markharwood is this a duplicate of https://github.com/elastic/elasticsearch/issues/4227 ? Seems like this issue asks for a specific parameter thats available onthe search API to be made available on the msearch API and the linked issue asks for the same but for all parameters supported by the search API?</comment><comment author="markharwood" created="2017-04-26T09:35:08Z" id="297319426">&gt; is this a duplicate of #4227 ?

Looks like 4227 highlights the broader problem. If Clinton's 4227 suggested solution of making _search an implicit _msearch with a count of 1 then this timeout issue and 4227 would be dealt with. I'm not sure if there are any gotchas in taking that approach though..</comment><comment author="colings86" created="2017-04-26T09:37:58Z" id="297320629">Closing in favour of #4227</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent "bad" shard from causing perpetual initialization tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11309</link><project id="" key="" /><description>Have come across this multiple times in the field.  For whatever reason (still to be determined - different issue), the index files for a shard are gone (for both the primary and replica).  The shard's index directory exists (but is empty) for the shard for both primary and replica.  When this occurs:
- If the end user checks the cluster health api, it will show that there is a shard initializing, but it will be like this forever as if it is taking a long time to initialize.  But in fact, it is in initializing state because it keeps trying and never stops trying.
- If you check the cat/shards, you will see that the shard keeps trying to initializing on the 2 nodes, basically, initialize on node 1 -&gt; fails -&gt; initialize on node 2 -&gt; fails, initialize on node 1 -&gt; fails, etc.. and so on.  If you look at head, you will see that the shard is "bouncing" between the 2 nodes:

![image](https://cloud.githubusercontent.com/assets/7216393/7776570/a8af3e76-0070-11e5-94c4-79b2b95b8857.png)

![image](https://cloud.githubusercontent.com/assets/7216393/7776564/a136a1a2-0070-11e5-839f-266f5aa9dfd8.png)
- And corresponding pending tasks keep showing up for each attempt against the 2 nodes:

```
{
  "tasks" : [ {
    "insert_order" : 14347,
    "priority" : "HIGH",
    "source" : "shard-failed ([default][0], node[5xR9MWlmSDKHjZPIZs4L-w], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[default][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_1/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]]; ]]",
    "executing" : true,
    "time_in_queue_millis" : 13,
    "time_in_queue" : "13ms"
  } ]
}
```
- And the logs on both nodes will start filling up with a ton of these warning messages:

```
[2015-05-22 10:56:27,812][WARN ][cluster.action.shard     ] [node2] [default][0] sending failed shard for [default][0], node[ktzJrukQQDux_edYck2tYA], [P], s[INITIALIZING], indexUUID [zdMk9SthR_OzhCJi27QzcQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[default][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_2/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]]; ]]
[2015-05-22 10:56:27,831][WARN ][indices.cluster          ] [node2] [default][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [default][0] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_2/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
```

To reproduce:
- Have a cluster with 2 node and an index with 1 replica.   Go to the file system and simulate the loss of shard data by deleting all files under a shard's index folder (keep the directory but empty it) from both nodes.
- Restart the 2 nodes
- Then you can see the continuous attempts to start the shard on both nodes and it will keep trying and keep failing and doesn't give up.

I suspect that the same behavior will occur if both primary and replica shards have index corruptions.

It will be nice to:
- Prevent it from trying perpetually - once it determines that there are no copies of the shard useable/available, stop trying repeatedly on the nodes
- Leave the problem shard unassigned and write an intuitive message to the master log with a summary of what nodes it tried on and that it is leaving the shard unassigned because it cannot find a usable copy of the shard data, etc..
</description><key id="79554522">11309</key><summary>Prevent "bad" shard from causing perpetual initialization tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-05-22T18:13:36Z</created><updated>2015-05-23T08:50:18Z</updated><resolved>2015-05-22T18:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-22T18:53:36Z" id="104742677">this is fixed by #11269 as of yesterday :) 

it will now try once this:

```
[2015-05-22 20:48:46,071][DEBUG][index                    ] [Erik Josten] [foo] [0] closed (reason: [failed recovery])
[2015-05-22 20:48:46,072][WARN ][indices.cluster          ] [Erik Josten] [[foo][0]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [foo][0] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [foo][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/simon/projects/elasticsearch/target/releases/elasticsearch-1.6.0-SNAPSHOT/data/elasticsearch/nodes/0/indices/foo/0/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:881)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:769)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:458)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:89)
    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:166)
    at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:152)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
```

then log this. I think we should upgrade this to a warn logging @kimchy WDYT?

```
[2015-05-22 20:48:46,135][DEBUG][gateway.local            ] [Erik Josten] [foo][0] found 0 allocations of [foo][0], node[null], [P], s[UNASSIGNED], highest version: [-1]

```
</comment><comment author="kimchy" created="2015-05-22T19:14:27Z" id="104746408">@s1monw the tricky bit on the logging is that this might be valid, as in, when you start nodes one by one on a fresh cluster, you will get to this state, which might be valid. I wonder if the indication of shard failures is enough to indicate there is a problem with this shard if its left unassigned? There is also an option of making it info.
</comment><comment author="s1monw" created="2015-05-22T19:22:21Z" id="104747775">ok fair enough
</comment><comment author="ppf2" created="2015-05-22T19:46:50Z" id="104752215">Thanks guys!
</comment><comment author="s1monw" created="2015-05-23T08:50:18Z" id="104869253">just for the record this is fixed in `1.6` &amp; `2.0`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not specialize TermQuery vs. TermsQuery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11308</link><project id="" key="" /><description>We had some code that created a TermQuery instead of a TermsQuery when there
was a single term, which is not useful anymore now that TermsQuery rewrites to
a disjunction when there are few terms:
  https://issues.apache.org/jira/browse/LUCENE-6360
</description><key id="79541228">11308</key><summary>Do not specialize TermQuery vs. TermsQuery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T17:34:01Z</created><updated>2015-06-07T17:09:52Z</updated><resolved>2015-05-29T07:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-22T18:09:37Z" id="104733081">to be honest i dont mind the explicit optimization here. I think its usually easier to reason about a base case in a parser or something like this, than rely on optimizations in multitermquery or other complex things going on. And I always think its good to be clear to just use the simplest query going to lucene.

I remember still adding code to rewrite a PhraseQuery with a single term to a TermQuery, and it really relatively recent, because historically lucene has just not done a very good job here and left it up to e.g. queryparsers to deal with.

Even if we ignore rewrite() optimizations, a TermQuery is still fundamentally simplest, as far as things like equals/hashcode and whatever. And the code here is not so hairy (especially compared to other optimizations in the codebase).

But I don't feel so strongly about it... especially if TermsQuery is only used special when the user explicitly asks for it, in that case you can see it as an optimization.
</comment><comment author="rjernst" created="2015-05-23T06:29:32Z" id="104853471">Personally I like the simplification. Just checking: does passing an empty terms list result in the same behavior as before (match no docs)? 
</comment><comment author="jpountz" created="2015-05-23T07:20:02Z" id="104856285">Thanks for having a look! If you pass an empty terms list, you will get an empty boolean query, which is what MatchNoDocsQuery rewrites to.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Merge pull request #11308 from jpountz/fix/term_vs_terms_query</comment></comments></commit></commits></item><item><title>Upgrade Jackson to 2.5.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11307</link><project id="" key="" /><description /><key id="79523193">11307</key><summary>Upgrade Jackson to 2.5.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T16:42:50Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-29T10:22:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-25T17:57:27Z" id="105279734">LGTM
</comment><comment author="spinscale" created="2015-05-29T09:46:22Z" id="106760219">The changes are really thin on jackson, see https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.5.3 and https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.5.2

I will get this into 1.x and push it to elasticsearch-parent master, once tests ran
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default detect_noop to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11306</link><project id="" key="" /><description>detect_noop is pretty cheap and noop updates compartively expensive so this
feels like a sensible default.

Closes #11282
</description><key id="79522647">11306</key><summary>Default detect_noop to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>breaking</label><label>review</label><label>v2.1.0</label></labels><created>2015-05-22T16:41:46Z</created><updated>2015-08-27T15:22:13Z</updated><resolved>2015-08-27T15:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-26T13:35:37Z" id="105525607">Updated with @clintongormley's language. Thanks. Its much clearer.
</comment><comment author="nik9000" created="2015-07-29T16:18:23Z" id="126005830">@jpountz, should I just take this one?
</comment><comment author="jpountz" created="2015-07-29T16:18:45Z" id="126005905">Yes, please :)
</comment><comment author="nik9000" created="2015-08-03T19:53:56Z" id="127387428">This badly needed a rebase so I did it. Rerunning tests.
</comment><comment author="nik9000" created="2015-08-11T14:40:56Z" id="129910997">I'd _love_ to get this into 2.0.0 somehow - please review?
</comment><comment author="jpountz" created="2015-08-11T16:05:40Z" id="129949477">LGTM
</comment><comment author="nik9000" created="2015-08-12T14:42:03Z" id="130325671">Ok - this looks to be approved for merge but needs conflicts resolved so I'll squash and rebase. And if the conflicts resolve easily I'll merge.
</comment><comment author="nik9000" created="2015-08-12T15:08:13Z" id="130336423">Ok - rebasing shows a failing Rest test. I'll work on that and ask for another review when I can.
</comment><comment author="nik9000" created="2015-08-13T17:37:38Z" id="130773504">OK - fixed the rest test and rebased so it can merge.
</comment><comment author="jpountz" created="2015-08-14T08:06:50Z" id="131012927">Is it possible to make `detect_noop` ttl-aware? If not, or if it would make things complicated, I don't think it would be too bad but then we should document it?
</comment><comment author="nik9000" created="2015-08-14T16:27:33Z" id="131169080">&gt; Is it possible to make detect_noop ttl-aware? If not, or if it would make things complicated, I don't think it would be too bad but then we should document it?

Yeah - I'll have a look. Also, since this isn't going to make 2.0 I'll have to move/change some documentation.
</comment><comment author="nik9000" created="2015-08-14T17:20:53Z" id="131187047">&gt; Yeah - I'll have a look. Also, since this isn't going to make 2.0 I'll have to move/change some documentation.

@jpountz I went with the documentation option. Its pretty easy to just always update the document if it has a ttl set but I'm not super clear on the interplay between default ttl and updates. Nor could I figure out how to clear the ttl with an update so I couldn't test to make sure I wasn't breaking that. So I went the easy way and just documented that the expiration doesn't change if the document doesn't by default.
</comment><comment author="nik9000" created="2015-08-24T20:01:58Z" id="134359678">Squashed and rebased onto master. Changed the migrate_2_1.asciidoc wording slightly.
</comment><comment author="nik9000" created="2015-08-24T20:06:34Z" id="134360956">@jpountz ready for another round of review I think.
</comment><comment author="jpountz" created="2015-08-25T09:30:14Z" id="134538840">Sorry for the delay. I left two minor comments but in general it looks good to ne
</comment><comment author="nik9000" created="2015-08-26T15:19:45Z" id="135059339">&gt; Sorry for the delay. I left two minor comments but in general it looks good to ne

Ok - moved the tests and fixed the docs as requested.
</comment><comment author="jpountz" created="2015-08-26T19:52:08Z" id="135152135">LGTM
</comment><comment author="nik9000" created="2015-08-27T14:24:24Z" id="135451292">OK! I'll resolve squash and rebase. If the rebase is clean enough I'll push to master.
</comment><comment author="nik9000" created="2015-08-27T15:22:02Z" id="135467263">All tests pass! Hurray! Merging.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/test/java/org/elasticsearch/document/BulkIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateNoopIT.java</file></files><comments><comment>Merge pull request #11306 from nik9000/default_detect_noop</comment></comments></commit></commits></item><item><title>Update RestRequest.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11305</link><project id="" key="" /><description>fix exception message
</description><key id="79508786">11305</key><summary>Update RestRequest.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">Iarwa1n</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T16:01:29Z</created><updated>2015-06-23T16:34:57Z</updated><resolved>2015-06-19T10:08:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-23T00:09:03Z" id="104805411">Hi @Iarwa1n, thanks for the change! Could you please sign the [Contributor's Agreement](https://www.elastic.co/contributor-agreement) so I can merge this in?
</comment><comment author="Iarwa1n" created="2015-06-05T15:27:06Z" id="109328240">done
</comment><comment author="colings86" created="2015-06-19T10:08:34Z" id="113459518">@Iarwa1n Thanks for the PR, I've merged this into the master branch
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/RestRequest.java</file></files><comments><comment>Fix exception message in RestRequest</comment></comments></commit></commits></item><item><title>Upgrade to Netty 3.10.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11304</link><project id="" key="" /><description /><key id="79500718">11304</key><summary>Upgrade to Netty 3.10.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>upgrade</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T15:38:43Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-22T16:42:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-22T15:49:45Z" id="104696087">LGTM, netty changelog looks good

for the record, there doesnt seem to have been a real 3.10.2 release, issue lists for [3.10.1](https://github.com/netty/netty/issues?q=milestone%3A3.10.1.Final), [3.10.2](https://github.com/netty/netty/issues?q=milestone%3A3.10.2.Final) and [3.10.3](https://github.com/netty/netty/issues?q=milestone%3A3.10.3.Final)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11304 from kimchy/upgrade_netty_3_10_3</comment></comments></commit></commits></item><item><title>Custom analyzer names and aliases must not start with _</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11303</link><project id="" key="" /><description>closes #9596
</description><key id="79487390">11303</key><summary>Custom analyzer names and aliases must not start with _</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Analysis</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T15:00:43Z</created><updated>2015-05-26T09:55:46Z</updated><resolved>2015-05-26T09:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-23T05:47:03Z" id="104848429">LGTM, just a couple minor suggestions.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Merge pull request #11303 from brwe/custom_analyzer_name</comment></comments></commit></commits></item><item><title>Add logging for failed TTL purges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11302</link><project id="" key="" /><description>In order to get some information if the TTL purger thread could
successfully delete all documents per bulk exection, this commit
adds some logging. TRACE level logging will potentially contain
a lot of information about all the bulk failures.

Closes #11019
</description><key id="79449543">11302</key><summary>Add logging for failed TTL purges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T13:10:25Z</created><updated>2015-06-07T10:48:33Z</updated><resolved>2015-05-22T15:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-22T13:10:34Z" id="104655809">@jaymode can you take a look?
</comment><comment author="jaymode" created="2015-05-22T13:13:51Z" id="104656316">LGTM. left a minor comment about capitialization. Thanks for taking care of this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only load a plugin once from the classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11301</link><project id="" key="" /><description>Today, when loading plugins from the classpath we take the enumeration
given to us by the classloader and attempt to load every URL. This can
cause issues as certain classloaders, such as groovy's, will return the same
URL multiple times in the enumeration. When this happens, startup can fail
with guice errors as bindings have already been registered.

To workaround this, we create a set from the URLs returned by the classloader
to provide uniqueness.
</description><key id="79429532">11301</key><summary>Only load a plugin once from the classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T12:03:17Z</created><updated>2015-06-08T00:16:09Z</updated><resolved>2015-05-22T19:29:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-22T12:07:07Z" id="104644933">LGTM
</comment><comment author="jaymode" created="2015-05-22T14:01:13Z" id="104667563">@s1monw what do you think about backporting this to the 1.5 branch as well?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Please default to listening on 127.0.0.1 in /etc/elasticsearch/elasticsearch.yml in the Debian package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11300</link><project id="" key="" /><description>CVE-2015-1427 would've been considerably less painful if my ElasticSearch weren't listening on 0.0.0.0 by default.
</description><key id="79427088">11300</key><summary>Please default to listening on 127.0.0.1 in /etc/elasticsearch/elasticsearch.yml in the Debian package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mgedmin</reporter><labels /><created>2015-05-22T11:55:57Z</created><updated>2015-06-04T16:32:14Z</updated><resolved>2015-06-04T16:32:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/network/MulticastChannel.java</file><file>src/main/java/org/elasticsearch/common/network/NetworkService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Default to binding to loopback address</comment></comments></commit></commits></item><item><title>Please default RESTART_ON_UPGRADE to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11299</link><project id="" key="" /><description>Hapless sysadmins such as myself are used to installing Debian security updates by running `sudo apt-get upgrade` and assume that anything that needs to be restarted will be restarted by the postinst script.

This is not a safe assumption to make when remote code execution vulnerabilities such as CVE-2015-1427 show up.

(Of course if would be even more helpful if [RESTART_ON_UPGRADE=true actually worked](https://github.com/elastic/elasticsearch/issues/11298).)
</description><key id="79426326">11299</key><summary>Please default RESTART_ON_UPGRADE to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mgedmin</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-05-22T11:53:24Z</created><updated>2016-01-18T19:34:22Z</updated><resolved>2016-01-18T19:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-22T14:25:03Z" id="104673363">I don't think this is a good idea as it'd be easy for some silly puppet
script to take the whole cluster down with an ensume =&gt; latest. Do
PostgreSQL and MySQL restart on upgrade?

On Fri, May 22, 2015 at 7:53 AM, Marius Gedminas notifications@github.com
wrote:

&gt; Hapless sysadmins such as myself are used to installing Debian security
&gt; updates by running sudo apt-get upgrade and assume that anything that
&gt; needs to be restarted will be restarted by the postinst script.
&gt; 
&gt; This is not a safe assumption to make when remote code execution
&gt; vulnerabilities such as CVE-2015-1427 show up.
&gt; 
&gt; (Of course if would be even more helpful if RESTART_ON_UPGRADE=true
&gt; actually worked https://github.com/elastic/elasticsearch/issues/11298.)
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11299.
</comment><comment author="mgedmin" created="2015-05-22T18:48:20Z" id="104741825">postgresql: yes (did a test on Ubuntu 14.04: pgrep -u postgres; sudo apt-get install postgresql-9.3=&lt;some other version&gt;; pgrep -u postgres -- all the PIDs had changed).

mysql: no idea, I don't use mysql.
</comment><comment author="clintongormley" created="2015-05-25T17:21:17Z" id="105272446">I agree with @nik9000 - it'd be too easy to take your cluster down.  I think `RESTART_ON_UPGRADE` should be opt-in for the wily sysadmin only.
</comment><comment author="clintongormley" created="2016-01-18T19:34:22Z" id="172630567">We've decided against doing this. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RESTART_ON_UPGRADE doesn't work in the Debian package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11298</link><project id="" key="" /><description>The postinst script in [elasticsearch-1.3.9.deb](http://packages.elasticsearch.org/elasticsearch/1.3/debian/pool/main/e/elasticsearch/elasticsearch-1.3.9.deb) does this:

``` sh
    # if $2 is set, this is an upgrade
    if ( [ -n $2 ] &amp;&amp; [ "$RESTART_ON_UPGRADE" = "true" ] ) ; then
        startElasticsearch
```

where `startElasticsearch` is defined in the same script as

``` sh
startElasticsearch() {
    if [ -x "/etc/init.d/elasticsearch" ]; then
        if [ -x "`which invoke-rc.d 2&gt;/dev/null`" ]; then
            invoke-rc.d elasticsearch start || true
        else
            /etc/init.d/elasticsearch start || true
        fi
    fi
}
```

Meanwhile the init script has the following bit:

``` sh
case "$1" in
  start)
    checkJava

    if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then
        log_failure_msg "MAX_LOCKED_MEMORY is set - ES_HEAP_SIZE must also be set"
        exit 1
    fi

    log_daemon_msg "Starting $DESC"

    pid=`pidofproc -p $PID_FILE elasticsearch`
    if [ -n "$pid" ] ; then
        log_begin_msg "Already running."
        log_end_msg 0
        exit 0
    fi
```

As you can see, even if you set RESTART_ON_UPGRADE=true in /etc/default/elasticsearch, this won't restart ElasticSearch if it's already running.
</description><key id="79424922">11298</key><summary>RESTART_ON_UPGRADE doesn't work in the Debian package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mgedmin</reporter><labels /><created>2015-05-22T11:49:19Z</created><updated>2015-05-25T17:20:11Z</updated><resolved>2015-05-25T17:20:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T17:20:09Z" id="105272370">Hi @mgedmin 

Thanks for reporting.  It looks like this is already fixed in 1.6 by https://github.com/elastic/elasticsearch/pull/10595: https://github.com/elastic/elasticsearch/blob/1.x/src/packaging/common/scripts/postinst#L76
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How could i get all record</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11297</link><project id="" key="" /><description>As title say, how could i get all record, except set a huge size .
</description><key id="79398928">11297</key><summary>How could i get all record</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmqwudi</reporter><labels /><created>2015-05-22T10:35:14Z</created><updated>2015-05-22T10:43:42Z</updated><resolved>2015-05-22T10:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-22T10:43:41Z" id="104615892">Please use discuss.elastic.co for questions, we can help better there.

Look at scan and scroll API

Not an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to lucene-5.2.0-snapshot-1681024</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11296</link><project id="" key="" /><description>This has been built from the 5.2 branch which has just been created.
</description><key id="79378967">11296</key><summary>Upgrade to lucene-5.2.0-snapshot-1681024</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T09:39:28Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-22T12:17:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-22T09:39:59Z" id="104596599">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11296 from jpountz/upgrade/lucene-5.2.0-snapshot-1681024</comment></comments></commit></commits></item><item><title>ResourceWatcher: Use native JDK7 APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11295</link><project id="" key="" /><description>The `ResourceWatcherService` supports watching files based on three different intervals. Since we require java 7 for Elasticsearch, we can use the `WatchService` for files, which - depending on the file system (osx and windows do not support this IIRC) - then is notified of immediate updates of files and does not require polling, but still falls back to polling in case this notification functionality is not available.

The JDK contains a `SensitivityWatchEventModifier` class, which could be used as default and we should be able to make this configurable as well, so we can still configure the different intervals.
</description><key id="79370843">11295</key><summary>ResourceWatcher: Use native JDK7 APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2015-05-22T09:16:54Z</created><updated>2016-01-18T19:33:44Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-22T17:26:30Z" id="104720675">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How do search on _all fields except X field?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11294</link><project id="" key="" /><description># Situation

Cluster contains several indices, types and documents with several fields. We want to search fulltext on-demand as opposed to normally since we have richer metadata. We don't want to list every field in `fields` in the query. I realise the fields property takes `city.*` format. Since listing every field that is not fulltext can be handled by Elastic as opposed to the application.
# Request

Can we provide NOT field syntax in fields property something like `!fulltext`?

``` javascript
POST _search
{
   "query": {
    "query_string": {
       "query": "*",
       "fields": [
          "!fulltext"
       ]
    }
   }
}
```
# My DIY experience

I tried to debug and find out if the syntax supported any other formats, as the [help document](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html) didn't cover **NOT fields**. I had this error

```
Description Resource    Path    Location    Type
Project build error: Non-resolvable parent POM: Failure to transfer org.sonatype.oss:oss-parent:pom:7 from http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision} was cached in the local repository, resolution will not be reattempted until the update interval of lucene-snapshots has elapsed or updates are forced. Original error: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM pom.xml /elasticsearch  line 27 Maven pom Loading Problem
```
</description><key id="79344462">11294</key><summary>How do search on _all fields except X field?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abibell</reporter><labels /><created>2015-05-22T08:04:26Z</created><updated>2015-09-27T22:26:34Z</updated><resolved>2015-09-27T22:26:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T17:06:44Z" id="105271221">I've opened #11334 to discuss this
</comment><comment author="jpountz" created="2015-09-27T22:26:34Z" id="143598757">Closing in favor of  #11334
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow disabling of sigar via settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11293</link><project id="" key="" /><description>Sigar can only be disabled by removing the binaries. This is tricky for our
tests and might cause a lot of trouble if a user wants or needs to do it.
This commit allows to disable sigar with a simple boolean flag in the settings.

This safes me 20% for tests with `tests.slow=true` 
</description><key id="79331986">11293</key><summary>Allow disabling of sigar via settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-22T07:30:31Z</created><updated>2017-06-26T23:17:55Z</updated><resolved>2015-05-22T08:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-22T07:34:54Z" id="104551177">LGTM, minor nitpick: maybe renaming it to `bootstrap.sigar.enabled` like `http.enabled`, `watcher.enabled` or `discovery.zen.ping.multicast.enabled` explains better what the setting is doing

This will also help OpenVZ users to run this, without removing the sigar directory, very nice! (#9582)
</comment><comment author="s1monw" created="2015-05-22T07:59:17Z" id="104556959">I'd like to keep it as all the other settings on that level like `bootstrap.ctrlhandler` and `bootstrap.mlockall` can we have a separate issue to rename all of them?
</comment><comment author="spinscale" created="2015-05-22T08:01:07Z" id="104557250">+1 on that, I think there are more setting which should be `foo.enabled` as they are boolean. Go ahead with this one!
</comment><comment author="kjavitz" created="2017-06-26T23:17:55Z" id="311208207">@s1monw sorry I'm new to elasticsearch and am having the openvz crash issue with elasticsearch 1.7.6 so it seems disabling sigar can fix this https://github.com/elastic/elasticsearch/issues/9582 but I don't understand what file do I edit to change this setting? This will affect regular elasticsearch and not just tests?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove generics from FieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11292</link><project id="" key="" /><description>FieldMapper is currently generic, where the templated type
is only used as the return of a single function, value(Object).
This change simply removes this generic type. It is not needed. The
implementations of value() now have a covariant return (so
those methods have not changed).
</description><key id="79197444">11292</key><summary>Remove generics from FieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T23:10:23Z</created><updated>2015-06-07T10:51:15Z</updated><resolved>2015-05-21T23:25:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T23:17:28Z" id="104447035">You're my hero! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/IndexIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/MergeResult.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/FieldContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SimpleFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #11292 from rjernst/remove/mapper-generic</comment></comments></commit></commits></item><item><title>JAVA_HOME added to $DEFAULTS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11291</link><project id="" key="" /><description>Both rpm and dpkg versions of init.d/elasticsearch
src/packaging/deb/init.d/elasticsearch
src/packaging/rpm/init.d/elasticsearch
reference that JAVA_HOME can be set in the DEFAULT file (/etc/default/elasticsearch on debian systems) -- which keeps process specific environment variables for elasticsearch

JAVA_HOME is not currently a #'ed option.

Not clear how the file deployed to /etc/default/elasticsearch is being generated, probably from src/packaging/common/env/elasticsearch?
</description><key id="79193112">11291</key><summary>JAVA_HOME added to $DEFAULTS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jzinner</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2015-05-21T22:51:12Z</created><updated>2016-04-06T21:06:41Z</updated><resolved>2016-04-06T21:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T16:41:40Z" id="105264575">Related to https://github.com/elastic/elasticsearch/pull/11288

Suggestion is to add `# JAVA_HOME=...` to https://github.com/elastic/elasticsearch/blob/master/src/packaging/common/env/elasticsearch
and to add docs to https://github.com/elastic/elasticsearch/blob/master/docs/reference/setup/as-a-service.asciidoc
</comment><comment author="clintongormley" created="2015-05-25T16:42:13Z" id="105264641">@jzinner you want to send this as a PR?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minor refactor of MultiValueMode removing apply and reduce</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11290</link><project id="" key="" /><description>Simplification of MultiValueMode by removing the apply and reduce
methods for each mode.  This creates a more consistent environment for
sorting methods since all sorting must now go through select methods.
This allows for better error handling and better encapsulation for
sorting fields with multiple values.

Note that apply and reduce had inconsistencies in the code base
prior to this change since different calls were assuming that the
accumulator for apply was the first input versus the second input.

Also added is an UnsortedNumericDoubleValues interface to allow
customized values to be input into the different sort modes.  This 
prevents the need for apply/reduce outside of MultiValueMode.
</description><key id="79175673">11290</key><summary>Minor refactor of MultiValueMode removing apply and reduce</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T21:50:48Z</created><updated>2015-06-07T17:10:05Z</updated><resolved>2015-05-22T17:26:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T22:51:36Z" id="104443750">This looks great.

One minor concern I have is whether the introduction of `UnsortedNumericDoubleValues` is worth it or if we should just reuse the `SortedNumericDoubleValues` api instead and sort on the fly. But I don't feel too strongly about it so if you like the `UnsortedNumericDoubleValues` approach better, feel free to ignore.
</comment><comment author="jdconrad" created="2015-05-21T23:28:38Z" id="104448412">I thought about sorting on the fly because it certainly seems cleaner to me as well; however, sorting on the fly could potentially eat up a lot of memory if there's a large number of values in a field, so it didn't seem worth it to me.  I also avoided allowing sorting on inner doc select methods with median because you'd have to sort on _all_ of the inner document values which could take a large amount of memory as well.
</comment><comment author="jpountz" created="2015-05-22T08:06:24Z" id="104559447">OK. Thanks for the explanation. LGTM
</comment><comment author="jdconrad" created="2015-05-22T17:27:05Z" id="104720799">@jpountz Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/search/MultiValueMode.java</file><file>src/test/java/org/elasticsearch/search/MultiValueModeTests.java</file></files><comments><comment>Search: Refactor of MultiValueMode removing apply and reduce</comment></comments></commit></commits></item><item><title>Aggregations: Add an `other` bucket to the `filter(s)` aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11289</link><project id="" key="" /><description>Spin-off from #5324. While we still need to figure the API, it should be easy to compute an `other` bucket for the `filters` aggregation. We probably wouldn't even need an option and could just compute it all the time, it should be fairly cheap.

Another virtue I can see in that feature is that it would encourage to use query filters instead of aggregation filters if the `other` part is not of interest, which is more efficient.
</description><key id="79167505">11289</key><summary>Aggregations: Add an `other` bucket to the `filter(s)` aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-05-21T21:26:04Z</created><updated>2015-07-01T09:44:48Z</updated><resolved>2015-07-01T09:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersTests.java</file></files><comments><comment>Aggregations: Adds other bucket to filters aggregation</comment></comments></commit></commits></item><item><title>Update setup.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11288</link><project id="" key="" /><description>Configuring JAVA_HOME for the elasticsearch process on *NIX systems.
It's common in  a DevOps environment to configure the JAVA_HOME environment variable for a specific  process (like elasticsearch) instead of system-wide.
Elasticsearch already has process specific environmental variables located in /etc/default/elasticsearch
I've tested the JAVA_HOME from /etc/default/elasticsearch and it works as expected.
</description><key id="79165999">11288</key><summary>Update setup.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jzinner</reporter><labels /><created>2015-05-21T21:20:24Z</created><updated>2015-05-25T16:42:25Z</updated><resolved>2015-05-25T16:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T16:32:32Z" id="105263233">Hi @jzinner 

Thanks for the PR, but setting an environment variable feels like basic sysadmin knowledge which we shouldn't need to explain.  By explaining too much, we run the risk of obscuring the more important information.  If anything, just a one line note adding it here (https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html) might be appropriate.

thanks
</comment><comment author="clintongormley" created="2015-05-25T16:42:23Z" id="105264673">Closing in favour of #11291
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify Translog Settings and Semantics in Docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11287</link><project id="" key="" /><description>with #11143 and #11011 we should really clarify the docs about semantics and defaults for the translog. 
</description><key id="79132812">11287</key><summary>Clarify Translog Settings and Semantics in Docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>blocker</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T19:38:14Z</created><updated>2015-07-07T13:37:39Z</updated><resolved>2015-07-07T13:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T12:11:01Z" id="114081337">in master we now do a lot of things differently by default which  requires some changes to the documentation:
- translog syncs are by default now synchronous
- Adjust how often we sync by default. In 1.x we synced every 5 sec. which essentially is an async commit. Now we sync on every high level operation (Single | Bulk) as well as every 5 seconds.
- we wait for fsync on all replicas before we return the doc to the user not just on the primary which gives additional safety.
- `index.translog.durability` is set to `request` by default but can also be changed dynamically if async fsyncs are not an issue. 
- if `index.translog.durability: async` guarantees to drop all unsynced data. Since we have checkpoints we only read up to the latest checkpoint, this means we drop everything that has been written but not fsynced. So in the case of a node failure we might loose all docs indexed since the last scheduled sync (5s by default)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated the translog docs to reflect the new behaviour/settings in master</comment></comments></commit></commits></item><item><title>upgrading a read-only index causes ClusterBlockException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11286</link><project id="" key="" /><description>I had been running Elasticsearch 1.3.2. A few of my indexes were set to read-only -- that is, `blocks.read_only` was set to true.

I upgraded to 1.5.2. Various end-points, like `/_aliases` and `/_cat/indices` all returned a ClusterBlockException like `ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]`.

I was able to fix the problem by unlocking the indices individually. However, there was -- to my admittedly-limited knowledge -- **no** way to list all my indices. It was only because I kept a separate list of indices that I was able to determine which indices to unlock. Conceivably, a user could get into a problematic state where they can't determine which index to unlock.

My guess is that the upgrade process attempted to upgrade the read-only indexes, which failed because they were set to read-only. Two possible solutions: one is that upgrades disregard the read-only flag; the other is that the `_aliases` etc. endpoints don't fail when some indexes are upgraded to 1.5.2 and others are still an older version.

I should note that this was a one-box, one-cluster install of Elasticsearch. Both 1.3.2 and 1.5.2 were installed via the .deb. My elasticsearch.yml is quite boring (just a few special HTTP settings). 

This was my production box, so I don't have it available for testing anymore. Let me know if you have questions about replicability, etc.
</description><key id="79101654">11286</key><summary>upgrading a read-only index causes ClusterBlockException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">jeremybmerrill</reporter><labels><label>:Cluster</label></labels><created>2015-05-21T17:59:49Z</created><updated>2015-05-26T07:10:30Z</updated><resolved>2015-05-26T07:10:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T16:11:27Z" id="105258457">@tlrx I think these issues have already been solved by https://github.com/elastic/elasticsearch/pull/9203, no?
</comment><comment author="tlrx" created="2015-05-26T07:10:27Z" id="105420990">@clintongormley @jeremybmerrill yes, this issue has been solved by #9203.

In 1.4 some cluster blocks were added to various endpoints (see #7740 and #7763) in order to have more control of what operation is allowed or rejected. Theses cluster blocks did not exist in 1.3.2 and that's why you saw a different behavior, the `ClusterBlockException`, in 1.5.2. Unfortunately some cluster blocks were not fine grain enough and block some endpoints like `/_aliases`, `/_settings`... #9203 add more precise cluster blocks level and fixed that.

I'm closing this issue since it will be resolved by the upcoming 1.6 release.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Infra for deprecation logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11285</link><project id="" key="" /><description>This is an enhancement to #11033

It adds a test, some documentation and allows to retrieve the logger via the `ESLoggerFactory`.
</description><key id="79066721">11285</key><summary>Infra for deprecation logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Logging</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T16:17:22Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-05-26T15:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-26T06:13:55Z" id="105410902">@kimchy @rjernst can you take a look?
</comment><comment author="rjernst" created="2015-05-26T06:29:33Z" id="105415041">@spinscale I left comments directly on your commit. My only concern is checking in with an ignore on that test?
</comment><comment author="spinscale" created="2015-05-26T06:41:15Z" id="105417611">@rjernst thanks for checking, agreed on the check in of a ignored test, removed it for now, I think the other one is sufficient
</comment><comment author="rjernst" created="2015-05-26T07:05:08Z" id="105420331">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix FSRepository location configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11284</link><project id="" key="" /><description>Locations of file system repositories has to be now registered using `path.repo` setting. This is back port of a similar 2.0 feature into 1.6 branch.

Closes #11068
</description><key id="79061404">11284</key><summary>Fix FSRepository location configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v1.6.0</label></labels><created>2015-05-21T16:07:16Z</created><updated>2015-08-05T09:13:59Z</updated><resolved>2015-05-29T05:43:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-21T16:08:37Z" id="104334914">I am unsure about proper location and handling of test file system repositories in InternalTestCluster. So, if somebody has a better idea where to put it, please comment.
</comment><comment author="dakrone" created="2015-05-28T15:05:06Z" id="106385248">LGTM, I think the test file system stuff works fine in InternalTestCluster
</comment><comment author="jettro" created="2015-06-16T06:22:30Z" id="112307744">Can someone explain why this change was required? I am curious why I have to configure the possible path. It breaks my current implementations and I would like to understand why.
</comment><comment author="imotov" created="2015-06-16T13:28:29Z" id="112427852">@jettro it was needed to make default elasticsearch installation more secure. You can find more information in [the 1.6.0 release blog post](https://www.elastic.co/blog/elasticsearch-1-6-0-released#fs-config). Please note that the paths in `repo.path` have to match the prefix of your repository location. You don't have to specify there the whole path. It can be just shared filesystem mount point if you create multiple repositories. Alternatively, if your elasticsearch installation is secure and only authorized users can register repositories, you can essentially turn off the repository sandboxing by simply setting `repo.path` to `"/"` in the elasticsearch configuration file. This will get you back to the pre-1.6.0 behavior where repositories can be created anywhere on the local file system. 
</comment><comment author="rikkit" created="2015-08-04T15:59:12Z" id="127658622">@imotov 

&gt; Alternatively, if your elasticsearch installation is secure and only authorized users can register repositories, you can essentially turn off the repository sandboxing by simply setting repo.path to "/" in the elasticsearch configuration file. This will get you back to the pre-1.6.0 behavior where repositories can be created anywhere on the local file system. 

This is not true. :confused:  The previous behaviour was any old path could be used... "/" restricts to the local system. I want to be able to restore snapshots from arbitrary network shares for my development cluster. There doesn't seem to be a way of configuring that behaviour.

edit: opened an issue at #12665 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to configure logging...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11283</link><project id="" key="" /><description>Hello, guys
They could help me this error?

```
/usr/share/elasticsearch# ./bin/elasticsearch
Failed to configure logging...
org.elasticsearch.ElasticsearchException: Failed to load logging configuration
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:139)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:89)
    at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:100)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:184)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
    at java.nio.file.Files.readAttributes(Files.java:1686)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
    at java.nio.file.Files.walkFileTree(Files.java:2602)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:123)
    ... 4 more
log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

```
cat /etc/centos-release 
CentOS release 6.5 (Final)
```

x86_64

Please;
</description><key id="79037543">11283</key><summary>Failed to configure logging...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">igoreliasm</reporter><labels /><created>2015-05-21T15:09:53Z</created><updated>2015-09-07T20:19:26Z</updated><resolved>2015-05-25T15:41:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T15:41:44Z" id="105253720">Hi @igoreliasm 

Questions like these are best asked in the forum https://discuss.elastic.co/ but I'm guessing that you installed Elasticsearch as an RPM, in which case your config files are elsewhere.  Try starting Elasticsearch using the init scripts, rather than directly on the command line
</comment><comment author="mikesparr" created="2015-09-07T20:19:26Z" id="138370074">@clintongormley I'll jump into forum but ran into same issue. Trying to figure how to run ES from command line so I can control it with Supervisord.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>In 2.0 detect_noop for updates should be on by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11282</link><project id="" key="" /><description>We added `detect_noop` to updates a while back but left it defaulting to `false` because there are use cases where you'd want it to be false. Things like where you want timestamps to be updated even if the document didn't change. Anyway, while 2.0 is making breaking changes is probably the time to switch from defaulting to `false` to `true`. Its worth switching because performing noop updates _are_ expensive.
</description><key id="79026405">11282</key><summary>In 2.0 detect_noop for updates should be on by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>breaking</label></labels><created>2015-05-21T14:42:50Z</created><updated>2015-08-27T15:22:13Z</updated><resolved>2015-08-27T15:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T16:31:05Z" id="104345405">It's not clear to me yet how common it is to run updates that don't change anything, but this no-op detection looks cheap to me, so +1 if it can help certain use-cases, I don't think it would hurt anyone.
</comment><comment author="nik9000" created="2015-05-21T17:14:17Z" id="104359760">&gt; It's not clear to me yet how common it is to run updates that don't change anything, but this no-op detection looks cheap to me, so +1 if it can help certain use-cases, I don't think it would hurt anyone.

Yeah - I don't know how common it is either. If it weren't cheap I wouldn't recommend it but it seems cheap and I think folks _expect_ noops to be cheap.
</comment><comment author="boysmovie" created="2015-05-22T14:07:11Z" id="104668757">+1 it's logical for noop to be defaulted true. There are many use cases where messages get repeated with just an updated timestamp. Helps with es performance too.
</comment><comment author="jpountz" created="2015-05-22T14:11:58Z" id="104670172">@aanuprab I'm confused: If a timestamp field is updated then noop detection will not help since the document changed?
</comment><comment author="boysmovie" created="2015-05-22T15:57:09Z" id="104697558">@jpountz for example we have thousands of sensor data that continually stream in. Lots of times the actually sensor readings don't change. But it's just the timestamp on the sensor message header. The data is all that we store in ES, excluding the timestamp header. So it will be a noop. Hope it helps.
</comment><comment author="jpountz" created="2015-05-22T15:58:35Z" id="104697849">OK, got it, the timestamp is not part of your documents.
</comment><comment author="nik9000" created="2015-05-22T16:22:30Z" id="104703692">I'm putting together a patch for this. I noticed that you've been collecting the breaking changes in the migrate_2.0.asciidoc file so I'll have a note in there too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/test/java/org/elasticsearch/document/BulkIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateNoopIT.java</file></files><comments><comment>Default detect_noop to true</comment></comments></commit></commits></item><item><title>Translog recovery can fail due to mappings not present on recovery target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11281</link><project id="" key="" /><description>There is a small window where a type or a field can not be published to the the replica due to a synced mapping update but we are already sending a document of that type to the replica during translog recovery. This is basically the same problem as we have with normal indexing where the first document introducing the type blocks until the mapping update is published but subsequent documents don't introduce the new mapping since the node receiving it already got the update. The window is small but we hit it once in tests today:

http://build-us-00.elastic.co/job/es_core_master_centos/4808/consoleFull

resulting in this:

``` Java

1&gt; RemoteTransportException[[node_t2][local[658]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[2] phase2 failed]; nested: RemoteTransportException[[node_t0][local[656]][internal:index/shard/recovery/translog_ops]]; nested: NullPointerException;
  1&gt; Caused by: [test][0] Phase[2] phase2 failed
  1&gt;    at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:145)
  1&gt;    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:127)
  1&gt;    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:53)
  1&gt;    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:136)
  1&gt;    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:133)
  1&gt;    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:279)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: RemoteTransportException[[node_t0][local[656]][internal:index/shard/recovery/translog_ops]]; nested: NullPointerException;
  1&gt; Caused by: java.lang.NullPointerException
  1&gt;    at org.elasticsearch.index.mapper.MapperAnalyzer.getWrappedAnalyzer(MapperAnalyzer.java:48)
  1&gt;    at org.apache.lucene.analysis.DelegatingAnalyzerWrapper$DelegatingReuseStrategy.getReusableComponents(DelegatingAnalyzerWrapper.java:74)
  1&gt;    at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:139)
  1&gt;    at org.elasticsearch.common.lucene.all.AllField.tokenStream(AllField.java:77)
  1&gt;    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:606)
  1&gt;    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:344)
  1&gt;    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:300)
  1&gt;    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:232)
  1&gt;    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:458)
  1&gt;    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1363)
  1&gt;    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1142)
  1&gt;    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:522)
  1&gt;    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:448)
  1&gt;    at org.elasticsearch.index.shard.TranslogRecoveryPerformer.performRecoveryOperation(TranslogRecoveryPerformer.java:112)
  1&gt;    at org.elasticsearch.index.shard.TranslogRecoveryPerformer.performBatchRecovery(TranslogRecoveryPerformer.java:72)
  1&gt;    at org.elasticsearch.index.shard.IndexShard.performBatchRecovery(IndexShard.java:812)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:306)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:297)
  1&gt;    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:279)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

somehow we also need to wait for a clusterstate update here during translog recovery.
</description><key id="79025677">11281</key><summary>Translog recovery can fail due to mappings not present on recovery target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Mapping</label><label>:Recovery</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T14:41:15Z</created><updated>2015-06-04T20:27:36Z</updated><resolved>2015-06-04T20:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Recovery: restart recovery upon mapping changes during translog replay</comment></comments></commit></commits></item><item><title>Remove the `compress`/`compress_threshold` options of the BinaryFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11280</link><project id="" key="" /><description>This option is broken currently since it potentially interprets an incoming
binary value as compressed while it just happens that the first bytes are the
same as the LZF header.

See https://github.com/elastic/elasticsearch/pull/11279 for more information.
</description><key id="79001203">11280</key><summary>Remove the `compress`/`compress_threshold` options of the BinaryFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T13:43:57Z</created><updated>2015-06-06T15:47:40Z</updated><resolved>2015-05-22T12:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-21T20:22:22Z" id="104410679">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file></files><comments><comment>Merge pull request #11280 from jpountz/fix/remove_binary_compress</comment></comments></commit></commits></item><item><title>Tighten up our compression framework.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11279</link><project id="" key="" /><description>We have a compression framework that we use internally, mainly to compress some
xcontent bytes. However it is quite lenient: for instance it relies on the
assumption that detection of the compression format can only be called on either
some compressed xcontent bytes or some raw xcontent bytes, but nothing checks
this. By the way, we are misusing it in BinaryFieldMapper so that if someone
indexes a binary field which happens to have the same header as a LZF stream,
then at read time, we will try to decompress it.

It also simplifies the API by removing block compression (only streaming) and
some code duplication caused by some methods accepting a byte[] and other
methods a BytesReference.
</description><key id="78979371">11279</key><summary>Tighten up our compression framework.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T12:47:24Z</created><updated>2015-06-07T10:13:38Z</updated><resolved>2015-05-29T15:23:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T12:53:07Z" id="104259546">I'm reading the description again and I've probably not been clear enough about what changed:
- If you call CompressorFactory.compressor(ChannelBuffer) and the compression format is not detected, you will get an exception. This was already working this way at the call site: MessageChannelHandler.messageReceived, now it's done directly in the framework.
- If you call CompressorFactory.compressor(BytesReference) and the bytes are not either some xcontent bytes or some compressed xcontent bytes, then you will get an exception. There is no way in general to detect if some bytes are compressed because nothing prevents an arbitrary bytes reference to have the same header as a LZF block so we enforce that this method be only used on xcontent bytes, which is how we are using this method (to the notable exception of BinaryFieldMapper, which is buggy as described in the description).
</comment><comment author="jpountz" created="2015-05-21T13:44:19Z" id="104281755">I opened #11280 to fix the BinaryFieldMapper issue.
</comment><comment author="jpountz" created="2015-05-22T16:16:40Z" id="104702259">I pushed a new commit to also use DEFLATE (with a compression level of 3) instead of LZF. The reason is that even if LZF might be faster, we have had some issues recently like https://github.com/elastic/elasticsearch/pull/7210 or https://github.com/elastic/elasticsearch/pull/7468 and expect that using something that is more widely used like DEFLATE will protect us better from corruptions in the future.
</comment><comment author="rjernst" created="2015-05-23T01:31:58Z" id="104811503">LGTM, I left a couple minor comments.
</comment><comment author="jpountz" created="2015-05-29T08:40:01Z" id="106745957">@rjernst Thanks for the review, I pushed a new commit.
</comment><comment author="rjernst" created="2015-05-29T09:05:45Z" id="106751328">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java</file><file>src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>src/main/java/org/elasticsearch/common/compress/Compressor.java</file><file>src/main/java/org/elasticsearch/common/compress/CompressorFactory.java</file><file>src/main/java/org/elasticsearch/common/compress/NotCompressedException.java</file><file>src/main/java/org/elasticsearch/common/compress/NotXContentException.java</file><file>src/main/java/org/elasticsearch/common/compress/deflate/DeflateCompressor.java</file><file>src/main/java/org/elasticsearch/common/compress/lzf/LZFCompressedIndexInput.java</file><file>src/main/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamInput.java</file><file>src/main/java/org/elasticsearch/common/compress/lzf/LZFCompressor.java</file><file>src/main/java/org/elasticsearch/common/io/stream/InputStreamStreamInput.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAlias.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MappingMetaDataParserTests.java</file><file>src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTests.java</file><file>src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTests.java</file><file>src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java</file><file>src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java</file><file>src/test/java/org/elasticsearch/common/compress/lzf/CompressedStreamOutput.java</file><file>src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamOutput.java</file><file>src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamTests.java</file><file>src/test/java/org/elasticsearch/common/compress/lzf/LZFTestCompressor.java</file><file>src/test/java/org/elasticsearch/common/compress/lzf/LZFXContentTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java</file><file>src/test/java/org/elasticsearch/search/compress/SearchSourceCompressTests.java</file></files><comments><comment>Merge pull request #11279 from jpountz/fix/simplify_compression</comment></comments></commit></commits></item><item><title>docbug: rolling restarts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11278</link><project id="" key="" /><description>There's several places in the reference documentation which duplicate the _rolling restart_ procedure:
- [setup-upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html)
- [_rolling_restarts](https://www.elastic.co/guide/en/elasticsearch/guide/current/_rolling_restarts.html)
- [cluster-nodes-shutdown](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-shutdown.html)

The latter is wrong, as it fails to mention the allocation needs to be re-enabled after every node restart.
## Proposed change:

Remove duplicates (especially the wrong ones), and reference the one true procedure from all other places
</description><key id="78952317">11278</key><summary>docbug: rolling restarts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels><label>docs</label></labels><created>2015-05-21T11:27:08Z</created><updated>2015-05-25T15:25:30Z</updated><resolved>2015-05-25T15:25:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T15:25:22Z" id="105251524">Hi @faxm0dem 

Thanks for pointing this out. I've left the docs in the Definitive Guide, because that's for a published book and I think it is OK to duplicate it there.  However, the shutdown API has been removed in master, so I've added a deprecation notice to the 1.x branch, and just deleted the rolling restart docs from that page in 1.x (62c5361ad8d09a043d4ad800c142362045c135fd)  and 1.5 (a8c46d68a863e4be33f1d2aba24843f7940dcaf2)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Searches do not return expected number of hits when using rescoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11277</link><project id="" key="" /><description>When trying to upgrade to elasticsearch 1.5.x we encountered this unexpected behaviour.

For example:

``` json
{
  "from": 3,
  "size": 4,
  "query": {
    "term" : { "text" : "hello" }
  },
  "rescore" : {
    "window_size" : 50,
    "query" : {
       "rescore_query" : {
          "match" : {
             "text" : {
                "query" : "world",
                "slop" : 2
             }
          }
       },
       "query_weight" : 0.7,
       "rescore_query_weight" : 1.2
    }
  }
}
```

Returns 4 hits on ES 1.4.5, but only one on 1.5.2

See https://gist.github.com/koenbollen/f7232e3b5e600c66b4b4 for a minimal example that returns unexpected results on elasticsearch &gt;= 1.5.0
</description><key id="78935658">11277</key><summary>Searches do not return expected number of hits when using rescoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">jurriaan</reporter><labels><label>:Search</label><label>bug</label></labels><created>2015-05-21T10:35:12Z</created><updated>2015-05-26T08:53:00Z</updated><resolved>2015-05-26T08:50:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T15:15:39Z" id="105249937">Hi @jurriaan 

Yes, i can replicate this.  Setting `from` to anything but zero returns the same list of documents, but skipping the first `from` documents.  It looks to be related to https://github.com/elastic/elasticsearch/pull/7707

@mikemccand could you have a look at this please?

A simpler recreation below:

```
DELETE test

PUT test 
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST test/item/_bulk
{"index": {"_id": 1}}
{ "text": "hello world"}
{"index": {"_id": 2}}
{ "text": "hello world"}
{"index": {"_id": 3}}
{ "text": "hello world"}
{"index": {"_id": 4}}
{ "text": "hello world"}
{"index": {"_id": 5}}
{ "text": "hello world"}

POST /test/item/_search
{
  "from": 1,
  "size": 4,
  "query": {
    "term": {
      "text": "hello"
    }
  },
  "rescore": {
    "window_size": 50,
    "query": {
      "rescore_query": {
        "match_all": {}
      }
    }
  }
}
```
</comment><comment author="mikemccand" created="2015-05-26T08:09:26Z" id="105436418">Thanks @jurriaan this is indeed wrong ... I opened #11342 to fix this.
</comment><comment author="jurriaan" created="2015-05-26T08:53:00Z" id="105451105">@mikemccand Thanks for the quick fix! Hopefully we can migrate to a newer version of elasticsearch soon :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport: remove support for reading/writing list of strings, use arrays instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11276</link><project id="" key="" /><description>We recently introduced support for reading and writing list of strings as part of #11056, but that was an oversight, we should be using arrays instead.
</description><key id="78924913">11276</key><summary>Transport: remove support for reading/writing list of strings, use arrays instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T10:05:10Z</created><updated>2015-06-08T12:55:00Z</updated><resolved>2015-05-21T13:09:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-21T11:24:06Z" id="104234043">@cbuescher can you have a look please?
</comment><comment author="cbuescher" created="2015-05-21T12:47:14Z" id="104257803">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>Transport: remove support for reading/writing list of strings, use arrays instead</comment></comments></commit></commits></item><item><title>Renaming reducers to Pipeline Aggregators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11275</link><project id="" key="" /><description>This PR renames reducers to Pipeline aggregators in both the codebase and the documentation
</description><key id="78919899">11275</key><summary>Renaming reducers to Pipeline Aggregators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T09:49:11Z</created><updated>2015-05-21T14:17:08Z</updated><resolved>2015-05-21T14:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T13:47:18Z" id="104282564">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactors SimpleQueryStringBuilder/Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11274</link><project id="" key="" /><description>Posting as early preview - in the current state makes SimpleQueryStringBuilder streamable, adds hashCode and equals. Next step adds Lucene query checks.

Note: Switched to using toLanguageTag/forLanguageTag when parsing Locales. Using LocaleUtils from either Elasticsearch or Apache commons resulted in Locales not passing the roundtrip test. For more info see https://issues.apache.org/jira/browse/LUCENE-4021 and the respective Java Locale.toString() documentation.

Relates to #10217

This PR goes against the feature/query-refactoring feature branch.
</description><key id="78869822">11274</key><summary>Refactors SimpleQueryStringBuilder/Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-05-21T07:23:03Z</created><updated>2015-09-03T20:01:04Z</updated><resolved>2015-06-23T20:01:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-26T12:39:49Z" id="105510547">done first round of review, left some comments.
</comment><comment author="MaineC" created="2015-05-28T07:56:38Z" id="106221214">I briefly chatted with @dakrone  - here's his take on adding the Boostable interface:

&gt; I think the SimpleQueryString stuff could implement Boostable, the
&gt; reason it didn't previously I think was that it allowed specifying
&gt; fields in the ["field1", "field2"^2.0] syntax. We should add support for
&gt; the "boost" field that boosts the entire query to the syntax as well.
</comment><comment author="javanna" created="2015-06-04T16:19:28Z" id="108955431">thanks @MaineC  I left some comments
</comment><comment author="MaineC" created="2015-06-09T09:51:43Z" id="110297826">Thanks for your comments. Incorporating the proposed changes.
</comment><comment author="MaineC" created="2015-06-10T09:33:36Z" id="110671260">@cbuescher @javanna Ready for another round.

Didn't switch to checking for empty strings on validation, reason here: https://github.com/elastic/elasticsearch/pull/11274#discussion-diff-31736540R322
</comment><comment author="javanna" created="2015-06-10T12:32:56Z" id="110732091">I did another review round, thanks @MaineC 
</comment><comment author="MaineC" created="2015-06-17T08:01:49Z" id="112706221">@cbuescher @javanna Latest update switches to complaining when users set fields with sensible defaults to null. This is mostly for illustration, let me know what you think.

Also I left the operator stuff in for now assuming #11345 will include factoring the operator into a separate class (what @alexksikes has there already has more functionality than what the class in this code provides, so probably it makes sense to use his instead). This needs to be changed once #11345 is merged.
</comment><comment author="MaineC" created="2015-06-18T10:01:45Z" id="113097558">sticky note for myself: After running the test suite w/o the dev profile I ran into issues. Keeping the broken test seed here so I find it again:

mvn test -Pdev -Dtests.seed=C8F79050CF446C0E -Dtests.class=org.elasticsearch.index.query.SimpleQueryStringBuilderTest -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en_US -Dtests.timezone=Europe/Berlin

java.security.AccessControlException"&gt;java.security.AccessControlException: access denied (&amp;quot;java.lang.RuntimePermission&amp;quot; &amp;quot;accessClassInPackage.sun.util.locale&amp;quot;)
        at __randomizedtesting.SeedInfo.seed([C8F79050CF446C0E]:0)
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)
        at java.security.AccessController.checkPermission(AccessController.java:559)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1525)
        at java.lang.Class.checkPackageAccess(Class.java:2265)
        at java.lang.Class.checkMemberAccess(Class.java:2245)
        at java.lang.Class.getDeclaredFields(Class.java:1805)
        at com.carrotsearch.randomizedtesting.rules.RamUsageEstimator.createCacheEntry(RamUsageEstimator.java:573)
        at com.carrotsearch.randomizedtesting.rules.RamUsageEstimator.measureSizeOf(RamUsageEstimator.java:537)
        at com.carrotsearch.randomizedtesting.rules.RamUsageEstimator.sizeOfAll(RamUsageEstimator.java:385)
        at com.carrotsearch.randomizedtesting.rules.StaticFieldsInvariantRule$1.afterAlways(StaticFieldsInvariantRule.java:108)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:43)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
        at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
        at java.lang.Thread.run(Thread.java:744)
</comment><comment author="javanna" created="2015-06-22T13:38:40Z" id="114106372">I did a last round of review, this is very close.
</comment><comment author="MaineC" created="2015-06-22T18:26:10Z" id="114208471">Will make the code changes tomorrow morning when awake - thanks for your comments. Will ping you when done.
</comment><comment author="javanna" created="2015-06-23T10:04:32Z" id="114428838">LGTM squash and push? ;)
</comment><comment author="javanna" created="2015-09-01T11:48:03Z" id="136685556">Marked this breaking. This PR contains a little breakage for the java api, as the SimpleQueryStringQueryBuilder#field(String) method doesn't allow to specify the boost in form field("field^2"). If you want to specify a boost you need to switch to SimpleQueryStringQueryBuilder#field(String, float) and do field("field", 2);
It breaks how we parse the locale on the REST layer, as we now do `Locale#forLanguageTag` and we output using `Locale#toLanguageTag`. This is more correct but not a backwards compatible change. Locales that were previously properly parsed might not get parsed anymore. See #13229.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file></files><comments><comment>Merge pull request #12810 from MaineC/feature/simple-query-string-test</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file></files><comments><comment>Fix SimpleQueryStringBuilder wildcard handling</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file></files><comments><comment>Merge pull request #11274 from MaineC/feature/simple-query-string-refactoring</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringTests.java</file></files><comments><comment>Add boost to SimpleQueryStringBuilder.</comment></comments></commit></commits></item><item><title>Remove useless outdated forbidden-api version in `-Pdev` config.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11273</link><project id="" key="" /><description>Pointed out by @uschindler on https://github.com/elastic/elasticsearch-parent/pull/43
</description><key id="78850517">11273</key><summary>Remove useless outdated forbidden-api version in `-Pdev` config.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T06:12:35Z</created><updated>2015-05-21T06:26:37Z</updated><resolved>2015-05-21T06:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-05-21T06:17:01Z" id="104151535">+1

I am not even sure if the "dev" config worked at all, because Maven selects profiles also depending on versions...
</comment><comment author="dadoonet" created="2015-05-21T06:24:51Z" id="104152304">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11273 from rmuir/useless_version</comment></comments></commit></commits></item><item><title>Cleanup field name handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11272</link><project id="" key="" /><description>This clarifies some of the uses of names, so that the ambiguous
"name" is mostly no longer used (does this include path or not?).
sourcePath is also removed as it was not used. Not all the
uses of .name() have been cleaned up because Mapper still has
this, and ObjectMapper depends on it returning the short name,
but I would like to leave finishing that cleanup for a future issue.
</description><key id="78752433">11272</key><summary>Cleanup field name handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-21T00:08:44Z</created><updated>2015-06-07T10:51:57Z</updated><resolved>2015-05-21T07:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T07:31:47Z" id="104165186">LGTM. I like how this renaming made it immediately obvious that we were wrongly using the short name instead of full name in some places.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/ContentPath.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Merge pull request #11272 from rjernst/refactor/mapper-names</comment></comments></commit></commits></item><item><title>Shards should rebalance across multiple path.data on one node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11271</link><project id="" key="" /><description>Follow-on from #11185:

As of #9398 we now allocate an entire shard to one path on node's path.data, instead of file by file.

Even though we do the initial allocation to the path.data with the most free space, if the shards then grow in lopsided ways (an "adversary"), we can get to a state where one path.data is nearly full while others are very empty, on a single node.  I suspect such adversaries would not be uncommon in practice...

Yet, DiskThresholdDecider only looks at total usable space on the node (not per-path on path.data) so it won't notice when only one path is close to full... we need to fix that?  Also, the shard allocation process needs to see/address each path.data separately somehow?

Sometimes a shard would just move from one path.data to another path.data on the same node (maybe we should bias for that, all other criteria being nearly equal, since we save on network traffic).

I think it's important to fix this but this is beyond my knowledge of ES now ...
</description><key id="78702475">11271</key><summary>Shards should rebalance across multiple path.data on one node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>discuss</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T21:20:58Z</created><updated>2015-09-16T13:35:04Z</updated><resolved>2015-08-21T06:34:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-20T21:27:49Z" id="104044401">&gt; DiskThresholdDecider only looks at total usable space on the node (not per-path on path.data) so it won't notice when only one path is close to full... we need to fix that?

+1, I think instead of doing the average for all the paths we can just use the `max` used data path on the node?
</comment><comment author="mikemccand" created="2015-05-20T21:41:57Z" id="104047713">&gt; I think instead of doing the average for all the paths we can just use the max used data path on the node?

I think so, at least for the logic that notices when a node is "getting full" and triggers a rebalance.
</comment><comment author="clintongormley" created="2015-06-19T09:42:46Z" id="113448133">I think there are only two possible solutions here:
1. Write a path-aware allocator
2. Remove multi-path support 
</comment><comment author="clintongormley" created="2015-06-19T09:46:37Z" id="113449271">If we remove multi-path support, users can run multiple instances per server, and we'd need to provide a tool to migrate data before starting the cluster (or use snapshot/restore)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java</file></files><comments><comment>Add `expectedShardSize` to ShardRouting and use it in path.data allocation</comment></comments></commit></commits></item><item><title>Add support for applying setting filters when displaying repository settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11270</link><project id="" key="" /><description>Currently all settings that were specified during repository creation are displayed. This commit enables  plugins such as cloud-aws to filter out sensitive settings from the repository.

Closes #11265
</description><key id="78688778">11270</key><summary>Add support for applying setting filters when displaying repository settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T20:40:21Z</created><updated>2015-06-07T17:26:48Z</updated><resolved>2015-05-30T00:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-20T20:54:05Z" id="104034456">Something I don't understand in your PR. What would happen if I have 2 plugins, one for Azure, one for S3. Which `SettingsFilter` will be injected in `RestGetRepositoriesAction`?
</comment><comment author="imotov" created="2015-05-20T21:50:07Z" id="104049113">@dadoonet the same, the `SettingsFilter` is a singleton, and each plugin should add their own patterns to it that they want to filter out to this singleton. 
</comment><comment author="dadoonet" created="2015-05-20T21:51:10Z" id="104049307">Ah I see. Thanks! 
The change looks good to me.
</comment><comment author="s1monw" created="2015-05-29T09:05:40Z" id="106751299">@imotov can you get this in please 
</comment><comment author="imotov" created="2015-05-29T23:25:17Z" id="106957671">Because 1.6 implementation is 100% different (except test) I have opened a new PR #11431 for it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check if the index can be opened and is not corrupted on state listing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11269</link><project id="" key="" /><description>We fetch the state version to find the right shard to be started as
the primary. This can return a valid shard state even if the shard is
corrupted and can't even be opened. This commit adds best effort detection
for this scenario and returns an invalid version for the shard if it's corrupted

Closes #11226
</description><key id="78682859">11269</key><summary>Check if the index can be opened and is not corrupted on state listing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T20:25:22Z</created><updated>2015-05-25T15:30:24Z</updated><resolved>2015-05-21T07:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-20T20:25:29Z" id="104024707">@kimchy can you look
</comment><comment author="bleskes" created="2015-05-21T06:53:34Z" id="104158416">LGTM (left two minor comments). 
</comment><comment author="s1monw" created="2015-05-21T07:46:04Z" id="104168655">pushed a new commit - I will push that to master soon
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pattern Analyzer pattern default does not seem to be '\W+'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11268</link><project id="" key="" /><description>I'm a bit confused by the documentation.  It seems that if I want an analyzer that tokenizes across non-word ([^a-zA-Z_0-9]) then pattern analyzer would do it by default w/ the pattern defaulting to '\W+'  However this does not seem to be the case.  I recently realized that the Simple analyzer does not produce number tokens.

```
DELETE /test
PUT /test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nonword": {
          "type": "pattern",
          "pattern": "\\W+" 
        }
      }
    }
  }
}

GET /test/_analyze?analyzer=pattern&amp;text=33
# does NOT return 33 as a token
GET /test/_analyze?analyzer=nonword&amp;text=33
# does return 33 as a token
```
</description><key id="78675607">11268</key><summary>Pattern Analyzer pattern default does not seem to be '\W+'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aochsner</reporter><labels /><created>2015-05-20T20:04:48Z</created><updated>2015-05-25T14:43:20Z</updated><resolved>2015-05-25T14:43:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T14:43:20Z" id="105245872">Hi @aochsner 

The pattern analyzer in Lucene had a "fast path" when the pattern was set to `.\W` which used (if I remember correctly) `isLetter()`.  This was clearly a bug.  The PatternAnalyzer class has now been reused, and the `pattern_analyzer` in the master branch of Elasticsearch is implemented using the PatternTokenizer, which doesn't suffer from this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide an HTTP API url, which will output all available API urls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11267</link><project id="" key="" /><description>Ruby on Rails, and many other software has a nice feature of automatically listing all available urls. When somebody new comes in to the project, runs `rake routes`, he/she gets a list of urls the application understands.

Elasticsearch is moving fast, and the http API is growing. It would be nice to have some HTTP endpoint which will list all available routes for each of the Elasticsearch APIs (Cluster API health/status, Nodes info API, indices API, Search API and every new feature too).

Instead of chasing the changelog and reading the changed documentation, you call something like `http://localhost:9200/_api_routes` and you will get a nice list of all API urls currently supported by the given Elasticsearch version.

Since this list is autogenerated, there is no risk of going out of sync between the correct urls and the documentation.
</description><key id="78649639">11267</key><summary>Provide an HTTP API url, which will output all available API urls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astropanic</reporter><labels /><created>2015-05-20T19:01:07Z</created><updated>2015-05-27T09:44:14Z</updated><resolved>2015-05-27T09:44:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T14:22:41Z" id="105242478">Hi @astropanic 

I know it isn't an API, but do you know about the REST API spec? https://github.com/elastic/elasticsearch/tree/master/rest-api-spec/api

It is manually maintained, but we use this spec when updating the officially supported clients.  It is also used by a community Swagger plugin: see https://github.com/timschlechter/swagger-for-elasticsearch
</comment><comment author="astropanic" created="2015-05-26T16:52:13Z" id="105600598">Hi @clintongormley,

Was not aware of this nice summary.

Thanks!
</comment><comment author="clintongormley" created="2015-05-27T09:44:14Z" id="105841911">Glad to help @astropanic - I think your need is already served, so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix NPE when streaming commit stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11266</link><project id="" key="" /><description>commit id is only written from lucene 5 on and therefore can be null.
</description><key id="78646942">11266</key><summary>Fix NPE when streaming commit stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Engine</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T18:53:58Z</created><updated>2015-06-08T12:55:43Z</updated><resolved>2015-05-21T06:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-05-20T18:59:28Z" id="103996185">LGTM
</comment><comment author="s1monw" created="2015-05-20T19:08:56Z" id="103999088">LGTM too 
</comment><comment author="s1monw" created="2015-05-21T07:46:50Z" id="104168763">@brwe I think you can remove the 1.6 label this stuff is not in 1.x no?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/CommitStats.java</file><file>src/test/java/org/elasticsearch/index/engine/CommitStatsTests.java</file></files><comments><comment>Merge pull request #11266 from brwe/npe-commitstats</comment></comments></commit></commits></item><item><title>Snapshot/Restore: Add support for applying setting filters when displaying repository settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11265</link><project id="" key="" /><description>Currently all settings that were specified during repository creation are displayed. We should allow plugins such as cloud-aws to filter out sensitive settings from the repository. See elastic/elasticsearch-cloud-aws/issues/184 for an example.
</description><key id="78636442">11265</key><summary>Snapshot/Restore: Add support for applying setting filters when displaying repository settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T18:23:50Z</created><updated>2015-05-30T00:05:56Z</updated><resolved>2015-05-30T00:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file></files><comments><comment>Don't show access_key and filter_key in S3 repository settings</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/get/RestGetRepositoriesAction.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepositoryPlugin.java</file></files><comments><comment>Snapshot/Restore: Add support for applying setting filters when displaying repository settings</comment></comments></commit></commits></item><item><title>Using a query in Percolate should implicitly enable track_scores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11264</link><project id="" key="" /><description>When using a percolator "metadata query", the desired behavior is almost always to use the query for sorting.  But the user needs to know that `track_scores` _also_ needs to be toggled.

It would be more user-friendly if using a query implicitly toggled `track_scores`.  The option should still remain if the user doesn't actually want it, but the default should switch to make it more convenient to use.

So instead of this:

``` shell
curl -XGET 'localhost:9200/news/item/_percolate' -d '{
    "doc" : {
        ...
    },
    "query" : {
        ...
    },
    "track_scores" : true
}'
```

A user would just specify the query:

``` shell
curl -XGET 'localhost:9200/news/item/_percolate' -d '{
    "doc" : {
        ...
    },
    "query" : {
        ...
    }
}'
```

And scores are tracked implicitly unless changed.
</description><key id="78619575">11264</key><summary>Using a query in Percolate should implicitly enable track_scores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Percolator</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-05-20T17:35:26Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T14:13:05Z" id="105240530">+1
</comment><comment author="camilojd" created="2015-09-04T03:32:05Z" id="137638844">I'd like to work on this issue.

Should `trackScores` be set after `PercolatorService.parseRequest()` in `percolate()`, or as part of `parseRequest()` itself?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/index/memory/ExtendedMemoryIndex.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhase.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateException.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorModule.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhaseTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorQueryCacheTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryShardContextTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorServiceTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/TTLPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/percolator/PercolatorQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolStatsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>percolator: Replace percolate api with the new percolator query</comment></comments></commit></commits></item><item><title>Uid#createTypeUids to accept a collection of ids rather than a list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11263</link><project id="" key="" /><description>The downside of having createTypeUids accept a list only is that if you do provide a collection nothing breaks at compile time, but you end up calling the same method that accepts an object as second argument. Renamed both methods to avoid clashes to `createUidsForTypesAndId` and `createUidsForTypesAndIds`. The latter accepts now a Collection of Objects rather than just a List.
</description><key id="78618430">11263</key><summary>Uid#createTypeUids to accept a collection of ids rather than a list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T17:31:33Z</created><updated>2015-06-07T10:13:47Z</updated><resolved>2015-05-21T08:52:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-20T23:21:30Z" id="104069844">@javanna Maybe we could also rename `createTypeUids` to `createTypeUid` (removing the trailing 's') when there is a single provided ID, this would avoid having again the same issue with eg. iterables?
</comment><comment author="javanna" created="2015-05-21T08:17:17Z" id="104174458">@jpountz indeed that is what I had done at first, but the plural makes sense because the uids generated are still more than one although the id provided as argument is only one (number of types \* number of ids). Shall we still do it?
</comment><comment author="jpountz" created="2015-05-21T08:21:54Z" id="104175901">Maybe another way to see it is that this method should not really take `Object's. Since all our ids are supposed to be strings, I think only`String`or`BytesRef` could really make sense?
</comment><comment author="javanna" created="2015-05-21T08:38:03Z" id="104182012">&gt; Maybe another way to see it is that this method should not really take Object's. Since all our ids are supposed to be strings, I think onlyStringorBytesRef` could really make sense?

I tend to agree, but this would be a bigger change, that has to do with these methods being called from `FieldMapper#termQuery` &amp; `FieldMapper#termsQuery`. I went for renaming both methods, hopefully that makes it clearer.
</comment><comment author="jpountz" created="2015-05-21T08:40:26Z" id="104182474">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file></files><comments><comment>Internal: Uid#createTypeUids to accept a collection of ids rather than a list, plus rename method variants to avoid clashes</comment></comments></commit></commits></item><item><title>Async fetch of shard started and store during allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11262</link><project id="" key="" /><description>Today, when a primary shard is not allocated we go to all the nodes to find where it is allocated (listing its started state). When we allocate a replica shard, we head to all the nodes and list its store to allocate the replica on a node that holds the closest matching index files to the primary.

Those two operations today execute synchronously within the GatewayAllocator, which means they execute in a sync manner within the cluster update thread. For large clusters, or environments with very slow disk, those operations will stall the cluster update thread, making it seem like its stuck.

Worse, if the FS is really slow, we timeout after 30s the operation (to not stall the cluster update thread completely). This means that we will have another run for the primary shard if we didn't find one, or we won't find the best node to place a shard since it might have timed out (listing stores need to list all files and read the checksum at the end of each file).

On top of that, this sync operation happen one shard at a time, so its effectively compounding the problem in a serial manner the more shards we have and the slower FS is...

This change moves to perform both listing the shard started states and the shard stores to an async manner. During the allocation by the GatewayAllocator, if data needs to be fetched from a node, it is done in an async fashion, with the response triggering a reroute to make sure the results will be taken into account. Also, if there are on going operations happening, the relevant shard data will not be taken into account until all the ongoing listing operations are done executing.

The execution of listing shard states and stores has been moved to their own respective thread pools (scaling, so will go down to 0 when not needed anymore, unbounded queue, since we don't want to timeout, just let it execute based on how fast the local FS is). This is needed sine we are going to blast nodes with a lot of requests and we need to make sure there is no thread explosion.

This change also improves the handling of shard failures coming from a specific node. Today, those nodes were ignored from allocation only for the single reroute round. Now, since fetching is async, we need to keep those failures around at least until a single successful fetch without the node is done, to make sure not to repeat allocating to the failed node all the time.

Note, if before the indication of slow allocation was high pending tasks since the allocator was waiting for responses, not the pending tasks will be much smaller. In order to still indicate that the cluster is in the middle of fetching shard data, 2 attributes were added to the cluster health API, indicating the number of ongoing fetches of both started shards and shard store.

closes #9502
closes #11101
</description><key id="78609800">11262</key><summary>Async fetch of shard started and store during allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>release highlight</label><label>v1.6.0</label></labels><created>2015-05-20T17:07:48Z</created><updated>2015-05-25T17:12:33Z</updated><resolved>2015-05-22T09:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-20T17:08:13Z" id="103963433">just merged to 1.x, still have failing tests, so, wip
</comment><comment author="kimchy" created="2015-05-22T09:42:33Z" id="104597901">pushed to 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add profile name to TransportChannel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11261</link><project id="" key="" /><description>Today, only the NettyTransportChannel implements the getProfileName method
and the other channel implementations do not. The profile name is useful for some
plugins to perform custom actions based on the name. Rather than checking the
type of the channel, it makes sense to always expose the profile name.

For DirectResponseChannels we use a name that cannot be used in the settings
to define another profile with that name. For LocalTransportChannel we use the
same name as the default profile.

Closes #10483
</description><key id="78600539">11261</key><summary>Add profile name to TransportChannel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T16:43:01Z</created><updated>2015-06-07T16:28:29Z</updated><resolved>2015-05-21T16:43:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-20T16:43:13Z" id="103954709">@spinscale can you review?
</comment><comment author="spinscale" created="2015-05-21T07:15:09Z" id="104162948">LGTM with the exception of the naming. The DirectChannel is named ".local" whereas the LocalChannel is named "default" - might be confusing?
</comment><comment author="jaymode" created="2015-05-21T10:07:22Z" id="104211516">What do you think about `.direct` for DirectChannel? The LocalTransportChannel profile should stay as `default` IMO because it is used with the LocalTransport and there is only one profile in that case.
</comment><comment author="spinscale" created="2015-05-21T12:22:03Z" id="104253693">@jaymode +1 for that
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic_date_formats ignore 'basicDateTimeNoMillis' date format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11260</link><project id="" key="" /><description>In my app I strongly use dynamic nature of ElasticSearch, but for dates it looks like not working properly. In my app all dates are stored in yyyyMMdd’T'HHmmssZ format, so I simply want dynamic date detection to use basicDateTimeNoMillis format. But neither dynamic_date_formats nor dynamic template for dates doesn't work. Dates are simply ignored and detected as strings. Here is part of mapping for _default_ type:

``` json
       "_default_" : {
            "dynamic_date_formats":"basicDateTimeNoMillis",     
            "properties" : {
                "uri" : {
                    "type" : "string"
                },
                "sn" : {
                    "type" : "string"
                },
                "acl" : {
                    "type" : "object"
                }               
            },
            "dynamic_templates": [
                { "date": {
                    "path_match": "*",
                    "match_mapping_type": "date",
                    "mapping": {
                    "type": "date",
                    "format": "basicDateTimeNoMillis"
                }
                }}
            ]
        }
```

It doesn't work for basicDate also, but works as expected for simple UTC formats like yyyy/MM/dd. Also it works fine if I put date fields description directly to  type mapping, like this:

``` json
      "DirectoryRepository_Contact" : {
            "properties" : {
                "properties" : {
                    "type" : "object",
                    "properties" : {
                        "bday": {
                            "type": "date",
                            "format": "basicDateTimeNoMillis"
                        }
                    }
                }
            }
        }
```

So looks like ISO formats are ignored in dynamic_date_formats setting.
</description><key id="78571142">11260</key><summary>dynamic_date_formats ignore 'basicDateTimeNoMillis' date format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PolinaMash</reporter><labels /><created>2015-05-20T15:31:35Z</created><updated>2015-05-25T14:08:49Z</updated><resolved>2015-05-25T14:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T14:08:48Z" id="105240086">Hi @PolinaMash 

This is a known issue (see https://github.com/elastic/elasticsearch/issues/1694).  The problem at the moment is that date-time parsing is not strict enough, so eg `1T` could be detected as the year `0001`.  

We'd like to make this date detection stricter and thus more reliable in 2.0.  Here's the issue to track these changes: https://github.com/elastic/elasticsearch/issues/10971
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field stats: added index_constraint option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11259</link><project id="" key="" /><description>Field stats index constraints allows to omit all field stats for indices that don't match with the index constraint. An index constraint can exclude indices' field stats based on a field's `min_value` and `max_value` statistics.

For example index constraints can be useful to find out the min and max value of a particular property of your data in a time based scenario. The following request only returns field stats for the `answer_count` property for indices holding questions created in the year 2014:

``` bash
curl -XPOST 'http://localhost:9200/_field_stats?level=indices' -d '{
   "fields" : ["answer_count"]
   "index_constraints" : {
      "creation_date" : {
         "min_value" : {
            "gte" : "2014-01-01T00:00:00.000Z",
         },
         "max_value" : {
            "lt" : "2015-01-01T00:00:00.000Z"
         }
      }
   }
}'
```

PR for #11187
</description><key id="78558640">11259</key><summary>Field stats: added index_constraint option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T14:56:46Z</created><updated>2015-07-01T06:47:41Z</updated><resolved>2015-07-01T06:47:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T14:11:37Z" id="104289674">I'm personally a bit confused by the API. For instance the documentation says `The following request only returns field stats for indices that have`_timestamp`date values between the defined range.` but if I read the code correctly, an index that only has two values: one that is less than the minimum value and another one that is greater than the max value, then it would match although it has no values in the range?

Instead, maybe the API should apply to the min/max values instead of the field themselves to be clearer about what it does? Eg.

``` bash
curl -XPOST 'http://localhost:9200/_field_stats?level=indices' -d '{
   "fields" : {
      "_timestamp.max" : {
         "gte" : "2014-01-01T00:00:00.000Z"
      },
      "_timestamp.min" : {
         "lt" : "2015-01-01T00:00:00.000Z"
      }
   }
}'
```
</comment><comment author="jpountz" created="2015-05-21T19:57:42Z" id="104403054">My above comment does not necessarily mean I think it's a bug, but I wanted to raise the corner case so that our API gurus like @clintongormley could help figure out if the API is right.
</comment><comment author="martijnvg" created="2015-05-21T20:05:27Z" id="104405568">@jpountz @clintongormley The field stats filtering doesn't really behave like a filter on docs in an index. It is rather more of an overlapping mechanism / range matching.

Lets say an index contains logs from `02-01-2015` to `03-01-2015`. A range from `01-01-2015` to `04-01-2015` should include the field stats for that index in the field stats response.

So maybe we should name and document it differently?
</comment><comment author="jpountz" created="2015-05-21T20:07:35Z" id="104406108">+1 I would like it better if the API made it clearer this performs range overlapping - the current one feels more like it will return indices that have a value in the specified range, which is not what it does.
</comment><comment author="clintongormley" created="2015-05-25T16:21:31Z" id="105260978">I like @jpountz 's suggestion a lot - makes things much clearer
</comment><comment author="martijnvg" created="2015-05-26T08:26:01Z" id="105443486">I like to make a small change to the format @jpountz is suggesting:

``` bash
curl -XPOST 'http://localhost:9200/_field_stats?level=indices' -d '{
   "fields" : {
      "_timestamp" : {
          "max" : {
              "gte" : "2014-01-01T00:00:00.000Z"
          },
          "min" : {
             "lt" : "2015-01-01T00:00:00.000Z"
          }
      }
   }
}'
```

By having on top level `_timestamp` element, it is clearer that we compute field stats for one field only. 
</comment><comment author="clintongormley" created="2015-05-28T17:51:43Z" id="106525962">See https://github.com/elastic/elasticsearch/issues/11187#issuecomment-106525757
</comment><comment author="martijnvg" created="2015-06-18T20:44:51Z" id="113283871">I've updated this PR based on the comments in #11187. Also the title and description have been updated to match index constraints instead of field stats filter.
</comment><comment author="martijnvg" created="2015-06-19T07:44:48Z" id="113415265">@rjernst Thanks for looking at this, I've updated the PR.
</comment><comment author="martijnvg" created="2015-06-29T15:52:51Z" id="116739561">If there is no more feedback then I like to merge this PR in the next 48 hours.
</comment><comment author="clintongormley" created="2015-06-30T09:41:56Z" id="117095783">@jpountz @rjernst @bleskes any more feedback?
</comment><comment author="jpountz" created="2015-06-30T12:36:40Z" id="117161303">I left some comments, otherwise LGTM. The one I'm most concerned about is what to do if a request has both a body and a `fields` parameter?
</comment><comment author="martijnvg" created="2015-06-30T12:41:08Z" id="117162866">On 30 June 2015 at 14:36, Adrien Grand notifications@github.com wrote:

&gt; I left some comments, otherwise LGTM. The one I'm most concerned about is
&gt; what to do if a request has both a body and a fields parameter?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/11259#issuecomment-117161303
&gt; .

good point, we should just fail the request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk cluster state updates on index deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11258</link><project id="" key="" /><description>This fix is related to https://github.com/elastic/elasticsearch/issues/7295

Currently a ClusterState update is issued for every index when deleting multiple indices, this causes timeout errors in most cases when deleting a large amount of indices. We changed the behavior so only one clusterState update (maximum two if some indices are locked ) is issued when deleting multiple indices. This makes large delete index operations much more faster and more fail-safe.
</description><key id="78552021">11258</key><summary>Bulk cluster state updates on index deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bogensberger</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-05-20T14:40:57Z</created><updated>2015-10-29T19:44:12Z</updated><resolved>2015-10-27T13:49:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T16:15:51Z" id="105259058">@s1monw could you review this please?
</comment><comment author="s1monw" created="2015-06-26T09:26:15Z" id="115606236">hey, this has not been forgotten... I wonder if we do need to do some higher level refactorings to this area of the code. I will look into it next week and come back to this.
</comment><comment author="s1monw" created="2015-10-16T18:42:37Z" id="148802342">@bogensberger I finally found the time to look closer at this. It always bugged me that we had a list of semaphores to acquire and maintain across the lifetime of the delete. This is super error prone and was the main reason why we didn't move forward here sooner. I spend the day looking into why it is needed and what the reasons were why it was added (in 0.17.0) ;) ie. 200 years ago... anyway I am sure we fixed all the problems that lead to the addition of these semaphores in other PRs in 1.x already so we can remove the locking and just go ahead without the locks. I opened https://github.com/elastic/elasticsearch/pull/14159 to get rid of it which I will port for 2.2 once it's in would you mind bringing this up to date? 
</comment><comment author="bogensberger" created="2015-10-19T09:21:20Z" id="149155448">@s1monw yes, i will do.
</comment><comment author="s1monw" created="2015-10-19T11:55:38Z" id="149193157">@bogensberger I pushed it to master... please go ahead!
</comment><comment author="bogensberger" created="2015-10-20T15:18:48Z" id="149599646">@s1monw thanks, i've updated the PR
</comment><comment author="s1monw" created="2015-10-23T10:57:51Z" id="150543359">it looks good to me @javanna can you please take a look at this as well
</comment><comment author="javanna" created="2015-10-23T14:13:59Z" id="150585288">I had a quick look at this and left a few minor comments, looks very good though. I might be getting confused, but I think this one makes #11189 obsolete, and it's a very good change, long overdue. @bogensberger do you have time to address those minor comments so we can pull this in?
</comment><comment author="s1monw" created="2015-10-27T10:44:48Z" id="151453233">@bogensberger if you don't have time to apply the changes I can do it... but I'd love if you could do it :)
</comment><comment author="bogensberger" created="2015-10-27T10:46:36Z" id="151453562">@s1monw i will do it! 
</comment><comment author="s1monw" created="2015-10-27T10:49:37Z" id="151454202">@bogensberger awesome thanks!
</comment><comment author="bogensberger" created="2015-10-27T13:08:32Z" id="151489347">@javanna @s1monw i'v done the changes and also squashed them. 
</comment><comment author="javanna" created="2015-10-27T13:21:17Z" id="151492194">looks great thanks @bogensberger !
</comment><comment author="s1monw" created="2015-10-27T13:50:06Z" id="151502228">merged into master - I will port to 2.x after it's seen some CI cycles thanks @bogensberger 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java</file></files><comments><comment>Deduplicate concrete indices after indices resolution</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file></files><comments><comment>Merge pull request #11258 from crate/b/reduce_cluster_state_updates_on_index_deletion</comment></comments></commit></commits></item><item><title>Tests: replacement test for missing elements in the diff-serialized cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11257</link><project id="" key="" /><description>The commit fd1954d74f79311e203315d62cdc5a25c9b9faab invalidated the assumption that if two cluster states are equal they must have the same binary serialized length. As a result our smoke test for cluster state diffs started to fail periodically and was removed by the commit 42ad6771277d7778a088db57fe512bb9ce885356. Therefore now we don't really have a test for a partially implemented elements in cluster state. In other words if a cluster state element is added to writeTo() but we forget to add it to equalTo and we don't json serialize it, we might miss this fact in this test. We need to come up with a better way to catch situations like this. 
</description><key id="78536477">11257</key><summary>Tests: replacement test for missing elements in the diff-serialized cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-05-20T13:55:29Z</created><updated>2016-03-16T13:39:18Z</updated><resolved>2016-03-16T13:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T16:23:47Z" id="104342958">I have plans to remove block compression as part of https://github.com/elastic/elasticsearch/pull/11279 which means that at least we should again get the same length of serialized cluster states. So if it gets in maybe it would be easiest to add the assertion back instead of working on other ways to compare cluster states.
</comment><comment author="jpountz" created="2015-06-10T11:39:21Z" id="110712610">@imotov Given recent changes we made to how we perform compression, this assertion should be less of an issue now. Should we just add it back?
</comment><comment author="imotov" created="2015-06-10T13:01:52Z" id="110742652">Yes, it would be great to add it back! Thanks!
</comment><comment author="clintongormley" created="2016-01-18T19:33:06Z" id="172630334">@jpountz @imotov  is this fixed by https://github.com/elastic/elasticsearch/pull/11279?
</comment><comment author="imotov" created="2016-01-18T19:46:03Z" id="172632791">@clintongormley Hmm, I don't think we ever added it back.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>Bring back tests for missing elements in the diff-serialized cluster state</comment></comments></commit></commits></item><item><title>Continually try to resolve a host that is unresolved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11256</link><project id="" key="" /><description>This is a basic fix for the issue described in https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/131

I'm not sure this behaviour is desirable as a default or whether it would make sense to honor Java's networkaddress.cache.ttl and networkaddress.cache.negative.ttl or make ES specific properties.
</description><key id="78515854">11256</key><summary>Continually try to resolve a host that is unresolved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">slaupster</reporter><labels><label>:Java API</label><label>discuss</label><label>review</label></labels><created>2015-05-20T13:00:45Z</created><updated>2017-06-09T05:03:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="slaupster" created="2015-05-20T13:30:19Z" id="103889444">Not sure if there is a delay but I've signed the CLA and had what appears to be confirmation.
</comment><comment author="clintongormley" created="2015-05-25T13:58:51Z" id="105238631">Hi @slaupster 

Thanks for the PR (and I can see that you have signed the CLA, thanks).  I wonder if this fix is sufficient.  As I understand it, Elasticsearch will never re-resolve DNS entries, so if the IP for a node changes, the only way to make the transport client resolve the IP address again is to restart it.  Wondering if we can improve this logic somehow.

@spinscale what do you think?
</comment><comment author="spinscale" created="2015-05-26T07:32:07Z" id="105426523">@clint you're right, we already had a couple of discussions about this, see [here](https://github.com/elastic/elasticsearch/issues/10337#issuecomment-89578303). Especially, once an IP address is part of the cluster state (like as part of a client node), dynamic changes are more tricky, as we do not serialize the current ip address over the wire.

In the case of using a TransportClient though, I could imagine the above change.
</comment><comment author="mirekingr" created="2016-02-12T10:21:29Z" id="183262608">This bug fix looks quite useful, any plans to release it?
</comment><comment author="dakrone" created="2016-04-06T21:08:18Z" id="206569008">@spinscale revisiting this, do you still think it's a good idea? Does it need another review?
</comment><comment author="spinscale" created="2016-04-08T07:44:21Z" id="207280599">@dakrone yes, I still think the change is useful for transport clients, however logstash is not affected by this due to using HTTP now. It does need another review, I did not follow any recent changes about networking code
</comment><comment author="dakrone" created="2016-09-12T21:19:38Z" id="246497381">@spinscale any update on this?
</comment><comment author="rjernst" created="2017-06-09T05:03:22Z" id="307296090">We should make a decision on this PR as it has been open for over 2 years. I have marked it for discussion.</comment><comment author="elasticmachine" created="2017-06-09T05:03:24Z" id="307296094">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default value for socket reuse should not be null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11255</link><project id="" key="" /><description>The default for reuse address can be null and casting to a boolean will result in a
NPE. This adds a check to only set the value if it should not be null.
</description><key id="78511503">11255</key><summary>Default value for socket reuse should not be null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T12:47:39Z</created><updated>2015-06-07T16:28:42Z</updated><resolved>2015-05-20T13:56:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-20T12:52:14Z" id="103873577">hm, doesnt it make more sense to fix the `NetworkUtils.defaultReuseAddress()` call? Does null make any sense there? I thought for a fallback on a settings call, but it's not used like that...
</comment><comment author="jaymode" created="2015-05-20T13:32:59Z" id="103890387">@spinscale that makes sense. I have switched this to return false instead of null and removed the null checks where this was called.
</comment><comment author="spinscale" created="2015-05-20T13:47:14Z" id="103894039">ok, so instead of not calling this code at all when the `Boolean` returns `null`, we call it always under windows and set `reuseAddress` to false.

LGTM, but no windows expert, so lets see, if this results in problems in CI
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Backport  #11179 to 1.x / 1.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11254</link><project id="" key="" /><description>This is a cherry-picked backport of  #11179 

Since this PR is a pretty intense one I don't wanna just push it but create a PR for it to ensure the backport doesn't miss anything.
</description><key id="78467160">11254</key><summary>Backport  #11179 to 1.x / 1.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2015-05-20T10:25:48Z</created><updated>2015-05-21T14:42:37Z</updated><resolved>2015-05-21T14:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-21T07:31:51Z" id="104165193">I will push this later today as is unless anybody objects
</comment><comment author="bleskes" created="2015-05-21T13:26:49Z" id="104277041">the back port looks good to me.  left some minor comments/ questions. I also made note of little things which are not part of the back port it self (I suspect they are also in master) . I'll try to get to the soon.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dentry cache bloating on ES instances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11253</link><project id="" key="" /><description>I had created a post in elasticsearch google group earlier
https://groups.google.com/forum/#!topic/elasticsearch/c8_BLOtFVhs

ES version 1.5.2
Arch Linux on Amazon EC2
of the available 16 GB, 8 GB is heap (mlocked). Memory consumption is continuously increasing (225 MB per day).
Total no of documents is around 800k, 500 MB.

```
cat /proc/meminfo has
    Slab: 3424728 kB 
    SReclaimable: 3407256 kB

curl -XGET 'http://localhost:9200/_nodes/stats/jvm?pretty'

    "heap_used_in_bytes" : 5788779888,
              "heap_used_percent" : 67,
              "heap_committed_in_bytes" : 8555069440,

slabtop
 OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                  
17750313 17750313 100%    0.19K 845253       21   3381012K dentry
```

I understand that the dentry cache isn't a bad thing and kernel can take back this memory if needed, but the unusual rate of increase is a bit worrisome. 

I have straced elasticsearch for a couple of minutes
strace -fp PID -o file.txt

out of the 40k+ events recorded
2.2k + events have resulted in errors like this
https://gist.github.com/vanga/55ca296f737b3c1fb9a2
Is this normal?

I think this is the reason for the dentry bloating, though I am not sure if there is some thing wrong with my cluster or not. 

Update: Is ES/lucene making requests to non existent files? or is it making requests to files which were there, but not any more? If yes why would it request to files which are not there? is there some thing wrong with the cluster?

Update: I see that access to a file which resulted in ENOENT succeeds at later point in time(based on strace output). 
Thanks.
</description><key id="78465363">11253</key><summary>Dentry cache bloating on ES instances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vanga</reporter><labels /><created>2015-05-20T10:21:15Z</created><updated>2015-05-25T14:04:21Z</updated><resolved>2015-05-25T14:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T13:17:27Z" id="105230239">@rmuir do you have any thoughts about what is going on here?
</comment><comment author="rmuir" created="2015-05-25T14:04:20Z" id="105239536">Not a problem.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pre sync flush cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11252</link><project id="" key="" /><description>I found some potential problems while backporting this to 1.x
</description><key id="78451042">11252</key><summary>Pre sync flush cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T09:35:07Z</created><updated>2015-06-08T12:56:16Z</updated><resolved>2015-05-20T09:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-05-20T09:41:45Z" id="103825983">+1
</comment><comment author="jpountz" created="2015-05-20T09:42:17Z" id="103826064">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Response codes for index sealing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11251</link><project id="" key="" /><description>We still have to decide which response codes to return if index sealing failed. Currently we return PARTIAL_CONTENT(206) for partial success and CONFLICT(409) for complete failure but this is not really right as discussed here: https://github.com/elastic/elasticsearch/pull/11179#discussion_r30437649
Sorry @nik9000 with all the excitement I actually forgot to fix that before merging.
</description><key id="78448857">11251</key><summary>Response codes for index sealing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T09:28:34Z</created><updated>2015-05-29T18:02:11Z</updated><resolved>2015-05-29T18:02:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-20T09:36:25Z" id="103825140">++ thanks britta
</comment><comment author="clintongormley" created="2015-05-25T13:13:17Z" id="105229794">PARTIAL_CONTENT isn't used anywhere else yet, but I'm wondering if we should, eg a search request where some shards failed?

CONFLICT we only use on version conflicts atm, which i kinda like, but I agree with @nik9000 that the description indicates that it is the best fit for this condition.  That said, wondering if we should just go for a plain 400 here?
</comment><comment author="bleskes" created="2015-05-25T19:40:19Z" id="105297074">The tricky part is that this is a multi-index operation which make it's tricky to decide what the top level rest response should be. 

400 is bad, because the spec explicitly says:

```
The request could not be understood by the server due to malformed syntax. The client SHOULD NOT repeat the request without modifications.
```

especially the later part is simply not true.

Since this is all a best effort api, I would vote to keep things simple and return 200 if nothing exploded on the top level and keep the more intricate "couldn't do  it, pending operation" or some other shard level failure to be apparent from the response. See #11336 for proposed improved, cleaner (imho) response where there is a single counter to watch for (`_shards.failed`) which is similar to flush.
</comment><comment author="nik9000" created="2015-05-26T13:04:44Z" id="105519376">Why is w3.org down when I need it?

&gt; 400 is bad

+1

I don't like 200 because it means:

```
The request has succeeded.
```

But it didn't. I mean, I asked for a seal (or a synced flush or whatever) and it only partially worked. My first instinct with the seal command is to stick it in a `do {response = seal()} while (response != 200);` and I suspect that is what other folks will think too. I figure if a user is thinking deeply enough about the process then they can do the json parsing and figure out the response - but most users should just spin until they get a good seal.

As to the actual code - I don't care so long as it doesn't scream "wrong!" like 200 or 400 do to me. `CONFLICT` is fine with me. Anyone who is ok with partial seals has the wherewithall to look at the result and can distinguish a total failure from a partial one without a different http code. And conflict implies retry-able to me.

It looks like webdav uses 207 for this kind of thing. It might be ok to ape it here somewhat. Or not. Sorry for being a pain about this.
</comment><comment author="bleskes" created="2015-05-26T13:13:46Z" id="105521202">re reading the spec for 409 conflict, it may be actually what we want:

```
The request could not be completed due to a conflict with the current state of the resource. This code is only allowed in situations where it is expected that the user might be able to resolve the conflict and resubmit the request. The response body SHOULD include enough ...
```

99% of failed synced flush (sorry Nik :) ) will be because of concurrent indexing.

I think @brwe already said she's OK with it (correct me if wrong!) . @clintongormley I think you had some reservations, but I think this is were we gravitate. Any objections left?

&gt; Sorry for being a pain about this.

Not at all - These are good discussions! we take good care to choose correctly here, so users in years to come will not be confused.. 
</comment><comment author="nik9000" created="2015-05-26T13:16:05Z" id="105521757">&gt; Any objections left?

None - 409 sounds like the best choice to me.
</comment><comment author="clintongormley" created="2015-05-26T13:59:30Z" id="105534392">&gt; Why is w3.org down when I need it?

There's quite a good page on this little website called wikipedia. you may have heard of it... :)

Yeah, I think 409 is best.  Only for complete failure? and 206 for partial failure? Or 409 for both?
</comment><comment author="bleskes" created="2015-05-26T14:11:32Z" id="105537419">There are indeed subtleties here:
- a complete failure we be an exception, which will carry it's own rest status
- a  failure that relates to an entire shard group (like no assigned primary, or in flight operation) . This will be 409.
- a shard level failure (one replica failed to sync-flush for what ever reason). For this one I'm torn a bit but will go with 409 for simplicity to avoid 200 on error. 

Note that 206 is tricky to me because the spec blames it on the client:

```
The server is delivering only part of the resource (byte serving) due to a range header sent by the client. 
```
</comment><comment author="nik9000" created="2015-05-26T17:00:44Z" id="105602861">&gt; There's quite a good page on this little website called wikipedia. you may have heard of it... :)

I wanted to use a primary source! :)

&gt; a complete failure we be an exception, which will carry it's own rest status

Sure. These are mostly 5xx level stuff I imagine and for the most part should stop any automated processes until a person looks at them.

&gt; a failure that relates to an entire shard group (like no assigned primary, or in flight operation) . This will be 409.

+1

&gt; a shard level failure (one replica failed to sync-flush for what ever reason). For this one I'm torn a bit but will go with 409 for simplicity to avoid 200 on error.

Thanks. I think 409 is best - if the user wants to get cute and say "some of my indexes can restore slow, I don't care" then they can issue the synced flush commands to the list of indexes they want - or they can parse the 409. I think, at least.
</comment><comment author="clintongormley" created="2015-05-29T18:02:09Z" id="106891125">Closed by #11336
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Newly turned on replicas don't honor same_shard.host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11250</link><project id="" key="" /><description>I have set `cluster.routing.allocation.same_shard.host: true`, in a cluster setup with two nodes per server.
This is working correctly, as I cannot relocate shards to hosts that already have a shard (primary or replica) of the same index on either node of that server.

However, when I turn on replicas for an index, I often see that two replica shards are being assigned to the same server (but to the two nodes).

ES Version: 1.4.5
</description><key id="78432855">11250</key><summary>Newly turned on replicas don't honor same_shard.host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wkoot</reporter><labels /><created>2015-05-20T08:41:41Z</created><updated>2015-05-20T08:48:47Z</updated><resolved>2015-05-20T08:48:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-20T08:43:40Z" id="103811807">Can you provide the output from _cat/shards?
</comment><comment author="wkoot" created="2015-05-20T08:47:32Z" id="103813208">Scratch this, I misunderstood `same_shard.host` as "shards from the same index". :-)
</comment><comment author="markwalkom" created="2015-05-20T08:48:46Z" id="103813389">All good :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Corrupted shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11249</link><project id="" key="" /><description>After restarting Elasticsearch 1.5.2 (just a `sudo service elasticsearch restart`) on my single node server with about 600GB data one shard cannot get recovered and the cluster stays in the red status. I haven't noticed any problems before the restart. I'm using the indices without replicas.

I am using a GCE instance with persistent disks and there is enough free space for Elasticsearch there (&gt;170GB). 

Error log:

```
[2015-05-20 07:48:29,774][WARN ][indices.cluster          ] [test1] [[gardenhose-2015-week20][2]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [gardenhose-2015-week20][2] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:162)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [gardenhose-2015-week20][2] failed to upgrade 3x segments
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:121)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:32)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1262)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1257)
        at org.elasticsearch.index.shard.IndexShard.prepareForTranslogRecovery(IndexShard.java:784)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:226)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
        ... 3 more
Caused by: java.io.EOFException: read past EOF: NIOFSIndexInput(path="/mnt/elasticsearch-test1/test/nodes/0/indices/gardenhose-2015-week20/2/index/segments_c9")
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:336)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:98)
        at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:183)
        at org.elasticsearch.common.lucene.Lucene.indexNeeds3xUpgrading(Lucene.java:767)
        at org.elasticsearch.common.lucene.Lucene.upgradeLucene3xSegmentsMetadata(Lucene.java:778)
        at org.elasticsearch.index.engine.InternalEngine.upgrade3xSegments(InternalEngine.java:1084)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:119)
        ... 9 more
```
</description><key id="78425410">11249</key><summary>Corrupted shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">Hocdoc</reporter><labels><label>:Core</label><label>bug</label><label>v1.6.0</label></labels><created>2015-05-20T08:16:05Z</created><updated>2015-06-08T16:37:30Z</updated><resolved>2015-06-08T16:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-20T08:21:06Z" id="103805862">This looks similar to #10680
</comment><comment author="Hocdoc" created="2015-05-20T08:32:13Z" id="103807901">Yes, shutdown of the cluster may have last 30s.
I have noticed there was a big query running before the restart which cause an OOM-Exception.

I have uploaded the full log file https://gist.githubusercontent.com/Hocdoc/0956a3675bd63e0af0d5/raw/elasticsearch.log . The (first) restart was at [2015-05-20 07:44:35,457].
</comment><comment author="clintongormley" created="2015-05-25T12:59:43Z" id="105228601">Actually, it says `failed to upgrade 3x segments`.  Makes me think this was an old previously-corrupted index.
</comment><comment author="mikemccand" created="2015-06-08T15:29:47Z" id="110042566">Hmm I think I see what's happening here ... Elasticsearch detects false corruption if the segments file is 0 bytes, and gives a confusing message about upgrading 3x segments.

Was `/mnt/elasticsearch-test1/test/nodes/0/indices/gardenhose-2015-week20/2/index/segments_c9` 0 bytes in this case?

I'll work on a fix.
</comment><comment author="mikemccand" created="2015-06-08T15:31:05Z" id="110043802">This only affects 1.x; we don't do this check in 2.0 (master).
</comment><comment author="mikemccand" created="2015-06-08T16:37:28Z" id="110068254">Fixed with better error reporting in #11539.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not kill process on service shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11248</link><project id="" key="" /><description>Following #10680 we should not kill the elasticsearch process when a SysV/init.d/SystemD/Win service is stopped.
</description><key id="78418064">11248</key><summary>Do not kill process on service shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T07:48:24Z</created><updated>2015-08-10T08:06:00Z</updated><resolved>2015-08-10T08:06:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Do not kill process on service shutdown</comment></comments></commit></commits></item><item><title>Review usage of CompressedString.equals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11247</link><project id="" key="" /><description>We just made this method slower to fix correctness issues: https://github.com/elastic/elasticsearch/pull/11233

We should review where we use CompressedString.equals to understand the impacts of this change. We might want to either change the way that we check for equality or improve equals to perform better in certain cases.
</description><key id="78410646">11247</key><summary>Review usage of CompressedString.equals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-05-20T07:27:10Z</created><updated>2015-06-10T11:38:08Z</updated><resolved>2015-06-10T11:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-20T11:02:37Z" id="103843435">After some testing I found that there are 3 classes where we use this method:
- MappingMetaData.equals because of cluster-state diffs
- IndicesClusterStateService.processMapping
- MetadataMappingService.putMapping

One good news is that hashCode is never used.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/compress/CompressedXContent.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>Internal: Make CompressedXContent.equals fast again.</comment></comments></commit></commits></item><item><title>Better exception if array passed to `term` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11246</link><project id="" key="" /><description>Example: in the query "term" : ["doc1", "doc2", "doc3"] only "doc3" will be searched. 

I think an error message would be more preferable in case of array, as this is not immediately obvious and might take some time to track down why the query is not returning the expected values (as it happened to me). The user probably meant to run a "terms" query. 

Steps to reproduce:

&lt;pre&gt;
curl -XDELETE localhost:9200/test 
curl -XPUT localhost:9200/test/test/1 -d ' { "title": "doc1"} '
curl -XPUT localhost:9200/test/test/2 -d ' { "title": "doc2"} '
curl -XPUT localhost:9200/test/test/3 -d ' { "title": "doc3"} '

// Query will only return doc3
curl -XGET localhost:9200/test/test/_search -d  '{"query":{"term":{"title": ["doc1", "doc2", "doc3"]}}}'
&lt;/pre&gt;
</description><key id="78386175">11246</key><summary>Better exception if array passed to `term` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spyk</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-05-20T05:52:55Z</created><updated>2015-10-15T15:28:06Z</updated><resolved>2015-05-29T11:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spyk" created="2015-05-22T21:54:02Z" id="104784628">Oops I actually gave a wrong example query. When "term" is used as a query it will throw an ElasticsearchParseException, but when used as a filter it will then actually filter only the last element of the array.

So the test case becomes:

&lt;pre&gt;
// ElasticsearchParseException[Expected field name but got VALUE_STRING \"null\"]; }]"
curl -XGET localhost:9200/test/test/_search -d  '{"query":{"term":{"title": ["doc1", "doc2", "doc3"]}}}'
// Query will only return doc3
curl -XGET localhost:9200/test/test/_search -d  {"query":{"filtered":{"query":{"match_all":{}}, "filter":{"term":{"title":["doc1", "doc2", "doc3"]}}}}}
&lt;/pre&gt;
</comment><comment author="clintongormley" created="2015-05-25T17:45:43Z" id="105276946">Hi @spyk 

Thanks for reporting.  The `term`  filter has been removed in master (all filters are now queries), but I have a sneaking suspicion that parsing of the `term` query goes haywire after it encounters the array. (eg the exception from this query indicates some other problem): 

```
GET /_search
{
  "query": {
    "bool": {
      "should": [
        {
          "term": {
            "foo": [
              "two",
              "one"
            ]
          }
        },
        {
          "term": {
            "foo": [
              "two",
              "one"
            ]
          }
        }
      ]
    }
  }
}
```

Either way, a better exception would be nice.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: throw an exception if array passed to `term` query.</comment></comments></commit></commits></item><item><title>Add the ability to change index settings on snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11245</link><project id="" key="" /><description>Currently, there's no way to change things like index shard allocation settings when restoring from a snapshot.  You can rename the index, but you can't do anything to change its settings until the restore is underway.

If a cluster's settings have diverged significantly since the snapshot was taken, it may initially put the shards on the wrong nodes, leading to cluster instability until the restore has finished and the shards reallocate.
</description><key id="78320665">11245</key><summary>Add the ability to change index settings on snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kbourgoin</reporter><labels /><created>2015-05-20T01:25:23Z</created><updated>2015-05-20T15:28:46Z</updated><resolved>2015-05-20T15:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-20T15:25:25Z" id="103925145">@kbourgoin this feature does exist, see: #9285

Feel free to re-open if I have misinterpreted the issue.
</comment><comment author="kbourgoin" created="2015-05-20T15:28:46Z" id="103925964">Yup. I searched the issues but somehow managed to completely miss that. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>http.max_content_length setting ignored if above 2^31-1 bytes (~2gb), fix or document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11244</link><project id="" key="" /><description>When doing very large bulk requests, increasing http.max_content_length from the default of 100mb is essential. But if you set it above 2gb or so, it'll just silently ignore the setting and default back to 100mb.

If this can't be fixed, the docs at https://www.elastic.co/guide/en/elasticsearch/reference/1.5/modules-http.html should mention it.
</description><key id="78288412">11244</key><summary>http.max_content_length setting ignored if above 2^31-1 bytes (~2gb), fix or document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SergeyTsalkov</reporter><labels><label>docs</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T23:17:19Z</created><updated>2015-05-20T19:08:55Z</updated><resolved>2015-05-20T19:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-20T19:03:32Z" id="103997387">Looks like we do this on purpose for anything over `Integer.MAX_VALUE`, see: https://github.com/elastic/elasticsearch/blob/a40ba3be5aecc00e9b0b7ef79a0220d09159e327/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java#L191-L195

Are you doing bulk inserts over 2gb in size? That is definitely higher than recommended.

I'll add a commit to mention this in the documentation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Mention Integer.MAX_VALUE limit for http.max_content_length</comment></comments></commit></commits></item><item><title>Remove document parse listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11243</link><project id="" key="" /><description>There was previously a single user of the parse listener, MLT API.
However, now that this is gone, there is no need for it.
</description><key id="78277621">11243</key><summary>Remove document parse listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T22:37:28Z</created><updated>2015-06-07T10:52:05Z</updated><resolved>2015-05-20T07:00:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-20T06:40:51Z" id="103779393">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file></files><comments><comment>Merge pull request #11243 from rjernst/remove/type-listener</comment></comments></commit></commits></item><item><title>Plugins should be able to add content types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11242</link><project id="" key="" /><description>We've talked about moving `token_count` into a plugin but I'm a bit stumped as to how I'd plug a new type into Elasticsearch with a plugin. It looks like DocumentMapperParser has the list hard coded. There are method to modify the list after the DocumentMapperParser is built but I can't see the right place to hook into to actually call them.
</description><key id="78243748">11242</key><summary>Plugins should be able to add content types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-05-19T20:42:42Z</created><updated>2015-05-19T22:09:28Z</updated><resolved>2015-05-19T22:07:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-19T22:03:45Z" id="103680600">@nik9000 This is already possible, see the mapper attachment plugin as an example:
https://github.com/elastic/elasticsearch-mapper-attachments/blob/master/src/main/java/org/elasticsearch/plugin/mapper/attachments/AttachmentsIndexModule.java
</comment><comment author="nik9000" created="2015-05-19T22:09:28Z" id="103681487">Thanks. I hadn't remembered that plugin or that technique. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add engine failure on recovery finalization corruption back</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11241</link><project id="" key="" /><description>This engine failure on finalization corruption was lost on refactorings and
should be added back.
</description><key id="78238592">11241</key><summary>Add engine failure on recovery finalization corruption back</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T20:25:50Z</created><updated>2015-06-08T15:43:32Z</updated><resolved>2015-05-20T12:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-19T20:26:02Z" id="103659029">@bleskes can you take a look if you have a sec
</comment><comment author="bleskes" created="2015-05-20T06:25:33Z" id="103777556">Good catch, LGTM. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file></files><comments><comment>Merge pull request #11241 from s1monw/readd_fail_on_recovery</comment></comments></commit></commits></item><item><title>Implement on-line reindexing </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11240</link><project id="" key="" /><description>I'm an **end user** and I'm new to ES and I've been immediately faced with the problem of getting my data processed correctly.
The problem with ES (vs. Splunk) is that data is being indexed/analyzed first, then stored, then you can't change it. (vs. Splunk, where almost everything happens during search time, thus you can adjust your plans).

Currently, in order to test my new mapping params, I have to  (using kopf plugin -- otherwise I'd throw ES immediately out of window for lack of usability):
1. Have my data ready -- either stored in some safe index or in local .json file on disk
2. delete index / index template  -- `/_plugin/kopf/#!/cluster`
3. adjust index appropriately and create it   -- `/_plugin/kopf/#!/createIndex`
4. Use **stream2es** in order to copy index from "safe place" to the index above -- `cat mydata.json | stream2es  stdin --target http://blah:9200/newidx/type1`
   OR 
   `stream2es es --source http://blah:9200/safe_idx/type1 --target http://blah:9200/newidx/type1`
5. Run some query, see it failing/misbehaving, read docs, see that you need mapping adjustment, GO TO step #2.

Now imagine doing this 50 times, because you can't get ES behave properly with your data....
This is very tedious for someone who expected modern easy-to-use software.

Ideally, I want to input my new mapping, press button, and let it do the reindex automatically. 
Of course, this would be useful for dev env only as on production, you'd sync the mapping changes with app schema changes.
</description><key id="78238208">11240</key><summary>Implement on-line reindexing </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">celesteking</reporter><labels /><created>2015-05-19T20:24:43Z</created><updated>2015-05-25T13:50:14Z</updated><resolved>2015-05-25T13:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-05-20T12:43:43Z" id="103870470">Ancillary point, but wanted to note that Logstash 1.5.0 added an [Elasticsearch input](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html). It's more flexible than stream2es since you have the full power of Logstash's filters and other outputs.
</comment><comment author="dadoonet" created="2015-05-20T12:53:17Z" id="103874013">+1 for what @drewr said. I just wrote a blog about it. In cas it helps: http://david.pilato.fr/blog/2015/05/20/reindex-elasticsearch-with-logstash/
</comment><comment author="clintongormley" created="2015-05-25T13:50:13Z" id="105237567">@celesteking we absolutely want to implement a reindex API which will do as you describe, and a whole lot more besides.  In fact this is a duplicate of a very old issue (https://github.com/elastic/elasticsearch/issues/492). Implementation is blocked by the need for a task management framework (https://github.com/elastic/elasticsearch/issues/6914) which will allow long running jobs to be paused, restarted, or cancelled.

Although both of these issues are old, this doesn't mean that they have been forgotten.  Both are on the roadmap and we will get to them as soon as we can.

I'm going to close this as a duplicate of #492
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Randomized Testing NPE in Seed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11239</link><project id="" key="" /><description>I'm attempting to test a customer ShardsAllocator plugin and am running into the following error when attempting to create an index:

```
java.lang.NullPointerException
at __randomizedtesting.SeedInfo.seed([BE85E9425BC7431B:F5AE237DAA0F1150]:0)
at org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider.canAllocate(ThrottlingAllocationDecider.java:109)
at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:146)
at org.elasticsearch.cluster.routing.allocation.allocator.SizeShardsAllocator$SizeBalancer.allocateUnassigned(SizeShardsAllocator.java:135)
at org.elasticsearch.cluster.routing.allocation.allocator.SizeShardsAllocator$SizeBalancer.balance(SizeShardsAllocator.java:107)
at org.elasticsearch.cluster.routing.allocation.allocator.SizeShardsAllocator.allocateUnassigned(SizeShardsAllocator.java:67)
at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:74)
at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)
at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:160)
at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:146)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:453)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:383)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
```

This issue appears to be a recurrence of https://github.com/elastic/elasticsearch/issues/7409 . I'm running Mac OS X 10.10.2 and Java 1.8.0_31. My elasticsearch repo is current up through commit 33fd25083e520e40cc9181a0d690e637b6eb04f4.

Here's a bit of the integration test I'm attempting to run: 

```
@ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE)
public class SizeAllocatorTests extends ElasticsearchIntegrationTest {

@Before
public void setup() throws IOException {
    createIndex("test"); // exception happens after this call
    ensureGreen("test");

    refresh();
}

@Override
protected Settings nodeSettings(int nodeOrdinal) {
    return settingsBuilder()
            .put(super.nodeSettings(nodeOrdinal))
            .put(ShardsAllocatorModule.TYPE_KEY, "org.elasticsearch.cluster.routing.allocation.allocator.SizeShardsAllocator")
            .build();
}
```
</description><key id="78233527">11239</key><summary>Randomized Testing NPE in Seed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sawyercade</reporter><labels /><created>2015-05-19T20:13:08Z</created><updated>2015-05-19T20:22:19Z</updated><resolved>2015-05-19T20:22:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sawyercade" created="2015-05-19T20:22:18Z" id="103658233">Closing, issue is in the custom ShardsAllocator, not production ES.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Limit duration on scan&amp;scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11238</link><project id="" key="" /><description>Scan and scroll could be made safer by limiting the amount of time the scan window remains open.  If it's open too long, it can lead to extra segments failing to merge and will consume resources on the system.  A maximum limit to the duration could help keep scan and scroll from breaking the cluster if used improperly.
</description><key id="78232056">11238</key><summary>Limit duration on scan&amp;scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2015-05-19T20:07:33Z</created><updated>2017-06-28T10:16:58Z</updated><resolved>2017-06-28T10:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-20T08:50:05Z" id="103813673">+1
</comment><comment author="nik9000" created="2015-05-20T12:32:29Z" id="103864932">More playing devil's advocate than anything, but why isn't the timeout that you have to put on each request enough to do that? Because you could theoretically drag the request out a long long time?

Also when I use scan/scroll for mapping updates and on some indexes that process can take the better part of a day. How long do we have to get for these requests to get scary?
</comment><comment author="clintongormley" created="2015-05-25T13:44:44Z" id="105236921">I'm not sure how we could enforce a maximum duration without breaking scroll requests.  
</comment><comment author="rjernst" created="2015-05-26T06:35:06Z" id="105415600">@clintongormley I think we could do this by first allowing a new parameter to add this total duration limit on the scroll, and then at a later time making some value the default for the limit instead of infinity?
</comment><comment author="clintongormley" created="2015-05-26T08:04:32Z" id="105434975">This sounds like a policy for admins rather than for users.  I'm wondering if a better approach would be the task management API #6914.  Admins would be able to check for long running jobs which have exceeded their permitted lifespan and to kill them via an API.  They could also choose not to kill them, as opposed to realising too late that they have started an important scan/scroll job without extending the default lifespan.
</comment><comment author="rjernst" created="2015-05-26T09:30:24Z" id="105464151">Task management seems to me more for background tasks. Isn't all we are talking about an extra setting on the initial request that limits the total time allowed to refresh the scroll? Having to do an extra check for `jobs which have exceeded their permitted lifespan and to kill them via an API` seems rather cumbersome.
</comment><comment author="clintongormley" created="2015-05-26T09:33:36Z" id="105465466">@rjernst As a user, why would you start a scan/scroll job and then abort it before it is finished? You'd just need to start it again.  This is why I think that this functionality is more for the admin who wants to force their users to obey certain policies (but may well want to exempt their own long running jobs from such policies).
</comment><comment author="adrianosimoes" created="2017-02-27T22:47:38Z" id="282882226">I noticed this is a old issue. Is this issue still relevant?</comment><comment author="clintongormley" created="2017-02-28T12:39:17Z" id="283028523">Yes it is, and in fact we discussed this again just the other day.  We should add a soft limit which prevents setting the `scroll` time to more than (eg) 5 minutes.</comment><comment author="colings86" created="2017-06-28T10:16:58Z" id="311618477">Closing in favour of https://github.com/elastic/elasticsearch/issues/23268</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>copy_to needs top level object </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11237</link><project id="" key="" /><description>You should specify that `{"copy_to": "top.child"}`  needs `top` field defined first, otherwise it would throw an error when indexing.

Also, I'm completely dissatisfied with the documentation on the site. 
</description><key id="78230777">11237</key><summary>copy_to needs top level object </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">celesteking</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2015-05-19T20:00:33Z</created><updated>2015-12-15T11:16:40Z</updated><resolved>2015-12-07T13:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T12:38:16Z" id="105225920">Hi @celesteking 

I'd say this is a bug.  Recreation:

```
PUT test 
{
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string",
          "copy_to": "top.child"
        }
      }
    }
  }
}

PUT test/test/1
{
  "foo": "bar"
}
```

returns:

```
{
   "error": "MapperParsingException[attempt to copy value to non-existing object [top.child]]",
   "status": 400
}
```

&gt; Also, I'm completely dissatisfied with the documentation on the site.

You're welcome to participate in this open source project by sending PRs to improve the documentation, or the code.
</comment><comment author="rjernst" created="2015-05-26T06:32:40Z" id="105415389">There is actually a TODO in the code just before that exception is thrown, stating that we should create the parent object dynamically. I agree it is a bug.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file></files><comments><comment>Fix copy_to when the target is a dynamic object field.</comment><comment>Fixes #11237</comment></comments></commit></commits></item><item><title>copy_to not clear</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11236</link><project id="" key="" /><description>You should specify in docs that `copy_to` can be retrieved/shown using `fielddata_fields` and not standard `fields` notation.
</description><key id="78230078">11236</key><summary>copy_to not clear</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">celesteking</reporter><labels /><created>2015-05-19T19:58:06Z</created><updated>2015-05-19T20:50:52Z</updated><resolved>2015-05-19T20:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="celesteking" created="2015-05-19T20:50:27Z" id="103664178">okay -- I made something, now  `"fields": ["fld1"]` works. I guess it's because I've specified `"store": "true"` in mapping.  This should really all be documented!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rewrote some _seal documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11235</link><project id="" key="" /><description>The first two paragraphs were confusing to me so I tried to rewrite them.

I removed some passive voice because it irks me.
</description><key id="78201613">11235</key><summary>Rewrote some _seal documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-05-19T18:31:39Z</created><updated>2015-05-22T16:24:25Z</updated><resolved>2015-05-22T16:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-05-20T09:54:23Z" id="103830576">Left a few suggestions, other than that I like it!
</comment><comment author="nik9000" created="2015-05-20T12:28:21Z" id="103863001">Reworded pretty much how you said.

If you like I can squash. Github says it'll need a rebase so I can do that too.
</comment><comment author="brwe" created="2015-05-22T09:59:55Z" id="104604921">Sorry for the late response. Left another suggestion above but not passionate about it. If you rebased and resolved the conflicts would be great then I will pull it in.
</comment><comment author="nik9000" created="2015-05-22T14:44:09Z" id="104678223">Meh - I'm ok leaving it as is. I'm rebasing/squashing now.

On Fri, May 22, 2015 at 6:00 AM, Britta Weber notifications@github.com
wrote:

&gt; Sorry for the late response. Left another suggestion above but not
&gt; passionate about it. If you rebased and resolved the conflicts would be
&gt; great then I will pull it in.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/11235#issuecomment-104604921
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11235 from nik9000/seal_docs</comment></comments></commit></commits></item><item><title>Issue #10588 Display low disk watermark to be consistent with documentaiton</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11234</link><project id="" key="" /><description>This PR is an attempted fix for Issue #10588. I have added altered log entries, so that messages involving disk usage percentages display used disk percentages, rather than free disk percentages.

Change log entries, add convenience methods, unit tests
</description><key id="78186284">11234</key><summary>Issue #10588 Display low disk watermark to be consistent with documentaiton</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkis-</reporter><labels /><created>2015-05-19T17:44:40Z</created><updated>2015-05-22T21:52:36Z</updated><resolved>2015-05-22T21:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mkis-" created="2015-05-22T21:52:35Z" id="104784385">Closing PR, reopening after I signed the CLA
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix CompressedString.equals.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11233</link><project id="" key="" /><description>CompressedString relied on the assumption that two CompressedString instanes
are equal if there compressed representation are equal. Unfortunately this is
not always true because the compressed representation also depends on when
flush() was called on the output stream or on the size of the hash table that
has been used at compression time.
</description><key id="78164706">11233</key><summary>Fix CompressedString.equals.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T16:45:23Z</created><updated>2015-06-07T18:15:12Z</updated><resolved>2015-05-20T06:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-19T16:56:29Z" id="103590292">LGTM
</comment><comment author="rjernst" created="2015-05-19T20:38:22Z" id="103661695">In practice I don't think the recomputing will matter, I just wanted to confirm my understanding. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/compress/CompressedString.java</file><file>src/test/java/org/elasticsearch/common/compress/CompressedStringTests.java</file></files><comments><comment>Merge pull request #11233 from jpountz/fix/compressedstring_equals</comment></comments></commit></commits></item><item><title>Snapshot delete hangs forever</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11232</link><project id="" key="" /><description>I hit strange issue on ES 1.4.1. When I started snapshot it hanged and I cannot see any progress. I use S3 plugin. The output from _snapshot API call is presented below.

```
  curl -XGET http:'//localhost:9200/_snapshot/aws-prod-elasticsearch-backup/snapshot-name?pretty'
  {
    "snapshots" : [ {
      "snapshot" : "snapshot-name",
      "indices" : [ ... list of indices ... ],
      "state" : "IN_PROGRESS",
      "start_time" : "2015-05-18T04:11:40.344Z",
      "start_time_in_millis" : 1431922300344,
      "failures" : [ ],
      "shards" : {
        "total" : 0,
        "failed" : 0,
        "successful" : 0
      }
    } ]
  }
```

When I try to delete the snapshot

```
curl -XDELETE 'http://localhost:9200/_snapshot/aws-prod-elasticsearch-backup/snapshot-name
```

the command hangs forever. I thought I can try to use the cleaning tool (https://github.com/imotov/elasticsearch-snapshot-cleanup) to make cleanup. It didn't help.

Then I modified source code (https://github.com/imotov/elasticsearch-snapshot-cleanup/blob/master/src/main/java/org/motovs/elasticsearch/snapshots/AbortedSnapshotCleaner.java) to cleanup all possible snapshots by removing this condition:

``` java
if (shard.getValue().state() == SnapshotMetaData.State.ABORTED &amp;&amp; clusterState.nodes().get(nodeId) == null) {
```

Does anyone also encountered similar issue?
</description><key id="78150522">11232</key><summary>Snapshot delete hangs forever</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels><label>feedback_needed</label></labels><created>2015-05-19T16:08:17Z</created><updated>2015-08-26T19:47:01Z</updated><resolved>2015-08-26T19:47:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-19T16:16:29Z" id="103571925">@prog8 the hanging delete issue should be fixed by #9981. Did you restart the master node while the snapshot was running?
</comment><comment author="prog8" created="2015-05-20T06:54:20Z" id="103782018">Hi @imotov  thanks for your reply. Yes master nodes were restarted. I hope upgrade to newer ES will resolve the issue.
</comment><comment author="vanga" created="2015-05-20T10:23:59Z" id="103836378">@prog8 we had this issue, an upgrade to 1.5.0 resolved the issue.
</comment><comment author="jpountz" created="2015-08-26T19:47:01Z" id="135150869">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removed generic types from ContextAndHeaderHolder and  HasHeaders#putHeader()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11231</link><project id="" key="" /><description /><key id="78140381">11231</key><summary>Removed generic types from ContextAndHeaderHolder and  HasHeaders#putHeader()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label></labels><created>2015-05-19T15:41:01Z</created><updated>2015-05-25T12:40:51Z</updated><resolved>2015-05-19T21:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-19T16:18:35Z" id="103573655">LGTM, tested against a plugin needing this (using this zip file and installed the plugin in question, as well as running the plugin tests against this one)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure we mark store as corrupted if we fail to read the segments info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11230</link><project id="" key="" /><description>This commit also reenables CorruptedFileTest#testReplicaCorruption which had
a missing `GatewayAllocator.INDEX_RECOVERY_INITIAL_SHARDS: "one"` setting.

Closes #11226
</description><key id="78137182">11230</key><summary>Ensure we mark store as corrupted if we fail to read the segments info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>resiliency</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T15:33:04Z</created><updated>2015-06-07T17:52:48Z</updated><resolved>2015-05-19T15:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-19T15:33:29Z" id="103554745">@kimchy can you look
</comment><comment author="kimchy" created="2015-05-19T15:37:12Z" id="103556294">minor comment, lgtm
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Endpoint to trigger loading shard info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11229</link><project id="" key="" /><description>Feature suggestion that complements #9502 #11101

Loading shard info during startup of a cluster can be quite slow. PR #11101 addresses one of the pain points by parallelizing the requests for loading shard state info. Still, data needs to be read from disk to compute checksums. Pre-warming the file system cache can dramatically speed this up.

I suggest an endpoint to load shard info that can be manually triggered before doing a full restart of a cluster (e.g. before upgrading). Depending on FS cache usage, the endpoint could also be regularly triggered (which would help in case of an unexpected master node failure).

A test on our production cluster (ES 1.4.5) shows that it takes approx. 3 minutes to load shard stores / states on one of the largest nodes with magnetic disk. With a warmed-up FS cache it takes only 3 seconds.
</description><key id="78135024">11229</key><summary>Endpoint to trigger loading shard info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch-t</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-05-19T15:28:21Z</created><updated>2016-01-19T08:29:16Z</updated><resolved>2016-01-19T08:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T12:03:04Z" id="105220893">Hi @ywelsch-t 

I'm not really understanding why this should be an end point which you have to call manually.  The system has to do the work regardless (and if you don't have enough FS cache it might have to do the work twice).  Why not just let it happen when it needs to?
</comment><comment author="ywelsch-t" created="2015-05-26T15:07:51Z" id="105557214">@clintongormley At the moment, the "loading shard info" operation is only triggered if a cluster starts up (or master failover). The longer it takes to read the shard info, the longer the cluster stays red. In a large cluster, it takes quite a while. The goal of this endpoint is to reduce downtime of a cluster in case of restart/failover/upgrade. It is realized by warming the filesystem caches before shutting down the cluster so that reading the shard info upon restart goes faster.
</comment><comment author="clintongormley" created="2016-01-18T19:30:52Z" id="172629650">@ywelsch do you still think we need to do something here?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Test] set socket reuse address when using unicast discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11228</link><project id="" key="" /><description>When we use unicast discovery in tests, we test for available ports by binding to ports and if
the bind was successful, we use that port. This has a timing issue on certain operating systems
the socket can still be in a TIME_WAIT causing subsequent binds to fail before a certain timeout.
Setting reuse address on the Java socket will instruct the underlying operating system to allow
the socket to be bound to immediately, usually by specifying the SO_REUSEADDR socket option.
</description><key id="78124388">11228</key><summary>[Test] set socket reuse address when using unicast discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T15:00:30Z</created><updated>2015-05-20T12:41:46Z</updated><resolved>2015-05-19T15:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-19T15:00:57Z" id="103538124">@spinscale can you review?
</comment><comment author="spinscale" created="2015-05-19T15:23:19Z" id="103548170">LGTM, we do the same in our transports already (transport &amp; HTTP) already
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Found Elasticsearch with Aliased Index with Routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11227</link><project id="" key="" /><description>We have multiple organization having identical structure of records in Database. We are using ES with aliased index and routing features for each organization. Now, when we clear the ES index and start with clean state, then search records is coming properly from ES. But when we adding multiple organizations, ES is behaving strangely. Sometimes it is sending data of another organization or sometimes it just suppress data. Even more alarming, sometimes it is sending data of record which is not even searched ! "john" it is retuning "Ringo", searching via "Mick" returning "John".

BTW, we are using Symfony2 and FosElasticabundle for all of the above operations.

Further Investigations :

```
Index only one org. Data is coming for that org.
Index another one org. Data is coming for that org too.
Index third org. Data is lost for first org !.
```

Images :

```
Clean state : http://imageshack.com/i/idNW3QVCp
With one org. being indexed: http://imageshack.com/i/f0zTd4YEp, http://imageshack.com/i/idKl1oQrp
With another org. being indexed: http://imageshack.com/i/pbWP7YiNp, http://imageshack.com/i/p8eUMiDxp
With third org. being index: http://imageshack.com/i/ipxfEXo3p, http://imageshack.com/i/eyeS46fGp
Log from Sf2: http://imageshack.com/i/f03ZQ8IOp
```

ES Server status :
{
  "_shards" : {
    "total" : 10,
    "successful" : 5,
    "failed" : 0
  },
  "indices" : {
    "organizations" : {
      "index" : {
        "primary_size_in_bytes" : 54928,
        "size_in_bytes" : 54928
      },
      "translog" : {
        "operations" : 106
      },
      "docs" : {
        "num_docs" : 50,
        "max_doc" : 73,
        "deleted_docs" : 23
      },
      "merges" : {
        "current" : 0,
        "current_docs" : 0,
        "current_size_in_bytes" : 0,
        "total" : 0,
        "total_time_in_millis" : 0,
        "total_docs" : 0,
        "total_size_in_bytes" : 0
      },
      "refresh" : {
        "total" : 125,
        "total_time_in_millis" : 204
      },
      "flush" : {
        "total" : 0,
        "total_time_in_millis" : 0
      },
      "shards" : {
        "0" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ficq9zDSRM2HJTGLk2BZ7g",
            "relocating_node" : null,
            "shard" : 0,
            "index" : "organizations"
          },
          "state" : "STARTED",
          "index" : {
            "size_in_bytes" : 15923
          },
          "translog" : {
            "id" : 1432033150513,
            "operations" : 42
          },
          "docs" : {
            "num_docs" : 21,
            "max_doc" : 21,
            "deleted_docs" : 0
          },
          "merges" : {
            "current" : 0,
            "current_docs" : 0,
            "current_size_in_bytes" : 0,
            "total" : 0,
            "total_time_in_millis" : 0,
            "total_docs" : 0,
            "total_size_in_bytes" : 0
          },
          "refresh" : {
            "total" : 25,
            "total_time_in_millis" : 114
          },
          "flush" : {
            "total" : 0,
            "total_time_in_millis" : 0
          }
        } ],
        "1" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ficq9zDSRM2HJTGLk2BZ7g",
            "relocating_node" : null,
            "shard" : 1,
            "index" : "organizations"
          },
          "state" : "STARTED",
          "index" : {
            "size_in_bytes" : 123
          },
          "translog" : {
            "id" : 1432033150524,
            "operations" : 0
          },
          "docs" : {
            "num_docs" : 0,
            "max_doc" : 0,
            "deleted_docs" : 0
          },
          "merges" : {
            "current" : 0,
            "current_docs" : 0,
            "current_size_in_bytes" : 0,
            "total" : 0,
            "total_time_in_millis" : 0,
            "total_docs" : 0,
            "total_size_in_bytes" : 0
          },
          "refresh" : {
            "total" : 25,
            "total_time_in_millis" : 1
          },
          "flush" : {
            "total" : 0,
            "total_time_in_millis" : 0
          }
        } ],
        "2" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ficq9zDSRM2HJTGLk2BZ7g",
            "relocating_node" : null,
            "shard" : 2,
            "index" : "organizations"
          },
          "state" : "STARTED",
          "index" : {
            "size_in_bytes" : 4782
          },
          "translog" : {
            "id" : 1432033150529,
            "operations" : 8
          },
          "docs" : {
            "num_docs" : 8,
            "max_doc" : 8,
            "deleted_docs" : 0
          },
          "merges" : {
            "current" : 0,
            "current_docs" : 0,
            "current_size_in_bytes" : 0,
            "total" : 0,
            "total_time_in_millis" : 0,
            "total_docs" : 0,
            "total_size_in_bytes" : 0
          },
          "refresh" : {
            "total" : 25,
            "total_time_in_millis" : 11
          },
          "flush" : {
            "total" : 0,
            "total_time_in_millis" : 0
          }
        } ],
        "3" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ficq9zDSRM2HJTGLk2BZ7g",
            "relocating_node" : null,
            "shard" : 3,
            "index" : "organizations"
          },
          "state" : "STARTED",
          "index" : {
            "size_in_bytes" : 123
          },
          "translog" : {
            "id" : 1432033150518,
            "operations" : 0
          },
          "docs" : {
            "num_docs" : 0,
            "max_doc" : 0,
            "deleted_docs" : 0
          },
          "merges" : {
            "current" : 0,
            "current_docs" : 0,
            "current_size_in_bytes" : 0,
            "total" : 0,
            "total_time_in_millis" : 0,
            "total_docs" : 0,
            "total_size_in_bytes" : 0
          },
          "refresh" : {
            "total" : 25,
            "total_time_in_millis" : 0
          },
          "flush" : {
            "total" : 0,
            "total_time_in_millis" : 0
          }
        } ],
        "4" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ficq9zDSRM2HJTGLk2BZ7g",
            "relocating_node" : null,
            "shard" : 4,
            "index" : "organizations"
          },
          "state" : "STARTED",
          "index" : {
            "size_in_bytes" : 33977
          },
          "translog" : {
            "id" : 1432033150546,
            "operations" : 56
          },
          "docs" : {
            "num_docs" : 21,
            "max_doc" : 44,
            "deleted_docs" : 23
          },
          "merges" : {
            "current" : 0,
            "current_docs" : 0,
            "current_size_in_bytes" : 0,
            "total" : 0,
            "total_time_in_millis" : 0,
            "total_docs" : 0,
            "total_size_in_bytes" : 0
          },
          "refresh" : {
            "total" : 25,
            "total_time_in_millis" : 78
          },
          "flush" : {
            "total" : 0,
            "total_time_in_millis" : 0
          }
        } ]
      }
    }
  }
}

It seems ES is overwriting data of existing aliases when another org. is being indexed. But that is deal breaker. Please advice.
</description><key id="78075926">11227</key><summary>Found Elasticsearch with Aliased Index with Routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbhattacharjee</reporter><labels /><created>2015-05-19T12:54:34Z</created><updated>2015-05-25T11:46:06Z</updated><resolved>2015-05-25T11:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:46:05Z" id="105216263">Hi @dbhattacharjee 

This sounds like a misunderstanding of how to use Elasticsearch, but you haven't provided any curl statements to help recreate the issue.  I suggest you ask your question in the forum instead (https://discuss.elastic.co/c/elasticsearch) and that you provide a recreation demonstrating the problem in curl.  That way it'll be easy for others to figure out where you are going wrong.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CorruptedFileTest#testReplicaCorruption fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11226</link><project id="" key="" /><description>This test fails since we get into a cycle of trying to recover the primary from replica 1 and replica 2 which are corrupted, and never end up reaching the non corrupted previous primary shard
</description><key id="78071171">11226</key><summary>CorruptedFileTest#testReplicaCorruption fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T12:39:19Z</created><updated>2015-05-21T07:53:33Z</updated><resolved>2015-05-21T07:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-19T15:53:57Z" id="103560933">I had local failures... need to investigate
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>Check if the index can be opened and is not corrupted on state listing</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>Ensure we mark store as corrupted if we fail to read the segments info</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file></files><comments><comment>[TEST] add await fix for #11226</comment></comments></commit></commits></item><item><title>Remove flushNeeded in favor of IW#hasUncommittedChanges()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11225</link><project id="" key="" /><description>We maintain a boolean that is obsolete since IW maintains the same
property that we can just piggyback on.
</description><key id="78064613">11225</key><summary>Remove flushNeeded in favor of IW#hasUncommittedChanges()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T12:20:05Z</created><updated>2015-06-07T10:04:57Z</updated><resolved>2015-05-19T12:53:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-19T12:20:15Z" id="103465206">@mikemccand can you take a look
</comment><comment author="mikemccand" created="2015-05-19T12:52:54Z" id="103474811">LGTM, nice
</comment><comment author="s1monw" created="2015-05-19T13:05:42Z" id="103478133">this is not possible to remove on 1.6 since there we have still the commit without translog flush :(
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file></files><comments><comment>Merge pull request #11225 from s1monw/remove_flush_needed</comment></comments></commit></commits></item><item><title>Make FilteredQuery a forbidden API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11224</link><project id="" key="" /><description>This commit makes FilteredQuery a forbidden API and also removes some more usage
of the Filter API. There are some remaining code using filters for parent/child
queries but I'm not touching this as they are already being refactored in #6511.
</description><key id="78063822">11224</key><summary>Make FilteredQuery a forbidden API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T12:17:43Z</created><updated>2015-06-07T17:10:16Z</updated><resolved>2015-05-19T13:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-19T12:25:21Z" id="103466868">Overall looks good. i added one comment for a WTF.
</comment><comment author="jpountz" created="2015-05-19T12:27:07Z" id="103467422">Good catch, this was a left-over on my end, I pushed a new commit.
</comment><comment author="rmuir" created="2015-05-19T12:30:02Z" id="103468718">thanks, looks good to me. i still wonder what these methods are doing. We should investigate in a future PR. If you don't know, then I think thats a good sign we should axe them.
</comment><comment author="jpountz" created="2015-05-19T12:37:20Z" id="103470778">If I read https://www.elastic.co/guide/en/elasticsearch/guide/current/_dealing_with_null_values.html#_missing_filter correctly, the goal of this option is to be able to make a difference between a document that is missing a value and a document that was explicitely provided `null` as a value.
</comment><comment author="rmuir" created="2015-05-19T12:37:52Z" id="103470997">Worthless.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/FilteredCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java</file></files><comments><comment>Merge pull request #11224 from jpountz/enhancement/filtered_query_forbidden</comment></comments></commit></commits></item><item><title>Fix highlighting apis to highlight more than 1 doc at once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11223</link><project id="" key="" /><description>Highlighting apis seem to only handle one document at a time, but this totally breaks PostingsHighlighter design on elasticsearch.

The whole idea behind this highlighter is `#queryterms` &lt; `#summarydocs`. But performance totally falls apart because today documents are highlighted one at a time.
</description><key id="78050387">11223</key><summary>Fix highlighting apis to highlight more than 1 doc at once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Highlighting</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label><label>PITA</label></labels><created>2015-05-19T11:32:49Z</created><updated>2016-12-10T17:40:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-20T08:20:05Z" id="103805589">+1 it might be tricky to move the postings hl to highlight multiple documents at a time while maintaining the per document usecase needed with plain and fvh, but it's worth investing in making the postings highlighter work as it should. I will look into this.
</comment><comment author="nik9000" created="2015-05-20T12:47:59Z" id="103872089">Cool! That'll be useful!

The experimental_highlighter will actually need to decide which order to do
them in based on the mapping definition so I _think_ the best thing to do
is to push that decision to the highlighters - like push the whole loop
over the documents into them.

For the postings highlighter I think the optimal ordering is segments,
fields, then docs, right? Or is it just fields then docs?

On Wed, May 20, 2015 at 4:20 AM, Luca Cavanna notifications@github.com
wrote:

&gt; +1 it might be tricky to move the postings hl to highlight multiple
&gt; documents at a time while maintaining the per document usecase needed with
&gt; plain and fvh, but it's worth investing in making the postings highlighter
&gt; work as it should. I will look into this.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11223#issuecomment-103805589
&gt; .
</comment><comment author="rmuir" created="2015-05-20T14:05:55Z" id="103899286">postingshighlighter in lucene takes your ordinary `TopDocs` and does any internal resorting itself. You should also give it all the fields to highlight at once too. It knows how to do the right thing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove need for generics in ContextAndHeaderHolder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11222</link><project id="" key="" /><description>Generics were only needed for setting a header, that returned
the object being set (most likely the request), but none of
the other methods did this.
</description><key id="78044968">11222</key><summary>Remove need for generics in ContextAndHeaderHolder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T11:17:58Z</created><updated>2015-06-07T10:14:00Z</updated><resolved>2015-05-20T08:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-19T17:28:41Z" id="103602816">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow reallocation on node where freediskthreshold is hit if shard size is the same</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11221</link><project id="" key="" /><description>From the below repro it looks like cluster.routing.allocation.decider just look at disk space usage, but doesn't take into account the size of the shards already existing into a node coming back online?
If a node has its shards all allocated, hits free disk threshold, then get restarted, all the existing shards will not be reallocated again.

The above behaviour does make fully sense to me if there is data continuously coming into the cluster, hence there will be a delta of new data between the nodes, once the node which hit the free disk threshold will come back online, and it will be safe and wise not to allocate shards there.

Though If there is no data coming and if it can be measured that shard have same sized (delta is 0), it should be safer to reallocate the shards to the node which hit the free disk threshold. Not sure if allocation.decider just checks sigar output or actually goes deeper and look at the size of the shards on node.

Most likely this is easier to be said than implemented, though I just wanted to raise the discussion, should there will be a way in the future to handle this specific scenario in a better way. 

2 nodes cluster , v1.5.2, Marvel installed (2 separate VMs: node-1 192.168.0.15, node-2 192.168.0.16)

node 1 - config

```
[elk@localhost elasticsearch-1.5.2]$ /sbin/ifconfig | egrep 192
        inet 192.168.0.15  netmask 255.255.255.0  broadcast 192.168.0.255

[elk@localhost elasticsearch-1.5.2]$ tail -5 config/elasticsearch.yml 
cluster.name: "test"
node.name: "node-1"
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["192.168.0.16"]

[elk@localhost elasticsearch-1.5.2]$ df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   41G  4.2G   36G  11% /
devtmpfs                 2.0G     0  2.0G   0% /dev
tmpfs                    2.0G   88K  2.0G   1% /dev/shm
tmpfs                    2.0G  8.8M  2.0G   1% /run
tmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/mapper/centos-home   20G   64M   20G   1% /home
/dev/sda1                497M  143M  354M  29% /boot
```

node 2 - config

```
[elk@localhost elasticsearch-1.5.2]$ /sbin/ifconfig | egrep 192
        inet 192.168.0.16  netmask 255.255.255.0  broadcast 192.168.0.255

[elk@localhost elasticsearch-1.5.2]$ tail -6 config/elasticsearch.yml 
cluster.name: "test"
node.name: "node-1"
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["192.168.0.15"]

[elk@localhost elasticsearch-1.5.2]$ df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   41G  4.2G   36G  11% /
devtmpfs                 2.0G     0  2.0G   0% /dev
tmpfs                    2.0G   88K  2.0G   1% /dev/shm
tmpfs                    2.0G  8.7M  2.0G   1% /run
tmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/mapper/centos-home   20G   64M   20G   1% /home
/dev/sda1                497M  143M  354M  29% /boot
```

node1 - start

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; ./bin/elasticsearch -Des.logger.level=DEBUG -d 
Mon May 18 14:12:27 CEST 2015
```

node2 - start

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; ./bin/elasticsearch -Des.logger.level=DEBUG -d
Mon May 18 14:12:28 CEST 2015
```

node 1 master

```
[elk@localhost elasticsearch-1.5.2]$ curl localhost:9200/_cat/nodes?v
host                  ip        heap.percent ram.percent load node.role master name   
localhost.localdomain 127.0.0.1            7          27 0.00 d         *      node-1 
localhost.localdomain 127.0.0.1            8          24 0.07 d         m      node-2
```

start logstash and index data

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/logstash*/_search?search_type=count&amp;pretty"
Mon May 18 14:22:15 CEST 2015
{
  "took" : 13,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 64294,
    "max_score" : 0.0,
    "hits" : [ ]
  }
}
```

stop logstash - no more data coming in from now on

node 1 - index created

```
[2015-05-18 14:18:08,823][DEBUG][cluster.service          ] [node-1] processing [create-index [logstash-2015.05.18], cause [auto(bulk api)]]: execute
[2015-05-18 14:18:08,825][DEBUG][indices                  ] [node-1] creating Index [logstash-2015.05.18], shards [5]/[1]
[2015-05-18 14:18:08,848][DEBUG][index.mapper             ] [node-1] [logstash-2015.05.18] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null] [node-1] [.marvel-kibana][0] updating index_buffer_size from [7.8mb] to [4.6mb]
```

node 2 index created

```
[2015-05-18 14:18:08,873][DEBUG][discovery.zen.publish    ] [node-2] received cluster state version 7
[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: execute
[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] cluster state updated, version [7], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]
[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] set local cluster state to version 7
[2015-05-18 14:18:08,875][DEBUG][indices.cluster          ] [node-2] [logstash-2015.05.18] creating index
[2015-05-18 14:18:08,875][DEBUG][indices                  ] [node-2] creating Index [logstash-2015.05.18], shards [5]/[1]
[2015-05-18 14:18:08,899][DEBUG][index.mapper             ] [node-2] [logstash-2015.05.18] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]
```

cluster is green, all shards STARTED

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cluster/health?pretty" 
Mon May 18 14:22:33 CEST 2015
{
  "cluster_name" : "test",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 6,
  "active_shards" : 12,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "number_of_pending_tasks" : 0
}

[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cat/shards?pretty" 
Mon May 18 14:23:25 CEST 2015
logstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 
logstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 
logstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 
.marvel-2015.05.18  0 r STARTED   479 1.7mb 127.0.0.1 node-2 
.marvel-2015.05.18  0 p STARTED   479 1.7mb 127.0.0.1 node-1 
```

node 2 - create large file to hit disk threshold

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; df -h
Mon May 18 14:23:54 CEST 2015
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   41G  4.2G   36G  11% /
devtmpfs                 2.0G     0  2.0G   0% /dev
tmpfs                    2.0G   88K  2.0G   1% /dev/shm
tmpfs                    2.0G  8.8M  2.0G   1% /run
tmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/mapper/centos-home   20G   64M   20G   1% /home
/dev/sda1                497M  143M  354M  29% /boot
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; fallocate -l 30G /tmp/largefile.img
Mon May 18 14:24:14 CEST 2015
```

node 1(master) sees the free disk space threshold hit

```
[2015-05-18 14:24:34,830][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [l4OMm3XtReStyBfRFtTZwA][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node
```

Everything still fine health/shard wise

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cluster/health?pretty" 
Mon May 18 14:25:15 CEST 2015
{
  "cluster_name" : "test",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 6,
  "active_shards" : 12,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "number_of_pending_tasks" : 0
}

[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cat/shards?pretty" 
Mon May 18 14:25:26 CEST 2015
logstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 
logstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 
logstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 
.marvel-2015.05.18  0 r STARTED   551 1.8mb 127.0.0.1 node-2 
.marvel-2015.05.18  0 p STARTED   551 2.5mb 127.0.0.1 node-1 
```

disable allocation

```
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disable_allocation": true
  }
}
```

node 1 updates cluster state

```
[2015-05-18 14:26:31,698][DEBUG][cluster.service          ] [node-1] processing [cluster_update_settings]: execute
[2015-05-18 14:26:31,699][DEBUG][cluster.service          ] [node-1] cluster state updated, version [17], source [cluster_update_settings]
[2015-05-18 14:26:31,699][DEBUG][cluster.service          ] [node-1] publishing cluster state version 17
[2015-05-18 14:26:31,721][DEBUG][cluster.service          ] [node-1] set local cluster state to version 17
[2015-05-18 14:26:31,723][INFO ][cluster.routing.allocation.decider] [node-1] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]
```

node 2 receives new state

```
[2015-05-18 14:26:31,709][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]
[2015-05-18 14:26:31,721][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 17)
```

everything still ok health/shards wise

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cluster/health?pretty" 
Mon May 18 14:28:02 CEST 2015
{
  "cluster_name" : "test",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 6,
  "active_shards" : 12,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "number_of_pending_tasks" : 0
}
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cat/shards?pretty" 
Mon May 18 14:28:04 CEST 2015
logstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 
logstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 
logstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 
logstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 
.marvel-2015.05.18  0 r STARTED   648 2.3mb 127.0.0.1 node-2 
.marvel-2015.05.18  0 p STARTED   648 2.2mb 127.0.0.1 node-1 
```

node 2 - Shutdown

```
[2015-05-18 14:29:19,622][DEBUG][indices                  ] [node-2] [logstash-2015.05.18] closing index service (reason [shutdown])
[2015-05-18 14:29:19,623][DEBUG][indices                  ] [node-2] [logstash-2015.05.18] closed... (reason [shutdown])
[2015-05-18 14:29:19,629][INFO ][node                     ] [node-2] stopped
[2015-05-18 14:29:19,629][INFO ][node                     ] [node-2] closing ...
[2015-05-18 14:29:19,635][INFO ][node                     ] [node-2] closed
```

node 1 (master) sees node 2 shutdown 

```
[2015-05-18 14:29:19,394][INFO ][cluster.service          ] [node-1] removed {[node-2][l4OMm3XtReStyBfRFtTZwA][localhost.localdomain][inet[192.168.0.16/192.168.0.16:9300]],}, reason: zen-disco-node_left([node-2][l4OMm3XtReStyBfRFtTZwA][localhost.localdomain][inet[/192.168.0.16:9300]])
[2015-05-18 14:29:19,394][DEBUG][cluster.service          ] [node-1] publishing cluster state version 18
[2015-05-18 14:29:19,394][DEBUG][cluster.service          ] [node-1] set local cluster state to version 18
```

node 2 - Restart

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; ./bin/elasticsearch -Des.logger.level=DEBUG -d
Mon May 18 14:30:16 CEST 2015

[2015-05-18 14:30:23,388][DEBUG][discovery.zen            ] [node-2] got first state from fresh master [60ojafViSJCoo3pt60ihmw]
[2015-05-18 14:30:23,388][DEBUG][cluster.service          ] [node-2] cluster state updated, version [19], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]
[2015-05-18 14:30:23,389][INFO ][cluster.service          ] [node-2] detected_master [node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]], added {[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]],}, reason: zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])
[2015-05-18 14:30:23,389][DEBUG][cluster.service          ] [node-2] set local cluster state to version 19
[2015-05-18 14:30:23,393][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]
[2015-05-18 14:30:23,506][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 19)
[2015-05-18 14:30:23,518][DEBUG][cluster.service          ] [node-2] processing [finalize_join ([node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]])]: execute
[2015-05-18 14:30:23,518][DEBUG][cluster.service          ] [node-2] processing [finalize_join ([node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]])]: no change in cluster_state
[2015-05-18 14:30:23,526][INFO ][http                     ] [node-2] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.16:9200]}
[2015-05-18 14:30:23,527][INFO ][node                     ] [node-2] started
[2015-05-18 14:30:30,218][DEBUG][marvel.agent.exporter    ] [node-2] deriving host setting from httpServer
[2015-05-18 14:30:30,288][DEBUG][marvel.agent.exporter    ] [node-2] accepting existing index template (version [6], needed [6])
```

node 1 - Sees again node 2 - however given disk threshold is hit , it will prevent the existing shards on node 2 to be re-allocated

```
[2015-05-18 14:30:23,356][DEBUG][transport.netty          ] [node-1] connected to node [[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]]
[2015-05-18 14:30:23,362][DEBUG][cluster.service          ] [node-1] processing [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]: execute
[2015-05-18 14:30:23,362][DEBUG][cluster.service          ] [node-1] cluster state updated, version [19], source [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]
[2015-05-18 14:30:23,363][INFO ][cluster.service          ] [node-1] added {[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]],}, reason: zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])
[2015-05-18 14:30:23,363][DEBUG][cluster.service          ] [node-1] publishing cluster state version 19
[2015-05-18 14:30:23,506][DEBUG][cluster.service          ] [node-1] set local cluster state to version 19
[2015-05-18 14:30:23,507][DEBUG][cluster                  ] [node-1] data node was added, retrieving new cluster info
[2015-05-18 14:30:23,507][DEBUG][cluster.service          ] [node-1] processing [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]: done applying updated cluster_state (version: 19)
[2015-05-18 14:30:23,508][DEBUG][river.cluster            ] [node-1] processing [reroute_rivers_node_changed]: execute
[2015-05-18 14:30:23,508][DEBUG][river.cluster            ] [node-1] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-05-18 14:30:23,523][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [Vobb_8yRQ5WZJKmJyAGWfw][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node
[2015-05-18 14:30:24,818][DEBUG][cluster.service          ] [node-1] processing [routing-table-updater]: execute
[2015-05-18 14:30:24,820][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,820][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)
[2015-05-18 14:30:24,822][DEBUG][cluster.service          ] [node-1] processing [routing-table-updater]: no change in cluster_state
[2015-05-18 14:30:34,834][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [Vobb_8yRQ5WZJKmJyAGWfw][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node
```

reenable allocation

```
{
   "acknowledged": true,
   "persistent": {
      "cluster": {
         "routing": {
            "allocation": {
               "disable_allocation": "false"
            }
         }
      }
   },
   "transient": {}
}
```

node 1

```
2015-05-18 14:31:36,142][INFO ][cluster.routing.allocation.decider] [node-1] updating [cluster.routing.allocation.disable_allocation] from [true] to [false]
.
.
.
[2015-05-18 14:31:36,156][DEBUG][cluster.routing.allocation.decider] [node-1] Less than the required 15% free disk threshold (14.6% free) on node [Vobb_8yRQ5WZJKmJyAGWfw], preventing allocation
[2015-05-18 14:31:36,156][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54326665423653% free disk (38496071680 bytes)
[2015-05-18 14:31:36,158][DEBUG][cluster.routing.allocation.decider] [node-1] Less than the required 15% free disk threshold (14.6% free) on node [Vobb_8yRQ5WZJKmJyAGWfw], preventing allocation
[2015-05-18 14:31:36,158][DEBUG][cluster.service          ] [node-1] processing [reroute_after_cluster_update_settings]: no change in cluster_state
```

node 2

```
[2015-05-18 14:31:36,088][DEBUG][discovery.zen.publish    ] [node-2] received cluster state version 20
[2015-05-18 14:31:36,089][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: execute
[2015-05-18 14:31:36,090][DEBUG][cluster.service          ] [node-2] cluster state updated, version [20], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]
[2015-05-18 14:31:36,090][DEBUG][cluster.service          ] [node-2] set local cluster state to version 20
[2015-05-18 14:31:36,091][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [true] to [false]
[2015-05-18 14:31:36,140][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 20)
```

node 2 has all unassigned shards

```
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cluster/health?pretty" 
Mon May 18 14:32:29 CEST 2015
{
  "cluster_name" : "test",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 6,
  "active_shards" : 6,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 6,
  "number_of_pending_tasks" : 0
}
[elk@localhost elasticsearch-1.5.2]$ date &amp;&amp; curl -XGET "localhost:9200/_cat/shards?pretty" 
Mon May 18 14:32:31 CEST 2015
logstash-2015.05.18 4 p STARTED    12878 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 4 r UNASSIGNED                              
logstash-2015.05.18 0 p STARTED    12868 1.8mb 127.0.0.1 node-1 
logstash-2015.05.18 0 r UNASSIGNED                              
logstash-2015.05.18 3 p STARTED    12825 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 3 r UNASSIGNED                              
logstash-2015.05.18 1 p STARTED    12852 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 1 r UNASSIGNED                              
logstash-2015.05.18 2 p STARTED    12871 1.9mb 127.0.0.1 node-1 
logstash-2015.05.18 2 r UNASSIGNED                              
.marvel-2015.05.18  0 p STARTED      810 2.7mb 127.0.0.1 node-1 
.marvel-2015.05.18  0 r UNASSIGNED                              
```
</description><key id="78030446">11221</key><summary>Allow reallocation on node where freediskthreshold is hit if shard size is the same</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-05-19T10:30:49Z</created><updated>2016-01-18T19:29:40Z</updated><resolved>2016-01-18T19:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:33:27Z" id="105214622">@dakrone do you have any thoughts about this one?
</comment><comment author="dakrone" created="2015-05-28T18:00:27Z" id="106533605">In order to allow this I believe the DiskThresholdDecider would need to know whether the shard was previously assigned and valid on the node, which we currently don't have. I think it would be a nice optimization, but would require something like the synced flush or maybe sequence ids to work.

Alternatively, we could make the information about shard state versions available to the decider, but we don't currently have that plumbing (hopefully in the future?)
</comment><comment author="clintongormley" created="2016-01-18T19:29:40Z" id="172629222">Synced flush now implements what was suggested.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11220</link><project id="" key="" /><description>Fix [REST in the uri
</description><key id="78021861">11220</key><summary>[docs] fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mendab1e</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-05-19T10:02:41Z</created><updated>2015-06-19T07:14:37Z</updated><resolved>2015-06-19T07:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:32:02Z" id="105214486">Hi @mendab1e 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="mendab1e" created="2015-05-31T13:16:11Z" id="107176159">Hi @clintongormley, I have signed the CLA.
</comment><comment author="clintongormley" created="2015-06-02T17:43:03Z" id="108027915">thanks @mendab1e - merged
</comment><comment author="clintongormley" created="2015-06-19T07:14:36Z" id="113404302">Closed by 6812ed0bb69dedc16dd3b8acb9f97b264f9b50af
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch 1.5.2 Cluster name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11219</link><project id="" key="" /><description>Cluster name is not being changed from default "Elasticsearch" to custom name.

ES Version 1.5.2
</description><key id="77997179">11219</key><summary>ElasticSearch 1.5.2 Cluster name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sandesh2014</reporter><labels><label>feedback_needed</label></labels><created>2015-05-19T08:44:44Z</created><updated>2015-05-27T10:00:26Z</updated><resolved>2015-05-27T10:00:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-20T10:13:03Z" id="103834648">Do you mean that changing the `cluster.name` setting in `elasticsearch.yml` has no effect?
</comment><comment author="boysmovie" created="2015-05-27T02:26:15Z" id="105727264">I'm on 1.5.2 and it seems to be working fine.  @sandesh2014 Any specifics you could add?
</comment><comment author="clintongormley" created="2015-05-27T10:00:25Z" id="105848970">No more info provided. Most likely a config issue.  I suggest asking for advice in the forum instead: https://discuss.elastic.co/c/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to lucene-5.2.0-snapshot-1680200.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11218</link><project id="" key="" /><description /><key id="77993556">11218</key><summary>Upgrade to lucene-5.2.0-snapshot-1680200.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T08:33:45Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-19T08:34:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-19T08:34:13Z" id="103398812">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/common/lucene/IndexCacheableQueryTests.java</file></files><comments><comment>Merge pull request #11218 from jpountz/upgrade/lucene-5.2.0-snapshot-1680200</comment></comments></commit></commits></item><item><title>Exception in Logstash 1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11217</link><project id="" key="" /><description>While reloading data (which worked ok on 1.4.2), this occured
Exception in filterworker {"exception"=&gt;#&lt;NoMethodError: undefined method `to_hash' for "*AS18144 Energia Communications,Inc.":String&gt;, "backtrace"=&gt;["/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-filter-geoip-0.1.9/lib/logstash/filters/geoip.rb:119:in `filter'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-1.5.0-java/lib/logstash/filters/base.rb:162:in`multi_filter'", "org/jruby/RubyArray.java:1613:in `each'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-1.5.0-java/lib/logstash/filters/base.rb:159:in`multi_filter'", "(eval):569:in `cond_func_7'", "org/jruby/RubyArray.java:1613:in`each'", "(eval):561:in `cond_func_7'", "(eval):412:in`filter_func'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-1.5.0-java/lib/logstash/pipeline.rb:219:in `filterworker'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-1.5.0-java/lib/logstash/pipeline.rb:156:in`start_filters'"], :level=&gt;:error}
</description><key id="77983524">11217</key><summary>Exception in Logstash 1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aimdev</reporter><labels /><created>2015-05-19T07:59:05Z</created><updated>2015-05-25T11:23:05Z</updated><resolved>2015-05-25T11:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:23:04Z" id="105213595">Hi @aimdev 

Please open this issue on the https://github.com/logstash-plugins/logstash-filter-geoip issues list instead
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove SmartNameFieldMappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11216</link><project id="" key="" /><description>This change simplifies the users of the mapper service to no
longer have access to multiple fields for a single name. While
FieldMappersLookup still stores and gives access to multiple
fields for a single name, the current users of SmartNameFieldMappers already all
assumed a single field. The arbitrary selection of that field
when multiple exist is now isolated to the mapper service.
</description><key id="77947100">11216</key><summary>Remove SmartNameFieldMappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-19T05:54:26Z</created><updated>2015-06-07T10:52:11Z</updated><resolved>2015-05-19T17:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-19T06:49:13Z" id="103369754">LGTM, I like it
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #11216 from rjernst/remove/smart-field-mappers</comment></comments></commit></commits></item><item><title>Docs: Fix typo on percolate_format description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11215</link><project id="" key="" /><description /><key id="77874237">11215</key><summary>Docs: Fix typo on percolate_format description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edgurgel</reporter><labels /><created>2015-05-19T00:53:38Z</created><updated>2015-05-26T02:05:59Z</updated><resolved>2015-05-25T11:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:18:54Z" id="105213257">thanks @edgurgel - merged
</comment><comment author="edgurgel" created="2015-05-26T02:05:58Z" id="105359259">:+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix typo on percolate_format description</comment></comments></commit></commits></item><item><title>Snapshot/Restore: Add repository-level throttling to the snapshot and restore operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11214</link><project id="" key="" /><description>In #4855 we added node-level throttling to snapshot and restore operation. While this helps to prevent an individual node from being overloaded, it doesn't protect overloading of the repository or the network that can be targeted by unpredictable load ranging from load from a single node to load from all data nodes in the cluster. See #11148 for an example of the issues that this is causing.
</description><key id="77817767">11214</key><summary>Snapshot/Restore: Add repository-level throttling to the snapshot and restore operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-05-18T21:47:38Z</created><updated>2016-01-18T19:27:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11213</link><project id="" key="" /><description>typo
</description><key id="77803077">11213</key><summary>[docs] typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2015-05-18T20:56:32Z</created><updated>2015-06-01T17:27:44Z</updated><resolved>2015-06-01T17:27:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T11:12:55Z" id="105211815">thanks @missinglink - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure java.io.tmpdir is created earlier in tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11212</link><project id="" key="" /><description>When testing, each jvm gets its own tmpdir set, so it may not exist
at all. A Lucene test rule ensures its created, but some tests (I
am looking at you rest tests) do a bunch of file stuff in static {},
in that case because its a parameterized test. And if you try to
extend it, it will fail if security manager is disabled...

Currently we ensure(java.io.tmpdir) very early when tests are running under
security manager, but otherwise we don't and it won't happen until the
test rule fires. So just do it early always.
</description><key id="77797622">11212</key><summary>Ensure java.io.tmpdir is created earlier in tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T20:41:27Z</created><updated>2015-05-19T00:50:40Z</updated><resolved>2015-05-19T00:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-18T21:03:03Z" id="103210245">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Merge pull request #11212 from rmuir/java.io.tmpdir.early</comment></comments></commit></commits></item><item><title>Too many TCP Connection Issue - transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11211</link><project id="" key="" /><description>Hi i use java transport client to connect to es- 1.4.2.
i use many connection configuration but i found that a single es query open more than 50 TCP connection , which never close until i stop tomcat.
connection increase on each query so if i query 4 times then more than 200 tcp connection is open and never close. so by this way i soon reached resource limit. and my tomcat kill by linux OOM ( out of memory ).
can anyone help me to figure it out , how to close a tcp connection after query?
i try many configuration but still no luck.
</description><key id="77729733">11211</key><summary>Too many TCP Connection Issue - transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linuzvipin</reporter><labels /><created>2015-05-18T17:06:04Z</created><updated>2015-05-25T10:50:10Z</updated><resolved>2015-05-25T10:50:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-25T10:50:09Z" id="105204664">Hi @linuzvipin 

It sounds like you keep creating new transport clients instead of reusing existing ones.  I suggest you ask about the proper way to use the client on the forum rather than here: https://discuss.elastic.co/c/elasticsearch

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`detect_noop` now understands `null` as a valid value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11210</link><project id="" key="" /><description>If the source contrains a null value for a field then detect_noop should
consider setting it to null again to be a noop.

Closes #11208
</description><key id="77725958">11210</key><summary>`detect_noop` now understands `null` as a valid value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T16:53:11Z</created><updated>2015-06-07T17:55:40Z</updated><resolved>2015-05-19T06:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-18T17:28:20Z" id="103142357">This looks good to me, I'll merge it tomorrow if nobody else has objections.
</comment><comment author="nik9000" created="2015-05-18T17:45:14Z" id="103146162">Updated based on your comment. I can squash the changes if you want.
</comment><comment author="jpountz" created="2015-05-18T17:48:25Z" id="103146935">That would be great. :-)
</comment><comment author="nik9000" created="2015-05-18T19:14:35Z" id="103179643">Squashed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/test/java/org/elasticsearch/update/UpdateNoopTests.java</file></files><comments><comment>Merge pull request #11210 from nik9000/detect_noop_null</comment></comments></commit></commits></item><item><title>Make DocumentMapper.refreshSource() private.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11209</link><project id="" key="" /><description>This method should not be public, we should refresh the source automatically
when we change mappings.
</description><key id="77691168">11209</key><summary>Make DocumentMapper.refreshSource() private.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T15:09:20Z</created><updated>2015-06-07T10:52:19Z</updated><resolved>2015-05-19T06:49:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-18T23:34:12Z" id="103265085">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/test/java/org/elasticsearch/index/mapper/camelcase/CamelCaseFieldNameTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file></files><comments><comment>Merge pull request #11209 from jpountz/fix/documentmapper_refreshsource_private</comment></comments></commit></commits></item><item><title>detect_noop doesn't work with null values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11208</link><project id="" key="" /><description>The `detect_noop` flag doesn't work if a field contains a `null` value:

```
PUT t/t/1
{
  "foo": "baz",
  "bar": "baz"
}
```

This update request always returns the same version:

```
POST t/t/1/_update
{
  "doc": {
    "foo": "baz",
    "bar": "baz"
  },
  "detect_noop": true
}
```

While this one always returns a new version:

```
POST t/t/1/_update
{
  "doc": {
    "foo": "baz",
    "bar": null
  },
  "detect_noop": true
}
```
</description><key id="77687154">11208</key><summary>detect_noop doesn't work with null values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-05-18T14:57:34Z</created><updated>2015-05-19T06:38:50Z</updated><resolved>2015-05-19T06:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-18T15:57:04Z" id="103112848">Eww. I can have a look in a bit.
</comment><comment author="nik9000" created="2015-05-18T16:53:53Z" id="103128203">low hanging fruit indeed.
</comment><comment author="ppf2" created="2015-05-18T17:11:11Z" id="103136760">@nik9000 @clintongormley  Thx for the super quick turnaround on this :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/test/java/org/elasticsearch/update/UpdateNoopTests.java</file></files><comments><comment>detect_noop now understands null as a valid value</comment></comments></commit></commits></item><item><title>No need to send mappings to the master node on phase 2.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11207</link><project id="" key="" /><description>Now that mapping updates are synchronous, it is not necessary to send mappings
to the master node during the recovery process anymore: they will already be on
the master node since we ensure mappings are on the master node before indexing.
</description><key id="77681444">11207</key><summary>No need to send mappings to the master node on phase 2.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T14:41:31Z</created><updated>2015-06-08T15:43:45Z</updated><resolved>2015-05-18T14:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-18T14:48:46Z" id="103084173">awesome LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file></files><comments><comment>Merge pull request #11207 from jpountz/fix/recovery_no_mapping_update</comment></comments></commit></commits></item><item><title>Contributing to elasticsearch page has html all over "Bug Reports"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11206</link><project id="" key="" /><description>Have a look at https://www.elastic.co/contributing-to-elasticsearch under the bug reports section. I think that is supposed to be highlighted bash script but its all `&lt;pre&gt;`ed now.
</description><key id="77680857">11206</key><summary>Contributing to elasticsearch page has html all over "Bug Reports"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-05-18T14:39:53Z</created><updated>2015-05-25T10:34:56Z</updated><resolved>2015-05-25T10:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-18T14:40:51Z" id="103082165">Ping @clintongormley  I think this is yours?
</comment><comment author="clintongormley" created="2015-05-25T10:34:55Z" id="105201249">thanks @nik9000 - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make mapping updates atomic wrt document parsing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11205</link><project id="" key="" /><description>When mapping updates happen concurrently with document parsing, bad things can
happen. For instance, when applying a mapping update we first update the Mapping
object which is used for parsing and then FieldNameAnalyzer which is used by
IndexWriter for analysis. So if you are unlucky, it could happen that a document
was parsed successfully without introducing dynamic updates yet IndexWriter does
not see its analyzer yet.

In order to fix this issue, mapping updates are now protected by a write lock
and document parsing is protected by the read lock associated with this write
lock. This ensures that no documents will be parsed while a mapping update is
being applied, so document parsing will either see none of the update or all of
it.
</description><key id="77670863">11205</key><summary>Make mapping updates atomic wrt document parsing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T14:12:02Z</created><updated>2015-06-07T10:52:27Z</updated><resolved>2015-05-21T16:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-18T18:20:24Z" id="103158203">LGTM. Any way to test this?
</comment><comment author="jpountz" created="2015-05-21T15:39:45Z" id="104323184">@s1monw @rjernst We now have a working test, if you comment out the locking on DocumentParser, it fails.
</comment><comment author="jpountz" created="2015-05-21T16:13:15Z" id="104337413">Merged manually.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>search_exists should be consistent with other *_exists methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11204</link><project id="" key="" /><description>- use `HEAD` request type (which also means to allow for `POST` when http client cannot send `HEAD` with body, as well as the `source` parameter)
- the client API should only return `true`/`false`
- the yaml tests need to be changed to reflect that
</description><key id="77663999">11204</key><summary>search_exists should be consistent with other *_exists methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>discuss</label><label>enhancement</label></labels><created>2015-05-18T13:49:08Z</created><updated>2016-03-10T18:37:41Z</updated><resolved>2016-01-18T19:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-18T13:55:21Z" id="103068168">Any reason not to have a HEAD request for a SEARCH default to search_exists? Maybe its just too weird do a HEAD with a body.

By the client API do you mean the Java api? All of them?
</comment><comment author="HonzaKral" created="2015-05-18T13:57:03Z" id="103068548">Unfortunately we cannot rely on `HEAD` working with body, the specs allow for it, but many clients and proxies don't so we need to be cautious and have a separate endpoint just to be sure.

I mean all of them, they should be consistent in this, but those will be updated once the java client is and the yaml test suite is changed to reflect that.
</comment><comment author="javanna" created="2015-05-18T14:05:34Z" id="103070229">Search exists api is different compared to search. Search (as well as count) does allow you to set `terminate_after`, but it will still go to each of the shard in the context of the search request and execute the query there, the `terminate_after` is effectively per shard. Search exists returns the response after the first shard reports existing documents, instead of blocking for responses from all the shards. This is the main reason why this is a separate endpoint I think. 

Supporting HEAD for consistency sounds good but this api is different compared to other exists\* api, as it has to accept a body (which can be provided as query_string encoded `source` parameter too though to work around any potential proxy/client issue).
</comment><comment author="javanna" created="2015-05-18T14:11:39Z" id="103071997">@HonzaKral I didn't notice at first your point around supporting `POST` too, makes perfect sense. +1
</comment><comment author="s1monw" created="2015-05-22T08:24:42Z" id="104564052">yeah lets do it - I think this is relatively simple to add. We have `org.elasticsearch.rest.action.exists.RestExistsAction` that is registered in `org.elasticsearch.rest.action.search.RestSearchAction` but only for `GET` and `POST`. If we register it for `HEAD` as well as omit the response it does when it is a `HEAD` request we are good to go? 

For the response part I think we can just omit the response if we `request.method() == RestRequest.Method.HEAD` 
</comment><comment author="s1monw" created="2015-05-22T08:27:32Z" id="104566542">I know @markharwood mentioned that @ESamir was looking for a low hanging fruit, this one could be one?
</comment><comment author="javanna" created="2015-10-21T17:02:09Z" id="149962895">I marked this back for discussion. The search/exists api has been removed from master and deprecated in 2.1. We can now discuss whether we want `_search` to work with HEAD or not. It currently doesn't. Seems like a different endpoint than `_search` would still be required for the reasons stated by @HonzaKral above though. And it would be the first api that responds to HEAD but accepts a body, which many clients don't support.
</comment><comment author="clintongormley" created="2015-10-23T17:45:14Z" id="150643900">&gt; And it would be the first api that responds to HEAD but accepts a body, which many clients don't support.

If it is going to be part of the Elasticearch API, then it needs to be part of the REST tests, which means that all clients should be able to support it.  This makes me lean towards: if you want to add a shortcut to a particular client then go for it, but it shouldn't be part of the official API.
</comment><comment author="clintongormley" created="2016-01-18T19:27:02Z" id="172628747">Search exists has been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the `ignore_conflicts` option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11203</link><project id="" key="" /><description>Mappings conflicts should not be ignored. If I read the history correctly, this
option was added when a mapping update to an existing field was considered a
conflict, even if the new mapping was exactly the same. Now that mapping updates
are smart enough to detect conflicting options, we don't need an option to
ignore conflicts.
</description><key id="77657211">11203</key><summary>Remove the `ignore_conflicts` option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T13:29:53Z</created><updated>2015-06-07T10:52:35Z</updated><resolved>2015-05-19T08:03:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-18T18:25:49Z" id="103161056">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file></files><comments><comment>Merge pull request #11203 from jpountz/fix/remove_ignore_conflicts</comment></comments></commit></commits></item><item><title>Node restart causes ClusterBlockException when Transport Client is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11202</link><project id="" key="" /><description>## Abstract

Transport Client can work correctly when shutdown a connecting node, however when the node starts ClusterBlockException raised.
## Environment
- cluster
  - version: 1.4.4
  - 3 nodes(phyical server)
  - Index
    - big index data(20gb, i couldnt reproduce this issue with no index data)
    - number_of_replicas: 1
    - number_of_shards: 9
- application
  - version: 1.4.4
  - Java using Transport Client
  - specify all IPs of 3 nodes
## Reproduce process
1. query through Transport Client continuosly, repeatedly(any query is ok. ex: search, get, index...)
2. shutdown a node (shutdown API or initd script)
   - In this phase sometime searching method returns a result that contains failed shards. But this is NOT fatail. I retry search and can get collect result.
3. start the node
4. Transport Client sometime causes following Exception. We cannot catch it bacause it happens in Elasticsearch Client Thread pool maybe.

```
[error] (run-main-0) org.elasticsearch.transport.RemoteTransportException: [es-node01][inet[/192.168.10.42:9300]][indices:data/read/search]
org.elasticsearch.transport.RemoteTransportException: [es-node01][inet[/192.168.10.42:9300]][indices:data/read/search]
Caused by: org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:151)
        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:141)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:111)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:70)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:61)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:51)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:100)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
        at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:63)
        at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:51)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```
## IMO

https://groups.google.com/d/msg/elasticsearch/qBw6mgCO_Nk/N77Oe9X2YdIJ is right.
Transport Client accesses starting client before it connects cluster.
But, I cannot find any settings to solve this.
I tried [Sniff Setting](https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.x/client.html#transport-client) but the exception is still caused.

By deleting TransportAddress of restarting node, exception didn't happen finally.
However it needs application restart.
I want to do rolling update elasticsearch cluster without application server shutdown.

Could anyone help us?
## Other Info
- I couldnt reproduce this issue using Node Client
- We cannot use Node Client by production enviroinment restriction (at least now, im trying to solve this)
- Almost same issue: https://groups.google.com/forum/#!msg/elasticsearch/qBw6mgCO_Nk/BLuikJhaiBsJ
</description><key id="77650429">11202</key><summary>Node restart causes ClusterBlockException when Transport Client is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bigwheel</reporter><labels /><created>2015-05-18T13:07:43Z</created><updated>2016-08-16T10:44:26Z</updated><resolved>2015-06-19T02:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gongweixin" created="2015-06-18T15:15:11Z" id="113189012">I also got this exception, you now have a solution for it ？
@bigwheel 
</comment><comment author="bigwheel" created="2015-06-18T18:31:56Z" id="113251633">@gongweixin 
After I wrote this issue, we found a mistake in this issue report.

&gt; Transport Client sometime causes following Exception. We cannot catch it bacause it happens in Elasticsearch Client Thread pool maybe.

This is not correct, we can catch it by such as following code.

``` java
try {
client.prepareSearch().execute().actionGet();
} catch (RemoteTransportException e) {
 ...
}
```

We decided to retry query simply when exception happened.

P.S.
This mistake happens by inappropriate exception handling.
In fact, catching this exception is not so difficult.
</comment><comment author="clintongormley" created="2015-06-18T20:02:43Z" id="113274134">@bigwheel thanks for letting us know.  Can this issue be closed?
</comment><comment author="hiranfdo" created="2016-08-16T10:44:26Z" id="240068007">can solve the issue 
ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/2/no master];]\",\"status\":503}
solution:
set the tcp_keepalive_time to a suitable value ( default 7200 seconds) . example change the value to 
tcp_keepalive_time=300

add this tcp_keepalive_time=300 end of the file /etc/sysctl.conf  ( permanent fix)  
of all the ES nodes and reboot the ES nodes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decrement reference even if IndexShard#postRecovery barfs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11201</link><project id="" key="" /><description>This can cause a reference leak if we call `IndexShard#postRecovery`
on an already closed shard.

a CI build failed here today:

```
1&gt; [2015-05-18 13:45:50,829][DEBUG][index.store              ] [node_s0] [test][0] store reference count on close: 1
  1&gt; [2015-05-18 13:45:50,829][TRACE][indices.recovery         ] [node_s0] [test][0] Got exception on recovery
  1&gt; [test][0] CurrentState[CLOSED] Closed
  1&gt;    at org.elasticsearch.index.shard.IndexShard.postRecovery(IndexShard.java:766)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryStatus.markAsDone(RecoveryStatus.java:191)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveriesCollection.markRecoveryAsDone(RecoveriesCollection.java:131)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:196)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:71)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:461)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

http://build-us-00.elastic.co/job/es_core_master_metal/9363/consoleFull
</description><key id="77636825">11201</key><summary>Decrement reference even if IndexShard#postRecovery barfs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T12:26:15Z</created><updated>2015-06-07T18:15:33Z</updated><resolved>2015-05-18T15:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-18T15:01:06Z" id="103087834">@jpountz pushed a new commit
</comment><comment author="jpountz" created="2015-05-18T15:02:38Z" id="103088148">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Some smallish translog cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11200</link><project id="" key="" /><description /><key id="77633739">11200</key><summary>Some smallish translog cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T12:15:21Z</created><updated>2015-05-25T10:46:52Z</updated><resolved>2015-05-18T15:41:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-18T12:15:32Z" id="103040465">@mikemccand can you take a look
</comment><comment author="mikemccand" created="2015-05-18T12:33:00Z" id="103042868">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/NoopStreamOutput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamOutput.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file></files><comments><comment>Merge pull request #11200 from s1monw/cleanup_translog</comment></comments></commit></commits></item><item><title>Fix multi level parent/child bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11199</link><project id="" key="" /><description>If the search hit is a child of the inner hit definition then an empty inner hit response was returned even if there is a matching parent document.
</description><key id="77527427">11199</key><summary>Fix multi level parent/child bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-18T06:45:20Z</created><updated>2015-05-29T16:25:33Z</updated><resolved>2015-05-19T09:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-18T23:28:50Z" id="103261115">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Count api to become a shortcut to the search api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11198</link><project id="" key="" /><description>The count api used to have its own execution path, although it would do the same (up to bugs!) as the search api. This commit makes it a shortcut to the search api with `size` set to `0`. The change is made in a backwards compatible manner, by leaving all of the java api code around too, given that you may not want to get back a whole `SearchResponse` when asking only for number of hits matching a query, also cause migrating from `countResponse.getCount()` to `searchResponse.getHits().totalHits()` doesn't look great from a user perspective. We can always decide to drop more code around the count api if we want to break backwards compatibility on the java api, making it a shortcut on the rest layer only.

The only breaking behaviour is around total failures, the search api returns an exception, while the count api used to return a normal response with `count` set to `0`, `successful` set to `0` and a list of shard failures. Maybe we can find a way to maintain this behaviour, but I'm not sure it makes sense.

If it wasn't for the above breaking aspect we could potentially backport this change to 1.x, for what it's worth. Let's discuss that.

Closes #9117
Closes #9110
</description><key id="77031690">11198</key><summary>Count api to become a shortcut to the search api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-16T12:46:10Z</created><updated>2015-08-13T13:49:33Z</updated><resolved>2015-05-26T17:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-16T13:57:00Z" id="102630734">@javanna this looks great!, can we add a test that makes sure that `CountRequest` converts itself to the right `SearchRequest`? other than that, LGTM
</comment><comment author="javanna" created="2015-05-16T16:42:47Z" id="102649988">Sure @kimchy will add a test. This is the first time (I think) that we make an api shortcut to one another in the java api. The way I did it is very specific to the count api, I wonder if we should support this in a more generic fashion e.g. having some kind of adapter mechanism built-in in `AbstractClient` rather than having to override three methods to do pretty much the same thing (redirect count to search). Thoughts?

Also, do we see the count api sticking around in the java api or will we remove it at some point? I am conflicted on this, I can't make up my mind.

Last thing, is the breaking aspect around total failures acceptable or is it going to be a problem?
</comment><comment author="kimchy" created="2015-05-16T17:01:57Z" id="102654581">&gt; I wonder if we should support this in a more generic fashion e.g. having some kind of adapter mechanism built-in in

I think that we can add it if we see it becoming a pattern, I doubt it will though.

&gt; Also, do we see the count api sticking around in the java api or will we remove it at some point? I am conflicted on this, I can't make up my mind.

Yea, its a tricky one, I think that this change is great without bothering with the question if we should remove it or not, we can decide later.

&gt; Last thing, is the breaking aspect around total failures acceptable or is it going to be a problem?

No, I think the consistency here between search and count is better.
</comment><comment author="jpountz" created="2015-05-17T09:32:54Z" id="102771805">Nice diff stats :)
</comment><comment author="s1monw" created="2015-05-17T10:06:07Z" id="102779345">why are we going through the trouble of having a dedicated Java API here? Java folks still need to recompile their code if they wanna use 2.0 so we can just remove it. REST is a different story but has a way lower footprint.
</comment><comment author="javanna" created="2015-05-18T08:23:23Z" id="102967125">&gt; why are we going through the trouble of having a dedicated Java API here?

I initially wanted to remove the java api layer too, till I realized how much we use the count api in our tests, and I found myself making a change that I didn't like too much: moving to search, adding `setSize(0)` and replacing `getCount()` with `getHits.getTotalHits()`. It seemed easy enough for us to keep request and response around. It's mainly a matter of usability, using the search api directly it's so easy to forget setting size to 0 (and everything still works but much more happens under the hood), count does it internally and allows to set only what is supported when counting only. Also it's nicer to retrieve the resulting count through `response.getCount()` rather than `response.getHits().totalHits()`. I personally think it's also nice for java api users to have a corresponding java api for count, rather than having to dig what they have to use instead.
</comment><comment author="javanna" created="2015-05-18T08:32:05Z" id="102974144">btw pushed another commit as requested by @kimchy 
</comment><comment author="javanna" created="2015-05-20T08:41:21Z" id="103809609">I would like to get this in, what do we do? Do we want to keep bwc on the java api layer or not?
</comment><comment author="clintongormley" created="2015-05-25T13:01:33Z" id="105228749">It's just sugar, but I think that frequency of use and conciseness is a good enough argument for keeping the count API.
</comment><comment author="javanna" created="2015-05-26T11:27:05Z" id="105493842">I agree with @clintongormley I will get this in , @jpountz can you have a last look please?
</comment><comment author="jpountz" created="2015-05-26T14:36:54Z" id="105546580">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Be more lenient in EIT#waitForDocs</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/CountAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/CountResponse.java</file><file>src/main/java/org/elasticsearch/action/count/ShardCountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/ShardCountResponse.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationResponse.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/count/CountRequestTests.java</file><file>src/test/java/org/elasticsearch/action/count/CountResponseTests.java</file><file>src/test/java/org/elasticsearch/broadcast/BroadcastActionsTests.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file></files><comments><comment>Internal: count api to become a shortcut to the search api</comment></comments></commit></commits></item><item><title>Fix cluster state task name for update snapshot task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11197</link><project id="" key="" /><description>This commit fixes the name of the upated_snapshot task from something like "update_snapshot [org.elasticsearch.cluster.metadata.SnapshotMetaData$Entry@de00bc50]" to a more readable "update_snapshot [test-snap]"

Basically instead of logging:

```
[2015-05-15 19:31:54,258][DEBUG][cluster.service] [node_s0] processing [create_snapshot[test-snap]]: took 6ms done applying updated cluster_state (version: 21, uuid: asbLMhdqRSKhFvV2-h9JUw)
[2015-05-15 19:31:54,265][DEBUG][cluster.service] [node_s0] processing [update_snapshot [org.elasticsearch.cluster.metadata.SnapshotMetaData$Entry@de00bc50]]: execute
```

It now logs:

```
[2015-05-15 19:49:48,254][DEBUG][cluster.service] [node_s0] processing [create_snapshot [test-snap]]: took 6ms done applying updated cluster_state (version: 21, uuid: asbLMhdqRSKhFvV2-h9JUw)
[2015-05-15 19:49:48,268][DEBUG][cluster.service] [node_s0] processing [update_snapshot [test-snap]]: execute
```
</description><key id="76900035">11197</key><summary>Fix cluster state task name for update snapshot task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-05-16T00:15:47Z</created><updated>2015-06-08T00:35:22Z</updated><resolved>2015-05-16T00:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-16T00:16:36Z" id="102552648">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: add serial differencing pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11196</link><project id="" key="" /><description>No need for assignment or review yet, still need to write tests!
### Serial Differencing

Serial differencing (or just differencing) is a technique where values in a time series are subtracted from itself at different time lags or periods.  For example, the datapoint f(x) = f(x&lt;sub&gt;t&lt;/sub&gt;) - f(x&lt;sub&gt;t-n&lt;/sub&gt;), where `n` is the period being used.

A period of 1 is equivalent to a derivative: it is simply the change from one point to the next.  Single periods are useful for removing constant, linear trends.

Single periods are also useful for transforming data into a stationary series.  In this example, the Dow Jones is plotted over ~250 days.  The raw data is not stationary, which would make it difficult to use with some techniques.  

But once we plot the first-difference, it becomes a stationary series (we know this because the first difference is randomly distributed around zero, and doesn't seem to exhibit any pattern/behavior).  The transformation reveals that the dataset is a random-walk model, which allows us to use further analysis.

![screen shot 2015-03-19 at 10 42 04 am](https://cloud.githubusercontent.com/assets/1224228/6756802/b4ff11b4-cf02-11e4-86e5-26d36dc9114c.png)

Larger periods can be used to remove seasonal / cyclic behavior.  In this example, a population of lemmings was synthetically generated with a sine wave + constant linear trend + random noise.  The sine wave has a period of 30 days.

The first-difference removes the constant trend, leaving just a sine wave.  The 30th-difference is then applied to the first-difference to remove the cyclic behavior, leaving a stationary series which is amenable to other analysis.

![screen shot 2015-03-19 at 12 15 06 pm](https://cloud.githubusercontent.com/assets/1224228/6756873/36be2cd0-cf03-11e4-9141-cdc96a51c118.png)

(Old PR and comments: #10190)
### API

``` json
{
   "aggs": {
      "my_date_histo": {
         "date_histogram": {
            "field": "timestamp",
            "interval": "day"
         },
         "aggs": {
            "the_sum": {
               "sum": {
                  "field": "lemmings"
               }
            },
            "first_difference": {
               "serial_diff": {
                  "buckets_path": "the_sum",
                  "lag" : 1
               }
            },
            "thirtieth_difference": {
               "serial_diff": {
                  "buckets_path": "first_difference",
                  "lag" : 30
               }
            }
         }
      }
   }
}
```
</description><key id="76857438">11196</key><summary>Aggregations: add serial differencing pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T21:08:05Z</created><updated>2015-07-10T22:26:20Z</updated><resolved>2015-07-10T22:26:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-07-07T19:25:23Z" id="119309506">@colings86 Low priority, but this is up for review whenever you have a few spare minutes.  It is blissfully simple compared to `moving_avg` :)

Open question:  currently, if there is not enough data (or the lag is too large), you just don't get any `serial_diff` metric values.  We could also throw an exception, but that seems like poor behavior (the rest of your aggs may work fine).  Thoughts?
</comment><comment author="colings86" created="2015-07-08T10:15:59Z" id="119532252">@polyfractal left some comment but I really like this aggregation and the documentation for it is great :)

To your question: I am struggling to decide what is best. As you say, throwing an exception seems unfriendly and would be different behaviour to other aggregations. But equally if we just don't output anything then it can easily confuse users as to why the aggregation is not working since there will be no message or indication anywhere of what caused the aggregation to not output any data.
</comment><comment author="polyfractal" created="2015-07-10T16:39:34Z" id="120454875">@colings86 cleaned up, ready at your leisure :)
</comment><comment author="colings86" created="2015-07-10T16:50:23Z" id="120457176">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffTests.java</file></files><comments><comment>Merge pull request #11196 from polyfractal/feature/aggs_2_0_diff</comment></comments></commit></commits></item><item><title>Remove filter from PhraseSuggester collate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11195</link><project id="" key="" /><description>This commit removes the ability to use `filter` for PhraseSuggester collate.
Only `query` can be used for collation.
Internally, a collate query is executed as an exists query. So specifying a
filter does not have any benefits.
</description><key id="76850555">11195</key><summary>Remove filter from PhraseSuggester collate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T20:36:01Z</created><updated>2015-05-29T16:42:12Z</updated><resolved>2015-05-29T16:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-29T09:19:21Z" id="106753759">@areek let's deprecate this in 1.6 and push this as is to 2.0 ok? LGTM though
</comment><comment author="areek" created="2015-05-29T16:42:11Z" id="106867270">committed to 2.0 (https://github.com/elastic/elasticsearch/commit/fb8cd53582fe6b91ab4f5a395034cfa7ab2c29fa)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>allow mockito to be used in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11194</link><project id="" key="" /><description>It needs a special permission for the reflection it does.
</description><key id="76849970">11194</key><summary>allow mockito to be used in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T20:34:10Z</created><updated>2015-05-15T20:52:45Z</updated><resolved>2015-05-15T20:52:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-15T20:35:22Z" id="102520768">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11194 from rmuir/mockito</comment></comments></commit></commits></item><item><title>Remove the ability to flush without flushing the translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11193</link><project id="" key="" /><description>This is a leftover from the times where we failed a flush when
recoveries are ongoing. This code is really not needed anymore and
we can luckily flush the translog all the time as well.
</description><key id="76847054">11193</key><summary>Remove the ability to flush without flushing the translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T20:25:57Z</created><updated>2015-06-07T10:05:10Z</updated><resolved>2015-05-18T19:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-15T20:30:27Z" id="102519456">LGTM
</comment><comment author="mikemccand" created="2015-05-15T21:07:23Z" id="102525856">LGTM, yay :)
</comment><comment author="s1monw" created="2015-05-18T09:09:10Z" id="102983243">@mikemccand I removed the commit before running checkindex in the last commits. The reason is that I don't want to force an artificial full commit including the translog since it's not what we do in reality. Yet,
we also commit on close randomly and/or if we shutdown etc. so I think this should be enough. I also think the reason why we had this was that there are sometimes no commits present which IMO is a bug if that is the case today so I think it's safe to skip.
</comment><comment author="mikemccand" created="2015-05-18T15:03:13Z" id="103088263">LGTM, thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/store/IndexStoreModule.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java</file></files><comments><comment>Merge pull request #11193 from s1monw/remove_commit_without_translog</comment></comments></commit></commits></item><item><title>don't exclude rest tests yamls from test-framework jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11192</link><project id="" key="" /><description>Some plugins subclass these rest tests in a hacky way, by having a separate checkout of elasticsearch and specifying lots of things with -Dsysprops.

Besides being hacky, it won't work under security manager.

Instead, don't exclude from the test jar (its only 150KB compressed to the test jar), so that subclassing the test just works.
</description><key id="76812063">11192</key><summary>don't exclude rest tests yamls from test-framework jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T18:24:53Z</created><updated>2015-05-15T20:15:10Z</updated><resolved>2015-05-15T20:14:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-15T18:36:23Z" id="102487583">I left a minor comment, I think it's pretty neat that you can run the test suite from just the test-jar.
</comment><comment author="rmuir" created="2015-05-15T20:05:57Z" id="102511307">@s1monw do you mind looking again? I had to dodge some... annoying bugs.
</comment><comment author="s1monw" created="2015-05-15T20:09:42Z" id="102512897">LGTM
</comment><comment author="rmuir" created="2015-05-15T20:14:43Z" id="102515367">thank you
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTestCase.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>include rest tests in test-framework.jar</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTestCase.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>include rest tests in test-framework.jar</comment></comments></commit></commits></item><item><title>Query refactoring: QueryBuilder to extend Writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11191</link><project id="" key="" /><description>QueryBuilders need to become streamable over the wire, so we can use them as our own intermediate query representation and send it over the wire. Using Writeable rather than Streamable allows us to restore some final fields and delete default constructor needed only for serialization purposes.

Taken the chance also to revise the internals of IdsQueryBuilder (modified some internal data type and added deduplication of ids to reduce bits to serialize). Expanded also IdsQueryBuilderTest, injected random types and improved comparison.

This PR is against the query-refactoring branch.
</description><key id="76808863">11191</key><summary>Query refactoring: QueryBuilder to extend Writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-15T18:17:54Z</created><updated>2015-05-21T11:37:51Z</updated><resolved>2015-05-21T11:37:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-15T18:18:37Z" id="102478167">@cbuescher I assigned this to you for review. While I was changing the serialization aspects I took the chance to change a bit IdsQueryBuilder and its tests, lemme know what you think.
</comment><comment author="cbuescher" created="2015-05-19T21:51:57Z" id="103678270">Left a few comments, I like the addition of the type mappings to the tests and also the way QueryBuilder now are Writable. There's just one issue mentioned above connected to changing ids in IdsQueryBuilder to Set.
</comment><comment author="javanna" created="2015-05-20T17:39:23Z" id="103970722">thanks for the review @cbuescher  I pushed new commits to address your comments, I also worked further on testing, to make sure we have more coverage now that we have mappings, improved range query tests and shared more code between term query and span term query tests. Let me know what you think. Waiting for review on #11263 to adress the createTypeUids problem that you spotted. 
</comment><comment author="javanna" created="2015-05-21T09:57:34Z" id="104209090">The problem around createTypeUids was solved in master, this is ready for review again. Tests are green although I had to comment out the use of the `_all` type due to https://github.com/elastic/elasticsearch/commit/e95097c33cc782a3ca83b7ea6cafedc3d6793fd3#commitcomment-11302169 
</comment><comment author="cbuescher" created="2015-05-21T11:10:48Z" id="104231718">Left only one comment regarding boost testing, but LGTM since you already mentioned looking at generifying boost and queryname elsewhere. We can probably also reduce the duplication in tests there even a bit more.
</comment><comment author="javanna" created="2015-05-21T11:37:50Z" id="104236584">Merged https://github.com/elastic/elasticsearch/commit/028ff7e49eeb6d0be97233a6dc915627356d8839
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java</file></files><comments><comment>Query refactoring: QueryBuilder to extend Writeable</comment></comments></commit></commits></item><item><title>External list of exclude values for terms aggregation filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11190</link><project id="" key="" /><description>Would be nice to be able to specify an external list of exclude values to read from (and not have to include all of them in the request in an array) when filtering values for use cases where they can be thousands of exclude values.

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_filtering_values
</description><key id="76800252">11190</key><summary>External list of exclude values for terms aggregation filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-05-15T17:43:16Z</created><updated>2015-09-27T22:27:53Z</updated><resolved>2015-09-27T22:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-21T16:38:11Z" id="104347230">I'm a bit worried about making it easier to have large exclude lists. Do you have more information about the use-case?
</comment><comment author="ppf2" created="2015-05-21T16:54:14Z" id="104353641">Use case is that the end user has some domain/industry specific stop words that they would like to exclude from the aggregated bucket values and would rather not index a separate analyzed field (lot of data) with a stop filter.  They are looking into other approaches now after realizing that the exclude feature may not perform well with large lists today until 2.x. 
</comment><comment author="clintongormley" created="2015-05-25T16:03:36Z" id="105257116">A query time solution is never going to perform as well as an index time solution.  This feels trappy to me.  I'd say a better solution for them would be to use the stopwords token filter, and maintain a "patch" list of extra terms which they pass to each aggregation.  Periodically, they could reindex their data in the background to update the stopwords list.
</comment><comment author="jpountz" created="2015-09-27T22:27:53Z" id="143598823">+1
Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deleting multiple indices can fail to report all failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11189</link><project id="" key="" /><description>currently a delete is performed for every concreteIndex within the `DeleteIndexRequest`.
If an error occurs the exception is assigned to `lastFailure` and after the last delete is performed the error is passed to the listener (if one did occur). 

The problem is that lastFailure is declared within the listeners passed to `deleteIndexService.deleteIndex` and so only the failure of the last deleted index is considered and the errors of previous deleted indices are ignored. This patch fixes this behavior by declaring `lastFailure` outside the loop.

i had problems creating a proper test for this, maybe one of you can point me into the right direction or show me a similar test
</description><key id="76780394">11189</key><summary>Deleting multiple indices can fail to report all failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bogensberger</reporter><labels><label>:Index APIs</label><label>bug</label></labels><created>2015-05-15T16:31:05Z</created><updated>2015-10-29T19:47:34Z</updated><resolved>2015-10-27T14:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-15T16:47:56Z" id="102456781">Thanks for sending this PR @bogensberger , this relates to #7295. We will still have to improve the delete index api at some point so that we return a list of errors rather than just the last one, but this fix already improves things. I left a small comment, can you also sign our [CLA](https://www.elastic.co/contributor-agreement) please?
</comment><comment author="clintongormley" created="2015-05-15T19:44:40Z" id="102506599">Could we also have a better description for the PR?  I don't understand what it is doing.

thanks
</comment><comment author="bogensberger" created="2015-05-18T11:13:59Z" id="103020098">@javanna i've signed the CLA and updated the commit so `ack` is also moved.
</comment><comment author="bogensberger" created="2015-05-18T11:14:29Z" id="103020301">@clintongormley i've updated the PR description. I hope it's more clear now.
</comment><comment author="javanna" created="2015-05-18T11:16:04Z" id="103020881">thanks @bogensberger I just left a small comment, thanks a lot for updating, I will look into writing a test for this and merge it.
</comment><comment author="bogensberger" created="2015-05-20T14:42:21Z" id="103910123">@javanna Thanks for review i've updated the pull request. I have also created another PR (https://github.com/elastic/elasticsearch/pull/11258) which also relates to #7295
</comment><comment author="s1monw" created="2015-10-16T13:15:07Z" id="148713721">@javanna can you come back to this? I added a comment but you worked on it before so I wonder if you have something to add?
</comment><comment author="javanna" created="2015-10-19T11:52:21Z" id="149192683">I agree with your comment @s1monw . I'd love to have a simple test for it as well, I meant to come up with one but I never got to it :(
</comment><comment author="s1monw" created="2015-10-27T13:50:51Z" id="151502416">is this fixed by #11258 ?
</comment><comment author="bogensberger" created="2015-10-27T14:49:45Z" id="151526844">@s1monw yes #11258 also fixes this issue.
</comment><comment author="javanna" created="2015-10-27T14:59:26Z" id="151530512">great closing then, thanks again @bogensberger ! Superseded by #11258 .
</comment><comment author="s1monw" created="2015-10-27T15:03:23Z" id="151531687">awesome!
</comment><comment author="s1monw" created="2015-10-27T15:04:00Z" id="151532037">@javanna I labeled it fixed in 2.2 and 3.0 no?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve `include`/`exclude` clause list speed and scalability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11188</link><project id="" key="" /><description>In the terms aggregations' IncludeExclude class the underlying automaton-backed implementation throws an error if there are too many states.
In these circumstances this fix falls-back to using an implementation based on Set lookups for the excluded terms.
If the global-ordinals execution mode is in effect this implementation also addresses the slowness identified in issue 11181 which is caused by traversing the TermsEnum - instead the excluded terms’ global ordinals are looked up individually and unset the bits of acceptable terms. This is significantly faster for high cardinality fields.

Closes #11176
</description><key id="76773680">11188</key><summary>Improve `include`/`exclude` clause list speed and scalability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T16:07:30Z</created><updated>2015-06-08T08:44:01Z</updated><resolved>2015-05-15T18:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-05-15T16:08:38Z" id="102449754">@jpountz This PR addresses both https://github.com/elastic/elasticsearch/issues/11181 and https://github.com/elastic/elasticsearch/issues/11176
</comment><comment author="jpountz" created="2015-05-15T16:21:45Z" id="102452102">Just left one question. Otherwise it looks good to me.
</comment><comment author="markharwood" created="2015-05-15T17:22:34Z" id="102462736">Thanks for review. Changed to only use AutomatonBackedStringFilter for regex cases. Performance still good.
</comment><comment author="jpountz" created="2015-05-15T17:24:44Z" id="102463368">LGTM
</comment><comment author="markharwood" created="2015-05-15T18:22:13Z" id="102481301">Pushed to master via https://github.com/elastic/elasticsearch/commit/caf723570d8e52c52b1f6bd378f197c0af338c13
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field stats filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11187</link><project id="" key="" /><description>Originally discussed in #10523 

Would it be possible to filter the fields/indices we get back based on a filter? Perhaps I want back all of the indices with `@timestamp` fields between 2014-01-01 and 2015-01-01:

```
curl -XGET "http://localhost:9200/_field_stats?fields=@timestamp&amp;min=2014-01-01&amp;max=2015-01-01"
```

The above is probably not the right syntax since it would only allow for a single field. I'm not sure if we want a post body here or some other syntax?

This would be incredibly useful for Kibana and let us get rid of some serious bugs that occur for users with large numbers of indices. In fact, we could entirely get rid of our notion of timestamped indices as we'd be able to reliably sort an index list based on time. Users would only need to know their wildcard pattern! It would also allow for weird indexing strategies where maybe they're indexing weekly at one point and they need to step up to daily, something that isn't currently possible. Heck, you wouldn't even need timestamped indices, you could just increase a counter!
</description><key id="76771015">11187</key><summary>Field stats filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rashidkpc</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T15:58:33Z</created><updated>2015-07-01T06:47:47Z</updated><resolved>2015-07-01T06:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T19:40:44Z" id="102504709">@rashidkpc do you need to know which indices match for any reason other than to reduce the number of indices you query?  I ask because I don't think you should need to do this at all.  Elasticsearch should do this for you at search time.  As @mikemccand said in https://github.com/elastic/elasticsearch/issues/5829#issuecomment-98085925 : 

&gt; I think higher level optimizations could be very worthwhile, e.g. for time-based indices, knowing that a given index won't have any hits because there is a top-level range filter, should be a big speed up in many cases ...
</comment><comment author="martijnvg" created="2015-05-17T19:08:36Z" id="102835936">&gt; I think higher level optimizations could be very worthwhile, e.g. for time-based indices, knowing that a given index won't have any hits because there is a top-level range filter, should be a big speed up in many cases ...

We should definitely implement this. In order to implement this, we need to keep track of each min and max value per field per shard, so that we can skip entire shards. However even with this big improvement in place we will still send the shard level requests to the nodes that hold the shards. Kibana and other apps can do the next best thing by not sending requests at all to nodes that hold indices outside the timestamp range and to do this stats filter can help.

Maybe the right syntax would be: 

```
curl -XGET "http://localhost:9200/_field_stats?fields=@timestamp&amp;min.@timestamp=2014-01-01&amp;max.@timestamp=2015-01-01"
```

So the field name is included in the min and max params. I'd prefer this over adding parameters to the request body for this particular filter option. 
</comment><comment author="clintongormley" created="2015-05-26T09:26:16Z" id="105462074">Just been talking to @martijnvg and @jpountz about the field-stats filtering PR in #11259.  I'm not liking the API at all, it feels like the wrong solution.

The intention of this change is to reduce the amount of work that Elasticsearch has to do to fulfill a search request, by only querying indices that contain documents that could possibly match certain conditions (eg a timestamp range).  However, the proposed solution requires a round trip to all indices to retrieve the field stats data before executing the query, so all shards get hit anyway.

While you could cache this information, Kibana is supposed to take new data into account, so any caching would interfere with this process. 

The problem with running a search request on all indices at the moment is that we don't have the optimizations in place to abort the search request as early as possible if no data can possibly match. Even if we use the pre-flight check suggested in https://github.com/elastic/elasticsearch/issues/5829#issuecomment-98085925 we need other optimizations such as only loading global ordinals if we actually need them.

@rashidkpc I think that today Kibana queries each index in turn, starting with the most recent and backfilling data.  Is that correct? If we implemented the optimizations suggested above, would you still want to do that in the same way and, if so, why?

If this is still a requirement, then I think we should look at a different API which just returns index names, rather than tacking this on to the field stats API, or do you need the field stats as well?  Could you describe your "dream process" in more detail?
</comment><comment author="clintongormley" created="2015-05-28T17:51:28Z" id="106525757">@rashidkpc and I had a chat about how Kibana would use this API:

Regardless of whether Elasticsearch uses the min/max field stats internally, Kibana would still query indices individually to show results for recent indices more quickly and with less concurrent load on ES. Kibana wouldn't cache the field stats information (as it needs to be aware of any new indices/data as it becomes available), so the API needs to be fast.

A user might select a time range, so Kibana would use the field stats API to show data from each index within the time range in turn.  Then, if a user selects a field for filtering, Kibana would show the min/max values available for that field based on the selected time range.

In summary, the API should return:
- a list of indices
- field stats for a list of fields, per index
- possibly filtered by a constraint on a field (eg timestamp)

A question that remains is: would we ever need to filter on more than one field and, if so, how should the constraints be applied: as AND or OR.  That said, we couldn't come up with a use case for filtering on multiple fields, so let's worry about that later and for the moment just AND constraints  (ie all must apply).

I think it is worth separating out the list of fields that should be returned  from the constraints, eg:

```
curl -XPOST "http://localhost:9200/_field_stats?level=indices" -d'
{
  "fields": [
    "foo",
    "bar"
  ],
  "constraints": {
    "_timestamp.max": {
      "gte": "2014-01-01T00:00:00.000Z"
    },
    "_timestamp.min": {
      "lt": "2015-01-01T00:00:00.000Z"
    }
  }
}'
```

Should it be `constraints`, `filters`, `restrictions`, `where`?  I'm avoiding `filters` because it doesn't accept the query DSL.
</comment><comment author="martijnvg" created="2015-05-28T18:34:20Z" id="106557566">@clintongormley I like `restrictions` or maybe just `restrict`? I think that the restrictions should be embed in the field, so it is clear what restriction belongs to what field. Something like this:

``` json
{
  "fields": {
    "foo" : {
       "restrict" : {
          "_timestamp.max": {
              "gte": "2014-01-01T00:00:00.000Z"
          }
       }
    },
    "bar" : {
      "restrict" : {
         "_timestamp.min": {
            "lt": "2015-01-01T00:00:00.000Z"
         }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-05-28T19:43:14Z" id="106576176">@martijnvg the constraints should be separate from the fields list, because they are two separate concerns:
- which fields should i filter by
- which fields should i return
</comment><comment author="martijnvg" created="2015-05-29T07:21:34Z" id="106720433">Ok, I see, makes sense.
</comment><comment author="bleskes" created="2015-05-29T07:35:42Z" id="106727155">reading this I got confused a bit but what we are filtering on. I'm afraid people will very easily interpret the restrictions/constraints as to apply to data (i.e., query DSL) but I understand this is not what we aim for due to the performance implications. Rather the idea is to filter the indices used for the stats (right? ). I wonder if we should name it something that implies that like `indices_filter` or `indices_constraints` and have it in  a structure that allows future extension. Something like:

```
POST /_field_stats?level=indices
{
  "fields": [
    "foo",
    "bar"
  ],
  "indices_constraints": {
    "field_range": {
      "_timestamp": {
        "max": {
          "gte": "2014-01-01T00:00:00.000Z"
        },
        "min": {
          "lt": "2015-01-01T00:00:00.000Z"
        }
      }
    }
  }
}
```
</comment><comment author="martijnvg" created="2015-05-29T09:24:57Z" id="106755658">@bleskes Yes, the idea is to filter on indices. I like your syntax, because it is descriptive. It is a bit more verbose, but I think that is okay.
</comment><comment author="clintongormley" created="2015-05-29T10:29:55Z" id="106767591">I like `indices_constraints` although i think it should be `index_constraints`.  I don't think we need the `field_range` layer: all we can do is filter on the values of stats for each field.  So in summary:

```
POST /_field_stats?level=indices
{
  "fields": [
    "foo",
    "bar"
  ],
  "index_constraints": {
    "_timestamp": {
      "max": {
        "gte": "2014-01-01T00:00:00.000Z"
      },
      "min": {
        "lt": "2015-01-01T00:00:00.000Z"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStats.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/FieldStatsShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/IndexConstraint.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/fieldstats/RestFieldStatsAction.java</file><file>core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTest.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java</file></files><comments><comment>field stats: added index constraints</comment></comments></commit></commits></item><item><title>Descriptions of fuziness overlap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11186</link><project id="" key="" /><description>On https://www.elastic.co/guide/en/elasticsearch/reference/master/common-options.html#fuzziness the auto value for fuziness says that 0..1 character terms is no edits allowed and 1..5 is one edit allowed. I suspect its 0..1 character terms is no edits allowed and 2..5 is one edit allowed but I haven't checked.
</description><key id="76756426">11186</key><summary>Descriptions of fuziness overlap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-05-15T15:13:34Z</created><updated>2015-05-15T20:05:11Z</updated><resolved>2015-05-15T19:25:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T19:25:56Z" id="102500338">Neither :) It's 0..2, 3..5

thanks, fixed
</comment><comment author="nik9000" created="2015-05-15T20:05:11Z" id="102510988">Thanks!

On Fri, May 15, 2015 at 3:26 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Neither :) It's 0..2, 3..5
&gt; 
&gt; thanks, fixed
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11186#issuecomment-102500338
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fixed explanation of AUTO fuzziness</comment></comments></commit></commits></item><item><title>Balance new shard allocations more evenly on multiple path.data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11185</link><project id="" key="" /><description>This change adds a simplistic heuristic to try to balance new shard allocations across multiple data paths on one node.

It very roughly predicts (guesses!) how much disk space a shard will eventually use, as the max of the current avg. size of shards across the cluster, and 5% of current free space across all path.data on the current node, and then reserves space by counting how many shards are now assigned to each path.data.

Picking the best path.data for a new shard is using the same "most free space" logic, except it now deducts the reserved space.

I tested this on an EC2 instance with 2 SSDs with nearly the same amount of free space and confirmed we now put 2 shards on one SSD and 3 shards on the other, vs all 5 shards on a single path with master today, but I'm not sure how to make a standalone unit test ... maybe I can use a MockFS to fake up N path.datas with different free space?

This is just a heuristic, and it easily has adversarial cases that will fill up one path.data while other path.data on the same node still have plenty of space, and unfortunately ES can't recover from that today. E.g., DiskThresholdDecider won't even detect any problem (since it sums up total free space across all path.data) ... I think we should separately think about fixing that, but at least this change improves the current situation.

Closes #11122
</description><key id="76729910">11185</key><summary>Balance new shard allocations more evenly on multiple path.data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T13:41:25Z</created><updated>2015-06-17T14:28:35Z</updated><resolved>2015-06-17T14:28:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-15T18:02:07Z" id="102473250">This looks ok to me, although I do hope we keep tweaking this heuristic. I've tried to think of a better one but can't for now..but I wish we could do something based on weighted shard count instead of the very hard to think about mixed/shifting logic of average/5%.
</comment><comment author="s1monw" created="2015-05-15T20:08:24Z" id="102512170">mike this looks good to me - can we somehow have a test for this?
</comment><comment author="mikemccand" created="2015-05-17T09:46:06Z" id="102773984">&gt;  can we somehow have a test for this?

I agree, but it's tricky: I think I need a new MockFS that fakes how much free space there is on each path.  I can try ...
</comment><comment author="mikemccand" created="2015-05-17T09:53:15Z" id="102774297">&gt; This looks ok to me, although I do hope we keep tweaking this heuristic. I've tried to think of a better one but can't for now..but I wish we could do something based on weighted shard count instead of the very hard to think about mixed/shifting logic of average/5%.

The challenge with only using current shard count is then we don't take into account how much space the already allocated shards are already using?  E.g maybe one path has only a few shards, and is nearly full, but another path has quite a few shards and has lots of free space.

No matter the heuristic here, there will be easy adversarial cases against it, so in the end this will be at best a "starting guess": we can't predict the future.

To fix this correctly we really need shard allocation to separately see / allocate across each path.data on a node, so we can move a shard off a path.data that is filling up even if other path.data on that same node have plenty of space.
</comment><comment author="rmuir" created="2015-05-17T13:48:36Z" id="102805354">&gt; I agree, but it's tricky: I think I need a new MockFS that fakes how much free space there is on each path. I can try ...

this requires QUITE a few steps, and please note that ES's file management (especially tests) is simply not ready to juggle multiple nio filesystems at once (various copy/move routines are unsafe there). 

Separately, an out-of-disk space mockfs would be great. But please don't add a complicated test, and please don't use multiple nio.2 filesystems when ES isn't ready for that yet.

Test what is reasonable to test and then if we want to do better, some cleanups are needed. I have been working on these things but it is quite difficult without straight up banning methods, because more tests are added all the time.
</comment><comment author="mikemccand" created="2015-05-18T21:54:50Z" id="103224905">&gt; this requires QUITE a few steps, and please note that ES's file management (especially tests) is simply not ready to juggle multiple nio filesystems at once (various copy/move routines are unsafe there).

OK this sounds hard :)  I'll try to make a more direct test that just tests the heuristic logic w/o needing MockFS ...
</comment><comment author="mikemccand" created="2015-05-20T20:50:10Z" id="104033608">I pushed a new commit addressing feedback (thanks!).

However, I gave up making a direct unit test for the ShardPath.selectNewPath ... I tried to simplify the arguments passed to it (e.g. replacing Iterator&lt;IndexShard&gt; with Iterator&lt;Path&gt; and extracting the shard's paths "up above") so that it was more easily tested, but it just became too forced ...

I did retest on EC2 and confirmed the 5 shards are split to 2 and 3 shards on each SSD.

I'll open a follow-on issue to allow for shards to relocated to different path.data within a single node.
</comment><comment author="mikemccand" created="2015-05-22T09:19:46Z" id="104588221">One problem with this change is it's making a "blind guess" about how big a shard will be, yet if it's a relocation of an existing shard, the source node clearly "knows" how large it is, so it's crazy not to use this.

@dakrone is working on getting this information published via ClusterInfo or index meta data or something (thanks!), then I'll fix this change to use that, so we can make a more informed "guess".

Even so, this is just the current size of the shard, and we still separately need to fix #11271 so shards on one path.data that's filling up will still be relocated even if other path.data on the same node have plenty of space.
</comment><comment author="s1monw" created="2015-06-12T12:28:30Z" id="111476057">I left some comments here again, I think we should move forward here without the infrastructure to make it perfect. It's a step in the right direction?
</comment><comment author="mikemccand" created="2015-06-12T17:57:34Z" id="111575301">OK I folded in feedback here.

I avoid this logic when a custom data path is set, and force the statePath to be NodePaths[0] in that case.

And I removed ClusterInfo/Service and instead take avg. of all shards already on the node.
</comment><comment author="s1monw" created="2015-06-16T18:18:52Z" id="112519354">left a tiny comment otherwise LGTM
</comment><comment author="s1monw" created="2015-06-17T09:36:34Z" id="112736816">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file></files><comments><comment>Balance new shard allocations more evenly across multiple path.data</comment></comments></commit></commits></item><item><title>XContent format Versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11184</link><project id="" key="" /><description>We have the concept of versioning on the transport wire format which allows us to make changes to the product whilst maintaining backwards compatibility with nodes on previous versions (but still on the same major version). We currently have no such versioning for the REST request/response format. For the request format we tend to add backward compatibility support in the Parsers to parse the old parameters as well as the new ones (these tend to be jumbled together with the new parameters with no clear indication they are deprecated formats still there for bwc), but for the REST Response format we don't have any way of knowing what version of the format the client expects so cannot maintain bwc with changes.

We could add a query parameter to every REST action called `format` whose value would specify which version the server should expect for request formats and which version the client expects for response formats. This would allow us to have the same kind of bwc logic that we have for the transport layer (if the format parameter is missing then it will default of the server's version or possibly to trying all available formats). 

We could then also have a new API which would return the XContentTypes and format versions supported by the server. Clients could then use this API to determine whether they are compatible with the server on client initialisation and work out which version they need to specify in the format parameter.

This would be a big and long term change and certainly would not happen overnight as we would have to slowly introduce the bwc code in the XContent parsers and renderers.
</description><key id="76713317">11184</key><summary>XContent format Versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:REST</label><label>adoptme</label><label>discuss</label></labels><created>2015-05-15T12:29:41Z</created><updated>2016-06-06T17:04:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-15T16:37:41Z" id="102454972">I agree we need such a thing if we want to be able to change the response formats across major versions.
</comment><comment author="nik9000" created="2015-05-15T16:48:57Z" id="102456960">&gt; We could add a query parameter to every REST action called format whose value would specify which version the server should expect for request formats and which version the client expects for response formats.

MediaWiki uses [formatversion](https://www.mediawiki.org/wiki/API:JSON_version_2) for something similar to this. It feels to me like exactly the kind of this that accept headers were built for though. Either way I'm +1 on the concept .
</comment><comment author="colings86" created="2016-06-03T10:10:37Z" id="223542195">We discussed this in FixItFriday and we decided that for now, as breaking changes in the REST API response are rare we will support a `legacy` flag in the query parameters which indicates which feature should respond in the legacy format e.g. `?legacy=aggs,completion`. The feature can then check in the `toXContent()` for the legacy flag if it has a legacy format and then respond appropriately
</comment><comment author="rjernst" created="2016-06-03T18:35:48Z" id="223658508">While I understand the need, I am concerned with the semantics. When do we ever remove the "legacy" format if we dont know what version(s) that format was associated with? What if there are two breaking changes that occur "close" together (say within a major release)? While that may be unlikely, not accounting for it could be a real problem if it ever did happen. Unless what we mean is "use the previous major version format" which assumes we do not break the rest api response format within a major version (which I think we already try to follow that rule)?
</comment><comment author="rjernst" created="2016-06-03T18:37:18Z" id="223658900">Also, why can it not be as simple as a boolean? I think if you want the "legacy" format (whatever that means) the entire response should be in that format, not just pieces?
</comment><comment author="clintongormley" created="2016-06-06T17:04:27Z" id="224021646">&gt; Unless what we mean is "use the previous major version format" which assumes we do not break the rest api response format within a major version (which I think we already try to follow that rule)?

Yes, exactly.

&gt; I think if you want the "legacy" format (whatever that means) the entire response should be in that format, not just pieces?

Maybe, not sure... We're not promising that the response syntax will be exactly the same as the previous major version, just that we'll revert a major breaking change. Also, it may be that somebody migrates their queries, but hasn't had time to do aggs yet.  not sure
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removed `id_cache` from stats and cat apis.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11183</link><project id="" key="" /><description>Also removed the `id_cache` option from the clear cache api.

PR for #5269
</description><key id="76709224">11183</key><summary>Removed `id_cache` from stats and cat apis.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T12:14:17Z</created><updated>2015-08-13T17:32:41Z</updated><resolved>2015-05-15T12:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-15T12:26:43Z" id="102385703">LGTM I like the migration notes
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdCacheStats.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/search/child/ParentFieldLoadingTest.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Merge pull request #11183 from martijnvg/parent-child/remove_id_cache_from_stats_and_clear_cache_apis</comment></comments></commit></commits></item><item><title>Making single bucket aggregations a special case of MultiBucket Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11182</link><project id="" key="" /><description>At the moment SingleBucketAggregator extends BucketAggregator and MultiBucketAggregation is a separate interface. This means that single bucket Aggregations are different from multi bucket aggregations which can lead to problems when traversing the aggregation response. The reason for this is that single bucket aggregations don't render the same way as multi-bucket ones and so while parsing a response you need to work out what type of aggregation you are looking at before you know how to parse it.

If would be better if SingleBucketAggregator was a multi-bucket aggregator which just happened to only have a single bucket. In the response there would still be a 'buckets' field but it would only contain a single entry for single-bucket aggregations. This would mean that single and multi bucket aggregation could be treated the same both by users of the REST API and users of the JAVA API.
</description><key id="76707075">11182</key><summary>Making single bucket aggregations a special case of MultiBucket Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2015-05-15T12:04:53Z</created><updated>2017-05-06T07:41:34Z</updated><resolved>2017-05-06T07:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-15T14:32:53Z" id="102414945">Similar to https://github.com/elastic/elasticsearch/issues/8510
</comment><comment author="javanna" created="2017-05-05T19:35:31Z" id="299556458">heya @colings86 should this be closed as a duplicate of #8510 ?</comment><comment author="colings86" created="2017-05-06T07:41:33Z" id="299622631">closed in favour of #8510 </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms agg exclude clauses slow on high-cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11181</link><project id="" key="" /><description>Even with a single `exclude` term, a `terms` aggregation can be very slow to execute on fields with high cardinality.
The culprit is the IncludeExclude.OrdinalsFilter.acceptedGlobalOrdinals method which loops over a globalTermsEnum to produce a set of accepted global ordinals. This can take several seconds and is a cost paid for every query.

The issue https://github.com/elastic/elasticsearch/issues/11176 is related to the same use case but is a different problem, relating to the scalability of the exclude list provided.
</description><key id="76694453">11181</key><summary>Terms agg exclude clauses slow on high-cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label></labels><created>2015-05-15T11:11:26Z</created><updated>2015-05-15T18:23:03Z</updated><resolved>2015-05-15T18:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-05-15T18:23:02Z" id="102481962">Fixed as part of https://github.com/elastic/elasticsearch/commit/caf723570d8e52c52b1f6bd378f197c0af338c13
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make SCAN faster.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11180</link><project id="" key="" /><description>When scrolling, SCAN previously collected documents until it reached where it
had stopped on the previous iteration. This makes pagination slower and slower
as you request deep pages. With this change, SCAN now directly jumps to the
doc ID where is had previously stopped.
</description><key id="76684291">11180</key><summary>Make SCAN faster.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T10:29:54Z</created><updated>2015-06-07T17:10:34Z</updated><resolved>2015-05-15T17:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-05-15T12:32:39Z" id="102386377">I like this approach! Left one minor comment, but other than that LGTM.
</comment><comment author="jpountz" created="2015-05-15T15:49:28Z" id="102443120">@martijnvg I pushed a new commit.
</comment><comment author="martijnvg" created="2015-05-15T16:06:28Z" id="102449373">LGTM
</comment><comment author="clintongormley" created="2015-05-15T19:47:48Z" id="102507620">wow!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/docset/AllDocIdSet.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/scan/ScanContext.java</file><file>src/test/java/org/elasticsearch/search/scan/ScanContextTests.java</file></files><comments><comment>Merge pull request #11180 from jpountz/enhancement/faster_scan</comment></comments></commit></commits></item><item><title>Seal indices for faster recovery </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11179</link><project id="" key="" /><description>Add the ability to seal an index. A seal is an id that is written to a primary and its replicas if there are no on going write operations on that shard while the sealing is in progress. Recovery skips file copying for a replica shard if the local copy on a node has the same seal id that the primary has.
Sealing can be triggered in two ways: Automatically, when the IndexingMemoryController detects that shard has become inactive and manually with an api.

relates to #10032
</description><key id="76661182">11179</key><summary>Seal indices for faster recovery </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>feature</label><label>release highlight</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-15T09:15:41Z</created><updated>2015-05-25T10:07:01Z</updated><resolved>2015-05-19T13:17:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-15T19:16:07Z" id="102497397">I added a bunch of comments nothing really major, looks pretty good though! I wonder if we can add some more assertions to the after test stuff and fetch the sync IDs and if two shards have the same sync ID they also need to have the same amount of committed and live documents?
</comment><comment author="brwe" created="2015-05-18T10:38:04Z" id="103012448">@nik9000 @s1monw thanks for the review! I addressed all comments or answered except for the recovery part here https://github.com/elastic/elasticsearch/pull/11179#discussion_r30437383 where I still have to dig how this can be done. 
I also added another check after each test to see if shards with same sync id also have same documents. 
Other than that I am unsure about the rest statuses that should be returned for the rest api. I now return CONFLICT(409) if al failed and PARTIAL_CONTENT(206) if only some shards succeeded but that seems odd to me since it is not about content. What would be appropriate here?
</comment><comment author="s1monw" created="2015-05-19T12:57:05Z" id="103475846">I think we are ready here LGTM @brwe 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file></files><comments><comment>Reduce shard inactivity timeout to 5m</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/TransportSealIndicesAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java</file><file>src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java</file><file>src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestSyncedFlushAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/seal/SealIndicesTests.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/indices/SealTests.java</file><file>src/test/java/org/elasticsearch/indices/flush/FlushTest.java</file><file>src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTest.java</file><file>src/test/java/org/elasticsearch/indices/flush/SyncedFlushUnitTests.java</file><file>src/test/java/org/elasticsearch/indices/flush/SyncedFlushUtil.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Move index sealing terminology to synced flush</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/TransportSealIndicesAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/index/engine/CommitStats.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/IndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/SyncedFlushService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/seal/RestSealIndicesAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/seal/SealIndicesTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java</file><file>src/test/java/org/elasticsearch/common/lucene/LuceneTest.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file><file>src/test/java/org/elasticsearch/indices/FlushTest.java</file><file>src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file><file>src/test/java/org/elasticsearch/indices/SealTests.java</file><file>src/test/java/org/elasticsearch/indices/SycnedFlushSingleNodeTest.java</file><file>src/test/java/org/elasticsearch/indices/SyncedFlushUtil.java</file><file>src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Merge pull request #11179 from elastic/feature/synced_flush</comment></comments></commit></commits></item><item><title>Query Refactoring: Moving parser NAME constant to corresponding query builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11178</link><project id="" key="" /><description>As part of the query refactoring, this PR moves the NAME field that currently identifies each query parser to the corresponding query builder. The reason for this, as discussed in https://github.com/elastic/elasticsearch/pull/11121#discussion_r30233757, is that we need still need to be able to link parser and builder implementations, but that the query builder is independent of the parser (queries don't necessarily need to be coverted to XContent any more). Builders don't need to know about their parsers, but parsers need to be linked to their respective builders.

This change does the following:
- move the NAME constants from the parsers to the builders, making sure they are unique
- rename the existing getParserName() in the Builder classes to queryId(), returning the unique NAME constant. This is now public so we can retrieve query name when coding against the QueryBuilder abstract class
-  modify the names() method in the parsers so it contains at least one member that is the query id
</description><key id="76648351">11178</key><summary>Query Refactoring: Moving parser NAME constant to corresponding query builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-15T08:32:19Z</created><updated>2015-05-22T12:09:07Z</updated><resolved>2015-05-22T12:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-15T08:42:03Z" id="102320323">As a further explanation: there were two cases where the 1:1 mapping from parsers to builders was not completely clear:
- FQueryFilterParser didn't have a corresponding Builder class, instead queries with the `"fquery"` id were build in the (already deprecated) QueryFilterBuilder. To make this link clearer I introduced a new FQueryFilterParser that extends QueryFilterBuilder but otherwise shares the existing code there, just carries the moved NAME constant
- TermsLookupQueryBuilder was a builder without a correspondingly named parser. It used the TermsQueryParser and so also its NAME. To have a unique id for each builder I introduced the new name `
  `"terms_lookup"` here and added this to the names() array in the TermsQueryParser. Because all these names are used to register the parsers this will link the builder to that parser now.
</comment><comment author="javanna" created="2015-05-15T10:01:29Z" id="102352010">good change! left a few comments
</comment><comment author="cbuescher" created="2015-05-21T16:40:10Z" id="104347889">I rebased this PR after recent changes on feature branch and addressed the two comments above. The two main changes here:
- TermsQueryBuilder covers all of former TermsLookupBuilder functionality, leaving deprecated Stub class, leading to a 1:1 relation between Builder and Parser here.
- FQueryFilterBuilder directly extends QueryBuilder, moved cases where queryName is set out of QueryFilterBuilder.
</comment><comment author="javanna" created="2015-05-21T20:03:29Z" id="104404838">I did another review round, thanks @cbuescher 
</comment><comment author="cbuescher" created="2015-05-22T11:04:16Z" id="104621755">@javanna I had another round, mostly reverting some of the changes in my prevoius commits.
</comment><comment author="javanna" created="2015-05-22T11:21:42Z" id="104626653">left a super minor comment on indentation, LGTM besides that
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportTests.java</file></files><comments><comment>Merge pull request #11178 from cbuescher/feature/query-refactoring-movename</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportTests.java</file></files><comments><comment>Query Refactoring: Moving parser NAME constant to corresponding query builder</comment></comments></commit></commits></item><item><title>Fixing flag name in doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11177</link><project id="" key="" /><description /><key id="76637704">11177</key><summary>Fixing flag name in doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>docs</label></labels><created>2015-05-15T07:50:13Z</created><updated>2015-05-15T20:07:26Z</updated><resolved>2015-05-15T07:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-15T07:50:34Z" id="102303738">LGTM
</comment><comment author="clintongormley" created="2015-05-15T19:06:38Z" id="102495657">Thanks @ycombinator. For future reference, you should merge these fixes into the other branches as well (at the moment: 1.x and master), otherwise these fixes get lost.  I've done this for this PR.
</comment><comment author="ycombinator" created="2015-05-15T19:12:07Z" id="102496530">Thanks @clintongormley for merging into other branches for me this time. I'll keep that in mind for future PRs.

BTW, as I can only target a PR to a specific branch, how do I accomplish merging into other branches as well? Do I merge the PR against the targeted branch first, then merge into other branches using the `git merge ...` CLI? That's how I'd do it unless you have a better way.
</comment><comment author="ycombinator" created="2015-05-15T20:07:26Z" id="102511887">Never mind, @rjernst pointed out to me that I should use `git cherry-pick` instead of `git merge` for backporting to older branches. Makes sense. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms agg fails with medium-large "exclude" clause lists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11176</link><project id="" key="" /><description>A `terms` agg with an `exclude` arrray of medium size (in my case, 86 strings) was sufficient to cause this error:

```
Caused by: org.apache.lucene.util.automaton.TooComplexToDeterminizeException: Determinizing automaton would result in more than 10000 states.
    at org.apache.lucene.util.automaton.Operations.determinize(Operations.java:743)
    at org.apache.lucene.util.automaton.RunAutomaton.&lt;init&gt;(RunAutomaton.java:138)
    at org.apache.lucene.util.automaton.ByteRunAutomaton.&lt;init&gt;(ByteRunAutomaton.java:32)
    at org.apache.lucene.util.automaton.ByteRunAutomaton.&lt;init&gt;(ByteRunAutomaton.java:27)
    at org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude$StringFilter.&lt;init&gt;(IncludeExclude.java:88)
```

Example query: 

```
{
    "query": {
        ...
    },
    "aggs": {
        "sample": {
            "sampler": {
                "shard_size": 100
            },
            "aggs": {
                "suggestions": {
                    "terms": {
                        "field": "links",
                        "exclude": [
                            "Triangle-free graph",
                            "Digraph (mathematics)",
                            "Clique (graph theory)",
                            "K-vertex-connected graph",
                            "Connected graph",
                            "Semi-symmetric graph",
                            "Complement (graph theory)",
                            "Tournament (graph theory)",
                            "Tree (graph theory)",
                            "Degree (graph theory)",
                            "Node (graph theory)",
                            "Heawood graph",
                            "Cycle graph",
                            "Cage graph",
                            "Star (graph theory)",
                            "Hoffman–Singleton graph",
                            "Edge (graph theory)",
                            "Shrikhande graph",
                            "Cubic graph",
                            "Cycle (graph theory)",
                            "Trees (graph theory)",
                            "Directed graph",
                            "Moore graph",
                            "Graph homomorphism",
                            "Hamiltonian graph",
                            "Regular graph",
                            "Vertex (graph theory)",
                            "Minor (graph theory)",
                            "Harries graph",
                            "Complement graph",
                            "K-edge-connected graph",
                            "Okapi BM25",
                            "Controlled vocabularies",
                            "Document retrieval",
                            "Recall (information retrieval)",
                            "C. J. van Rijsbergen",
                            "Subject indexing",
                            "Enterprise Search",
                            "Natural Language Processing",
                            "Information Retrieval",
                            "Relevance",
                            "SMART Information Retrieval System",
                            "Text mining",
                            "Inverse document frequency",
                            "Ranking functions",
                            "Binary classification",
                            "Enterprise search",
                            "Document classification",
                            "SIGIR",
                            "Query language",
                            "Gerard Salton",
                            "Gain (information retrieval)",
                            "Relevance (information retrieval)",
                            "Text Retrieval Conference",
                            "Compound term processing",
                            "Latent semantic indexing",
                            "Query expansion",
                            "Information retrieval#Recall",
                            "Vector Space Model",
                            "Stop words",
                            "Search engine",
                            "Index (search engine)",
                            "Tf-idf",
                            "Precision and recall",
                            "Query",
                            "Precision (information retrieval)",
                            "Latent semantic analysis",
                            "Summary statistics for contingency tables",
                            "Term frequency",
                            "Text retrieval",
                            "Natural language processing",
                            "Inverted index",
                            "Standard Boolean model",
                            "Question answering",
                            "Search engine indexing",
                            "Rocchio Classification",
                            "Jaccard index",
                            "Searching",
                            "Ranking function",
                            "Information retrieval",
                            "Relevance feedback",
                            "Special Interest Group on Information Retrieval",
                            "Information need",
                            "Gerard Salton Award",
                            "Vector space model",
                            "Automatic summarization"
                        ]
                    }
                }
            }
        }
    },
    "size": 1
}
```

I can see how consulting large sets of terms can be expensive and we might want to cap it but in the above case this risk is mitigated through the use of the "sampler" agg to consider a maxiumum of 100 top-matching docs.

The use case for this type of query is looking for new terms outside of a set that has already been gathered by the client eg aiding graph exploration by looking for connections beyond what you already have collected:

![excludebug](https://cloud.githubusercontent.com/assets/170925/7648108/a023170e-fad6-11e4-81fd-1ed43f7a3a6d.png)

These sorts of exclude lists can grow large so a cap of around 85 terms seems low for this use case.
</description><key id="76623256">11176</key><summary>Terms agg fails with medium-large "exclude" clause lists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label></labels><created>2015-05-15T06:54:17Z</created><updated>2015-05-15T18:40:59Z</updated><resolved>2015-05-15T18:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-15T10:27:52Z" id="102362603">We currently handle lists of terms as an automaton just like we handle regular expressions for simplicity. I guess we could specialize the terms list case if we want large lists to work.
</comment><comment author="ppf2" created="2015-05-15T17:27:41Z" id="102463896">Per @jpountz , this error is only affecting the master branch.  Just want to confirm that this will not get introduced to 1.6 when it gets released for we already have users using the exclude exact value with large number of values :)
</comment><comment author="ppf2" created="2015-05-15T18:40:59Z" id="102488415">@markharwood cool thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java</file></files><comments><comment>Aggregations improvement: exclude clauses with a medium/large number of clauses fail.</comment><comment>The underlying automaton-backed implementation throws an error if there are too many states.</comment></comments></commit></commits></item><item><title>Rename watcher.interval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11175</link><project id="" key="" /><description>This potentially conflicts with settings for the new Watcher plugin, but could be renamed to script.refresh.interval or similar to better reflect the actual use.

See http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html#_automatic_script_reloading for where this is in use
</description><key id="76574909">11175</key><summary>Rename watcher.interval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>:Settings</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-05-15T03:20:11Z</created><updated>2015-06-09T08:03:34Z</updated><resolved>2015-06-09T08:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-22T08:36:28Z" id="104571409">We just discussed this issue and came up with `script.file.reload.interval` to make clear it only applies to script files. This would also need to leverage the deprecation infrastructure: https://github.com/elastic/elasticsearch/pull/11285
</comment><comment author="uboness" created="2015-05-22T09:00:18Z" id="104581412">+1 on rename... `watcher` in the context of elasticsearch doesn't say much. `script.file.reload.interval` focuses on script reloading, while the watcher service in es has a broader function - it's basically a reload service for resources where a "resource" can be anything. So we need to be careful with the rename here and come up with an appropriate more generic name
</comment><comment author="jpountz" created="2015-05-22T09:11:36Z" id="104585527">How about `resource.reload.interval` then? @imotov since you worked on this feature, do you have any opinions on this?
</comment><comment author="imotov" created="2015-05-22T12:44:46Z" id="104650561">Indeed it was meant to be a generic file/url resource reloading mechanism, so I think `resource.reload.interval` would work better.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java</file><file>core/src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java</file></files><comments><comment>ResourceWatcher: Rename settings to prevent watcher clash</comment></comments></commit></commits></item><item><title>20 minutes and stop service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11174</link><project id="" key="" /><description>274.9 MB of 2 GB Used / 1.73 GB Free

And each 20 minutes i need do this:

sudo /etc/init.d/elasticsearch start
curl -XGET 'localhost:9200'

Please give me instructions
Tnks
</description><key id="76550146">11174</key><summary>20 minutes and stop service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnfelipe</reporter><labels /><created>2015-05-15T01:31:32Z</created><updated>2015-05-19T16:11:39Z</updated><resolved>2015-05-15T02:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-05-15T02:29:25Z" id="102226244">Hi @johnfelipe 
Please join us on https://discuss.elastic.co/ for troubleshooting help, we reserve github for confirmed bugs and feature requests.
</comment><comment author="johnfelipe" created="2015-05-19T16:11:39Z" id="103570091">https://discuss.elastic.co/t/service-stop-each-15-or-20-minutes/765/4, no respond
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster not indexing new documents when in "yellow" state.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11173</link><project id="" key="" /><description>I restarted a node in my cluster but new documents are not indexing.  I thought "yellow' state meant that unassigned replicas are getting initialized but the cluster is still fully functional.  Is this a bug?

+=== debug

$ curl -XGET 'es.internal.company.com:9200/_cluster/health?pretty'
{
  "cluster_name" : "es_cluster_a",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 9,
  "number_of_data_nodes" : 6,
  "active_primary_shards" : 1265,
  "active_shards" : 3041,
  "relocating_shards" : 0,
  "initializing_shards" : 12,
  "unassigned_shards" : 725,
  "number_of_pending_tasks" : 0
}

curl -XGET 'es.internal.company.com:9200/_cat/indices?pretty'|sort -k 3n

yellow open web-logs-2015.05.11              8 2 577100361   0 697.3gb  346.7gb
yellow open web-logs-2015.05.12              8 1 737742512   0 640.3gb  496.3gb
yellow open web-logs-2015.05.13              8 1 632722103   0 557.6gb  402.5gb
yellow open web-logs-2015.05.14              8 2 240925903   0 309.3gb  148.5gb
yellow open web-logs-2015.05.15              8 2         0   0    920b     920b
</description><key id="76537241">11173</key><summary>Cluster not indexing new documents when in "yellow" state.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rsomcio</reporter><labels /><created>2015-05-15T00:27:14Z</created><updated>2015-05-15T02:48:22Z</updated><resolved>2015-05-15T02:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-15T02:48:18Z" id="102233088">@rsomcio `yellow` means that all primary shards have started but it doesn't mean that you have enough shards to achieve default [write consistency](http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-consistency) for all indices if you have more than one replica per index. Your assumption is only correct if indices in the cluster have no more then 1 replica. It looks like in your case at least some of the indices have 2 replicas or more. For such indices a primary shard and at least one other replica have to be available to perform index operation with default level of write consistency. The `yellow` health status doesn't guarantee that. This issue is discussed in details in #4755. So, I am going to close this one as a duplicate. Please feel free to reopen it if my assessment of the issue was incorrect and you think that you have a different issue. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify and validate features relying on _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11172</link><project id="" key="" /><description>Today, we have some features that depend on `_source` being complete.  We now have the ability to know whether `_source` exists and has all original fields (no includes/excludes) with #11171.

We now need to add checks to features that rely on source and simplify them.  For example, highlighting currently has code to try and highlight based on stored fields if source does not exist.  We should not do this. As discussed in #8142, disabling source is an advanced feature, and comes with a cost. That cost is some features don't work, and we should simplify highlighting to reflect that.
</description><key id="76508282">11172</key><summary>Simplify and validate features relying on _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-05-14T22:20:21Z</created><updated>2016-01-18T19:22:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T19:22:35Z" id="172627829">also the new reindex API
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add back support for `enabled`/`includes`/`excludes` in `_source` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11171</link><project id="" key="" /><description>This adds back the ability to disable _source, as well as set includes
and excludes. However, it also restricts these settings to not be
updateable. enabled was actually already not modifiable, but no
conflict was previously given if an attempt was made to change it.

This also adds a check that can be made on the source mapper to
know if the the source is "complete" and can be used for
purposes other than returning in search or get requests. There is
one example use here in highlighting, but more need to be added
in a follow up issue (eg in the update API).

closes #11116
</description><key id="76474363">11171</key><summary>Add back support for `enabled`/`includes`/`excludes` in `_source` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T20:08:37Z</created><updated>2015-06-08T08:57:54Z</updated><resolved>2015-05-14T21:41:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-14T21:37:27Z" id="102177499">@jpountz I pushed a commit simplifying includes/excludes so they can no longer be null.
</comment><comment author="jpountz" created="2015-05-14T21:40:28Z" id="102177991">LGTM
</comment><comment author="uschindler" created="2015-05-14T23:30:44Z" id="102199036">Many thanks! -- Uwe
</comment><comment author="karmi" created="2015-05-15T07:02:27Z" id="102291075">@rjernst Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file></files><comments><comment>Merge pull request #11171 from rjernst/fix/11116</comment></comments></commit></commits></item><item><title>Cluster startup deadlock involving Lucene static initialization bits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11170</link><project id="" key="" /><description>From https://issues.apache.org/jira/browse/LUCENE-6482

&gt; during startup of Elasticsearch nodes: From the last 2 stack traces, you see that there are 2 things happening in parallel: Loading of Codec.class (because an Index was opened), but in parallel, Elasticsearch seems to initialize the PostingsFormats.class in the class CodecModule (Elasticsearch). In my opinion, this should not happen in parallel, but a fix would maybe that CodecModule should also call Codecs.forName() so those classes are initialized sequentially at one single place. The problem with Codec.class and PostingsFormat.class clinit running in parallel in different threads may have similar effects like you have seen in the blog post (Codecs depend on PostingsFormat and some PostingsFormats depend on the Codec class, which then hangs if PostingsFormats and Codecs are initialized from 2 different threads at same time, waiting for each other). But we have no chance to prevent this (unfortunately).
&gt; 
&gt; I cannot say for sure, but something seems to be fishy while initializing Elasticsearch, because there is too much happening at the same time. In my opinion, Codecs and Postingsformats and Docvalues classes should be initialized sequentially, but I have no idea how to enforce this.
</description><key id="76465192">11170</key><summary>Cluster startup deadlock involving Lucene static initialization bits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shikhar</reporter><labels><label>:Core</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-05-14T19:30:10Z</created><updated>2015-06-23T19:31:09Z</updated><resolved>2015-06-23T19:31:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T18:43:15Z" id="102488817">@shikhar We haven't seen this issue reported before.  From your last comment:

&gt; It might have been due to using a custom Elasticsearch discovery plugin which is purely asynchronous that those 2 bits ended up happening in parallel, and caused the deadlock.

Did you determine whether this happened when you weren't using the custom plugin?
</comment><comment author="shikhar" created="2015-05-16T07:44:55Z" id="102584661">@clintongormley Not yet, but I'll try to ascertain whether this is still happening by doing a ton of cluster restarts with and without the plugin. I'll reopen if I find this is still an issue. Thanks :)
</comment><comment author="shikhar" created="2015-05-19T12:59:15Z" id="103476355">I have been able to reproduce this with Zen. The only interesting setting is probably `discovery.zen.publish_timeout=0` which makes it similar to [eskka](http://github.com/shikhar/eskka) in that master does not block for acks before processing more updates. But I'm not sure if this is relevant to the problem at all, just thought I'd mention in case it is.

To describe what happens, the cluster is stuck in the RED state while starting up. On the master node, there are many errors like:

```
[2015-05-19 12:48:34,460][WARN ][gateway.local            ] [search45-es1] [mfg-1431690242][0]: failed to list shard stores on node [vrHV_EVZSuSFdCGEC7lUsg]
org.elasticsearch.action.FailedNodeException: Failed node [vrHV_EVZSuSFdCGEC7lUsg]
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178)
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [search44-es1][inet[/172.31.240.55:8301]][internal:cluster/nodes/indices/shard/store[n]] request_id [4273] timed out after [30001ms]
        ... 4 more
```

A thread dump from the `vrHV_EVZSuSFdCGEC7lUsg` node that it is complaining about: 
https://gist.github.com/shikhar/8ad5c166c6a20458c4d2 -- grepping for `Codec` or `&lt;init&gt;` should reveal the threads where the static initialization deadlock is happening. These threads are shown as RUNNABLE while in `Object.wait()`

My hunch is that this can happen while the cluster is starting up and an index create event lands (we have some crons that are creating new indexes in the background as a way of reindexing). You can see that one of the threads that is deadlocked landed in the Codec-ey bits via:

```
org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:311)
```
</comment><comment author="shikhar" created="2015-05-19T13:05:33Z" id="103478094">I think the static initialization of the deadlock-prone bits that @uschindler mentioned should happen in a deterministic manner at ES startup, rather than e.g. when creating indexes.
</comment><comment author="clintongormley" created="2015-05-19T13:10:14Z" id="103479359">@shikhar thanks for coming back to us with the details
</comment><comment author="shikhar" created="2015-05-19T13:13:58Z" id="103481282">Forgot to mention that this was reproduced on ES 1.5.2
</comment><comment author="uschindler" created="2015-05-19T13:24:06Z" id="103488195">The discussion when and why this might happen is here:
https://issues.apache.org/jira/browse/LUCENE-6482

The proposal is to load the Codec, PostingsFormat and DocValuesFormat in the Elasticsearch Boostrap class. So something like calls to:

``` java
Codec.reloadCodecs();
PostingsFormat.reloadPostingsFormats();
// and others, see Solr's SolrResourceLoader startup
```

somewhere at the beginning of Boostrap class. In Solr this is already done in the startup code. This prevents dependency problems if those index classes are started in a concurrent way.
</comment><comment author="rmuir" created="2015-05-19T13:43:16Z" id="103499533">Uwe, why should users of lucene have to do this? Tracking down a class loading deadlock like this is something i do not wish on anyone (thank you very much @shikhar for digging here). So I don't like that you need a "magical sequence" at the startup of your app, or you might get hangs.

Is there no better way?
</comment><comment author="uschindler" created="2015-05-19T14:10:55Z" id="103512816">The problem here is that on Elasticsearch startup the big builder is initializing a lot of shit in a unforseeable order, and concurrent in addition. The main problem here is that the Codec/Postingsformat/... stuff has a large spaghetti of load-time dependencies, so if called in the wrong order concurrently, it hangs.

To solve the issue here, my suggestion is to simply call those static initializes at a defined place in Elasticsearch's bootstrap, so it cannot happen that index A is initialized at the same time while index B is also booting up and both are loading SimpleText codec and ElasticsearchFooBar codec in parallel.

We can then look into Lucene's code to cleanup the loading of codecs. The big problem is the dependency graph and static initializers. The main problem is the Codec.getDefault() value that is initialized with Codec.forName(). I think the problem would be much easier if we would just initialize the default codec with a simple new LuceneXYCodec() instead of SPI on the &lt;clinit&gt; phase. Currently we have code like "static Codec defaultCodec = forName("Lucene50");"

So two steps:
- prevent deadlocks in ES caused by Lucene for now by explicitely initializing Codec class in Bootstrap.java
- change Lucene's Codec/PostingsFormat to initialize the default codecs/formats without forName(), because this can lead to deadlock. Just use a simple "new" call to constructor.

Similar propblems are regularirly happening in ICU4J: You remember the case where ICU4J caused NullPointerExceptions because it was trying to print a message using the default codec before the default codec was actually loaded and initialized... The problems in Lucene are the same, just happening in multithreaded code.
</comment><comment author="rmuir" created="2015-05-19T14:16:36Z" id="103516254">&gt; I think the problem would be much easier if we would just initialize the default codec with a simple new LuceneXYCodec() instead of SPI on the phase. Currently we have code like "static Codec defaultCodec = forName("Lucene50");"

Is that really all thats needed to fix it in Lucene? If so, +1.

But for master branch, we shouldn't need a hack in bootstrap, lets just fix it in lucene and upgrade to a newer lucene snapshot jar.

For any backport fixes (elasticsearch 1.X) your solution is practical. But i would not be upset to see a 4.10.5 either.
</comment><comment author="uschindler" created="2015-05-19T14:20:38Z" id="103518678">In my opinion this should work fine. We should just not use SPI to initialize the defaults.

The problem with forName() is: You are loading a lot of classes by that, which initialize themselves and may reference to the class doing the forName(). If we initialize the default codec statically, we dont need to wait for forName() to complete, the default codec is there ASAP. If some index file needs another codec, it can load it with forName() but there is no risk in deadlocking.
</comment><comment author="rmuir" created="2015-05-19T14:32:57Z" id="103524951">I see. I think this is a lot simpler in 5.x, because forName is not "abused" to load "impersonator" codecs in tests. You remember, in that case we had a Lucene3x that was writeable and we reordered jar files in tests and all that... all gone now.

So I think its too tricky to fix as a bugfix in lucene 4.10.x, and we should apply your logic to bootstrap for ES 1.x. But lets fix lucene 5.x as proposed, then simply update our snapshot for ES master.
</comment><comment author="uschindler" created="2015-05-19T14:46:14Z" id="103531711">I would wish we could write a test to check this out... But as usual, this test would need to run isolated (new and fresh JVM trying to open several indexes in parallel without loading Codec class before).

Thank you for remembering me why we did this Codec.forName() for the static initialization! That makes sense. I think we should kill this in 5.x, its a one-line change (and maybe a change in smoketester that validates that the defaultCodec line was updated after release). PostingsFormat is not affected, all of that is caused by Codec's &lt;clinit&gt;.

I analyzed the stack traces provided. What happens in the example is exactly as described: One of the threads is initializing Codec class (&lt;clinit&gt;), as side effect of opening an index. 3 other threads are opening other indexes at same time. Because the Codec class is not yet initialized in clinit, those threads are waiting before SegmentInfos's call to Codec.forName(), which is blocked because the SegmentInfos's call cannot access the Codec class yet. Because of concurrent class loading the newInstance() call in NamedSPILoader to the codec class actually loaded is blocked (too). This happens because JVM has internal locks on that (not 100% sure why and where this blocks, but seems to be the reason).
</comment><comment author="rmuir" created="2015-05-19T14:52:06Z" id="103534065">Related to that, there was some previous discussion about implementing a static detector here: https://issues.apache.org/jira/browse/LUCENE-5573

I am just hoping some policeman finds himself bored one day and implements the ASM logic so we could try to detect these, even if it wouldn't find this particular one.
</comment><comment author="uschindler" created="2015-05-19T15:40:04Z" id="103557469">I know this issue :-) I wonder that there is no Eclipse plugin that can detect this!
</comment><comment author="rmuir" created="2015-05-19T16:10:40Z" id="103569781">I did a quick survey, nothing exhaustive but I couldn't find anything. I haven't tried to play around with writing a detector. But IMO ideally it would be a policeman-tool like forbidden API, and we just scan for it in builds and fail the build.
</comment><comment author="shikhar" created="2015-06-22T17:59:27Z" id="114199972">hey @rmuir what are the plans for tackling this? It is fixed in Lucene 5.2.1 (https://issues.apache.org/jira/browse/LUCENE-6482) -- if only ES 2.0 can use 5.x, there probably either needs to be that ES Bootstrap workaround or a backport to Lucene 4.x
</comment><comment author="uschindler" created="2015-06-22T18:05:50Z" id="114202517">I would go with the Bootstrap workaround for ES 1.5 and 1.6. This is very easy to implement and there is no need to release several 4.x versions of Lucene. Should I provide a PR?
</comment><comment author="clintongormley" created="2015-06-23T14:37:43Z" id="114528634">@uschindler yes please! :)
</comment><comment author="s1monw" created="2015-06-23T14:37:50Z" id="114528697">@uschindler can you open a PR for this against 1.x branch please
</comment><comment author="uschindler" created="2015-06-23T17:00:24Z" id="114572705">OK, will provide one later! I am currently in the U.S. and busy :-)
</comment><comment author="uschindler" created="2015-06-23T17:00:33Z" id="114572740">Ah which branch?
</comment><comment author="uschindler" created="2015-06-23T17:57:50Z" id="114590773">Found it out: 1.x
</comment><comment author="uschindler" created="2015-06-23T19:08:18Z" id="114613088">I created a PR: #11837

This one does a "fake" Codecs.availableCodecs() on InternalNode startup. I did not do it in the Boostrap, so also people embedding ES can make use of it. In any case, the method call is cheap and does not slowdown, it just returns an immutable list.

Is there a problem that InternalNode directly depends on lucene-core.jar?
</comment><comment author="uschindler" created="2015-06-23T19:24:36Z" id="114616190">Thanks for merging!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change the default for `http.cors.allow-origin`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11169</link><project id="" key="" /><description>We currently have CORS disabled by default since #7151 was merged, but once it is enabled we still allow any origin (`http.cors.allow-origin` defaults to `*`). This is a bad default as a user may just enable CORS without specifying the origins to allow, which means that any site can still issue requests to elasticsearch. We should probably default `http.cors.allow-origin` to `null` like @rashidkpc suggested in https://github.com/elastic/elasticsearch/issues/7151#issuecomment-53098415
</description><key id="76458252">11169</key><summary>Change the default for `http.cors.allow-origin`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T19:07:24Z</created><updated>2015-07-12T10:30:44Z</updated><resolved>2015-07-10T18:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-15T14:49:29Z" id="102419440">+1
</comment><comment author="clintongormley" created="2015-05-15T19:20:25Z" id="102499233">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java</file></files><comments><comment>change CORS allow origin default to allow no origins</comment></comments></commit></commits></item><item><title>When using `recover_on_any_node` on shared filesystem, respect Deciders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11168</link><project id="" key="" /><description>When in a shared filesystem environment and recovering the primary to
any node. We should respect the allocation deciders if possible (still
force-allocting to another node if there aren't any "YES" decisions).

The AllocationDeciders should take precedence over the shard state
version when force-allocating an unassigned primary shard.
</description><key id="76457480">11168</key><summary>When using `recover_on_any_node` on shared filesystem, respect Deciders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T19:03:48Z</created><updated>2015-06-07T17:25:11Z</updated><resolved>2015-05-15T22:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-15T13:15:29Z" id="102393753">left a few comments
</comment><comment author="dakrone" created="2015-05-15T15:59:14Z" id="102446272">@kimchy pushed a new commit addressing the feedback, thanks for taking a look!
</comment><comment author="kimchy" created="2015-05-15T22:03:36Z" id="102535143">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixing flag name in doc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11167</link><project id="" key="" /><description /><key id="76433491">11167</key><summary>Fixing flag name in doc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels /><created>2015-05-14T17:38:28Z</created><updated>2015-05-15T07:38:00Z</updated><resolved>2015-05-15T07:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Properly export JAVA_HOME as defined in defaults.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11166</link><project id="" key="" /><description>Specifying JAVA_HOME in init defaults is not exported in RedHat init.d script.

There is no issue when a system java is available in $PATH. The issue occurs when java is not installed in $PATH and the which java command returns a JAVA version. If you try setting up JAVA_HOME without java in the path the init script fails to export JAVA_HOME for use in the elasticsearch user environment. This leaves elasticsearch lost as to where either JAVA_HOME or JAVA is.
</description><key id="76404076">11166</key><summary>Properly export JAVA_HOME as defined in defaults.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dforste</reporter><labels><label>:Packaging</label><label>review</label></labels><created>2015-05-14T15:57:20Z</created><updated>2016-03-08T19:44:05Z</updated><resolved>2016-03-08T19:44:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T18:35:21Z" id="102487374">Hi @dforste 

Thanks for the PR.  Please could I ask you to sign the CLA? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dforste" created="2015-05-15T19:06:34Z" id="102495644">@clintongormley CLA all signed and ready to go. 
</comment><comment author="clintongormley" created="2016-03-08T19:44:05Z" id="193938239">Hi @dforste 

Sorry it has taken a while to look at this.  JAVA_HOME is exported at the start of the script, and it isn't altered later on in the script, so this PR doesn't do anything.

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When master is running 1.5.2, master data cannot be retrieved from nodes running 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11165</link><project id="" key="" /><description>Hi, I am upgrading cluster from 1.4.4 to 1.5.2 following the Rolling upgrade instructions and I faced this. I first upgraded all master nodes to 1.5.2, then I started with data nodes and when I wanted to disable shard reallocation, I got `{"error":"MasterNotDiscoveredException[]","status":503}`. I do the "disable reallocation" request on that node that is being upgraded by sending it to localhost.

I analyzed this a bit and observed that any request that is related to "master data" fails with the same error when sending this from 1.4.4 node. On nodes that were already upgraded to 1.5.2, this works just fine.
</description><key id="76351051">11165</key><summary>When master is running 1.5.2, master data cannot be retrieved from nodes running 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinhynar</reporter><labels /><created>2015-05-14T12:57:30Z</created><updated>2015-05-14T16:10:28Z</updated><resolved>2015-05-14T16:10:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martinhynar" created="2015-05-14T16:10:27Z" id="102087666">It turned out that there was a network instability that caused that some of the nodes were unresponsive and number master nodes was fluctuating around min_master_nodes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unify script and template requests across codebase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11164</link><project id="" key="" /><description>This change unifies the way scripts and templates are specified for all instances in the codebase. It builds on the Script class added previously and adds request building and parsing support as well as the ability to transfer script objects between nodes. It also adds a Template class which aims to provide the same functionality for template APIs.

Note: This PR maintains backwards compatibility with versions 1.5 and before. There will be a separate PR to remove the backwards compatibility in 2.0 once this is merged (this will also include updating the "Breaking changes in 2.0" doc.

Closes #11091
Closes #10810
Closes #10113
</description><key id="76321251">11164</key><summary>Unify script and template requests across codebase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T11:00:45Z</created><updated>2015-08-13T13:56:58Z</updated><resolved>2015-05-29T15:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-05-26T14:33:31Z" id="105545473">LGTM - I've had to skim through a lot of stuff without review on the assumption that
1) Some changes are formatting only and
2) much of the "new" code is a cut and paste of old test code renamed to testFooUsingOldScriptApi
</comment><comment author="colings86" created="2015-05-26T15:05:04Z" id="105556210">@markharwood thanks for the review, I've pushed  new commit
</comment><comment author="jpountz" created="2015-05-29T09:59:59Z" id="106762637">This PR is mostly impossible to review due to its size, but I skimmed through the patch and it looks good overall. The main concern is about some exception changes that could trigger serialization errors on clusters that have mixed versions, but that should be easy to fix. Let's not wait to long before pushing, such large pull requests can easily become painful to rebase.
</comment><comment author="jpountz" created="2015-05-29T13:54:23Z" id="106809477">LGTM, I think we just need to rename scriptObject() to script() on UpdateRequest
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use our provided JNA library, versus one installed on the system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11163</link><project id="" key="" /><description>The JNA provided by the system might be older/incompatible and not work.

It happened easily with my system openjdk because some package installed an older JNA .so (from version 3). This means things like mlockall don't work with current master and that JDK because of "dll hell".

By specifying -Djna.nosys=true, we always use the bundled native library over anything else that happens to be on the system.  In most cases this is just the same logic already happening, unless you have DLL hell.

See docs in the source code: https://github.com/twall/jna/blob/master/src/com/sun/jna/Native.java#L64-L77

Only potential downside is as described in those docs: users with special security configurations might have to configure this differently. But they have to do special configuration for JNA to work anyway! On the other hand, it will work out of box better in the general case, where there is a conflict.

Tested on linux (including my JDK with conflicting JNA library), mac, and windows.
</description><key id="76227705">11163</key><summary>Use our provided JNA library, versus one installed on the system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T04:36:30Z</created><updated>2016-02-17T14:47:12Z</updated><resolved>2015-05-14T13:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-14T06:54:12Z" id="101942274">looks great!, I can't imagine how long it too to find it...
</comment><comment author="jimmyjones2" created="2015-05-19T14:41:29Z" id="103529637">Won't this break systems with noexec /tmp? These already don't work out of the box, but at least the library can be extracted and moved into a library path, whereas this will require another undocumented step. Or even better, could the ES package already have the library extracted out under the ES directory hierarchy?

While noexec /tmp isn't the default in any mainstream distro, it's security 101 to not allow code to be executed from /tmp so is reasonably common.
</comment><comment author="rmuir" created="2015-05-19T14:43:28Z" id="103530578">Right, as I described in my comment that is the tradeoff here. I think its far more common that someone would have a conflict with a system JNA. Someone with a special security config can just change the sysprop to do what they like.
</comment><comment author="rmuir" created="2015-05-19T14:46:05Z" id="103531629">&gt; Or even better, could the ES package already have the library extracted out under the ES directory hierarchy?

Just set JAVA_OPTS="-Djava.io.tmpdir=/personal/preference" for this. Or set it via the other mechanisms in your OS: TMPDIR, TEMP, TMP, whatever it respects depending. We don't hijack this in any way and just respect what you configure in the JVM/OS environment.
</comment><comment author="rmuir" created="2015-05-19T15:02:14Z" id="103538559">By the way, there is also a separate system property `jna.tmpdir` just for unpacking to a temporary place. So you can set JAVA_OPTS="-Djna.tmpdir=/personal/preference" to somewhere, if you want fine-grained control just over that.

Again, this was all done before, so its totally unrelated to this PR. Unpacking could always potentially happen, the problem was conflicts from so-called `system JNA`s which we now ignore by default.

A pull request that gave `jna.tmpdir` a different default would be reasonable IMO, if we think it improves things out of box. It seems a lot less extreme than hijacking all temporary file configuration from what the user expects!
</comment><comment author="jlecour" created="2016-02-17T14:29:21Z" id="185228858">Hi,

I hope I'm not off-topic, but i've found this issue when looking for a solution to an impossible startup of Elasticsearch on a Debian server with a `/tmp` mounted with `noexec` and a `/usr/share/elasticsearch` mounted with `ro`.

Your comments helped me. I've added this line in `/etc/default/elasticsearch` : 

```
ES_JAVA_OPTS="-Djava.io.tmpdir=/personal/preference"
```

I was thinking that ES might be able to detect that `TMPDIR` is `ro` or `noexec` and use a directory inside it's `DATADIR`.

In the meantime, I've changed my java options like this : 

```
ES_JAVA_OPTS="-Djava.io.tmpdir=${DATA_DIR}/tmp"
```

and it works great.

Also @rmuir suggested to simply change the `TMPDIR` environment variable, but I didn't find how to do this in the `/etc/default/elasticsearch` or /etc/init.d/elasqticsearch` files. Maybe some guidance could be added there.

Thanks anyway for the great work on ES. I love using it.
</comment><comment author="nik9000" created="2016-02-17T14:36:55Z" id="185231453">&gt; Also @rmuir suggested to simply change the TMPDIR environment variable

Lots of us are used to running Elasticsearch from the tar distribution so such things are simpler. I suspect you'd want to change the systemd service definition for elasticsearch to set that environment variable if you are using the package. Or the init script.

&gt; I was thinking that ES might be able to detect that TMPDIR is ro or noexec and use a directory inside it's DATADIR.

We probably could catch the write failures but DATA_DIR/tmp feels a bit hacky. If this is a common problem I think we should do something, but if its super uncommon just having this github issue come up in google searches might be enough.
</comment><comment author="rmuir" created="2016-02-17T14:44:39Z" id="185234924">&gt; We probably could catch the write failures but DATA_DIR/tmp feels a bit hacky.

This is not possible, because elasticsearch is overconfigurable. java.io.tmpdir must be supplied as a -D to the jvm initially. It cannot be set with System.setProperty after the fact.

hence this path cannot be configurable in es.yml. that is why i suspect it will not get fixed.
</comment><comment author="rmuir" created="2016-02-17T14:47:12Z" id="185235676">https://github.com/elastic/elasticsearch/issues/14372
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11163 from rmuir/jna_nosys</comment></comments></commit></commits></item><item><title>Cannot add filters to percolator from Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11162</link><project id="" key="" /><description>When performing a percolator query from the java API, adding a filter to the percolate operation causes no results being returned. Runnable code reproducing this issue can be found [here](https://github.com/leourbina/ElasticSearchTests/blob/master/src/test/java/elasticsearch/ElasticSearchTests.java#L128). The relevant portion is the below:

We setup two `matchAll` queries and index them with some accompanying metadata (`userId`)

``` java
MatchAllQueryBuilder query1 = QueryBuilders.matchAllQuery();
MatchAllQueryBuilder query2 = QueryBuilders.matchAllQuery();

client.prepareIndex(INDEX, PERCOLATOR_TYPE, "query1")
    .setSource(XContentFactory.jsonBuilder()
        .startObject()
        .field("query", query1)
        .field("userId", 1)
        .endObject())
    .execute().actionGet();

client.prepareIndex(INDEX, PERCOLATOR_TYPE, "query2")
    .setSource(XContentFactory.jsonBuilder()
        .startObject()
        .field("query", query1)
        .field("userId", 2)
        .endObject())
    .execute().actionGet();
```

Percolating a document yields both queries without filtering yields both queries, as expected:

``` java
PercolateResponse result = client.preparePercolate()
    .setIndices(INDEX)
    .setDocumentType(type)
    .setSource(anyDoc)
    .execute().actionGet();
```

However, adding a term filter by `userId`, yields no results:

``` java
PercolateResponse filteredResult = client.preparePercolate()
    .setIndices(INDEX)
    .setDocumentType(tweet.getType())
    .setSource(anyDoc)
    .setPercolateFilter(FilterBuilders.termFilter("userId", 1))
    .execute().actionGet();
```

Instead, I'm seeing the following shard failures being returned in the `PercolateResponse`.

```
[my-index-1][0] failed, reason [BroadcastShardOperationFailedException[[my-index-1][0] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate]; ]
```

I have verified that _it is_ possible to perform filtering/querying on the percolator directly via the REST api. At this point I'm not sure whether this is a legitimate bug, or whether I'm somehow misusing the Java API (I must admit, the docs relevant docs/javadocs are [pretty light on the subject](http://www.elastic.co/guide/en/elasticsearch/client/java-api/current/percolate.html)). Any ideas?
</description><key id="76224587">11162</key><summary>Cannot add filters to percolator from Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leourbina</reporter><labels /><created>2015-05-14T04:22:03Z</created><updated>2015-05-15T18:26:52Z</updated><resolved>2015-05-15T18:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="leourbina" created="2015-05-14T04:24:54Z" id="101905685">The same is true for queries, although currently my tests don't include it.
</comment><comment author="dadoonet" created="2015-05-14T05:34:11Z" id="101917405">Is "doc" different than "anyDoc" here?
</comment><comment author="martijnvg" created="2015-05-14T08:56:19Z" id="101977187">@leourbina This is a bit confusing, but instead of using `PercolateRequestBuilder#source()` you should use the `PercolateRequestBuilder#setPercolateDoc()` method to set the document to be percolated.

The source method assumes that you build the source entirely yourself (via a string, map or bytes reference).
</comment><comment author="leourbina" created="2015-05-14T15:12:41Z" id="102067646">Sorry, `doc` should have been `anyDoc`. Just making a point that since the percolated queries are `matchAll`, they should be returned as results unless filtered. Namely, the document percolated should not matter.
</comment><comment author="leourbina" created="2015-05-14T15:19:45Z" id="102069554">Ah! That makes more sense. Changing the code to 

``` java
PercolateResponse filteredResult = client.preparePercolate()
    .setIndices(INDEX)
    .setDocumentType(tweet.getType())
    .setPercolateDoc(new DocBuilder().setDoc(doc))
    .setPercolateFilter(FilterBuilders.termFilter("userId", 1))
    .execute().actionGet();
```

Actually works! The documentation doesn't seem to be clear that using setSource assumes that the end user needs to craft the whole source by hand, and there weren't many sources around the internet showcasing how to do filtering in percolate queries with the Java API. I would be happy to submit a PR with an updated version of [the percolate API](http://www.elastic.co/guide/en/elasticsearch/client/java-api/current/percolate.html) including an example of using filters. Thanks again!
</comment><comment author="martijnvg" created="2015-05-15T09:32:44Z" id="102343057">@leourbina +1 I'd be happy to pull this in!
</comment><comment author="clintongormley" created="2015-05-15T18:26:51Z" id="102484172">@leourbina a PR would be gratefully received :)  thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update ShapeBuilder and GeoPolygonQueryParser to accept non-closed GeoJSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11161</link><project id="" key="" /><description>While the GeoJSON spec does say a polygon is represented as an array of LinearRings (where a LinearRing is defined as a 'closed' array of points), it is a simple change to close the polygon for users. This addresses situations like those integrated with twitter (where GeoJSON polygons are not closed) such that our users do not have to write extra code to close the polygon.

closes #11131 
</description><key id="76214549">11161</key><summary>Update ShapeBuilder and GeoPolygonQueryParser to accept non-closed GeoJSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T03:27:29Z</created><updated>2016-04-13T05:16:41Z</updated><resolved>2015-07-17T15:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-14T20:43:14Z" id="102163832">How can you tell the difference between they left off the closing point, vs their serialization to json didn't write the entire polygon? Maybe this behavior should be controlled by a setting, as it relaxes restrictions but allows potentially incorrect data to be added?
</comment><comment author="lababidi" created="2015-05-30T14:57:00Z" id="107049773">@rjernst I understand what you mean. That then puts the onus on the person managing elasticsearch to have the correct configuration or setting. I believe the responsibility should be on the the person handling the data in the first place. If they write data to elasticsearch and find it's incorrect, they can check their serialization method to verify it's producing correct outputs. This will still fail if the polygon only contains 1,2, or 3 points. I can see why this type of validation is necessary, but again, I believe that validation should be on the json serialization not in elasticsearch.
</comment><comment author="nknize" created="2015-06-09T15:19:28Z" id="110399644">Since before ElastiCon there has been a lot of talk about maturing ES to make it harder to "hurt yourself".  The more I think about this issue the more I feel like we're opening a door for users to do exactly that.

In the spirit of user safety I like the idea of adding a setting to control this behavior. Though we're also trying to thin out the already massive number of settings for geo types. @clintongormley thoughts?
</comment><comment author="lababidi" created="2015-06-09T17:32:15Z" id="110441272">@nknize @clintongormley I hear what you're saying.

If you do add a setting, is the default going to allow "loose" GeoPoints, such as the non-closed ones? 

To elaborate:
The use case I have in mind would be an out of the box install of elasticsearch to then push twitter/gnip data right in after setting the mapping. Prior to this merge request ElasticSearch will fail this use case on this ingest. If this setting is created, I would strongly encourage the default setting to allow GeoPoints to be non-strict/non-closed.
</comment><comment author="clintongormley" created="2015-06-09T17:35:59Z" id="110442105">@nknize we have two settings on (eg) numeric fields:
- `ignore_malformed` - don't throw an exception if the data is malformed, but just ignore this field value
- `coerce` - try to coerce eg a string to a number, or a double to a long

Wondering if we should allow the same settings for geo-shapes, where `coerce:true` would auto-close?  (I think I'd default this setting to `false` though)
</comment><comment author="rjernst" created="2015-06-09T17:40:16Z" id="110443066">+1 to reusing settings (with documentation for what it means to "coerce" for geo points) and to keep the default sane (ie dont coerce by default).
</comment><comment author="nknize" created="2015-06-09T20:48:56Z" id="110500044">++ I like this idea for consistency across mappings.
</comment><comment author="lababidi" created="2015-07-10T20:25:50Z" id="120513220">What's the status on this merge?
</comment><comment author="nknize" created="2015-07-14T21:34:48Z" id="121397729">Code updated to use new mappings enhancements and optional coerce parameter
</comment><comment author="colings86" created="2015-07-15T12:18:02Z" id="121599623">Left a comment. I actually think `coerce` should default to `true` since thats the default for numbers. Should we not be consistent with numerics? /cc @clintongormley 
</comment><comment author="clintongormley" created="2015-07-15T14:46:56Z" id="121638938">I think I agree.  It'll also give a better OOB experience with the multitude of bad geodata out there.
</comment><comment author="rjernst" created="2015-07-15T15:04:15Z" id="121645111">I disagree. This would give a worse OOB experience. The point of this setting is to allow _not_ following the standard (closing the shapes) which leaves room for corrupt serialization of shapes (eg omitted some points) leaving a corrupt shape (because we "auto closed" it). We shouldn't be silent about it, the shape is corrupt! We should call the setting something different if we can't agree on this. The OOB experience needs to be "follow the standard or you get an error", otherwise we would silently allow corrupt data into the index.
</comment><comment author="nknize" created="2015-07-15T22:05:40Z" id="121763754">I agree that geodata differs from numerics in the context of 'coerce'. Even though its fundamentally two numeric elements they're also bounded by the spherical coordinate system. Defaulting coerce to true means ES will happily do more work just to accomodate non-sensical geodata. If the user really wants ES to "fix" bad data then they can explicitly set coerce to true. But IMHO I don't think we should take the default expectation that all geodata ES receives will be bad.
</comment><comment author="clintongormley" created="2015-07-16T09:50:44Z" id="121910078">OK - then coerce false
</comment><comment author="martijnvg" created="2015-07-16T10:34:19Z" id="121921897">Bumping the version up to 1.7.1 for the today's release.
</comment><comment author="colings86" created="2015-07-17T07:33:56Z" id="122203577">LGTM
</comment><comment author="cnwarden" created="2016-04-13T05:16:41Z" id="209230220">_coerce_: https://www.elastic.co/guide/en/elasticsearch/reference/current/coerce.html, does it only support latest version? how about 1.7.3?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Partial_fields should not reorder document properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11160</link><project id="" key="" /><description>When using partial fields the document's properties are reordered, we don't want that.
# Steps to replicate
1. Suppose I stored document schema is

``` javascript
{ "first": "abcd", "second": "xyz"}
```

``` javascript
POST _search
{
   "partial_fields" : {
        "_source" : {
            "exclude" : "field1"
        }
    }
}
```
# Expected

Document with property order is preserved same as they were stored after excluding or including properties as per query

``` javascript
{ "first": "abcd", "second": "xyz"}
```
# Actual

Document properties are jumbled

``` javascript
{ "second": "xyz", "first": "abcd" }
```
# Notes

We use document like this:

``` javascript
{
               "header": {
                  "identifier": "oai:nla.gov.au:nla.map-edlc1-10",
                  "datestamp": "2012-11-26T04:17:25Z",
                  "setSpec": "Map",
                  "status": null
               },
               "title": "Some title",
               "creator": [
                  "Sally Smith"
               ],
               "subject": [
                  "Geological",
                  "Geological cross sections -- France",
                  "Charts"
               ],
               "description": "Section in Noeux",
               "publisher": [
                  "1920"
               ],
               "contributor": [
                  "John Smith",
                  "Sally Smith"
               ],
               "date": "1977-01-01",
               "type": [
                  "Image"
               ],
               "format": "2 ms. sections on 1 sheet : col. ; sheet 41.8 x 34.3 cm.",
               "identifier": "http://nla.gov.au/nla.map-edlc11-10",
               "source": "Item held by National Library of Australia",
               "language": ["English"],
               "relation": "Part of: [Collection of Edgeworth David's maps] [cartographic material].",
               "coverage": "1977-01-01"
            }
         }
```
</description><key id="76205845">11160</key><summary>Partial_fields should not reorder document properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abibell</reporter><labels /><created>2015-05-14T02:45:11Z</created><updated>2015-05-22T01:45:38Z</updated><resolved>2015-05-15T18:23:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T18:23:22Z" id="102482270">Hi @abibell 

Two things to note:
- A JSON object is *"an unordered set of name/value pairs. There is no guarantee that order will be maintained, even when deserializing and reserializing an object.
- The ability to do updates on documents that do not store the entire original source (ie no `includes` or `excludes` allowed) will not be support the update API from version 2.0 onwards (see https://github.com/elastic/elasticsearch/issues/11116)
</comment><comment author="abibell" created="2015-05-15T21:36:47Z" id="102531078">@clintongormley Elastic search is a great product. In fact it is the best search product. The founders have done a great job.

When people say there is a problem there most likely can be a problem. Can be their own usage. The human element makes it harder to see it. Many things are man made. Including the definition of json. I need you to objectively look at the problem. Let's take another look at it.

We use elasticsearch, we store fulltext content in the document. We don't want to see it in results as it is too bulky. Currently we have source code that takes the data and strips out the fulltext content before sending it to content. This logic should have been in elastic, which it does But it is not quite predictable behavior.
- JSON definition doesn't say it is *unstructured according to Wikipedia or the RFC specification. Regardless of what it says. Why not make it better? Standard says json can be serialized and validated using xsd (see xs:sequence in XSD which requires the order to be maintained). If json was unstructured it would have failed.
- that's useful information. We are talking about data retrieval and partial updates doesn't matter in this context.

I looked at the source code it is complex and simple to retain the order. I will help fix the source code for you.
</comment><comment author="clintongormley" created="2015-05-18T11:35:38Z" id="103025964">&gt; JSON definition doesn't say it is *unstructured according to Wikipedia or the RFC specification. Regardless of what it says. Why not make it better? Standard says json can be serialized and validated using xsd. If json was unstructured it would have failed.

The spec at http://json.org/ says:

&gt; An object is an unordered set of name/value pairs.

The Wikipedia article at http://en.wikipedia.org/wiki/JSON#Data_types.2C_syntax_and_example says:

&gt; Object — an unordered collection of name/value pairs where the names (also called keys) are strings.

Most languages use hash randomization to avoid hash collision attacks (see http://lemire.me/blog/archives/2012/01/17/use-random-hashing-if-you-care-about-security/ ) so it is quite likely that deserealizing then reserealizing a JSON object will result in a different key order. This is by design.

Also see http://stackoverflow.com/a/4515863/819598 about why you shouldn't depend on order.
</comment><comment author="abibell" created="2015-05-18T16:44:34Z" id="103122824">I don't want this to be a "who is technically competent match". We can
simply provide facts invalidating each other, continue to say wrong source,
unrelated, wrong interpretation and maybe wonder why the other person is
not able to see the point. I don't take everything that is written as
Truth. To me technology is born to solve problems and not to be a
bottleneck.

All I am saying is JSON structure was preserved in all languages I have
used. It will make our lives better if we addressed the order. There is no
issue if we preserve the order. Random Hashing issue you mentioned is only
implemented by 3/3000+ languages and only recently. Plus randomised hashing
is not going to make any difference to the order preservation. If we
repeatedly serialize and deserialize the hash will change, which is wrong
because it is same object. It's not the problem of randomised hashing. It
is serializing processes.
On 18/05/2015 9:36 pm, "Clinton Gormley" notifications@github.com wrote:

&gt; JSON definition doesn't say it is *unstructured according to Wikipedia or
&gt; the RFC specification. Regardless of what it says. Why not make it better?
&gt; Standard says json can be serialized and validated using xsd. If json was
&gt; unstructured it would have failed.
&gt; 
&gt; The spec at http://json.org/ says:
&gt; 
&gt; An object is an unordered set of name/value pairs.
&gt; 
&gt; The Wikipedia article at
&gt; http://en.wikipedia.org/wiki/JSON#Data_types.2C_syntax_and_example says:
&gt; 
&gt; Object — an unordered collection of name/value pairs where the names (also
&gt; called keys) are strings.
&gt; 
&gt; Most languages use hash randomization to avoid hash collision attacks (see
&gt; http://lemire.me/blog/archives/2012/01/17/use-random-hashing-if-you-care-about-security/
&gt; ) so it is quite likely that deserealizing then reserealizing a JSON object
&gt; will result in a different key order. This is by design.
&gt; 
&gt; Also see http://stackoverflow.com/a/4515863/819598 about why you
&gt; shouldn't depend on order.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11160#issuecomment-103025964
&gt; .
</comment><comment author="jpountz" created="2015-05-18T17:18:42Z" id="103139728">I'm all for user friendliness but enforcing order has a cost. Not only in terms of complexity because we would need to ensure that all json manipulations that we perform maintain order, but also in terms of efficiency. For instance lots of people would like elasticsearch to be more space-efficient, and having the ability to reorder fields could be very useful for compression (eg. by grouping together fields that have the same type together to record them type only once, putting similar fields close to each other to help LZ77 compression, etc.).

I agree with the stackoverflow article that if you need order, the right data-structure is a json list, not a hash.
</comment><comment author="abibell" created="2015-05-21T03:01:40Z" id="104109586">Ok. The information I didn't knew is that we have **_source_include** &amp; **_source_exclude** which will solve the problem of removing our fulltext from elasticSearch results without custom code, saving network transfers. This ~~doesn't~~ does have the problem of **JSON reordering**. ~~Yay!~~ Oh no.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bake Lucene GeoPointField into GeoHash Grid Aggregations and GeoPoint Queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11159</link><project id="" key="" /><description>[LUCENE-6450](https://issues.apache.org/jira/browse/LUCENE-6450) introduces a new GeoPointField to core. This is a lightweight GeoPoint type with a simple API that indexes geopoints only and provides GeoPointInBBoxQuery and GeoPointInPolygonQuery classes.  It is currently an experimental sandbox feature that needs to be baked before full release (targeting ES 2.0).

This feature adds GeoPointField as the default indexing FieldType for `geo_point` mappings. It also updates Grid Aggregations, BoundingBox, and Polygon queries to use the new GeoPointInBBoxQuery and GeoPointInPolygonQuery classes.  This will be exposed on a feature branch and added as an experimental feature prior to full release.
</description><key id="76202165">11159</key><summary>Bake Lucene GeoPointField into GeoHash Grid Aggregations and GeoPoint Queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>feature</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-05-14T02:30:28Z</created><updated>2015-11-13T05:09:13Z</updated><resolved>2015-11-13T05:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure empty string completion inputs are not indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11158</link><project id="" key="" /><description>This PR ensures completion entries with an input of an empty string never gets indexed into the underlying FST. 

There is no way to retrieve an entry associated with an empty string through the _suggest API, so these entries should never be part of the underlying index, as they can never be retrieved but might bloat the FST with their context, weight, payload, etc.

closes #10987
</description><key id="76197537">11158</key><summary>Ensure empty string completion inputs are not indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T02:11:21Z</created><updated>2015-05-14T20:58:04Z</updated><resolved>2015-05-14T20:58:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-14T18:47:17Z" id="102132993">LGTM
</comment><comment author="areek" created="2015-05-14T20:58:03Z" id="102167402">Thanks @s1monw for the review! 
committed to v2.0.0 (https://github.com/elastic/elasticsearch/commit/af6b69e79106219d7c464cd4b130f89103c3a9e2), v1.6 (https://github.com/elastic/elasticsearch/commit/f830fd6858ebfc68101c7877af359b33a03248e9) and v1.5.3 (https://github.com/elastic/elasticsearch/commit/2979e2020be1771ef702a5ef5ddfee719d570c1e)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix FSRepository location configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11157</link><project id="" key="" /><description>Closes #11068
Closes #10828
</description><key id="76182691">11157</key><summary>Fix FSRepository location configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-14T01:03:01Z</created><updated>2015-06-06T17:32:05Z</updated><resolved>2015-05-21T04:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-14T02:02:50Z" id="101881154">looks good to me.
</comment><comment author="kimchy" created="2015-05-14T07:04:02Z" id="101946347">looks great, where can we add a note on the new setting and what to do when upgrading?
</comment><comment author="imotov" created="2015-05-14T13:46:21Z" id="102042390">@kimchy oh, I had that feeling that I forgot to document something! Now I know what it was. I will add a note to [migrate_2_0.asciidoc](https://github.com/elastic/elasticsearch/blob/master/docs/reference/migration/migrate_2_0.asciidoc). 
</comment><comment author="kimchy" created="2015-05-14T14:23:07Z" id="102049910">@imotov cool, if we backport it to 1.x, we should probably have it there as well in the upgrade notes?
</comment><comment author="imotov" created="2015-05-15T15:43:12Z" id="102440545">@rmuir I pushed another change and updated documentation to reflect the change in behavior. Could you take another look?
</comment><comment author="rmuir" created="2015-05-19T03:13:49Z" id="103314303">Thank you, I like it much better. Sorry for the delay, I am behind on email notifications.
</comment><comment author="tlrx" created="2015-05-19T08:03:47Z" id="103390080">This also closes #10828 I think
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure collate option in PhraseSuggester only collates on local shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11156</link><project id="" key="" /><description>Previously, collate query would be executed on all shards of an index using the client,
this leads to a deadlock when concurrent collate requests are run from the _search API,
due to the fact that both the external request and internal collate requests use the
same search threadpool.

As phrase suggestions are generated from the terms of the local shard, in most cases the
generated suggestion, which does not yield a hit for the collate query on the local shard
would not yield a hit for collate query on non-local shards.

Instead of using the client for collating suggestions, collate query is executed against the `ContextIndexSearcher`. This PR removes the ability to specify a `preference` for a collate 
query, as the collate query is only run on the local shard.

closes #9377
</description><key id="76139567">11156</key><summary>Ensure collate option in PhraseSuggester only collates on local shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T22:08:07Z</created><updated>2015-05-14T22:11:44Z</updated><resolved>2015-05-14T22:11:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-05-13T22:09:33Z" id="101831879">@s1monw would be awesome if you could take a look.
</comment><comment author="s1monw" created="2015-05-14T20:24:21Z" id="102157237">I left a tiny minor comment other than that LGTM you need to add 1.6.0 as a label too though
</comment><comment author="areek" created="2015-05-14T22:11:40Z" id="102184188">Thanks for the review @s1monw, wondering if this should also be back ported to  v1.5.3? This can be considered an enhancement and it removes the `preference` option, thoughts?
Committed to v2.0.0 (https://github.com/elastic/elasticsearch/commit/7efc43db25495b92833c4bc804f604b82597c157) and 1.6 (https://github.com/elastic/elasticsearch/commit/4ddc747ec5ee5fcf2bf5850d464acbf89244ddaa).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implement toXContent on ShardOpertionFailureException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11155</link><project id="" key="" /><description>ShardOperationFailureException implementations alread provide structured
exception support but it's not yet exposed on the interface. This change
allows nice rendering of structured REST exceptions also if searches fail on
only a subset of the shards etc.

Closes #11017

for instance 

``` JSON

PUT test2/test/1
{
  "foz": "bar"
}

GET _search?sort=foo
```

gives me now:

``` JSON

{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 7,
      "successful": 5,
      "failed": 2,
      "failures": [
         {
            "shard": 0,
            "index": "foo",
            "node": "2KRJGnOCRniCyyOcKw_ypg",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\"sort\":[{\"foo\":{}}]}]",
               "caused_by": {
                  "type": "search_parse_exception",
                  "reason": "No mapping found for [foo] in order to sort on"
               }
            }
         },
         {
            "shard": 0,
            "index": "foobar",
            "node": "2KRJGnOCRniCyyOcKw_ypg",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\"sort\":[{\"foo\":{}}]}]",
               "caused_by": {
                  "type": "search_parse_exception",
                  "reason": "No mapping found for [foo] in order to sort on"
               }
            }
         }
      ]
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": [
         {
            "_index": "test2",
            "_type": "test",
            "_id": "1",
```

similar for `_msearch`

``` JSON
GET _msearch
{"index": "test"}
{"sort": "foo"}
{"index": "_all"}
{"sort": "foo"}
```

now returns:

``` JSON
{
   "responses": [
      {
         "error": "SearchPhaseExecutionException[all shards failed]"
      },
      {
         "took": 28,
         "timed_out": false,
         "_shards": {
            "total": 12,
            "successful": 5,
            "failed": 7,
            "failures": [
               {
                  "shard": 0,
                  "index": "foo",
                  "node": "2KRJGnOCRniCyyOcKw_ypg",
                  "reason": {
                     "type": "search_parse_exception",
                     "reason": "Failed to parse source [{\"sort\": \"foo\"}]",
                     "line": 1,
                     "col": 2,
                     "caused_by": {
                        "type": "search_parse_exception",
                        "reason": "No mapping found for [foo] in order to sort on"
                     }
                  }
               },
```

bulk unfortunately still doesn't really work well here but it seems like a bigger thing and I wonder if we should do it at least for now
</description><key id="76117770">11155</key><summary>Implement toXContent on ShardOpertionFailureException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T20:44:23Z</created><updated>2015-06-07T18:40:04Z</updated><resolved>2015-05-18T08:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-13T20:45:36Z" id="101810865">@clintongormley happy?
</comment><comment author="kimchy" created="2015-05-13T20:48:32Z" id="101812105">left a minor comment, LGTM
</comment><comment author="s1monw" created="2015-05-13T20:55:57Z" id="101814562">I pushed the status - need to add some tests to ensure we don't loose this functionality before I push
</comment><comment author="clintongormley" created="2015-05-15T18:55:22Z" id="102491194">Looking much better.

@s1monw would it be possible to group shard failures in the same way that we do for top-level exceptions?
</comment><comment author="s1monw" created="2015-05-15T20:00:18Z" id="102509694">@clintongormley I pushed a new commit adding structured exceptions to bulk as well as the grouping to all the other relevant APIs:

``` JSON
{
   "took": 166,
   "errors": true,
   "items": [
      {
         "index": {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "_version": 1,
            "_shards": {
               "total": 2,
               "successful": 1,
               "failed": 0
            },
            "status": 201
         }
      },
      {
         "index": {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "status": "CONFLICT",
            "error": {
               "type": "version_conflict_engine_exception",
               "reason": "[test][1]: version conflict, current [1], provided [2]",
               "shard": 3,
               "index": "test"
            }
         }
      }
   ]
}
```

@kimchy I changed quite some other classes - wanna do another review?
</comment><comment author="clintongormley" created="2015-05-15T20:21:41Z" id="102516705">@s1monw looks awesome!  Only problem is in the bulk response.  The status is now being stringified to `CONFLICT` instead of 409 (while success still returns 200)
</comment><comment author="s1monw" created="2015-05-15T20:22:54Z" id="102517105">so we want the number @clintongormley not sure what the success comment means?
</comment><comment author="s1monw" created="2015-05-15T20:27:32Z" id="102519046">I see never mind, I fixed the stringifying problem in the last commit
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make SearchFactory static class in InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11154</link><project id="" key="" /><description>Now that lucene provides a way to identify if the warming reader is
the first initial opened reader we can detach this class from the
enclosing and make it static. This is important since it might access
not fully initialized members of the enclosing class since it's initialized
and used during constructor invocation.
</description><key id="76113190">11154</key><summary>Make SearchFactory static class in InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T20:27:40Z</created><updated>2015-05-15T20:02:53Z</updated><resolved>2015-05-15T20:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-13T20:27:53Z" id="101805791">@mikemccand @jpountz can you please take a look?
</comment><comment author="jpountz" created="2015-05-15T15:49:10Z" id="102443011">LGTM
</comment><comment author="mikemccand" created="2015-05-15T16:44:44Z" id="102456195">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_size does not support doc_values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11153</link><project id="" key="" /><description>Currently, if you try to create a mapping that tries to set `"doc_values" : true`, then it will either ignore the `_size` field or ignore that setting:

``` json
PUT /test
{
  "mappings": {
    "type": {
      "_size": {
        "enabled": true,
        "doc_values" : true,
        "store": true
      }, 
      "properties": {
        "field": {
          "type": "string"
        }
      }
    }
  }
}
```

This results in:

```
GET /test/_mapping
{
  "test": {
    "mappings": {
      "type": {
        "_size": {
          "enabled": true,
          "store": true
        },
        "properties": {
          "field": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

After adding some documents, then sorting on `_size`, you can see that it does indeed ignore the doc values setting (rather than just not showing it set, which was unlikely):

``` json
PUT /test/type/1
{
  "field" : "value1"
}

PUT /test/type/2
{
  "field" : "value2"
}

GET /test/_search
{
  "fields": ["_size"],
  "sort": [
    {
      "_size": {
        "order": "desc"
      }
    }
  ]
}
```

Fielddata:

```
GET /_cat/fielddata?fields=*&amp;v
id                     host        ip           node   total  _size 
2xF59inESXKOIkzK7GkavA my-hostname 192.168.1.31 Kl'rt 16.5kb 16.5kb 
```

This is not critical as `_size` must be enabled to even be used, but in that case, it would be useful to make use of doc values.
</description><key id="76109237">11153</key><summary>_size does not support doc_values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels /><created>2015-05-13T20:15:54Z</created><updated>2015-05-13T23:10:09Z</updated><resolved>2015-05-13T23:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-13T23:10:08Z" id="101844841">In #8143 we locked down meta fields, so even `store` here is no longer possible to set (it is always stored). It is important the configuration of meta fields are managed, so that we can depend on them for features.  Now that doc values are enabled by default for 2.0, for non-tokenized and indexed fields, `_size` will have doc values when enabled.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add test group for third-party tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11152</link><project id="" key="" /><description>We have quite a few integration tests across plugins that interact with third-party components. These can't run without additional setup/information: for example AWS/GCE/Azure cloud tests need you to pass a configuration file with credentials. RabbitMQ, CouchDB tests need you to start those processes up on your machine before running the test, and so on.

All of these plugins have duplicate test configurations in pom.xml to handle this case, yet its a common and general one. I think instead we should support `-Dtests.thirdparty=true / @ThirdParty` as a test group/annotation in the es test framework, and plumb it through es-parent, so that its consistent, and so we can remove all the duplicate build logic in these plugins.

See https://github.com/elastic/elasticsearch-parent/issues/41
</description><key id="76082663">11152</key><summary>Add test group for third-party tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-05-13T18:40:44Z</created><updated>2015-05-14T20:28:34Z</updated><resolved>2015-05-13T20:11:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-13T18:46:37Z" id="101773643">Should we change as well the pom.xml even if not used here?
</comment><comment author="rmuir" created="2015-05-13T18:49:16Z" id="101774127">It is not necessary, as es-core doesn't need this (same with tests.config which is already supported). 

We just need to add some lines to elasticsearch-parent and remove a bunch of duplicated code from plugins. I already did these steps locally with elasticsearch-aws and it works great.

And hopefully soon we are just using elasticsearch-parent!
</comment><comment author="dadoonet" created="2015-05-13T18:57:03Z" id="101777405">Agreed! Just that I don't want to forget it when "merging" both pom. :)
I used to adapt changes from core to parent :p

LGTM
</comment><comment author="rmuir" created="2015-05-13T18:57:19Z" id="101777460">@dadoonet I added the two props in a commit, just to keep es-core config synchronized with parent until its deduplicated.
</comment><comment author="dadoonet" created="2015-05-13T19:09:00Z" id="101779802">Yeah saw that. Thanks a lot!

2*LGTM :)
</comment><comment author="s1monw" created="2015-05-13T19:40:07Z" id="101789923">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #11152 from rmuir/party_time</comment></comments></commit></commits></item><item><title>multi_field "default" field not behaving as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11151</link><project id="" key="" /><description>From the [Multi Field Type docs](http://www.elastic.co/guide/en/elasticsearch/reference/0.90/mapping-multi-field-type.html):

&gt; With `multi_field` mapping, the field that has the same name as the property is treated as if it was mapped without a multi field. That’s the "default" field. It can be accessed regularly, for example using name or using typed navigation `tweet.name`.

Thus, given the following mapping, I would expect that querying `test.name` would be equivalent to `test.name.name` but this does not appear to be the case.

``` json
{
  // ...
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "string",
          "fields": {
            "name": {
              "analyzer": "full_name",
              "type": "string",
              "index": "analyzed"
            },
            "partial": {
              "search_analyzer": "full_name",
              "index_analyzer": "partial_name",
              "type": "string",
              "index": "analyzed"
            }
          }
        },
        // ...
      }
    }
  }
}
```

I get different results from the following queries:

```
GET tests/_search
{
  "query": {
    "match": {
      "name": "rett syndrome"
    }
  }
}

GET tests/_search
{
  "query": {
    "match": {
      "name.name": "rett syndrome"
    }
  }
}
```
</description><key id="76032353">11151</key><summary>multi_field "default" field not behaving as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marshall007</reporter><labels /><created>2015-05-13T15:48:42Z</created><updated>2015-05-15T18:17:34Z</updated><resolved>2015-05-15T18:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-13T22:17:20Z" id="101835353">First, you should make sure to use fully qualified field names in your queries, ie `test.name` and `test.name.name`. Second, I think you are confusing "old" multi-fields (pre 1.0) with current multi-fields.  See http://www.elastic.co/guide/en/elasticsearch/reference/1.5/mapping-core-types.html#_multi_fields_3.
</comment><comment author="marshall007" created="2015-05-13T23:16:23Z" id="101845781">@rjernst ok so to get the behavior I'm looking for my mapping should instead look like this?

``` json
{
  // ...
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "string",
          "analyzer": "full_name",
          "index": "analyzed",
          "fields": {
            "partial": {
              "search_analyzer": "full_name",
              "index_analyzer": "partial_name",
              "type": "string",
              "index": "analyzed"
            }
          }
        },
        // ...
      }
    }
  }
}
```

I still think if you define a multi-field with the same name as it's parent property that should override the default mapping for that field.
</comment><comment author="rjernst" created="2015-05-13T23:35:23Z" id="101849416">&gt; I still think if you define a multi-field with the same name as it's parent property that should override the default mapping for that field.

If you want to have different behavior for the original field, then change the settings for the original field? The name of sub fields should not matter. The `fields` behavior only means "copy the original value to some more fields, and access them as an offshoot of the original name".
</comment><comment author="clintongormley" created="2015-05-15T18:17:33Z" id="102477452">&gt; I still think if you define a multi-field with the same name as it's parent property that should override the default mapping for that field.

And, if you have a sub-field with the same name as its parent, it will just add a new field `my_field.my_field`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggs: No collected documents does not mean an empty aggregation should be built</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11150</link><project id="" key="" /><description>Reported in https://discuss.elastic.co/t/can-you-explain-this-to-me/592/1

The issue is that we assume that calling `buildEmptyAggregation()` on sub aggregators is ok if no documents were collected. However this is not true eg. in the case that `min_doc_count=0` on a sub terms aggregation. The bug is in `BucketsAggregator.bucketAggregations` which does this:

```
    protected final InternalAggregations bucketAggregations(long bucketOrd) {
        final ArrayList&lt;InternalAggregation&gt; childAggs = new ArrayList&lt;&gt;();
        final long bucketDocCount = bucketDocCount(bucketOrd);
        if (bucketDocCount == 0L) {
            for (int i = 0; i &lt; subAggregators.length; i++) {
                childAggs.add(subAggregators[i].buildEmptyAggregation()); // &lt;-- This is wrong
            }
        } else {
            // deal with non-empty buckets
        }

        return new InternalAggregations(childAggs);
    }
```

Reproducible with the following Sense script on 1.x:

```
PUT test 
{
  "settings": {
    "number_of_shards": 2
  }
}

PUT test/test/1
{
  "minutes": 300,
  "megabytes": 1000
}

PUT test/test/2
{
  "minutes": 200,
  "megabytes": 5000
}

GET test/_search
{  
   "post_filter":{  
      "term":{  
         "megabytes":1000
      }
   },
   "aggs":{  
      "minutes":{  
         "filter":{  
            "term":{  
               "megabytes":1000
            }
         },
         "aggs":{  
            "minutes":{  
               "terms":{  
                  "field":"minutes",
                  "min_doc_count":0
               }
            }
         }
      }
   }
}
```
</description><key id="76020405">11150</key><summary>Aggs: No collected documents does not mean an empty aggregation should be built</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-05-13T15:13:50Z</created><updated>2016-01-18T19:21:56Z</updated><resolved>2016-01-18T19:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="belendel" created="2015-05-13T18:03:31Z" id="101760930">Thank you for creating the issue @jpountz ! :-)
</comment><comment author="clintongormley" created="2016-01-18T19:21:56Z" id="172627709">This appears to have been fixed by https://github.com/elastic/elasticsearch/pull/10785
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail recovery if retry recovery if resetRecovery fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11149</link><project id="" key="" /><description>This might fail if the shard is closed for instance. This will leak
a shard lock causing the shard being locked on this node forever.

```
  1&gt; [2015-05-13 16:08:52,118][DEBUG][indices.recovery         ] [node_s1] unexpected error during recovery, but recovery id [420] is finished
  1&gt; [test_index2][0] CurrentState[CLOSED] Shard not in recovering state
  1&gt;    at org.elasticsearch.index.shard.IndexShard.performRecoveryRestart(IndexShard.java:870)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryStatus.resetRecovery(RecoveryStatus.java:233)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.retryRecovery(RecoveryTarget.java:151)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:237)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:72)
  1&gt;    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:462)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

followed by

```
  1&gt; [2015-05-13 16:08:52,123][DEBUG][indices                  ] [node_s1] [test_index2] failed to delete index store - at least one shards is still locked
  1&gt; org.apache.lucene.store.LockObtainFailedException: Can't lock shard [test_index2][0], timed out after 0ms
  1&gt;    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:520)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:448)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:392)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.deleteIndexDirectorySafe(NodeEnvironment.java:342)
  1&gt;    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:496)
  1&gt;    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:403)
  1&gt;    at org.elasticsearch.indices.IndicesService.deleteIndex(IndicesService.java:445)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndex(IndicesClusterStateService.java:844)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:243)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:180)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:489)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="76012432">11149</key><summary>Fail recovery if retry recovery if resetRecovery fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T14:51:58Z</created><updated>2015-05-29T17:04:41Z</updated><resolved>2015-05-13T14:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-13T14:54:04Z" id="101701242">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fine-granular manipulation of snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11148</link><project id="" key="" /><description>###### Suggestion for more fine-granular manipulation of snapshots:

Snapshots can be partially restored by selecting only a subset of the indices. They can at the moment, however, only be fully deleted. This means that unwanted parts of a snapshot cannot be easily removed. Deleting indices from snapshots would complement the existing functionality.

Other primitive operations such as moving / copying indices between snapshots might be useful. At the moment, only a single snapshot operation can be active in the cluster. This determines the level of concurrency for backing up as well as restoring data. In a large cluster, the backing up of data might be partitioned into smaller snapshot tasks. This is relevant when prioritizing certain indices or backing up indices with different frequencies. It also helps to minimize the interference with re-balancing processes by backing up individual indices as fast as possible (reduces the amount of transfer time for each individual index). Now if I snapshot the first ten indices and then ten more (e.g. to consider priorities), I will not be able to restore index A of the former snapshot and index B of the latter snapshot at the same time. Moving indices between snapshots would allow the combination of both snapshots.
###### What about immutability of snapshots?

At the moment, snapshots are immutable (they are created as a fixed set of indices). Adding delete / move / copy operations would change that. However, other operations could be imagined that conform to the "immutable" model. For example, creating a new snapshot based on the content of other snapshots (by giving an ordered list of (snapshot, indices) pairs). With the current implementation of snapshots, this would be very fast (only writing meta-data).

I can make a pull request if this is a feature that others deem useful. Also, the specifics of the operations need to be clarified first.
</description><key id="76010708">11148</key><summary>Fine-granular manipulation of snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch-t</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label></labels><created>2015-05-13T14:47:55Z</created><updated>2016-07-26T18:16:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-13T17:28:51Z" id="101753157">I think we can keep the immutability property by creating a new snapshot from pieces of existing snapshots, so instead of move/copy you can have functionally equivalent partial merge. However, currently a snapshot represents a point in time state of a cluster (or part of a cluster) at the moment the snapshot was created. It's nice property to have for several reasons. The most important reason is knowing the version of the cluster that wrote the snapshot and therefore being able to make different discussions based on this version. Slicing and gluing pieces of different snapshots together removes this property and makes handling older versions of the snapshots more complicated. So, considering the additional complexity that this feature will introduce I am not sure if the use case of restoring data from two snapshots at the same time is worth it. @ywelsch-t, could you elaborate a bit more on the benefits of combining two separate snapshots into one comparing to restoring them one after another. 
</comment><comment author="ywelsch-t" created="2015-05-18T10:10:54Z" id="103006711">Let me try and explain some more. The setting we have is backing up data using the Hadoop HDFS Snapshot/Restore plugin. Source of the backup is a large ES cluster (&gt; 500 nodes) with lots of data and destination is an HDFS cluster (many nodes as well).

The tricky part is finding the right amount of throughput between the ES cluster and the HDFS cluster such that
- our indices that we want to backup / restore are transfered as fast as possible but
- without impacting the performance of the cluster too much (i.e. saturating the shared network for regular ES communication).

The HDFS cluster is specifically set up to store ES backups and has no resource limitations.

Currently, the only way to control speed and concurrency with ES is using three parameters:
- max_snapshot_bytes_per_sec / max_restore_bytes_per_sec which trottles the bandwidth per node.
- The parameter concurrent_streams for the HDFS plugin determines the amount of parallel transfers of low-level files (e.g. Lucene segments) per node.
- Amount of nodes participating in the snapshot / restore process. This cannot be directly selected but is determined 
  - by the number of shards that are part of the snapshot and
  - by the distribution of shards in the cluster (i.e. allocation to node)

A limitation which we face is that multiple snapshots / restore actions cannot run concurrently.

This means that:
- To reach a certain throughput, we must snapshot / restore as many shards as possible in parallel.
- To not oversaturate our network, we cannot have too many nodes (and hence shards) as part of the backup action. This means that only max. 10% of the nodes in our cluster should be involved at the same time in backup operations.

Our solution is thus to backup batches of similarly sized indices (and hence shards) to have constant throughput.

In conclusion, our snapshots are sized based on transfer capabilites and network concerns.

However, this leads to the following conflicts if a snapshot can only be manipulated / deleted in its whole:
1) Some indices (in a snapshot) have different life spans than others. The operation "Delete index from snapshot" is useful in that case to "clean up" older snapshots. Deleting some of the indices in a snapshot also does not violate the property that a snapshot represents a point in time of a cluster (or part of a cluster) at the moment the snapshot was created.

2) In case of an emergency restore, we want to restore as fast as possible (i.e. fully saturate the network). Additionally, we need to prioritize the restore process of the most relevant indices. With the possibility to create a new snapshot based on the contents of other snapshots (or to merge snapshots), we can selectively pick the indices from different snapshots that we want to restore, create fake snapshots based on them and restore the fake snapshots in custom order.

We have patched ES with an implementation to create fake snapshots based on the contents of other snapshots and to delete parts of a snapshot. We believe this fine-granular snapshot manipulation gives us the flexibility for more customized backup strategies.
</comment><comment author="imotov" created="2015-05-18T19:44:16Z" id="103188092">@ywelsch-t this is a great explanation. Thank you! So, if we assume for a moment that besides node-level throttling you have a cluster-level (or better to say repository-level) throttling for both snapshot and restore operations, would it make this feature unnecessary? 
</comment><comment author="ywelsch-t" created="2015-05-19T14:05:12Z" id="103509258">Repository-level throttling is only part of the solution. It solves the issue of snapshot transfer impacting the overall performance of the cluster too much (i.e. saturating the shared network for regular ES communication).

Two issues I have described in the previous comment are remaining, however.
1. To reach a certain throughput, we must snapshot / restore multiple indices in parallel.
   - For backups, this means that we may have to combine different indices into a single snapshot although they have different lifetimes.
   - For restores, this means that we have to combine the restore of indices in different snapshots.
2. Prioritization. This is particularly important for restore operations (see point 2 of previous post).

For SNAPSHOT operations, I see three solutions:
a) allow concurrent snapshots (of disjoint indices sets). We then group indices by lifetimes / prioritization and do concurrent backups.
b) give possibility to dissect snapshot after the backup. The initial snapshot is then only a means of concurrent transfer. If lifetime expires, we delete the "expired" indices from the snapshot. For that we need an operation "delete index from snapshot" (which, as previously said, does not conflict with the original intent of snapshots). Advantage is that lifetime of certain indices in the backup can be controlled not only before but also after snapshotting.
c) allow both a) and b). My preferred solution.

For RESTORE operations, I see two solutions:
a) allow concurrent snapshot restores (of disjoint indices sets). My preferred solution.
b) give possibility to combine snapshots before the restore. This is a poor substitute for a).
</comment><comment author="imotov" created="2015-05-19T17:46:33Z" id="103612197">@ywelsch-t ok, so if we will enable concurrent snapshots of non-intersecting sets of indices into different repositories and concurrent restore plus repository-level throttling, will it completely solve the issue?
</comment><comment author="ywelsch-t" created="2015-05-20T08:57:02Z" id="103815265">1) Why the restriction to different repositories? If snapshots are concurrently done on the same repository but for disjoint sets of indices, isn't it only the update of the snapshot list at the end that needs to be synchronized (as far as BlobStoreRepository is concerned)?

2) I would prefer to also have official support for the "delete index from snapshot" operation (Advantage is that lifetime of certain indices in the backup can be controlled not only before but also after snapshotting.) . A workaround would be to do snapshots that are only comprised of single indices.

3) Repository-level throttling becomes actually less important if we have concurrent snapshots/restores because we can then approximately control the level of network throughput by sizing the snapshot / restore operations appropriately. Also, I am not sure whether throttling at the repository level makes the most sense. Should we do it at a cluster level, or even at a per-snapshot or per-restore level? How does it relate to node-level throttling? Could nodes as well be throttled at a per-snapshot or per-restore level? For me, this feature has the most unknowns right now.

In conclusion, what would solve our issues:
- concurrent snapshots of disjoint sets of indices
- concurrent restore of disjoint sets of indices from different snapshots
- "delete index from snapshot" operation

A nice-to-have:
- some form of throttling for snapshot and restore operations.
</comment><comment author="ywelsch-t" created="2015-05-26T15:17:08Z" id="105562452">@imotov I could help contribute on the following items:
- concurrent snapshots of disjoint sets of indices
- concurrent restore of disjoint sets of indices from different snapshots
- "delete index from snapshot" operation
  Is that something you would consider accepting as PR?
</comment><comment author="imotov" created="2015-05-26T15:38:59Z" id="105570668">@ywelsch-t concurrent snapshot and concurrent restore is definitely on the roadmap. The infrastructure is there that can be enabled with a couple of lines of code, but it is lacking good test coverage. So, you can definitely start working on that if you wish. 

I am not completely sold on the "delete index from snapshot" operation to be honest. I would wait for others to comment. 
</comment><comment author="tlrx" created="2015-05-27T07:27:55Z" id="105797340">My current understanding is that we need a) concurrent snapshot and concurrent restore of non-intersecting sets of indices b) throttling at repository level in order to not saturate the repository (ie limit the number of shards that are concurrently snapshot to/restored from the repo) c) throttling at cluster level in order to limit impact on cluster performance.

With concurrent restore and the fact it’s already possible to select indices that should be restored by using `indices` I'm not sure we need a "delete index from snapshot" operation. The only advantage I see is to clean and reduce a too large snapshot to free space. If indices have different lifespans, maybe they should be put into different snapshots?
</comment><comment author="4ydx" created="2015-07-10T03:47:10Z" id="120215774">Just to speak up concurrent restoration of non-intersecting sets of indices would be much appreciated.  I am not at all familiar with the internals of ES, but if somebody were willing to point me in the right direction, I would be happy to take a look at how that could be enabled in the code.  My company needs this feature in the coming months.
</comment><comment author="4ydx" created="2015-07-15T03:25:11Z" id="121474002">I was wondering if somebody might be able to take a look at

https://github.com/4ydx/elasticsearch/commit/e10ac70ef7c5f0bc1d06478dd9efda94fcf85cd5

and let me know if those minor changes seem reasonable for allowing concurrent restoration.  I am currently running mvn clean test as per the "Contributing to Elasticsearch" docs.  As a side note an unaltered clone followed by running mv clean test is producing failures so I submitted a separate issue to find out if this is common or not.  At the end of the day I would like to be able to contribute something if possible, but please remember that I am new to Java as well as the Elasticsearch codebase.
</comment><comment author="clintongormley" created="2015-07-15T10:50:31Z" id="121574249">Hi @4ydx 

Best idea is to send your changes as a pull request - here it'll just get lost.
</comment><comment author="balooka" created="2016-03-04T07:23:55Z" id="192155861">I'll add my vote regarding the "delete indice from snapshot" feature,
As it'll definitely allow for cleanup in 2 occasions I've encountered in my current environment:
- as described by @ywelsch-t, allowing better management of indices with different life spans, although in our case the reason for snapshots containing such a mix relates more to original strategy vs actual need. Reviewing and amending the snapshot strategy,  I'm still "locked" by the older snapshots created, where fairly small indices that should be kept are mixed with larger indices that consume all the spaces but could/should be deleted because there is no need to keep them any longer
- due to filesystem space issue, some snapshots have only completed partially, so we wouldn't be able to use safely the snapshot for some of the indices anyway, better to be able to clean them then

Another approach that would be completely satisfactory to our use case (but doesn't cover the throughput aspect brought up in this discussion) would be to allow for snapshot generations based on existing snapshots, only allowing splitting (new snapshot has a subset of the indices of the older snapshot)
</comment><comment author="DravenJohnson" created="2016-07-26T18:16:32Z" id="235357297">1. How about add combine/merge feature that you can organize your snapshot every month (daily snapshot merge into a monthly one.)
2. I would prefer if we have retake option for failed snapshot so I don't have to spend more time retaking entire snapshot. (Time consuming and also not well organized.)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: remove the ability to append raw bytes to XContent instances.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11147</link><project id="" key="" /><description>This optimization is mostly useful to render `_source` documents: we store it
verbatim and at request time, if the same format is used as the one that was
used at indexing time we just have to copy raw bytes instead of re-serializing
the document. However, such an optimization is dangerous because you need to
make sure to not insert eg. smile-serialized content inside of a json and also
it is in the way of other optimizations such as reformatting `_source` upon
indexing (eg. removing useless line breaks and spaces) or storing it in a more
space-efficient format.
</description><key id="75993133">11147</key><summary>Internal: remove the ability to append raw bytes to XContent instances.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-05-13T13:58:50Z</created><updated>2015-07-08T12:52:46Z</updated><resolved>2015-07-08T12:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-13T15:54:03Z" id="101723249">I don't think we should remove it, this is an optimization that is well worth it. It doesn't require to re-parse the docs all the time, we just copy them over. For the majority of the use cases, which is JSON in, JSON out, this makes a big difference based on my tests when I added it (garbage memory generated, CPU, ... under concurrent load).

&gt; smile-serialized content inside of a json

I believe we protected against it, no? We parse the content if we need to generate it in a different format, no way around it.

&gt; optimizations such as reformatting _source upon indexing

If our client libraries are used, they don't index data in "pretty" format by default, I haven't seen data indexed in ES that is prettified for a long time, I think its common now.
</comment><comment author="jpountz" created="2015-07-08T12:52:39Z" id="119568657">Closing for now. I'll reopen eg. if we change elasticsearch to not reuse the incoming json (in favour of eg. something more compact) in the future.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove caching of the mapping source.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11146</link><project id="" key="" /><description>DocumentMapper today caches the mapping source, which is a bit error-prone as
you need to make sure to update the cached value when a change is performed.

Additionally, this is less useful now that cluster state updates and dynamic
mapping updates send diffs.
</description><key id="75976127">11146</key><summary>Remove caching of the mapping source.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-05-13T13:01:44Z</created><updated>2015-07-08T13:21:21Z</updated><resolved>2015-07-08T13:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-13T22:01:20Z" id="101829184">Nice cleanup. LGTM
</comment><comment author="kimchy" created="2015-05-14T06:59:31Z" id="101943308">how does the diff affects things here? I am confused. For example, here: https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java#L430
</comment><comment author="jpountz" created="2015-07-08T13:21:11Z" id="119576277">I'm closing it as there are several places that rely on the serialized mapping source to check whether two mappings are equal, and I don't see how to fix it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log query when circuit breaker trips</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11145</link><project id="" key="" /><description>If a query caused a circuit breaker to trip, it would be useful to log the full query for troubleshooting purposes. 
</description><key id="75965386">11145</key><summary>Log query when circuit breaker trips</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jjfalling</reporter><labels><label>:Circuit Breakers</label><label>adoptme</label><label>enhancement</label></labels><created>2015-05-13T12:21:27Z</created><updated>2016-06-29T13:40:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-13T15:42:34Z" id="101720168">@jjfalling this is a great idea, it might be tricky to figure out what level we can log it at, but hopefully it is doable.
</comment><comment author="colings86" created="2015-07-28T10:13:28Z" id="125539800">So it looks like we can add this in to the onFailure methods of TransportSearchQueryThenFetchAction (and the equivalent for other search types) however logging the request would mean we would need to make SearchRequest and all the objects it contains (queries, aggregations, highlighters, suggesters etc.) implement ToXContent so we can re-serialise the request to JSON to include it in the response (I propose adding a setRequestSource() method to CircuitBreakingException which will add the content to the JSON response in toXContent). adding toXcontent to all these object would be a lot of work and should probably be done as part of the query re-factoring if we wanted to do it.

Alternatively, we could just support this in the REST layer, and add logic to RestSearchAction's onFailure() method to add the request JSON to the response if the caught exception is a CircuitBreakingException. Thoughts?
</comment><comment author="clintongormley" created="2015-07-30T10:29:47Z" id="126267725">What about adding this exception to the query slowlog?
</comment><comment author="colings86" created="2015-07-30T11:41:56Z" id="126284595">The problem isn't sending the exception somewhere, the circuitBreakerException is already sent back to the client at the moment. The problem is how to render the query in the exception because at the moment we can't convert from the SearchRequest back to the JSON. We would need to do this wherever we end up sending the exception (to the logs or to the response).

The question is whether we need to do this for the Java API? If we do, we will need the SearchRequest to be serialised to JSON which will be a lot of work as there are a lot of classes that it depends on (all queries and aggs for a start).

Another option is for the SearchRequest to keep the original JSON string around. This seems inefficient as the search request would essentially be duplicating its content (once as the JSON string and once as the requests internal state). 

The last option is to only support this in the REST layer only and have the logic in the RestSearchAction instead of the TransportSearchActions.
</comment><comment author="dakrone" created="2015-10-28T20:02:57Z" id="151973960">@colings86 I'm curious if this has changed any with the new query refactoring work? Is is still something that could only happen in the `RestSearchAction`?
</comment><comment author="colings86" created="2015-11-02T11:45:46Z" id="152996423">@dakrone yes, we should now be able to do this since with the refactoring work we have ended up implementing ToXContent on the SearchRequest object (and all its attributes).
</comment><comment author="vineet01" created="2016-06-29T13:40:20Z" id="229358612">No updates on it for 2.3.3?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove dependency on hppc:esoteric.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11144</link><project id="" key="" /><description>The esoteric classifier contains in particular maps that take bytes or doubles
as keys. In the byte case, we can just use integer, and in the double case we
can use their long bits instead.
</description><key id="75964648">11144</key><summary>Remove dependency on hppc:esoteric.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T12:18:56Z</created><updated>2015-06-08T08:51:54Z</updated><resolved>2015-05-15T14:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-13T12:19:20Z" id="101645250">@dweiss Would you mind having a look?
</comment><comment author="dweiss" created="2015-05-13T12:23:39Z" id="101647491">Sure, I'll take a look later today, Adrien.
</comment><comment author="jpountz" created="2015-05-15T14:25:31Z" id="102412993">Thanks @dweiss !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java</file></files><comments><comment>Merge pull request #11144 from jpountz/fix/remove_hppc_esoteric_dep</comment></comments></commit></commits></item><item><title>Add translog checkpoints to prevent translog corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11143</link><project id="" key="" /><description>Today we are almost intentionally corrupt the translog if we loose
a node due to powerloss or similary disasters. In the translog reading
code we simply read until we hit an EOF exception ignoring the rest of the
translog file once hit. There is no information stored how many records
we are expecting or what the last written offset was.

This commit restructures the translog to add checkpoints that are written
with every sync operation recording the number of synced operations as well
as the last synced offset. These checkpoints are also used to identify the actual
transaction log file to open instead of relying on directory traversal.

This change adds a significant amount of additional checks and pickyness to the translog
code. For instance is the translog now associated with a specific engine via a UUID that is
written to each translog file as part of it's header. If an engine opens a translog file it
was not associated with the operation will fail.

Closes to #10933
Relates to #11011
</description><key id="75942639">11143</key><summary>Add translog checkpoints to prevent translog corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T10:50:18Z</created><updated>2015-05-18T07:42:55Z</updated><resolved>2015-05-18T07:42:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-13T18:34:40Z" id="101770504">@mikemccand I addressed all your comments, thanks!!
</comment><comment author="kimchy" created="2015-05-13T18:53:57Z" id="101776282">this looks great @s1monw!
</comment><comment author="s1monw" created="2015-05-14T20:59:25Z" id="102167864">@mikemccand pushed a new commit
</comment><comment author="mikemccand" created="2015-05-15T14:52:10Z" id="102419959">LGTM
</comment><comment author="s1monw" created="2015-05-15T15:14:57Z" id="102428632">i will push this in the on monday unless anybody objects
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/io/stream/ByteBufferStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngineFactory.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamInput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReader.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/ImmutableTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReaderBase.java</file><file>src/main/java/org/elasticsearch/index/translog/MultiSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/TruncatedTranslogException.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>src/test/java/org/elasticsearch/transport/TransportMessageTests.java</file></files><comments><comment>Merge pull request #11143 from elastic/feature/translog_checkpoints</comment></comments></commit></commits></item><item><title>Query DSL: Add `filter` clauses to `bool` queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11142</link><project id="" key="" /><description>These clauses filter the document space without affecting scoring and map to
Lucene's BooleanClause.Occur.FILTER. The `filtered` query is now deprecated and

``` json
{
  "filtered": {
    "query": { //query },
    "filter": { //filter }
  }
}
```

should be replaced with

``` json
{
  "bool": {
    "must": { //query },
    "filter": { //filter }
  }
}
```
</description><key id="75929575">11142</key><summary>Query DSL: Add `filter` clauses to `bool` queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T10:09:05Z</created><updated>2015-05-13T10:31:11Z</updated><resolved>2015-05-13T10:25:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-13T10:14:41Z" id="101609860">Also for the record: even if the `filtered` query is still supported for backward-compatibility reasons, it now parses to a `bool` query and the `strategy` parameter is ignored.
</comment><comment author="kimchy" created="2015-05-13T10:19:11Z" id="101612075">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #11142 from jpountz/feature/bool_filter</comment></comments></commit></commits></item><item><title>Make FieldNameAnalyzer less lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11141</link><project id="" key="" /><description>In case FieldNameAnalyzer does not find an explicit analyzer for a given
field name, it returns the default analyzer. This behaviour can hide bugs
where the analyzer fails to be propagated to FieldNameAnalyzer or an
analyzer is requested for a field which is not mapped.
</description><key id="75908624">11141</key><summary>Make FieldNameAnalyzer less lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T09:04:02Z</created><updated>2015-06-07T10:52:47Z</updated><resolved>2015-05-15T14:41:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-13T15:44:54Z" id="101720743">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/FieldNameAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperAnalyzer.java</file></files><comments><comment>Merge pull request #11141 from jpountz/fix/fieldnameanalyzer_leniency</comment></comments></commit></commits></item><item><title>First query is slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11140</link><project id="" key="" /><description>while I use es to search something, I find it behave much slower in first query.can sb tell me why?
</description><key id="75907151">11140</key><summary>First query is slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmqwudi</reporter><labels /><created>2015-05-13T08:55:36Z</created><updated>2015-05-13T09:00:25Z</updated><resolved>2015-05-13T09:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-13T09:00:24Z" id="101579827">That is because during the first query Elasticsearch pulls data into cache, so that subsequent queries are faster.
You can use [Warmers](http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-warmers.html) to help with this, but be careful of using them too much.

Also please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add missing rewrite parameter to FuzzyQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11139</link><project id="" key="" /><description>We parse the rewrite field in `FuzzyQueryParser` but we don't allow to set it via `FuzzyQueryBuilder` for our java api users. Added missing field and setter.

Similar problems will be resolved with the query parser refactoring that we have been working on. The parser will only return an intermediate `Streamable` object, the same object that the java api users will use directly. No more building json through java api then, which can geet out of sync with what we actually parse.

Closes #11130
</description><key id="75905204">11139</key><summary>Add missing rewrite parameter to FuzzyQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T08:46:38Z</created><updated>2015-06-07T18:21:49Z</updated><resolved>2015-05-13T14:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-13T10:21:44Z" id="101613363">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file></files><comments><comment>Java api: add missing rewrite parameter to FuzzyQueryBuilder</comment></comments></commit></commits></item><item><title>Prevent PercolateResponse from serializing negative VLong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11138</link><project id="" key="" /><description>We are using a a VLong to serialize the PercolateResponse#tookInMillis. This
can due to several `System.currentTimeMillis()` implemenation details be negative.
We should prevent the negavite value for being serialized as a VLong and make sure
we use a valid value for this in the first place

Our CI ran into this here: http://build-us-00.elastic.co/job/es_core_master_suse/608/

```
  1&gt; java.lang.RuntimeException: failed to check serialization - version [0.20.5] for streamable [org.elasticsearch.action.percolate.PercolateResponse@129e9df5]
  1&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertVersionSerializable(ElasticsearchAssertions.java:644)
  1&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertVersionSerializable(ElasticsearchAssertions.java:624)
  1&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures(ElasticsearchAssertions.java:326)
  1&gt;    at org.elasticsearch.percolator.RecoveryPercolatorTests$2.run(RecoveryPercolatorTests.java:334)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.lang.AssertionError
  1&gt;    at org.elasticsearch.common.io.stream.StreamOutput.writeVLong(StreamOutput.java:159)
  1&gt;    at org.elasticsearch.action.percolate.PercolateResponse.writeTo(PercolateResponse.java:178)
  1&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.serialize(ElasticsearchAssertions.java:617)
  1&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertVersionSerializable(ElasticsearchAssertions.java:637)
  1&gt;    ... 4 more
```
</description><key id="75892422">11138</key><summary>Prevent PercolateResponse from serializing negative VLong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T08:06:58Z</created><updated>2015-05-29T17:05:18Z</updated><resolved>2015-05-13T08:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-13T08:11:10Z" id="101564915">You are changing both the serialization (using long instead of vlong) and how the elapsed time is computed (taking the max with 0), I think we should do only one of these changes?

Looking at other actions, they take the max with 1, see eg. AbstractAsyncAction.buildTookInMillis
</comment><comment author="martijnvg" created="2015-05-13T08:15:18Z" id="101565771">+1 this can go into master. But when backporting the serialization change shouldn't go into 1.x and 1.5 branches.
</comment><comment author="s1monw" created="2015-05-13T08:17:32Z" id="101567016">&gt; You are changing both the serialization (using long instead of vlong) and how the elapsed time is computed (taking the max with 0), I think we should do only one of these changes?

I am ok with this but then I turn the asserts into real checks and keep the vLong
</comment><comment author="s1monw" created="2015-05-13T08:20:18Z" id="101567577">@jpountz @martijnvg pushed a new commit
</comment><comment author="jpountz" created="2015-05-13T08:20:47Z" id="101567764">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file></files><comments><comment>Prevent PercolateResponse from serializing negative VLong</comment></comments></commit></commits></item><item><title>Change geo filters into queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11137</link><project id="" key="" /><description>This commit replaces geo filters with queries that support two-phase iteration.
</description><key id="75877671">11137</key><summary>Change geo filters into queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T07:14:49Z</created><updated>2015-06-15T18:48:44Z</updated><resolved>2015-05-13T15:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-13T15:26:20Z" id="101714818">+1, looks great.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/docset/AndDocIdSet.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxQuery.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/AndDocIdSetTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #11137 from jpountz/enhancement/geo_queries</comment></comments></commit></commits></item><item><title>specify -Djna.nosys=true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11136</link><project id="" key="" /><description>We should use the version of JNA that we ship and test with, not some arbitrary one that happens to be installed on the machine.

Both when running tests, and for bin/elasticsearch.

If i use the system openjdk on my linux box, JNA natives such as mlockall do not work correctly and JNA can't be loaded, because i have /usr/lib/jnidispatch.so from an ancient JNA version required by some-package-or-another. There is no reason to use the system one ever, it is only something that can break.
</description><key id="75871163">11136</key><summary>specify -Djna.nosys=true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label></labels><created>2015-05-13T06:45:02Z</created><updated>2015-05-15T11:50:59Z</updated><resolved>2015-05-15T11:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-15T11:50:57Z" id="102377096">Fixed by #11163
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NEST multuple aggregations not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11135</link><project id="" key="" /><description>I have a use case where I need to do aggregation on multiple columns using C#. I am using NEST libraries for this and I am facing the following issue

Query C# :

```
 var searchRequest = new SearchRequest
        {
            SearchType = SearchType.Count, 
            Query = query,
            Aggregations = new Dictionary&lt;string, IAggregationContainer&gt;
            {
                { "a", new AggregationContainer
                            {
                                ExtendedStats = new ExtendedStatsAggregator()
                                {
                                    Field = "a"
                                }
                            }
                },
               { "b", new AggregationContainer
                            {
                                ExtendedStats = new ExtendedStatsAggregator()
                                {
                                    Field = "b"
                                }
                            }
                }
            }
        }; 
```

When I receive response from NEST, however I am getting only result for one aggregation. I am looking at SearchResult.Agg dictionary but it has only one entry for one aggregation field instead of two. Let me know if I am missing soemthing or is it some issue with NEST libraries

When I print the SearchResponse.ConnectionStatus I can see that both the aggregations are returned. The problem I am facing is that my SearchResults.Aggs.ExtendendStats() has only details of first aggregation.

Some how the deserialization in NEST is dropping the second aggregation though the response correctly came from ES
</description><key id="75847430">11135</key><summary>NEST multuple aggregations not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">riksteiner</reporter><labels /><created>2015-05-13T05:00:13Z</created><updated>2015-05-13T10:31:08Z</updated><resolved>2015-05-13T10:31:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2015-05-13T10:31:06Z" id="101617503">Hi @riksteiner this looks like a bug in NEST mind opening your issue there: 

https://github.com/elastic/elasticsearch-net/issues

thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add tests.config support to BootstrapForTesting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11134</link><project id="" key="" /><description>Several plugins (e.g. elasticsearch-cloud-aws, elasticsearch-cloud-azure, elasticsearch-cloud-gce)
have integration tests that run with actual credentials to a remote service, so test runs
need access to this file.

These all require the tester (or jenkins) to supply the file with -Dtests.config. I think we should just add a simple rule here. 

I considered two alternatives:
1. solve it in maven by treating the external config as a test resource. I don't like this since its credentials in these cases, think about jenkins workspaces and so on.
2. just don't run these special integration test groups with security manager. I don't like this since then we can't be confident these plugins work!

With this PR, these aws and azure tests work with security manager enabled. I also had to add a permission for some crazy stuff in aws-core-sdk, I will look into that later and see if I can remove it.
</description><key id="75804076">11134</key><summary>Add tests.config support to BootstrapForTesting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T01:30:18Z</created><updated>2015-06-08T12:56:57Z</updated><resolved>2015-05-13T15:35:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-13T03:00:54Z" id="101491553">I created a PR to AWS for the sun-internal reflection: https://github.com/aws/aws-sdk-java/pull/432
</comment><comment author="jpountz" created="2015-05-13T06:26:00Z" id="101528790">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Merge pull request #11134 from rmuir/tests_config</comment></comments></commit></commits></item><item><title>Check that reading indices is allowed before creating their snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11133</link><project id="" key="" /><description>Clean up of Snapshot/Restore cluster and index block handling.
</description><key id="75799231">11133</key><summary>Check that reading indices is allowed before creating their snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-13T01:10:18Z</created><updated>2015-06-07T17:26:57Z</updated><resolved>2015-05-26T04:59:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-13T01:12:22Z" id="101474645">@tlrx, @javanna I cleaned up blocks for snapshot and restore actions a bit. Could you check if these changes make sense? 
</comment><comment author="tlrx" created="2015-05-18T11:07:55Z" id="103019223">@imotov yes, the changes make sense. 

I also find it more clean to have a 403 `"blocked by: [FORBIDDEN/4/index closed];"` exception rather than a failed snapshot with `"reason":"Indices don't have primary shards"` when snapshotting a closed index.

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: check that reading indices is allowed before creating their snapshots</comment></comments></commit></commits></item><item><title>Remove unnecessary permissions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11132</link><project id="" key="" /><description>Also document remaining permissions, so we have an explanation for each one.

For the case of LinkPermission, its special, as it allows access to any file on the system. We don't really need the possibility to create symlinks, and the two tests doing it need guards anyway for the windows case, etc.
</description><key id="75774921">11132</key><summary>Remove unnecessary permissions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T23:18:47Z</created><updated>2015-06-08T12:57:12Z</updated><resolved>2015-05-13T00:03:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-12T23:57:59Z" id="101463233">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>src/test/java/org/elasticsearch/common/PidFileTests.java</file></files><comments><comment>Merge pull request #11132 from rmuir/lockdown6</comment></comments></commit></commits></item><item><title>Accept non-closed GeoJSON polygons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11131</link><project id="" key="" /><description>@nknize I'm creating a new issue here based on a comment on issue #3909 

Twitter provides the following object. It is rejected by elasticsearch for only having 4 elements and not 5. This strict requirement is not consistent with GeoJSON. Please allow elasticsearch to accept this Object, out of the box, because technically it is legal GeoJSON. Currently I must write a middle-man to add a 5th-element (cue Bruce Willis jokes here) to this array of coordinates. This is a bit kludgy in terms of workflow:

```
"bounding_box": {
        "type": "Polygon",
        "coordinates": [
        [
        [
        -9.0915413,
        38.6713816
        ],
        [
        -9.0915413,
        38.9313732
        ],
        [
        -8.8102479,
        38.9313732
        ],
        [
        -8.8102479,
        38.6713816
        ]
        ]
        ]
        },
```
</description><key id="75717555">11131</key><summary>Accept non-closed GeoJSON polygons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">lababidi</reporter><labels><label>:Geo</label></labels><created>2015-05-12T19:51:20Z</created><updated>2016-05-06T20:18:11Z</updated><resolved>2015-07-17T15:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dimroc" created="2015-05-30T12:38:33Z" id="107036309">+1. Got here from issue https://github.com/elastic/elasticsearch/issues/3909.
</comment><comment author="gustin" created="2015-06-03T03:51:36Z" id="108180709">:+1: We're running into this storing data from gnip, their geo enrichment uses the same 4 point polygons. 
</comment><comment author="vsilva89" created="2016-05-06T20:18:11Z" id="217547872">Does this problem is solved ?  I have the same problem and need your help
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoPolygonTests.java</file></files><comments><comment>[GEO] Update ShapeBuilder and GeoPolygonQueryParser to accept unclosed GeoJSON</comment></comments></commit></commits></item><item><title>FuzzyQueryBuilder is missing rewrite</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11130</link><project id="" key="" /><description>FuzzyQueryBuilder is a MultiTermQueryBuilder, so it should support a `rewrite` parameter. Indeed, `rewrite` works when I specify a JSON `fuzzy` query. But I can't set it if I want to use a `FuzzyQueryBuilder`.

My workaround is to use `MatchQueryBuilder`.
</description><key id="75697553">11130</key><summary>FuzzyQueryBuilder is missing rewrite</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">adamhooper</reporter><labels><label>:Java API</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T18:38:16Z</created><updated>2015-05-13T14:21:34Z</updated><resolved>2015-05-13T14:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-13T14:21:34Z" id="101683537">Thanks @adamhooper good catch, fixed in master, 1.x and 1.5 branches.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file></files><comments><comment>Java api: add missing rewrite parameter to FuzzyQueryBuilder</comment></comments></commit></commits></item><item><title>Percolator with transform script doesn't match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11129</link><project id="" key="" /><description>Hi

I think I found a bug in the percolator from elaticsearch. The problem happens when you have a transform script registered in your mapping. The following code reproduces this bug.

Create an elasticsearch.yml config file to activate dynamic scripts:

``` yaml
script.inline: on
script.indexed: on
script.disable_dynamic: false
```

Start an elasticsearch instance (I do it with docker)

``` bash
docker run -d -p 9200:9200 -v $PWD/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml --name es elasticsearch:1.5.2
```

The following script then reproduces the bug. The registered transform script does nothing (you can also replace it by something else). I expect that the percolator matches the document, since the same query matches the document after it has been added to the index. After removing the transform script, the percolator works as exptected. I also added some sleep statements since I wasn't sure if it's a timing problem.

``` bash
ES_HOST=localhost:9200

echo "Deleting the percolator"
curl -XDELETE $ES_HOST/myindex/.percolator/1

echo -e "\nDelete index"
curl -XDELETE $ES_HOST/myindex/

echo -e "\nCreating index"
curl -XPUT $ES_HOST/myindex

echo -e "\nInsert mapping"
curl -XPUT $ES_HOST/myindex/_mapping/mytype?pretty -d '
{
    "mytype" : {
        "properties": {
            "message": {
                "type": "string"
            }
        },
        "transform": {
            "script" : "println ctx",
            "lang": "groovy"
        }
    }
}
'

echo -e "\nCheck mapping"
curl -XGET $ES_HOST/myindex/_mapping/?pretty

echo -e "\nAdd a percolator query"
curl -XPUT "$ES_HOST/myindex/.percolator/1?pretty" -d '
{
    "query" : {
        "match" : {
            "message" : "bonsai"
        }
    }
}'

sleep 2

echo -e "\nSend a document to the percolator"
curl -XGET "$ES_HOST/myindex/mytype/_percolate?pretty" -d '{
    "doc" : {
        "message" : "A new bonsai tree in the office"
    }
}'

# HERE THE RESPONSE IS WRONG

echo -e "\nIndexing the document"
curl -XPOST "$ES_HOST/myindex/mytype?pretty" -d '{
    "message" : "A new bonsai tree in the office"
}'

sleep 2

echo -e "\nRun a query with the same settings as the percolator"
curl -XGET "$ES_HOST/myindex/mytype/_search?pretty" -d '
{
    "query" : {
        "match" : {
            "message" : "bonsai"
        }
    }
}'

# HERE THE SAME DOCUMENT IS FOUND WITH THE QUERY
```
</description><key id="75695154">11129</key><summary>Percolator with transform script doesn't match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">idlefella</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2015-05-12T18:31:19Z</created><updated>2016-01-18T19:17:41Z</updated><resolved>2016-01-18T19:17:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sbeaupre" created="2015-08-15T08:07:04Z" id="131314269">I think this does not work for the same reason @nik9000 mentioned in this discussion:
https://discuss.elastic.co/t/mapping-transform-only-for-creating-new-and-not-for-updating/27312/9
</comment><comment author="clintongormley" created="2016-01-18T19:17:41Z" id="172626925">Transform scripts have been removed.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Encryption of snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11128</link><project id="" key="" /><description>Is it possible to encrypt snapshots before it leaves the node ?
So that data is encrypted in flight ? May be a config variable to
set up password for encryption and decryption ?
</description><key id="75686759">11128</key><summary>Encryption of snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pwli</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-05-12T18:04:59Z</created><updated>2017-06-05T00:14:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T17:22:10Z" id="102462589">@imotov @jaymode what do you think?
</comment><comment author="pandujar" created="2015-12-16T12:32:16Z" id="165092592">+1
</comment><comment author="yarmand" created="2016-01-05T15:30:36Z" id="169035273">+1
</comment><comment author="clintongormley" created="2016-01-18T19:17:24Z" id="172626871">@imotov @jaymode thoughts?
</comment><comment author="yarmand" created="2016-01-19T10:12:27Z" id="172803987">We needed encryption for our Azure snapshots.
As some backends have encryption directly as a server side option, I implemented encryption directly in the Azure plugin.
For reference, here is the [Pull Request for this implementation against elasticsearch 1.7](https://github.com/elastic/elasticsearch-cloud-azure/pull/112).
When we update to 2.1 I will port this and will submit a PR in this repository.
</comment><comment author="pmazeris" created="2017-06-05T00:14:52Z" id="306077225">+1

We're using the **repositiry-gcs** elasticsearch plugin to push to encrypted / acl'ed gcr.io buckets in GKE/Google. Having data already encrypted during transit time would be perfect. 

@yarmand, what would be a reasonable ETA for this addtion? Thks</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Acquire IndexWriter's `write.lock` lock before shard deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11127</link><project id="" key="" /><description>In `NodeEnvironment.deleteShardDirectoryUnderLock`, we will now attempt
to acquire, then release, the `write.lock` file for the Lucene index in
question to ensure that no other `IndexWriter` has the directory open
before deleting the data.

Note that the `write.lock` file must be released before the actual
deletion in order to allow the directory to be deleted.

Fixes #11097
</description><key id="75677628">11127</key><summary>Acquire IndexWriter's `write.lock` lock before shard deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T17:32:00Z</created><updated>2015-06-01T22:35:14Z</updated><resolved>2015-05-12T19:28:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-12T17:54:53Z" id="101365362">So its intentionally best effort right? As soon as we release the lock, we open up a race condition while the IOUtils.rm actually happens. But its not clear we can do any better even if we make it complicated (at least not on all platforms/lockfactories).
</comment><comment author="dakrone" created="2015-05-12T17:57:01Z" id="101365828">&gt; So its intentionally best effort right?

Yep, it's a sanity check for cases where a node might be disconnected from other nodes, but the shard's dir is still opened by a different machine's `IndexWriter`

&gt; its not clear we can do any better even if we make it complicated (at least not on all platforms/lockfactories).

Yeah, I think it's a reasonable trade-off for a sanity check versus building something really complicated.
</comment><comment author="rmuir" created="2015-05-12T17:58:58Z" id="101366264">yes, I am +1 for this approach. Maybe we can add a line to the javadocs just mentioning this is the case. Especially if you think about locking on shared filesystems, we should not rush to do something complex.
</comment><comment author="rmuir" created="2015-05-12T18:16:50Z" id="101372751">looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/rescore/RescorePhase.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file></files><comments><comment>Don't truncate TopDocs after rescoring</comment></comments></commit></commits></item><item><title>Make the script filter a query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11126</link><project id="" key="" /><description>This change changes the script filter so that it produces scorers with two-phase
iteration support instead of doc id sets with random-access.
</description><key id="75674694">11126</key><summary>Make the script filter a query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T17:21:35Z</created><updated>2015-06-07T16:52:33Z</updated><resolved>2015-05-12T17:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-12T17:28:10Z" id="101358292">+1, looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file></files><comments><comment>Merge pull request #11126 from jpountz/fix/script_query</comment></comments></commit></commits></item><item><title>Upgrade to lucene-5.2.0-snapshot-1678978.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11125</link><project id="" key="" /><description>This new revision has some sugar to build queries with two-phase iteration support.
</description><key id="75666755">11125</key><summary>Upgrade to lucene-5.2.0-snapshot-1678978.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T16:51:08Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-12T16:53:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-12T16:51:30Z" id="101346089">looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11125 from jpountz/upgrade/lucene-5.2.0-snapshot-1678978</comment></comments></commit></commits></item><item><title>Add index name to log statements when settings update fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11124</link><project id="" key="" /><description>When an index setting is invalid and fails to be set, a WARN statement is logged but it doesn't contain the index name, making tracking down and fixing the problem more difficult. This commit adds the index name to the log statement.

To see the differences in the log statements:

``` sh
# Create an index
curl -XPOST localhost:9200/foo
# Response
{"acknowledged":true}

# Add a setting value that isn't dash delimited
curl -XPUT localhost:9200/foo/_settings -d '{ "auto_expand_replicas": 2 }'
# Response
{"acknowledged":true}

# Current log statement
[2015-05-12 12:07:03,538][WARN ][cluster.metadata         ] [Hypnotia] Unexpected value [2] for setting [index.auto_expand_replicas]; it should be dash delimited

# Updated log statement
[2015-05-12 12:04:31,460][WARN ][cluster.metadata         ] [Spider-Ham] failed to set [index.auto_expand_replicas] for index [foo], it should be dash delimited [2]

# Add a setting value with a min value not a number
curl -XPUT localhost:9200/foo/_settings -d '{ "auto_expand_replicas": "a-b" }'
# Response
{"acknowledged":true}

# Current log statement
[2015-05-12 12:10:09,297][WARN ][cluster.metadata         ] [Hypnotia] failed to set [index.auto_expand_replicas], minimum value is not a number [a]

# Updated log statement
[2015-05-12 12:09:10,056][WARN ][cluster.metadata         ] [Spider-Ham] failed to set [index.auto_expand_replicas] for index [foo], minimum value is not a number [a]

# Add a setting value with a max value not a number or all
curl -XPUT localhost:9200/foo/_settings -d '{ "auto_expand_replicas": "0-a" }'
# Response
{"acknowledged":true}

# Current log statement
[2015-05-12 12:12:01,934][WARN ][cluster.metadata         ] [Hypnotia] failed to set [index.auto_expand_replicas], maximum value is neither [all] nor a number [a]

# Updated log statement
[2015-05-12 12:11:37,191][WARN ][cluster.metadata         ] [Spider-Ham] failed to set [index.auto_expand_replicas] for index [foo], maximum value is neither [all] nor a number [a]

```
</description><key id="75656272">11124</key><summary>Add index name to log statements when settings update fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nickcanz</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T16:21:28Z</created><updated>2015-05-30T10:54:50Z</updated><resolved>2015-05-14T22:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-14T22:26:28Z" id="102187264">@nickcanz this looks good, but there are two identical commits (and a merge commit) which is kind of strange, can you squash this to a single commit and I'll merge it?
</comment><comment author="nickcanz" created="2015-05-14T22:47:18Z" id="102191228">@dakrone doh, sorry about that. I think I was messing around with origins or something.

I've fixed the commits into a single commit with only the changes in them.
</comment><comment author="dakrone" created="2015-05-14T23:07:05Z" id="102195801">Merged this to master and 1.x, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added an R client to community clients page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11123</link><project id="" key="" /><description>I maintain an R client called `elastic` at https://github.com/ropensci/elastic
</description><key id="75631522">11123</key><summary>Added an R client to community clients page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sckott</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-05-12T14:58:02Z</created><updated>2015-05-15T19:48:46Z</updated><resolved>2015-05-15T19:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T17:10:34Z" id="102460559">Hi @sckott 

Thanks for the PR. Please can I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="sckott" created="2015-05-15T17:28:40Z" id="102464047">@clintongormley thanks, I signed the CLA. 
</comment><comment author="clintongormley" created="2015-05-15T19:48:45Z" id="102507809">thanks @sckott - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard allocation should work harder to balance new shards across multiple path.data paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11122</link><project id="" key="" /><description>In 2.0 as of #10461 we now allocate each shard on a node to whichever path.data has the most free space at the moment the shard is assigned to the node.

But in the case when a new node just started up, and suddenly gets a bunch of new shards assigned (new index), we will put all those shards onto a single path (the one with the most free space at that moment).

I saw this happen on a node that had 2 identically sized SSDs on the path.data, but on one of them I had installed ES so it had a wee bit less disk space, and then all 5 shards of my new index were assigned to the other SSD.
</description><key id="75629414">11122</key><summary>Shard allocation should work harder to balance new shards across multiple path.data paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T14:50:45Z</created><updated>2015-06-17T14:28:35Z</updated><resolved>2015-06-17T14:28:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T15:12:55Z" id="101316525">@mikemccand I made this a blocker
</comment><comment author="mikemccand" created="2015-05-12T16:27:55Z" id="101339594">&gt; @mikemccand I made this a blocker

OK, I agree.  I can try to fix this ...

I think there are at least 2 cases here: 1) new index (shard) allocated to this node, and 2) shard relocated/recovered from another node.

For case 2), I think IndexService.createShard needs to be told how large the incoming shard will be (it's not today?), since we must already know this somewhere, but I don't know how to get this information in IndicesClusterStateService.applyInitializingShard when it calls createShard.

For case 1) we need to be a bit smarter when our node is allocated more than 1 shard for the same index, and use some heuristic e.g. "the size of the overall index will be 50% of total free space across all path.data paths", and budget each shard as 1/Nth of that.

Maybe there are other cases, e.g. restoring from a snapshot?

For both of these cases we need some way to record "reserved space" against each path.data, much like how a credit card company pre-charges you to "authorize" a purchase but not charge you until you actually need to pay.  Maybe this "tracking of disk space that will be consumed soon" needs to be in NodeEnvironment?
</comment><comment author="dakrone" created="2015-05-12T16:57:45Z" id="101347511">&gt; For case 2), I think IndexService.createShard needs to be told how large the incoming shard will be (it's not today?), since we must already know this somewhere, but I don't know how to get this information in IndicesClusterStateService.applyInitializingShard when it calls createShard.

I think we could average the shard sizes for all other shards in the cluster. This information can be retrieved from the `ClusterInfoService`, which will periodically fetch it (on the master node).
</comment><comment author="mikemccand" created="2015-05-12T20:21:35Z" id="101409366">Thanks for the pointer @dakrone, I'll try to use that for the case when there are already some shards in the cluster.
</comment><comment author="mikemccand" created="2015-05-13T12:41:25Z" id="101651763">At the end of the day, the fix here will be heuristicky, since we essentially must "guess" how big this shard will grow to in the future.  So, there will be adversarial cases which fill up one data path while others remain very empty...

Maybe we should (separately) make the separate path.datas on a given node visible to the cluster state?  I think we don't do this today?  E.g. DiskThresholdDecider looks at the net free bytes on a node, so it won't notice if one path is nearly full and another is very empty?  It will see that node has still having plenty of space?

If we did that, it would handle the adversarial cases where we "guessed wrong", and accidentally put N tiny shards on one path and N huge shards on another ... we'd be able to correct it later by relocating shards from the nearly full path.data ...
</comment><comment author="s1monw" created="2015-05-13T12:58:42Z" id="101654493">I mean we have a general problem here that we don't know on which node we should put the shard if the index is freshly created. I wonder if we should do the same thing we do for shard allocation as well for disk allocaiton and make sure shards of an index are balanced across disks if there is enough space on all the disks and as a second heuristic use the number of shards in total on that node to balance. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file></files><comments><comment>Balance new shard allocations more evenly across multiple path.data</comment></comments></commit></commits></item><item><title>Query refactoring: BoolQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11121</link><project id="" key="" /><description>Refactors BoolQueryBuilder and Parser. Splits the parse() method into a parsing and a query building part, adding NamedWriteable implementation for serialization and hashCode(), equals() for testing.

This change also adds tests using fixed set of leaf queries (Terms, Ids, MatchAll) as nested Queries in test query setup. Also QueryParseContext is adapted to return QueryBuilder instances for inner filter parses instead of previously returning Query instances, keeping old methods in place but deprecating them.

Relates to #10217

PR goes agains query-refacoring feature branch.
</description><key id="75619871">11121</key><summary>Query refactoring: BoolQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-12T14:20:34Z</created><updated>2015-06-11T11:04:24Z</updated><resolved>2015-06-11T11:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-13T13:31:42Z" id="101664271">I left a few comments. I think I would split this into two PRs, one for adding Streamable support to all query builders, and streamline serialization support, second one around actually refactoring the bool query.

As I said in my comments I think we should move all query builders to `StreamableReader` and change some of the existing query builders to have final fields when it make sense, lose the default constructors when possible etc.
</comment><comment author="cbuescher" created="2015-06-10T13:26:54Z" id="110750400">Did a rebase and squash after merge of NamedWritable interface on feature branch. I think this is good for another round of reviews now.
</comment><comment author="javanna" created="2015-06-10T15:59:15Z" id="110811013">looks good, left some comments
</comment><comment author="cbuescher" created="2015-06-11T09:33:40Z" id="111063830">Adressed last round of comments, putting reading/writing of NamedWritable lists in the Streaming classes amongst other things. Let me know if you think this is good to go.
</comment><comment author="javanna" created="2015-06-11T10:11:03Z" id="111076273">left a couple of tiny comments, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file></files><comments><comment>Merge pull request #11121 from cbuescher/feature/query-refactoring-boolquery</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/guice/MyJsonQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/PluginJsonQueryParser.java</file></files><comments><comment>Query refactoring: Adding getBuilderPrototype() method to all QueryParsers</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportTests.java</file></files><comments><comment>Query Refactoring: Moving parser NAME constant to corresponding query builder</comment></comments></commit></commits></item><item><title>Query DSL: Fix `bool` parsing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11120</link><project id="" key="" /><description>In #10985 I introduced a bug that should clauses are parsed as filters while
must_not clauses should be parsed as filters.
</description><key id="75614480">11120</key><summary>Query DSL: Fix `bool` parsing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T14:01:50Z</created><updated>2015-06-08T12:57:50Z</updated><resolved>2015-05-13T09:09:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-13T08:46:40Z" id="101577279">LGTM, although I'm not sure if I like the introduction of the dummy parser for the test for the reasons given above. Still I don't see a simple alternative, so we'll have to deal with it I guess.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #11120 from jpountz/fix/bool_parsing</comment></comments></commit></commits></item><item><title>Query with size=zero changes aggregations that rely on scores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11119</link><project id="" key="" /><description>If a query has the parameter for size set to zero it interferes with the scoring logic used by aggregations such as `top_hits` and `sampler` that rely on scoring.

Example GIST here: https://gist.github.com/markharwood/d66ea14a081fa584d3ca
</description><key id="75606898">11119</key><summary>Query with size=zero changes aggregations that rely on scores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-05-12T13:40:03Z</created><updated>2015-05-28T13:49:58Z</updated><resolved>2015-05-28T13:49:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-05-27T17:05:26Z" id="105998090">An additional side-effect is that the `min_score` setting is also currently broken if size=0. As a result aggregations that don't need scores e.g. `terms` agg will collect nothing if min_score is &gt;0 and the size=0 is set.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file></files><comments><comment>Aggregations fix: queries with size=0 broke aggregations that require scores.</comment><comment>Aggregations like Sampler and TopHits that require access to scores did not work if the query has size param set to zero. The assumption was that the Lucene query scoring logic was not required in these cases.</comment><comment>Added a Junit test to demonstrate the issue and a fix which relies on earlier creation of Collector wrappers so that Collector.needsScores() calls work for all search operations.</comment></comments></commit></commits></item><item><title>has_child and inner_hits for grandchild hit doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11118</link><project id="" key="" /><description>I'm trying to get `inner_hits` to work on an a two level deep `has_child` query, but the grandchild hits just seem to return an empty array.  If I specify the inner hits at the top level I get the results I want, but its rather too slow. Using inner hits on the has child query seems much faster, but doesn't return the grandchild hits.

Below is an example query:

``` json
{
  "from" : 0,
  "size" : 25,
  "query" : {
    "has_child" : {
      "query" : {
        "has_child" : {
          "query" : {
            "filtered" : {
              "query" : {
                "multi_match" : {
                  "query" : "asia",
                  "fields" : [ "_search" ],
                  "operator" : "AND",
                  "analyzer" : "library_synonyms",
                  "fuzziness" : "1"
                }
              },
              "filter" : {
                "and" : {
                  "filters" : [ {
                    "terms" : {
                      "range" : [ "Global" ]
                    }
                  } ]
                }
              }
            }
          },
          "child_type" : "document-ref",
          "inner_hits" : {
            "name" : "document-ref"
          }
        }
      },
      "child_type" : "class",
      "inner_hits" : {
        "size" : 1000,
        "_source" : false,
        "fielddata_fields" : [ "class" ],
        "name" : "class"
      }
    }
  },
  "fielddata_fields" : [ "name" ]
}
```

The `document-ref` inner hits just always returns an empty array.  Should this work (and, if so, any ideas why it isn't?), or is it beyond the means of what inner hits can currently do?
</description><key id="75601121">11118</key><summary>has_child and inner_hits for grandchild hit doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">lukens</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-05-12T13:25:49Z</created><updated>2017-04-27T05:37:57Z</updated><resolved>2016-04-29T09:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukens" created="2015-05-12T13:52:39Z" id="101290241">I've created a simpler test case for this, and it seems a little clearer that this doesn't currently work.

**Add mappings:**

```
curl -XPOST 'http://localhost:9200/grandchildren' -d '{
  "mappings" : {
    "parent" : {
      "properties" : {
        "parent-name" : {
          "type" : "string",
          "index" : "not_analyzed"
        }
      }
    },
    "child" : {
      "_parent" : {
        "type" : "parent"
      },
      "_routing" : {
        "required" : true
      },
      "properties" : {
        "parent-name" : {
          "type" : "string",
          "index" : "not_analyzed"
        }
      }
    },
    "grandchild" : {
      "_parent" : {
        "type" : "child"
      },
      "_routing" : {
        "required" : true
      },
      "properties" : {
        "grandchild-name" : {
          "type" : "string",
          "index" : "not_analyzed"
        }
      }
    }
  }
}'
```

**Populate:**

```
curl -XPOST 'http://localhost:9200/grandchildren/parent/parent' -d '{ "parent-name" : "Parent" }'
curl -XPOST 'http://localhost:9200/grandchildren/child/child?parent=parent&amp;routing=parent' -d '{ "child-name" : "Child" }'
curl -XPOST 'http://localhost:9200/grandchildren/grandchild/grandchild?parent=child&amp;routing=parent' -d '{ "grandchild-name" : "Grandchild" }'
```

**Query:**

```
curl -XGET 'http://localhost:9200/grandchildren/_search?pretty' -d '{
  "query" : {
    "has_child" : {
      "query" : {
        "has_child" : {
          "query" : {
            "match_all" : {}
          },
          "child_type" : "grandchild",
          "inner_hits" : {
            "name" : "grandchild"
          }
        }
      },
      "child_type" : "child",
      "inner_hits" : {
        "name" : "child"
      }
    }
  }
}'
```

**Result:**

```
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "grandchildren",
      "_type" : "parent",
      "_id" : "parent",
      "_score" : 1.0,
      "_source":{ "parent-name" : "Parent" },
      "inner_hits" : {
        "grandchild" : {
          "hits" : {
            "total" : 0,
            "max_score" : null,
            "hits" : [ ]
          }
        },
        "child" : {
          "hits" : {
            "total" : 1,
            "max_score" : 1.0,
            "hits" : [ {
              "_index" : "grandchildren",
              "_type" : "child",
              "_id" : "child",
              "_score" : 1.0,
              "_source":{ "child-name" : "Child" }
            } ]
          }
        }
      }
    } ]
  }
}
```

Not only is the grandchild hits empty, but they're also not nested within the child hits, so aren't going to give me what I want anyway. I'm not sure what the intended/expected behaviour would be here, but guess I need to try something else for now.
</comment><comment author="lukens" created="2015-05-12T14:03:05Z" id="101294573">Above with inner hits at top-level:

**Query:**

```
curl -XGET 'http://localhost:9200/grandchildren/_search?pretty' -d '{
  "query" : {
    "has_child" : {
      "query" : {
        "has_child" : {
          "query" : {
            "match_all" : {}
          },
          "child_type" : "grandchild"
        }
      },
      "child_type" : "child"
    }
  },
  "inner_hits" : {
    "child" : {
      "type" : {
        "child" : {
          "inner_hits" : {
            "child" : {
              "type" : {
                "grandchild" : {}
              }
            }
          }
        }
      }
    }
  }
}'
```

**Result:**

```
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "grandchildren",
      "_type" : "parent",
      "_id" : "parent",
      "_score" : 1.0,
      "_source":{ "parent-name" : "Parent" },
      "inner_hits" : {
        "child" : {
          "hits" : {
            "total" : 1,
            "max_score" : 1.0,
            "hits" : [ {
              "_index" : "grandchildren",
              "_type" : "child",
              "_id" : "child",
              "_score" : 1.0,
              "_source":{ "child-name" : "Child" },
              "inner_hits" : {
                "child" : {
                  "hits" : {
                    "total" : 1,
                    "max_score" : 1.0,
                    "hits" : [ {
                      "_index" : "grandchildren",
                      "_type" : "grandchild",
                      "_id" : "grandchild",
                      "_score" : 1.0,
                      "_source":{ "grandchild-name" : "Grandchild" }
                    } ]
                  }
                }
              }
            } ]
          }
        }
      }
    } ]
  }
}
```

Here the grandchild inner hit is correctly found, and nested
</comment><comment author="lukens" created="2015-05-12T14:12:57Z" id="101296674">[I'm using 1.5.2]
</comment><comment author="lukens" created="2015-05-13T12:44:35Z" id="101652264">I managed to track down why this wasn't working. When parsing 'has child' queries, inner hits were always being added to the `parseContext`, and never as child inner hits to the parent inner hits.

I've put together a very quick and dirty fix for this (https://github.com/lukens/elasticsearch/commit/fb22d622e7b24074b5f4fe3e26cffd4c38cff75b) which has got it working for my case, but I don't think is suitable for inclusion in a release.

Issues I see with my with my fix:
1. It's just messy, it doesn't really fix the issue, but just cleans up after it. Children still add their inner hits to the `parseContext`, the parent just removes them again afterwards, and adds them as children to its own inner hits.
2. The parent removes them again afterwards by mutating a Map it gets from the current `SearchContext`'s `InnerHitsContext`. This is obviously bad and messy, and would be broken if `InnerHitsContext` was changed to return a copy of the map, rather than the map itself. Nasty dependencies between classes.
3. I think a child can specify inner hits even if a parent doesn't. These would currently get lost. I'm not sure what the behaviour should be here, it should probably be considered an invalid query.
4. If this was done properly, descendants at different levels could add inner hits with the same name, whereas currently this could cause issues. Maybe you shouldn't be able to have the same name at different levels, but if implemented correctly, there should be no need to enforce this. 

I considered submitting as a pull request, but felt it was far too rough and ready.
</comment><comment author="martijnvg" created="2015-05-13T16:41:55Z" id="101740871">@lukens If you want nested inner hits then you need to use the top level inner hits, the inner_hits on a query doesn't support nesting. The fact that your grandparents inner hits is empty is clearly a bug, thanks for bringing this up!
</comment><comment author="lukens" created="2015-05-14T09:30:04Z" id="101983466">Hi, it's grandchild, rather than grandparent, that isn't working. Though you also seem to be suggesting it shouldn't be.  Either way, the change I committed shows that it can work, my change just isn't a very nice way to make it work.
</comment><comment author="lukens" created="2015-05-14T09:36:15Z" id="101984539">Ah, or are you saying it should work, but just shouldn't be nested? I'm not really sure what the point of it would be if it wasn't nested, though that may just be because it doesn't fit my use case, and I can't think of a use case where it would be useful.

I think it would be good if nesting did work, as that would still allow either use case, really.

The problem with top level inner_hits is that I have to apply the query once in the has_child query, and then again in the inner_hits query, which makes everything slower than it would otherwise need to be.
</comment><comment author="martijnvg" created="2015-05-14T10:20:30Z" id="101995382">@lukens yes, I meant grandchild. The reason it is a bug is, because the inner_hits in your response shouldn't be empty.

The top level inner hits and inner hits defined on a query internally to ES is the same thing and either way of defining inner hits will yield the same performance in terms of query time. The nested inner hits support in the query dsl was left out to reduce complexity and most of the times there is just a single level relationship. Obviously that means for your use case that you need to use top level inner hits. 

Maybe the inner hits support in the query dsl should support multi level relationships too, but I think the parsing logic shouldn't be get super complex because this. I need to think more about this. Like you said if it the grandchild isn't nested its hits in the response, then it isn't very helpful.

The only overhead of top level inner hits is that queries are defined twice, so the request body gets larger. If you're concerned with that, you can consider using search templates, so that you don't you reduce the amount of data send to your cluster.
</comment><comment author="lukens" created="2015-05-14T10:46:55Z" id="102001876">Isn't there also an overhead in running the query twice with the top level inner_hits, or does Elasticsearch do something clever so that it only gets run once? (and, if so, does it need to be specified again in the inner hits?)

I've not yet come across search templates, are these compatible with highly dynamic searches?

I'd like try and spend a bit more time on getting nesting working in a less hacky manner, but am under enormous pressure just to get a project completed at the moment. For the case when grandchildren aren't nested in the query, I guess the simple solution is to also not nest them in the response. I don't think this would overcomplicate the parsing too much.

I have a fairly nice way to handle all this in my mind, just not the time to implement it at the moment.
</comment><comment author="lukens" created="2015-05-14T11:12:21Z" id="102005489">OK, switching to top level hits doesn't seem to have affected performance, so I can work with that for now. It had seemed much slower before, but once I'd actually got inner_hits on the query working, that ended up just as slow, until I tweaked some other things.

The "fix" I've committed above is probably only really of any use as a reference if you want to go with nesting, as it doesn't solve the problem when not nested. I expect there's something in the inner workings of inner_hits that currently relies on them being nested.
</comment><comment author="martijnvg" created="2015-05-15T09:55:48Z" id="102351002">&gt; Isn't there also an overhead in running the query twice with the top level inner_hits, or does Elasticsearch do something clever so that it only gets run once? (and, if so, does it need to be specified again in the inner hits?)

inner_hits runs as part of the fetch phase and always executes an additional search to fetch the inner hits. The search being executed is cheap. It only runs an a single shard and just runs a query that fetches top child docs that matches with `_parent:[parent_id]` (all docs associated with parent `parent_id`) and the inner query defined in the `has_child` query. This is a query that ES (actually Lucene) can execute relatively quickly. This mini search is executed for each hit being returned.

&gt; I've not yet come across search templates, are these compatible with highly dynamic searches?

Yes, the dynamic part of the search request can be templated.

&gt; The "fix" I've committed above is probably only really of any use as a reference if you want to go with nesting, as it doesn't solve the problem when not nested. I expect there's something in the inner workings of inner_hits that currently relies on them being nested.

Yes, the inner hits features relies on the fact that grandchild and child are nested. When using the top-level inner hits notation this works out, but when using inner_hits as part of the query dsl this doesn't work out, because grandchild inner hit definition isn't nested under the child inner hit definition.

In order to fix this properly the query dsl parsing logic should just support nested inner hits. I think the format doesn't need to change in order to support this. Just because the fact the two `has_child` queries are nested should be enough for automatically nest the two inner hit definitions.
</comment><comment author="alvinchow86" created="2015-09-01T23:28:25Z" id="136890849">Allso seeing this issue, in my case multi-level nested documents (as in https://github.com/elastic/elasticsearch/issues/13064). Would be great to get a solution to this.
</comment><comment author="lipixun" created="2015-11-02T11:08:31Z" id="152989716">Will this issue be fixed at 2.x? Really looking forward to see the nested inner hits in query dsl.
</comment><comment author="sumitjainn" created="2015-11-26T13:49:40Z" id="159920657">When will this be fixed, its a blocker for us !! 
</comment><comment author="yehosef" created="2015-12-20T13:30:05Z" id="166117232">+1
</comment><comment author="jCalamari" created="2016-01-09T11:29:34Z" id="170228081">+1
</comment><comment author="clintongormley" created="2016-01-18T19:16:53Z" id="172626781">@martijnvg just pinging you about this as a reminder
</comment><comment author="abulhol" created="2016-02-01T13:21:45Z" id="177969286">+1
</comment><comment author="JanSchroeder" created="2016-03-01T09:54:42Z" id="190640006">+1
</comment><comment author="abhishek5678" created="2017-04-27T05:37:57Z" id="297616801">Hi Everyone,
where is document data </comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/InnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/InnerHitsBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/InnerHitBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/support/InnerHitsBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file></files><comments><comment>Drop top level inner hits in favour of inner hits defined in the query dsl.</comment></comments></commit></commits></item><item><title>Renamed `ignore_like` to `unlike`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11117</link><project id="" key="" /><description>This changes the parameter name `ignore_like` to the more user friendly name
`unlike`. This later feature generates a query from the terms in `A` but not
from the terms in `B`. This translates to a result set which is like `A` but
unlike `B`. We could have further negatively boosted any documents that have
some `B`, but these documents already do not receive any contribution from
having `B`, and would therefore negatively compete with documents having `A`.
</description><key id="75595424">11117</key><summary>Renamed `ignore_like` to `unlike`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T13:07:12Z</created><updated>2015-06-17T22:32:13Z</updated><resolved>2015-06-17T22:32:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-17T14:19:25Z" id="112821290">I'm a bit uncomfortable with this renaming given that `ignore_like` better describes the current behaviour, given that we just ignore the negative terms? `unlike` would feel to me like we give negative boosts to documents that contain the negative terms, which is not what we do?
</comment><comment author="alexksikes" created="2015-06-17T15:47:33Z" id="112856830">So suppose we have like `A` but unlike `B` type of query. Under the hood we form a query made of the terms in `A - B`. So we are interested about the documents (vectors) near the vector `A - B`, which corresponds to a result set in which the user gets documents which are like `A` but unlike `B`. Sure some documents could have some terms in `B` but they would not contribute to the score of the document, only the terms in `A - B` contribute to such a score. Also if we wanted to further negatively boost the terms in `B`, the user could always use boosting queries. I think that `unlike` reads much better than `ignore_like` from user perspective.
</comment><comment author="clintongormley" created="2015-06-17T15:53:40Z" id="112858521">It's tricky. I kinda read `ignore_like` as a stronger promise: don't return anything like FOO.  `unlike` feels a bit weaker, so I think I'd stick with `unlike`
</comment><comment author="jpountz" created="2015-06-17T15:56:36Z" id="112859318">OK, let's merge then!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file></files><comments><comment>More Like This: renamed `ignore_like` to `unlike`</comment></comments></commit></commits></item><item><title>Alternatives to disabling or filtering the `_source` field at index time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11116</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/10915 we removed the ability to disable the `_source` field, and in https://github.com/elastic/elasticsearch/pull/10814 we removed the ability to use `includes` and `excludes` to remove selected fields from the `_source` field that is stored with each document.

The reason for this is that a number of important existing and future features rely on having the complete original `_source` field available in Elasticsearch, such as:
- the `update` API
- on-the-fly highlighting
- reindexing (either to change mappings/analysis or in to upgrade an index over major versions)
- automated repair of index corruption 
- the ability to debug problems by viewing the original source used for indexing

In our experience, many new users disable `_source` just to save disk space, or because it seemed like a nice optimisation. Almost all of them later regret it, and found themselves unable to move forward because rebuilding the index from the original data store was too costly.

Instead, we have the ability to:
- filter the contents of the `_source` field that is returned to the user (#3302)
- enable a higher compression ratio (https://github.com/elastic/elasticsearch/pull/8863)
- filter down the entire search response with the `path` parameter (https://github.com/elastic/elasticsearch/pull/10980)

The above changes are good for the most common use cases, probably 90% of our user base.  However, there are two use cases in particular where controlling how or whether the source is indexed would beneficial to the more expert user:
## No source needed

High volume indexing of documents used almost exclusively for analytics. The source field is not required in the search results, indices can be rebuilt from fast primary data stores, minimising disk usage and write performance matters. In this case, we can provide an index setting to completely disable the storage of the `_source` field and all of the benefits that come with having the original source.

**Why an index setting?**

Previously, users could do this just by setting `_source.enabled: false`, so why switch this to an index setting?  Doing this in the mapping was too convenient, so users who didn't understand the consequences used the option and ended up suffering for it.  By making it an index setting, it (1) invalidates the behaviour that has been recommended in blog posts, making users go back to read the documentation and (2) allows us to use a scary enough name (with accompanying docs) that will make users think twice.
## Reading a large `_source` is slow and unnecessary

Users who are indexing a large field (like the contents of a PDF) plus several small fields (eg title, creation date, tags, etc) are likely to want to return just the small fields plus highlighted snippets.  However, returning just the `title` field necessitates reading (and then filtering out) the large `contents` field as well.

Previously, users used the `source.includes` and `source.excludes` filters to remove these large fields from the `_source`, but as a consequence, this disables all of the features mentioned above.   As an alternative, the user can still disable the `_source` field and set individual fields to `store: true`.

It would be nice to do better though: to keep the original `_source` but make search responses requiring just a few fields faster than they are today.  Two proposals:

**Add  a `_response_source` field**

The original  `_source` would still be stored, but the `_response_source` would be a second stored field with a filtered list of fields (behaving like the old `includes/excludes).  The user could choose which field should be returned with their search requests.  Compression would minimise the amount of extra storage required because the fields in the`_response_source`would be a subset of those in the`_source`.

**Store top-level fields as separate stored fields**

As suggested in https://github.com/elastic/elasticsearch/issues/9034, the `_source` field would be stored as separate stored fields, one for top-level field in the JSON document.  This would allow Elasticsearch to efficiently skip over filtered out fields to return just the required subset, yet it preserves the original JSON so that values such as `[1,null,1]` or `[]` etc can be returned correctly.

An advantage of this solution is that the decision about which fields to return is query time, while the `_response_source` option is set at index time.

This also opens up the possibility to enable more efficient compression techniques for individual fields, depending on the type of data contained in each field.

Thoughts?
</description><key id="75593609">11116</key><summary>Alternatives to disabling or filtering the `_source` field at index time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>Meta</label></labels><created>2015-05-12T12:58:49Z</created><updated>2015-08-19T09:32:58Z</updated><resolved>2015-05-14T21:41:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-05-12T13:30:55Z" id="101278692">I like the discussion here, thank for raising it.

In general I still think that it should up to the user if he wants to disable the _source field of filter it. The problem with this was just because there was a lot of documentation around that suggested to do this (same wrong suggestions like those tons of blog posts telling Apache Solr users to commit and optimize the whole index after each document insert...).

_I would suggest to still allow to disable or filter the_source field, but then atomatically all those services like document update API or reindexing just throw UnsupportedOperationException._

I am perfectly fine with that. I don't rely on Elasticsearch as a primary data store. I just index all documents and can reindex the stuff without the need for _source documents. I just need to store the search result snippet - and I am fine. This is why I want to filter _source. This is just the classical use-case for lucene: Index large documents and store just the snippet in the index. This is the classical full text search engine use-case. If this one is no longer supported, sorry: Elasticsearch is no longer an option for this type of stuff.
</comment><comment author="HonzaKral" created="2015-05-12T13:34:01Z" id="101279748">I generally don't like the "protect users from doing the wrong thing" approach. To me these issues should be solved by documentation, but should not limit the options users have unless usage of these provide security/stability issues (like `minimum_master_nodes`).

To change `_source.enabled` into an index settings we take away a lot of flexibility - for example we cannot have parent/child documents with different settings (since those have to be in the same index) or generally have different types with different settings in the same index - something that is very useful in many cases (think users, blog posts and comments where index ).

Same goes for `_source` filtering - it can be very useful if we have, for example, a large text that we wish to search but not display/highlight. It decreases the space taken on disk and the speed of the search itself (no need to retrieve, parse and discard this blob). Imagine a use case of people indexing documents (as in word/pdf/... a fairly common use case) - they might want to store the metadata in ES, but there is no need to store the text extracted from some binary file. If such document were to be reindexed, it would most likely be from the binary source again.

I understand _some_ of the complications and the desire to limit code paths. My compromise suggestion would be to always keep `_source`, but allow for the index-time filtering essentially allowing people to say: `"_source": {"exclude": "*"}`. It can be clearly documented as to its effects ("no reindex, update, highlighting" in bold, friendly letters) and should still allow us to remove a bunch of functionality while keeping the flexibility people like about elasticsearch.
</comment><comment author="uschindler" created="2015-05-12T13:38:26Z" id="101282247">&gt; I would suggest to still allow to disable or filter the_source field, but then atomatically all those services like document update API or reindexing just throw UnsupportedOperationException.

This should be possible to implement quite easily by a simple check on the mapping like: if (mapping.isSourceDisabled() || mapping.isSourceFiltered()) throw new UnsupportedOperationException(...)

This is in my opinion the easiest to do. And of course in the documentation clearly state that those options disable all those services that rely on full _source field availability.
</comment><comment author="nik9000" created="2015-05-12T14:17:09Z" id="101297931">&gt; Reading a large _source is slow and unnecessary

We haven't hit this, btw. We do 130 million prefix searches a day (edge ngram query, not completion suggester) that just return a handful of small fields. We load them from _source without trouble. I tried it with stored fields a year ago and didn't see any improvement. I'm sure there is a breaking point, but its a couple of times bigger than the average size of wikipedia pages. My instinct is once you start talking about single digit MB size fields.

It'd be nice to those large fields out of the working set but that's a harder thing to measure.
</comment><comment author="uschindler" created="2015-05-12T15:12:43Z" id="101316429">In the PDF case this is often the issue. I have some documents with like 80 MiB of text (I know this is a problem completely), but we just have them there to allow actually finding them, but the score of those hits is quite small (forcefully boosted down). I would never ever load those fields from source, so it is for sure an issue to store this completely useless information. As a user I want to have the power to prevent them to go as plain text into a field, sorry. I am using Elasticsearch not as a database or data store, just as a search engine. PERIOD.

As said on the other issue: One important thing is to store the _source field as CBOR instead of JSON, this improved a lot if you are scanning the whole index.

The other issue: I just don't want to exhaust my I/O cache just because I load 80 MiB of data for nonsense, just to display the title of a document.
</comment><comment author="nik9000" created="2015-05-12T15:19:45Z" id="101318516">&gt; The other issue: I just don't want to exhaust my I/O cache just because I load 80 MiB of data for nonsense, just to display the title of a document.

80mb will do that, yeah.

In #9034 @jpountz describes smooshing the _source into multiple stored fields so you only load what you need. Its still pretty useless dropping the 80mb document onto the disk but at least in the you don't blow out the cache. That feels like it'd be good enough for me.

I like simplicity of that proposal because it feels like you really could hide stored fields behind a _source abstraction.
</comment><comment author="clintongormley" created="2015-05-12T15:29:22Z" id="101320891">For 2.0, we're going to:
- add back the ability to disable storing source
- add back the `includes` and `excludes` parameters
- allow these settings to be set per-type
- only allow these settings to be set when creating a type (currently they are dynamically updatable)
- if `enabled` is false, or `includes/excludes` is set, throw an unsupported exception when trying to use features which require source.
- add better documentation so that the user understands the cost of disabling source
</comment><comment author="uschindler" created="2015-05-12T15:37:26Z" id="101322979">I agree with @HonzaKral because I also hate the "protect users from doing the wrong thing" approach. Documentation is the right approach. And failing early! If I disable or filter my source field because I don't ever want to load the data or reindex my stuff its my own decision.

It is just important to:
- document this
- throw UnsupportedOperationException if you try to do something that requires full _source.  It was indeed bad to silently do the wrong thing. Just be clear and fail early if you try to use the update document or reindexing API or whatever API. If you do this, developers will get error and will think 2 times before they disable or filter the _source field. But those who really want to do this still have the possibility.

There is nothing more from my side. I just want to have this simple possibility to decide on my own and have the full flexibility. The code is self-contained and was not even removed in the "disable source filtering" patch (#10814). The whole source filtering code is still there!!! It was just "forbidden" to use. The only actual code change next to cleanups was a single if-statement that disallowed to use of "include/exclude" in the mapping. I want this one to be reverted.
</comment><comment author="uschindler" created="2015-05-12T15:38:03Z" id="101323284">@clintongormley looks like a nice proposal!
</comment><comment author="karmi" created="2015-05-12T15:58:02Z" id="101330188">&gt; Doing this in the mapping was too convenient, so users who didn't understand the consequences used the option and ended up suffering for it.

I agree with @HonzaKral that issues like that should be solved by documentation and instructions, not by the implementation. 

However, if there's a general consensus to move this configuration from index mappings to settings, I'm OK with that. Let's just keep the configuration _option_ available to users.

(Regarding filtering `_source`, the `_response_source` approach is something which doesn't feel right to me... The second option is better.)
</comment><comment author="blakeparker" created="2015-08-18T17:18:12Z" id="132284802">It would be very helpful to also add back the ability to disable compression as well.  We have our ES nodes virtualized and stored on a Pure Storage array that does inline dedup and compression, however the ES compression completely breaks its dedup features.
</comment><comment author="jpountz" created="2015-08-19T09:32:58Z" id="132508957">The ability to disable compression won't come back. By the way it only applied to a part of the index (stored fields and term vectors), while it has never been possible to disable compression on the terms dictionary, postings, position data, doc values, etc. According to the documentation, PureStorage does deduplication of blocks of 512 bytes and uses LZO for compression so I don't think that disabling compression in elasticsearch would bring any significant gains on realistic data when running on PureStorage.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file></files><comments><comment>Mappings: Add back support for enabled/includes/excludes in _source</comment></comments></commit></commits></item><item><title>MLT depricated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11115</link><project id="" key="" /><description>According to #10736 More Like This is deprecated, not Fuzzy Like This. I tried to find issue about FLT deprecation, but I could not.
</description><key id="75584130">11115</key><summary>MLT depricated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klappvisor</reporter><labels /><created>2015-05-12T12:23:48Z</created><updated>2015-05-13T08:08:42Z</updated><resolved>2015-05-12T16:47:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-12T16:47:00Z" id="101345155">Actually, the more-like-this query will stay, we just removed the _mlt endpoint. Regarding fuzzy-like-this, it will be removed in 2.0, see https://github.com/elastic/elasticsearch/pull/10391
</comment><comment author="klappvisor" created="2015-05-13T08:07:33Z" id="101563931">Oh, sorry. I did not find this issue. I hope there will be migration guide in documentation. We are extensively using FLT for full text search though entire document which handles user typos. Is there any alternatives?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Batching for GatewayAllocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11114</link><project id="" key="" /><description>Patch to fetch shard states / stores info in batches. Batching is only done for shard states / stores requests that go to all nodes. Batch size and timeout can be configured for initial queries (when system is started up, and we want the highest degree of parallelism) and for subsequent queries.

Implements second part of #9502
</description><key id="75543888">11114</key><summary>Batching for GatewayAllocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch-t</reporter><labels /><created>2015-05-12T09:54:52Z</created><updated>2015-05-13T12:24:50Z</updated><resolved>2015-05-13T12:24:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-13T09:28:57Z" id="101590605">hey @ywelsch-t, thanks for the effort!. I have been working on a chance that removes the sync nature from the gateway allocator completely, see more info here: #11101. I think this will probably be the best approach since it solves the problem completely.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't throw an exception if repositories are unregistered with `*`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11113</link><project id="" key="" /><description>Today we barf if repositories are unregistered with a `*` pattern. This
happens on almost every test and adds weird log messages. I dont' think
we should barf in that case.
</description><key id="75514696">11113</key><summary>Don't throw an exception if repositories are unregistered with `*`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T08:22:14Z</created><updated>2015-06-07T17:27:29Z</updated><resolved>2015-05-13T15:06:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T10:14:50Z" id="101222503">@imotov can you take a look at this?
</comment><comment author="s1monw" created="2015-05-13T07:30:18Z" id="101547496">pushed a new commit
</comment><comment author="imotov" created="2015-05-13T13:35:33Z" id="101665117">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file></files><comments><comment>Don't throw an exception if repositories are unregister with match all</comment></comments></commit></commits></item><item><title>Query DSL: deprecate BytesFilterBuilder in favour of WrapperFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11112</link><project id="" key="" /><description>BytesFilterBuilder got removed from master and is now deprecated in 1.x. WrapperFilterBuilder should be used instead.

Relates to #10919
</description><key id="75513196">11112</key><summary>Query DSL: deprecate BytesFilterBuilder in favour of WrapperFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>deprecation</label><label>enhancement</label><label>v1.6.0</label></labels><created>2015-05-12T08:16:37Z</created><updated>2015-05-12T08:57:57Z</updated><resolved>2015-05-12T08:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-12T08:46:29Z" id="101191712">@jpountz can you have a quick look please?
</comment><comment author="jpountz" created="2015-05-12T08:51:10Z" id="101194521">LGTM Thanks @javanna !
</comment><comment author="javanna" created="2015-05-12T08:57:46Z" id="101199913">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make synced flush attempt async to run it easily from a TransportAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11111</link><project id="" key="" /><description>Today we enforce blocking which doesnt' really fit in the elasticsearch model
this commit adds async execution to the synced flush service by passing a
ActinListener to the service returing immediately.
</description><key id="75511283">11111</key><summary>Make synced flush attempt async to run it easily from a TransportAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-05-12T08:10:08Z</created><updated>2015-05-15T17:06:22Z</updated><resolved>2015-05-12T09:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T08:32:07Z" id="101187744">@kimchy as discussed moved the blocking version into test util
</comment><comment author="kimchy" created="2015-05-12T08:35:28Z" id="101188326">@s1monw this is great, thanks!, LGTM
</comment><comment author="brwe" created="2015-05-12T08:57:12Z" id="101199382">LGTM too
</comment><comment author="clintongormley" created="2015-05-15T17:04:44Z" id="102459632">@s1monw Please put labels on this
</comment><comment author="clintongormley" created="2015-05-15T17:06:21Z" id="102459894">sorry - i see it was a feature branch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix NPE when checking for active shards before deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11110</link><project id="" key="" /><description>The timeout value that was added to ShardActiveRequest in #10172 is
also used as a timeout for the cluster state observer. When
streaming from older versions this timeout value was null
and this caused an NPE when the cluster state observer was called.

I noticed this while looking at http://build-us-00.elastic.co/job/es_bwc_1x/10115/ although I do not think this caused the failure.
</description><key id="75510230">11110</key><summary>Fix NPE when checking for active shards before deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Store</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T08:05:13Z</created><updated>2015-06-08T15:55:12Z</updated><resolved>2015-05-12T08:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T08:18:13Z" id="101185311">LGTM
</comment><comment author="clintongormley" created="2015-05-15T17:02:40Z" id="102459293">@brwe please put version labels on this (plus type of fix, etc)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] add backwards path to security policy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11109</link><project id="" key="" /><description>@rmuir Is that the way to do it?
</description><key id="75506800">11109</key><summary>[TEST] add backwards path to security policy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-05-12T07:51:36Z</created><updated>2015-05-12T08:28:42Z</updated><resolved>2015-05-12T08:02:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T08:01:47Z" id="101180494">I think we agreed that we won't run BWC tests with sec manager since it requires adding permission to spawn proceses which we don't want to do?
</comment><comment author="brwe" created="2015-05-12T08:02:59Z" id="101180955">sorry, missed that discussion.
</comment><comment author="brwe" created="2015-05-12T08:28:42Z" id="101187024">I added a comment to the TESTING.asciidoc.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: RangeQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11108</link><project id="" key="" /><description>Split the parse(QueryParseContext ctx) method into a parsing and a query building part, adding Streamable for serialization and hashCode(), equals() for better testing.
Add basic unit test for Builder and Parser.

PR goes agains query-refacoring feature branch.
</description><key id="75500527">11108</key><summary>Query refactoring: RangeQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-12T07:34:33Z</created><updated>2015-05-18T10:45:57Z</updated><resolved>2015-05-18T10:45:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-12T07:45:39Z" id="101171756">There are two issues with this PR I'd like to get a second opinion on. 
- In order to stream the `timezone` and the corresponding `format` field I chose to use internal String representation and just create the DateMathParser and DateTimeZone objects if needed right before the query is build. However, for early validation of these fields I saw no other way than to create the same objects in `validate()` already. Would like to hear thoughts about that.
- In order to test the whole query construction part when there are mappers for the field, I'd have to add mappings to the parseContext. Not sure how to do that in the setup of the Base test though.
</comment><comment author="rjernst" created="2015-05-12T17:52:31Z" id="101364876">@cbuescher You can add mappings by constructing your own MapperService and adding types? Right now the dummy parse context you have just has null for the MapperService IIRC, or something effectively like that.
</comment><comment author="javanna" created="2015-05-13T10:01:55Z" id="101605606">left a few comments, looks good though!
</comment><comment author="javanna" created="2015-05-13T12:25:26Z" id="101648362">@cbuescher to answer your question above:

&gt; In order to stream the timezone and the corresponding format field I chose to use internal String representation and just create the DateMathParser and DateTimeZone objects if needed right before the query is build. However, for early validation of these fields I saw no other way than to create the same objects in validate() already. Would like to hear thoughts about that.

looks good to me, fail fast fail often ;) if the timezone or format are broken, we want to know asap, let's pay the price for the object creation on the coord node then.
</comment><comment author="cbuescher" created="2015-05-13T14:00:37Z" id="101672647">Went through your comments and adressed most of them, rebased on current head of feature branch also. I'd still like to have a look if the `toQuery()` branch for the `DateFieldMapper`case can be tested somehow.
</comment><comment author="javanna" created="2015-05-13T15:35:22Z" id="101717646">left a couple of minor comments, besides those LGTM though I think it's ready 
</comment><comment author="cbuescher" created="2015-05-15T13:23:52Z" id="102396724">Added mappings for a field with date type to the base test and extended the RangeQueryBuilderTest to include the code path where date mapper is used. Unfortunately the resulting lucene query is very difficult to access, so I fall back on checking the resulting toString() representation of the query.
</comment><comment author="cbuescher" created="2015-05-15T15:26:48Z" id="102432230">@javanna I went through your last comments one more time and decided to add an additional test mapping for the integer type case. This way we cover all three cases that are possible in RangeQueryBuilder. I'm a little hesitant in these cases with that level of tests because they are likely to break fast with subtle changes in the lucene query code, but maybe thats a good thing.
</comment><comment author="javanna" created="2015-05-15T15:38:46Z" id="102437082">I went over it again, left a few more comments, it's very close though, just a few minor changes to make I guess
</comment><comment author="cbuescher" created="2015-05-18T09:04:20Z" id="102981996">I went through your last comments regarding test class and rebased the whole PR on top of current feature branch.
</comment><comment author="javanna" created="2015-05-18T09:16:54Z" id="102987381">left a couple of minor comments, looks great besides those
</comment><comment author="cbuescher" created="2015-05-18T09:29:28Z" id="102991488">@javanna thanks, adressed last couple of changes.
</comment><comment author="javanna" created="2015-05-18T09:53:30Z" id="103000452">LGTM
</comment><comment author="cbuescher" created="2015-05-18T10:45:22Z" id="103013743">Merged with feature branch with b99cb2103cf112ee799c4f2a9ecd434dcc5c86ec
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file></files><comments><comment>Query Refactoring: Add RangeQueryBuilder and Parser refactoring and test.</comment></comments></commit></commits></item><item><title>Relocation performing extra work?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11107</link><project id="" key="" /><description>The following is the reported scenario on 1.4.2 (have cat shards output if you would like to see it):

A node "6" was rebooted (unexpectedly), this node was hosting 4 shards, 2 primaries and 2 replicas.  When "6" stopped, 2 other nodes in the cluster had replicas promoted to primaries, and the 2 replicas that used to be on "6" were then allocated to another 2 nodes in the cluster - as expected.

When node "6" came back, cluster started to relocate 2 shards from 2 other nodes to it.  It decided to move 2 big 34Gb shards from 2 other nodes to "6".  Then shortly after, it seems like it then realized that the cluster was unbalanced and proceeded to move one of the 34Gb shard from "6" to a different node "4".  Note that at the time the 2 34Gb shards were relocating to "6", there were no other relocations going on and "4" was showing that it had room for one of the 34Gb shard.  It seems like it could have just relocated the 34Gb shard directly to "4" without first moving it to "6".
</description><key id="75467814">11107</key><summary>Relocation performing extra work?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-05-12T05:18:56Z</created><updated>2016-01-18T19:12:41Z</updated><resolved>2016-01-18T19:12:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-12T13:44:57Z" id="101286877">In general the balance algorithm is "simple" in that it doesn't look into the future. It just tries to make the best decision it can at the time and it doesn't try to take a long view. I expect this kind of thing, unfortunately.

Note also that it doesn't take into account the size of the shards except to make sure that it doesn't fill up the disk.
</comment><comment author="ppf2" created="2015-05-12T17:00:40Z" id="101348346">@nik9000 thx
</comment><comment author="ppf2" created="2015-05-16T00:18:45Z" id="102552803">Does it try to balance the # of shards per index across the nodes as part of this "intermediate" move?  For this incident, the 2 34Gb shards that got moved to "6" in the beginning are 2 replica shards (different shard #s) for the same index.  And this index has 5 shards (1 replica) = 10 shards total, but the cluster has 15 data nodes.  Not sure if it decided to move one of the 2 shards immediately to another node because of the size imbalance, or the shards per index per node imbalance.
</comment><comment author="nik9000" created="2015-05-16T02:45:05Z" id="102566247">I'm not sure why. Its sometimes difficult to be sure why the movement code
makes the decisions it makes. You should probably have a look at the
total_shards_per_node setting - it might be useful to prevent these large
shards from moving together.

On Fri, May 15, 2015 at 8:18 PM, Pius notifications@github.com wrote:

&gt; Does it try to balance the # of shards per index across the nodes as part
&gt; of this "intermediate" move? For this incident, the 2 34Gb shards that got
&gt; moved to "6" in the beginning are 2 replica shards (different shard #s) for
&gt; the same index. And this index has 5 shards (1 replica) = 10 shards total,
&gt; but the cluster has 15 data nodes. Not sure if it decided to move one of
&gt; the 2 shards immediately to another node because of the size imbalance, or
&gt; the shards per index per node imbalance.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11107#issuecomment-102552803
&gt; .
</comment><comment author="clintongormley" created="2016-01-18T19:12:41Z" id="172625970">This process has been greatly improved since 1.4. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve path mgmt on init, better error messages, symlink support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11106</link><project id="" key="" /><description>Improve path management on init:
- Properly support symlinks for configured paths (e.g. /tmp -&gt; /mnt/tmp)
- Check all configured paths up front and deliver the best exception we can when things are wrong.
- Initialize securitymanager earlier.
- Fix too-loud error logging of Natives root check.
- Make bootstrap src/java code package private and src/test code public, rather than the other way around.
- Add some unit tests for this stuff.

Thank you @mrsolo for reporting symlink issues.
</description><key id="75464558">11106</key><summary>Improve path mgmt on init, better error messages, symlink support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-12T05:03:43Z</created><updated>2015-05-15T17:23:58Z</updated><resolved>2015-05-12T18:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-05-12T15:45:01Z" id="101325687">Hi looks good from my side. The checkDirectoryExists code is really crazy! Thanks for the discussion. Maybe we should add a similar code to Lucene's FSDirectory, so we don't fail in Files.createDirectories() because of symlink pointing to index directory itsself.
</comment><comment author="rmuir" created="2015-05-12T15:55:40Z" id="101329555">yeah, its unfortunately crazy. it could be simpler at the cost of not having good exceptions, immediately.  My idea is that since all user-configured paths come thru this one place at startup, it is good to "try our best" up-front to make misconfigurations obvious, at the expense of the crazy method.

So if you configure some path to /actually/a/file, NotDirectoryException is way better than a confusing FileAlreadyExistsException. 

And if you configure a path that is already there, its good to try to do some check that permissions are reasonable, so that you get immediate AccessDenied rather than some potentially more confusing exception later.

For the lucene FSDirectory case, where typically some read/write will happen immediately after (e.g. lockfactory initialization), we may be fine with something simpler, like createDirectories(toRealPath()), so its just changing the order of two lines of code there if we want to do that.

Thanks for looking at this @uschindler !
</comment><comment author="uschindler" created="2015-05-12T16:36:43Z" id="101342781">&gt; For the lucene FSDirectory case, where typically some read/write will happen immediately after (e.g. lockfactory initialization), we may be fine with something simpler, like createDirectories(toRealPath()), so its just changing the order of two lines of code there if we want to do that.

Actually I am glad that I repaired the LockFactory shit last September for Lucene 5 :-) The LockFactory has no initialization anymore, it is a singleton. But the actual makeLock call passes the directory, so we are sure that everything fits each other!

But I agree, we should do the tests just before we do the mkdirs() with the already fully resolved path.
</comment><comment author="rmuir" created="2015-05-12T17:15:47Z" id="101354983">&gt; But I agree, we should do the tests just before we do the mkdirs() with the already fully resolved path.

@uschindler this won't work because toRealPath() requires the target exist or itself will fail. I just played around, trying to restructure this logic to remove the conditional, but i think its unavoidable.
</comment><comment author="rjernst" created="2015-05-12T18:10:43Z" id="101371444">LGTM. I like the `BootstrapForTesting` name a lot better!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/main/java/org/elasticsearch/common/jna/Natives.java</file><file>src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTokenStreamTestCase.java</file></files><comments><comment>Merge pull request #11106 from rmuir/symlink</comment></comments></commit></commits></item><item><title>Add Multi-Valued Field Methods to Expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11105</link><project id="" key="" /><description>Add methods to operate on multi-valued fields in the expressions language.
Note that users will still not be able to access individual values
within a multi-valued field in expressions at this time.

The following methods will be included:
- min
- max
- avg
- med
- count
- sum

Additionally, changes have been made to MultiValueMode to support the
new med and count methods.
</description><key id="75398273">11105</key><summary>Add Multi-Valued Field Methods to Expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T23:52:12Z</created><updated>2015-06-06T18:54:05Z</updated><resolved>2015-05-14T15:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-12T09:36:25Z" id="101212728">The median impl looks fine but count looks more challenging since it doesn't work like other modes. Maybe we should only focus on exposing min/max/sum/avg/med in this PR and think about `count` later.

Regarding apply/reduce, I think we should nuke them: they are only used for function score and look easy to replace. We can do it in another PR though.

Could you update `docs/reference/search/request/sort.asciidoc` to mention support of `med` when sorting?
</comment><comment author="jdconrad" created="2015-05-13T18:23:58Z" id="101767461">@jpountz @rjernst Ready for the next round of review.  As noted above, I moved count into its own separate ValueSource/FunctionValues pair for expressions, and responded to all other requests for changes as well.
</comment><comment author="rjernst" created="2015-05-13T22:55:08Z" id="101841437">LGTM
</comment><comment author="jdconrad" created="2015-05-14T15:49:19Z" id="102079714">@jpountz @rjernst Thanks for the reviews!
</comment><comment author="clintongormley" created="2015-05-15T18:33:40Z" id="102486694">w00t
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java</file><file>src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java</file><file>src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java</file><file>src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java</file><file>src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java</file><file>src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java</file><file>src/main/java/org/elasticsearch/search/MultiValueMode.java</file><file>src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file><file>src/test/java/org/elasticsearch/search/MultiValueModeTests.java</file></files><comments><comment>Scripting: Add Multi-Valued Field Methods to Expressions</comment></comments></commit></commits></item><item><title>Generate access to tests paths like other paths.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11104</link><project id="" key="" /><description>Currently we have some static paths in our access rules for tests, that use the troublesome sysprop substitution mechanism (e.g. previous java.io.tmpdir issues) in the policy file.

Instead I would like to dynamically generate the paths like we do for any other paths. Besides actually working reliably (i am able to get some plugins working with security manager enabled with this PR, after building a custom JDK build to debug this nightmare), this also gives us additional control (e.g. throw a clear exception if pom does not pass the necessary settings through).

We also remove the unnecessary java.home permission (thats outdated).
</description><key id="75374925">11104</key><summary>Generate access to tests paths like other paths.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T22:09:38Z</created><updated>2015-05-15T16:25:17Z</updated><resolved>2015-05-11T22:32:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-11T22:20:34Z" id="101065165">Also Dawid Weiss is looking into the underlying debug issue that made this so much fun: https://issues.apache.org/jira/browse/LUCENE-6478
</comment><comment author="dakrone" created="2015-05-11T22:23:59Z" id="101065644">LGTM
</comment><comment author="rmuir" created="2015-05-11T22:31:17Z" id="101066732">Thanks @dakrone 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/SecurityBootstrap.java</file></files><comments><comment>Merge pull request #11104 from rmuir/generate_paths</comment></comments></commit></commits></item><item><title>Add Spark write RDD exemple</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11103</link><project id="" key="" /><description>There isn't any write example in Spark support documentation showing how to use `saveAsNewAPIHadoopDataset` and `saveAsHadoopDataset`.

Exemple of `saveAsHadoopDataset` in Java:

```
        JobConf jobConf = new JobConf();
        jobConf.set(ES_NODES, "localhost:9200");
        jobConf.set(ES_RESOURCE, "requests/simple");
        jobConf.setOutputFormat(EsOutputFormat.class);
        jobConf.setMapOutputValueClass(MapWritable.class);
        jobConf.setMapOutputKeyClass(Text.class);
        requests.mapToPair(req -&gt; {
                    String id = req.getId();
                    Map&lt;String, Object&gt; document = new HashMap&lt;&gt;();
                    document.put("title", req.getTitle());
                    document.put("path", req.getPath());
                    document.put("duration", req.getDuration());
                    return new Tuple2&lt;&gt;(id, EsJavaUtils.toWritable(document));
                })
                .saveAsHadoopDataset(jobConf);
```
</description><key id="75369931">11103</key><summary>Add Spark write RDD exemple</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gquintana</reporter><labels /><created>2015-05-11T21:46:52Z</created><updated>2015-05-15T16:24:11Z</updated><resolved>2015-05-15T16:24:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T16:24:10Z" id="102452487">Please reopen on https://github.com/elastic/elasticsearch-hadoop/issues
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Hadoop Configuration setMapOutputValueClass method doesn't exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11102</link><project id="" key="" /><description>In http://www.elastic.co/guide/en/elasticsearch/hadoop/2.0/mapreduce.html#writing-json-new-api code exemple

```
Configuration conf = new Configuration();
conf.set("es.input.json", "yes");                 
conf.setMapOutputValueClass(BytesWritable.class); 
```

The `setMapOutputValueClass` method doesn't exist on class`Configuration`
</description><key id="75365520">11102</key><summary>Hadoop Configuration setMapOutputValueClass method doesn't exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gquintana</reporter><labels /><created>2015-05-11T21:33:23Z</created><updated>2015-05-15T16:23:51Z</updated><resolved>2015-05-15T16:23:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T16:23:51Z" id="102452444">Hi @gquintana 

Please could you reopen your issue here: https://github.com/elastic/elasticsearch-hadoop/issues
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Async fetch of shard started and store during allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11101</link><project id="" key="" /><description>Today, when a primary shard is not allocated we go to all the nodes to find where it is allocated (listing its started state). When we allocate a replica shard, we head to all the nodes and list its store to allocate the replica on a node that holds the closest matching index files to the primary.

Those two operations today execute synchronously within the GatewayAllocator, which means they execute in a sync manner within the cluster update thread. For large clusters, or environments with very slow disk, those operations will stall the cluster update thread, making it seem like its stuck.

Worse, if the FS is really slow, we timeout after 30s the operation (to not stall the cluster update thread completely). This means that we will have another run for the primary shard if we didn't find one, or we won't find the best node to place a shard since it might have timed out (listing stores need to list all files and read the checksum at the end of each file).

On top of that, this sync operation happen one shard at a time, so its effectively compounding the problem in a serial manner the more shards we have and the slower FS is...

This change moves to perform both listing the shard started states and the shard stores to an async manner. During the allocation by the GatewayAllocator, if data needs to be fetched from a node, it is done in an async fashion, with the response triggering a reroute to make sure the results will be taken into account. Also, if there are on going operations happening, the relevant shard data will not be taken into account until all the ongoing listing operations are done executing.

The execution of listing shard states and stores has been moved to their own respective thread pools (scaling, so will go down to 0 when not needed anymore, unbounded queue, since we don't want to timeout, just let it execute based on how fast the local FS is). This is needed sine we are going to blast nodes with a lot of requests and we need to make sure there is no thread explosion.

This change also improves the handling of shard failures coming from a specific node. Today, those nodes were ignored from allocation only for the single reroute round. Now, since fetching is async, we need to keep those failures around at least until a single successful fetch without the node is done, to make sure not to repeat allocating to the failed node all the time.

Note, if before the indication of slow allocation was high pending tasks since the allocator was waiting for responses, not the pending tasks will be much smaller. In order to still indicate that the cluster is in the middle of fetching shard data, 2 attributes were added to the cluster health API, indicating the number of ongoing fetches of both started shards and shard store.

Closes #9502
</description><key id="75360907">11101</key><summary>Async fetch of shard started and store during allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T21:18:26Z</created><updated>2015-05-29T15:15:33Z</updated><resolved>2015-05-20T15:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-11T21:19:53Z" id="101050770">@martijnvg I need your help here, every once in a while, the test `IndicesStoreIntegrationTests#testShardActiveElseWhere` in network mode fails every once in a while. I don't full understand the test, since it creates fake started shards that causes the system to barf... . Can you have a look at it, I think the test should be rewritten.
</comment><comment author="martijnvg" created="2015-05-12T07:48:52Z" id="101174909">@kimchy yes, the test is confusing, because of inconsistencies... the idea behind this test is that the internal shard exist api is used to verify if we actually can the shards a node has on disk. 

By injecting fake started shards we let a node think that shards it has have been started on another node, It then verifies with the shard exist api if those shards do really exist on another node, in this test that isn't the case, so the node shouldn't delete its local shards.
</comment><comment author="kimchy" created="2015-05-12T07:54:28Z" id="101176795">@martijnvg yea, I gathered, it just creates an unexpected state in ES side. I am working on it now to see how far I go, will keep you posted.
</comment><comment author="kimchy" created="2015-05-12T08:26:22Z" id="101186419">@martijnvg I just rebased against master and pushed a fix for this (I think), where failed shards should be ignored for the next successful fetch phase, lets see how it goes
</comment><comment author="kimchy" created="2015-05-13T09:29:57Z" id="101591567">I found the problem in the `IndicesStoreIntegrationTests#testShardActiveElseWhere` test, pushed a fix here: https://github.com/elastic/elasticsearch/commit/ba20d4b6ba3902b97e0691e7fd965c0c0f684f42
</comment><comment author="imotov" created="2015-05-14T19:51:08Z" id="102149195">@kimchy I left a few minor comments, otherwise it looks good to me. However, I don't have an intimate knowledge of some aspects of the allocation code that might be affected here (i.e. shadow replicas). So, I would feel much better if somebody else who worked with this code recently took another look. @dakrone, maybe?
</comment><comment author="kimchy" created="2015-05-15T13:42:23Z" id="102400925">@imotov removed the unneeded import, and responded to your comments. @dakrone it would be great if you can take a look
</comment><comment author="s1monw" created="2015-05-15T14:58:57Z" id="102423102">I like this change - it really touches the problem we have today with this code and fixes it. Yet, I think we need to work on some code internals and naming before we get this in. left some comments.
</comment><comment author="kimchy" created="2015-05-15T22:22:26Z" id="102538390">@s1monw I pushed another round
</comment><comment author="kimchy" created="2015-05-17T22:02:48Z" id="102863877">@s1monw I pushed another round, tell me what you think...
</comment><comment author="s1monw" created="2015-05-18T07:59:02Z" id="102955340">LGTM great stuff @kimchy 
</comment><comment author="kimchy" created="2015-05-18T09:43:14Z" id="102997521">thanks @s1monw, there were a few failures on CI for this pull which I wasn't happy with, I believe I tracked it down to not handling properly (as in, the same way as previously) the case of failed shards. I changed the code to have AsyncShardFetch to be shard level, and cleaned it up from having to mark the node as failed. I will let CI run a bit on it and see how it goes
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/NodesOperationResponse.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file><file>src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Async fetch of shard started and store during allocation</comment><comment>Today, when a primary shard is not allocated we go to all the nodes to find where it is allocated (listing its started state). When we allocate a replica shard, we head to all the nodes and list its store to allocate the replica on a node that holds the closest matching index files to the primary.</comment></comments></commit></commits></item><item><title>Cluster state update takes a long time due to lots of aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11100</link><project id="" key="" /><description>When taking a snapshot or creating an index takes a very long time. For example when taking a snapshot takes 18 hours. The cluster has 23 nodes &amp; 655 indexes with 400 alias per index. Each user is an alias (to an index). They track the current Index (in zookeeper). Once the current index has 400 alias, they create a new index. When looking at the pending tasks while the snapshot is running we see a lot of long run tasks like:

```
{
"insert_order" : 30872,
"priority" : "NORMAL",
"source" : "update snapshot state",
"executing" : false,
"time_in_queue_millis" : 62424008,
"time_in_queue" : "17.3h"
}
```

This is an example of the alias that is created:

```
"63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80" : {
    "filter" : {
        "term" : {
            "user_id" : "63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80"
        }
    },
    "index_routing" : "63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80",
    "search_routing" : "63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80"
},
```

According to Igor, publishing of cluster state updates always takes about 14-15 seconds no matter what type of update it is. Here is one example when updating aliases:

```
[03:23:37,861][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [index-aliases]: execute
[03:23:40,144][DEBUG][cluster.service ] [Mad Dog Rassitano] cluster state updated, version [39957], source [index-aliases]
[03:23:40,144][DEBUG][cluster.service ] [Mad Dog Rassitano] publishing cluster state version 39957
[03:23:55,545][DEBUG][cluster.service ] [Mad Dog Rassitano] set local cluster state to version 39957
[03:23:55,554][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [index-aliases]: done applying updated cluster_state (version: 39957)
```

Here is another example of a node leaving the cluster:

```
[07:37:21,822][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]: execute
[07:37:58,920][DEBUG][cluster.service ] [Mad Dog Rassitano] cluster state updated, version [40189], source [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]
[07:37:58,921][INFO ][cluster.service ] [Mad Dog Rassitano] removed {[Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false},}, reason: zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[07:37:58,921][DEBUG][cluster.service ] [Mad Dog Rassitano] publishing cluster state version 40189
[07:38:12,901][DEBUG][cluster.service ] [Mad Dog Rassitano] set local cluster state to version 40189
[07:38:12,942][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]: done applying updated cluster_state (version: 40189)
```

Slow cluster state propagation causes issues with index creation and slow snapshots.

The hot threads provided shows a lot of activity in reading cluster state from the network with a majority of the hot threads being busy reading aliases:

```
threads-0007: 79.9% (399.7ms out of 500ms) cpu usage by thread 'elasticsearch[Dormammu][transport_client_worker][T#2]{New I/O worker #2}'
threads-0007- 4/10 snapshots sharing following 31 elements
threads-0007- org.elasticsearch.common.hppc.ObjectObjectOpenHashMap.get(ObjectObjectOpenHashMap.java:564)
threads-0007- org.elasticsearch.common.collect.ImmutableOpenMap$Builder.get(ImmutableOpenMap.java:255)
threads-0007- org.elasticsearch.cluster.metadata.MetaData.&lt;init&gt;(MetaData.java:222)
threads-0007- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)
threads-0007- org.elasticsearch.cluster.metadata.MetaData$Builder.readFrom(MetaData.java:1382)
threads-0007- org.elasticsearch.cluster.ClusterState$Builder.readFrom(ClusterState.java:620)
threads-0007- org.elasticsearch.action.admin.cluster.state.ClusterStateResponse.readFrom(ClusterStateResponse.java:58)
threads-0007- org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:150)
threads-0007- org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)
threads-0007- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
threads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
threads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
threads-0007- org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
threads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
threads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
threads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
threads-0007- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
threads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
threads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)

threads-0008: 88.7% (443.7ms out of 500ms) cpu usage by thread 'elasticsearch[Tower][clusterService#updateTask][T#1]'
threads-0008- 2/10 snapshots sharing following 11 elements
threads-0008- org.elasticsearch.common.hppc.ObjectObjectOpenHashMap.get(ObjectObjectOpenHashMap.java:564)
threads-0008- org.elasticsearch.common.collect.ImmutableOpenMap$Builder.get(ImmutableOpenMap.java:255)
threads-0008- org.elasticsearch.cluster.metadata.MetaData.&lt;init&gt;(MetaData.java:222)
threads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)
threads-0008- org.elasticsearch.cluster.ClusterState$Builder.metaData(ClusterState.java:525)
threads-0008- org.elasticsearch.discovery.zen.ZenDiscovery$8.execute(ZenDiscovery.java:690)
threads-0008- org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
threads-0008- org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
threads-0008- java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
threads-0008- java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
threads-0008- java.lang.Thread.run(Unknown Source)
threads-0008- 4/10 snapshots sharing following 12 elements
threads-0008- org.elasticsearch.common.hppc.AbstractIterator.hasNext(AbstractIterator.java:34)
threads-0008- org.elasticsearch.common.hppc.AbstractObjectCollection.toArray(AbstractObjectCollection.java:86)
threads-0008- org.elasticsearch.common.hppc.ObjectArrayList.toArray(ObjectArrayList.java:44)
threads-0008- org.elasticsearch.cluster.metadata.MetaData.&lt;init&gt;(MetaData.java:232)
threads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)
threads-0008- org.elasticsearch.cluster.ClusterState$Builder.metaData(ClusterState.java:525)
threads-0008- org.elasticsearch.discovery.zen.ZenDiscovery$8.execute(ZenDiscovery.java:690)

threads-0008: 100.0% (500ms out of 500ms) cpu usage by thread 'elasticsearch[Freak][transport_server_worker][T#6]{New I/O worker #15}'
threads-0008- 6/10 snapshots sharing following 33 elements
threads-0008- org.elasticsearch.cluster.metadata.MetaData.&lt;init&gt;(MetaData.java:205)
threads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)
threads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.readFrom(MetaData.java:1382)
threads-0008- org.elasticsearch.cluster.ClusterState$Builder.readFrom(ClusterState.java:620)
threads-0008- org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:175)
threads-0008- org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:156)
threads-0008- org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
threads-0008- org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
threads-0008- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
threads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
threads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
threads-0008- org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
threads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
threads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
threads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
threads-0008- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
threads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
threads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
threads-0008- org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
```

I realize this issue is related to these two issues:

https://github.com/elastic/elasticsearch/pull/10212 
https://github.com/elastic/elasticsearch/pull/10295

However I am looking for suggestions as how to improve the performance of the cluster state propagation. 
</description><key id="75349483">11100</key><summary>Cluster state update takes a long time due to lots of aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msimos</reporter><labels><label>:Aliases</label></labels><created>2015-05-11T20:36:48Z</created><updated>2015-05-15T22:08:45Z</updated><resolved>2015-05-15T22:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T16:22:25Z" id="102452197">Hi @msimos 

I've marked this for discussion, but I think the outcome will be: you're using too many aliases.  These aliases are consuming a big chunk of memory on every node in your cluster, and making cluster state updates very heavy.  Even with cluster state diffs, it's still going to be slow.  I'd really consider adopting another plan which doesn't involve that many aliases (eg just adding the filter and routing values dynamically on to each request via your application)
</comment><comment author="msimos" created="2015-05-15T22:08:45Z" id="102535734">Hi @clintongormley,

This is what we recommended. So I'll close this now.

Mike
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect example in CamelCase tokenizer description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11099</link><project id="" key="" /><description>"pattern" on page http://www.elastic.co/guide/en/elasticsearch/reference/1.4/analysis-pattern-analyzer.html#_camelcase_tokenizer currently reads:" "([^\\p{L}\\d]+)|(?&lt;=\\D)(?=\\d)|(?&lt;=\\d)(?=\\D)|(?&lt;=[\\p{L}&amp;&amp;[^\\p{Lu}]])(?=\\p{Lu})|(?&lt;=\\p{Lu})(?=\\p{Lu}[\\p{L}&amp;&amp;[^\\p{Lu}]])"

should read:
"([^\p{L}\d]+)|(?&lt;=\D)(?=\d)|(?&lt;=\d)(?=\D)|(?&lt;=[ \p{L} &amp;&amp; [^\p{Lu}]])(?=\p{Lu})|(?&lt;=\p{Lu})(?=\p{Lu}[\p{L}&amp;&amp;[^\p{Lu}]])"
</description><key id="75337758">11099</key><summary>Incorrect example in CamelCase tokenizer description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>docs</label></labels><created>2015-05-11T19:50:57Z</created><updated>2015-05-15T16:44:29Z</updated><resolved>2015-05-15T16:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-11T20:09:52Z" id="101034079">I laughed so much at seeing this because my eyes just unfocus when I see a regex this long. Comparing them is something I can only do with great patience. I see the change from \ to \ which feels right. Is that the only change?
</comment><comment author="seang-es" created="2015-05-12T00:25:26Z" id="101080533">It looks like github is correcting the regexp as it renders it.  

Here's the source in a code block:

```
 "pattern":"([^\\\\p{L}\\\\d]+)|(?&lt;=\\\\D)(?=\\\\d)|(?&lt;=\\\\d)(?=\\\\D)|(?&lt;=[\\\\p{L}&amp;&amp;[^\\\\p{Lu}]])(?=\\\\p{Lu})|(?&lt;=\\\\p{Lu})(?=\\\\p{Lu}[\\\\p{L}&amp;&amp;[^\\\\p{Lu}]])"
```

The original has four backslashes instead of two.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fixed the backslash escaping on the pattern analyzer docs</comment></comments></commit></commits></item><item><title>Synced flush api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11098</link><project id="" key="" /><description>Dedicated api for synced flush. Triggers a synced flush and gives a detailed explanation on why it did not succeed if it did not.

request: 

```
POST test/_syncedflush
```

response: 

```
{
   "[test]/2": {
      "ZpOArkc7TmuLBxElH9Epsw": "pending operations",
      "rTtq-MHmS1qq-i5YdcvegQ": "pending operations"
   },
   "[test]/1": {
      "rTtq-MHmS1qq-i5YdcvegQ": "pending operations",
      "ZpOArkc7TmuLBxElH9Epsw": "pending operations"
   },
   "[test]/0": "operation counter on primary is non zero [2]",
   "[test]/4": {
      "ZpOArkc7TmuLBxElH9Epsw": "pending operations",
      "rTtq-MHmS1qq-i5YdcvegQ": "pending operations"
   },
   "[test]/3": {
      "ZpOArkc7TmuLBxElH9Epsw": "SUCCESS",
      "rTtq-MHmS1qq-i5YdcvegQ": "SUCCESS"
   }
}
```
</description><key id="75291289">11098</key><summary>Synced flush api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-05-11T17:10:20Z</created><updated>2015-05-13T12:46:40Z</updated><resolved>2015-05-13T12:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-12T08:15:42Z" id="101184889">@brwe I reviewed this one time and opened #11111 for you to review
</comment><comment author="brwe" created="2015-05-12T15:29:22Z" id="101320894">Thanks for the review! Addressed all comments except for the naming - waiting for input from @clintongormley 
</comment><comment author="clintongormley" created="2015-05-12T15:30:14Z" id="101321099">I think `_seal` is the perfect name
</comment><comment author="brwe" created="2015-05-12T16:13:51Z" id="101333842">ok, renamed to _seal. I also renamed all classes from synced flush -&gt; seal. 
@s1monw want to take another look?
</comment><comment author="s1monw" created="2015-05-13T07:39:56Z" id="101552613">did another review and left some comments
</comment><comment author="brwe" created="2015-05-13T12:16:34Z" id="101644693">thanks again! addressed all comments.
</comment><comment author="s1monw" created="2015-05-13T12:41:07Z" id="101651710">LGTM
</comment><comment author="brwe" created="2015-05-13T12:46:38Z" id="101652607">pushed to feature/synced_flush
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/SealIndicesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/seal/TransportSealIndicesAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/indices/SyncedFlushService.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/seal/RestSealIndicesAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/seal/SealIndicesTests.java</file><file>src/test/java/org/elasticsearch/indices/FlushTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>api for synced flush</comment></comments></commit></commits></item><item><title>Acquire `write.lock` when deleting shard directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11097</link><project id="" key="" /><description>We currently protect against multiple deletions by acquiring the Engine's writelock, however, we should also ensure that we acquire the `write.lock` write lock before deleting a shard to ensure that we are the only node writing to the directory.

This will prevent nodes that shared a filesystem from modifying the shard directory in the event that they cannot communicate, assuming that the shared filesystem's locking semantics work correctly.
</description><key id="75281436">11097</key><summary>Acquire `write.lock` when deleting shard directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label></labels><created>2015-05-11T16:31:08Z</created><updated>2015-05-12T19:28:59Z</updated><resolved>2015-05-12T19:28:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/index/store/FsDirectoryService.java</file><file>src/test/java/org/elasticsearch/index/IndexWithShadowReplicasTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>[CORE] Acquire IndexWriter's `write.lock` lock before shard deletion</comment></comments></commit></commits></item><item><title>[CI Failure] org.elasticsearch.common.jna.NativesTests.testMlockall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11096</link><project id="" key="" /><description>Fails on *nix because the actual mlockall call fails:

```
[2015-05-11 17:35:41,417][INFO ][test                     ] Test testTryMlockall(org.elasticsearch.common.jna.NativesTests) started
[2015-05-11 17:35:41,450][WARN ][common.jna               ] Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out. Increase RLIMIT_MEMLOCK (ulimit).
[2015-05-11 17:35:41,453][ERROR][test                     ] FAILURE  : testTryMlockall(org.elasticsearch.common.jna.NativesTests)
```

http://build-us-00.elastic.co/job/es_core_master_regression/2093/testReport/junit/org.elasticsearch.common.jna/NativesTests/testMlockall/

I modified the test to only assertTrue for Windows for now, but we should be able to test this on *nix as well.
</description><key id="75276733">11096</key><summary>[CI Failure] org.elasticsearch.common.jna.NativesTests.testMlockall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-05-11T16:19:16Z</created><updated>2016-01-19T08:49:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-11T17:20:36Z" id="100983305">Should we really assert this? Before adding the windows impl, we only checked that it was FALSE on unsupported platforms. Otherwise all build infrastructure and developers machines would need to be configured correctly, so that mlockall works.

if we REALLY REALLY want to check that mlockall succeeded, I think we need to do a lot more. At the least, we need assume(tests.maven) because people aren't going to configure their IDEs correctly. Sigar tests currently use this for example, because IDEs generally will not have the right configuration. We could also add a tests.mlockall sysprop, that controls the boolean we pass to SecurityBootstrap (https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/test/SecurityBootstrap.java#L47). 

It could default to false (so developers machines don't test it), and we could set it to true for jenkins jobs where we know it should be working. Even in that properly configured case I'm suspicious, because the machine could just be busy and not have enough memory available. 

So I'm worried that asserting mlockall success will only cause annoying test failures.
</comment><comment author="gmarz" created="2015-05-11T18:01:08Z" id="100999937">I've gone ahead and removed the assert.  Let's maybe keep this issue open until we have the above setup in place to test it properly?
</comment><comment author="clintongormley" created="2016-01-19T08:49:51Z" id="172778291">@rmuir 8 months later, do you think we still need to do anything here, or can I close the issue?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: Upgrade API should write minimum compatible version that the index was upgrade to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11095</link><project id="" key="" /><description>In #11072 we are adding a check that will prevent opening of old indices. However, this check doesn't take into consideration the fact that indices can be made compatible with the current version through upgrade API. In order to make compatibility check aware of the upgrade, the upgrade API should write a new setting `index.version.minimum_compatible` that will indicate the minimum compatible version of elasticsearch/lucene this index is compatible with. 
</description><key id="75262420">11095</key><summary>Core: Upgrade API should write minimum compatible version that the index was upgrade to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T15:35:14Z</created><updated>2015-05-28T19:34:37Z</updated><resolved>2015-05-28T15:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-11T17:12:42Z" id="100981521">I don't think this should just be on upgrade? It should be all merges really, since it may be true older segments are simply merged away naturally, without needing to call upgrade. I'm not sure how feasible that is though...
</comment><comment author="rjernst" created="2015-05-11T17:13:14Z" id="100981611">Perhaps on every flush, since that is the "durable" part of an upgrade?
</comment><comment author="clintongormley" created="2015-05-15T16:10:28Z" id="102450081">Doing a cluster state update on every flush seems excessive.  

The only time this setting is used is when we move to a new major version, while having indices from 2 major versions before.  

If all your ancient segments have been merged away, then calling the upgrade API with `only_ancient_segments` would only update this setting.  

@s1monw what do you think of the naming for this setting? And what should go in it? ES version or Lucene version?
</comment><comment author="rjernst" created="2015-05-15T16:12:54Z" id="102450530">&gt; Doing a cluster state update on every flush seems excessive.

It would only need to do the update if the min version changed?
</comment><comment author="clintongormley" created="2015-05-15T16:17:47Z" id="102451359">true :)
</comment><comment author="s1monw" created="2015-05-19T09:03:57Z" id="103407075">&gt; Perhaps on every flush, since that is the "durable" part of an upgrade?

I think we can actually do that since we only need to change the index settings / metadata iff we have a change in the min version. I think this should be rather rare?
</comment><comment author="imotov" created="2015-05-20T19:07:38Z" id="103998608">Adding a setting during as part of Upgrade API is relatively simple and straightforward change. Updating setting on each flush/merge will require 1) plug a segment version check into merge process and checking the version of all segments at the end of merge, 2) sending the minimum segment version to some centralized location, probably a service running on master, 3) the said service will need to keep track of minimum segment version across all shards, which is challenge in itself because replicas don't really have identities. 4) as soon as the minimum version across all shards changes this service will update `index.version.minimum_compatible` setting. Can anyone see a simpler solution?
</comment><comment author="clintongormley" created="2015-05-21T14:32:33Z" id="104298752">Honestly, this is a one-time thing, when you do a major upgrade.  I'd go the simple route and just set the setting when called by the upgrade API
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/ShardUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/TransportUpgradeStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsResponse.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeReallyOldIndexTest.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeTest.java</file></files><comments><comment>Core: refactor upgrade API to use transport and write minimum compatible version that the index was upgraded to</comment></comments></commit></commits></item><item><title>Remove pointless term frequency lookups.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11094</link><project id="" key="" /><description>If the user has set a shard_min_doc_count setting then avoid looking up background frequencies if the term fails to meet the foreground threshold on a shard.

Closes #11093
</description><key id="75257214">11094</key><summary>Remove pointless term frequency lookups.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T15:21:40Z</created><updated>2015-06-06T19:00:01Z</updated><resolved>2015-05-12T08:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-05-11T15:22:26Z" id="100942138">@colings86 any chance of a quick review?
</comment><comment author="colings86" created="2015-05-11T15:24:27Z" id="100943175">LGTM
</comment><comment author="markharwood" created="2015-05-12T08:04:04Z" id="101181712">Pushed to master https://github.com/elastic/elasticsearch/commit/89b95dccc8eaf04927c307d76931236236b11d02
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Significant terms aggregation has pointless lookups if shard_min_doc_count set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11093</link><project id="" key="" /><description>Zipf's law tells us many terms occur very infrequently and so setting shard_min_doc_count to a value &gt;1 can eliminate a lot of candidate terms from the analysis (assuming we are not interested in very low-frequency terms). 
This can be a significant performance gain but is not fully realised today because the existing code _always_ looks up the background frequency of terms in the index
The logic should change to only lookup background frequencies for terms if they pass the shard_min_doc_count threshold.
</description><key id="75245740">11093</key><summary>Significant terms aggregation has pointless lookups if shard_min_doc_count set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2015-05-11T14:43:22Z</created><updated>2015-05-15T17:00:55Z</updated><resolved>2015-05-12T08:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T17:00:25Z" id="102458922">@markharwood please put version labels on this PR
</comment><comment author="clintongormley" created="2015-05-15T17:00:55Z" id="102459007">sorry, ignore me - it's not a PR :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java</file></files><comments><comment>Aggregations enhancement - remove pointless term frequency lookups.</comment><comment>If the user has set a shard_min_doc_count setting then avoid looking up background frequencies if the term fails to meet the foreground threshold on a shard.</comment></comments></commit></commits></item><item><title>Marvel is trying to query closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11092</link><project id="" key="" /><description>I'm using Marvel 1.3.1 running on an ES 1.5.2 cluster.

My ES logs are seeing this every 30 seconds or so:

```
[2015-05-11 09:16:52,990][ERROR][marvel.agent             ] [dev-es1] error while collecting indices stats
org.elasticsearch.indices.IndexClosedException: [logstash-2015.03.27] closed
    at org.elasticsearch.cluster.metadata.MetaData.concreteIndices(MetaData.java:790)
    at org.elasticsearch.cluster.metadata.MetaData.concreteIndices(MetaData.java:723)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.&lt;init&gt;(TransportBroadcastOperationAction.java:120)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:71)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
    at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:77)
    at org.elasticsearch.client.support.AbstractIndicesAdminClient.stats(AbstractIndicesAdminClient.java:524)
    at org.elasticsearch.action.admin.indices.stats.IndicesStatsRequestBuilder.doExecute(IndicesStatsRequestBuilder.java:178)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)
    at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:80)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportIndicesStats(AgentService.java:293)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:260)
    at java.lang.Thread.run(Thread.java:745)
```

It is true that the index is closed, but shouldn't marvel skip closed indices instead of throwing an error when it tries to process them?
</description><key id="75240753">11092</key><summary>Marvel is trying to query closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bradvido</reporter><labels><label>enhancement</label><label>feedback_needed</label></labels><created>2015-05-11T14:26:26Z</created><updated>2015-08-26T19:50:08Z</updated><resolved>2015-08-26T19:50:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T15:58:24Z" id="102445567">Makes sense @bradvido 
</comment><comment author="bleskes" created="2015-05-20T12:21:47Z" id="103861540">@bradvido do you have a custom `marvel.agent.indices` ? the default (empty) one shouldn't expand to closed indices... 
</comment><comment author="bradvido" created="2015-06-02T15:52:48Z" id="107997659">Yes, I do have custom wildcard/all to query all indices:
`marvel.agent.indices: "*"`

Still, I'd expect closed indices to be skipped.
</comment><comment author="bleskes" created="2015-06-08T12:57:38Z" id="109984046">@bradvido thx. This is equivalent to the default and it works for me, so something else is in place. Any chance you can create a small reproduction?
</comment><comment author="jpountz" created="2015-08-26T19:50:08Z" id="135151736">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve request structure for scripts and templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11091</link><project id="" key="" /><description>Created from the discussion at https://github.com/elastic/elasticsearch/pull/8393#issuecomment-96388815

The ScriptParameterParser added in 1.5 was an effort to make the parsing of scripts in Elasticsearch consistent but did not go far enough. The way scripts are defined in requests is still not one consistent method across the codebase. We want scripts to be defined in an object containing the script, params, lang, and type. This will be defined in the request in the following way:

Inline Scripts:

``` json
...
"script": {
    "inline": "doc[foo] + 1",
    "lang": "groovy",
    "params": {
        ...
    }
}
...
```

File Scripts:

``` json
...
"script": {
    "file": "my_file_script",
    "lang": "groovy",
    "params": {
        ...
    }
}
...
```

Index Scripts:

``` json
...
"script": {
    "id": "my_indexed_script",
    "lang": "groovy",
    "params": {
        ...
    }
}
...
```

Inline scripts can also be specified using the following short form, but the language will be the default language and the params will be empty:

``` json
...
"script": "doc[foo] + 1"
...
```

We should have similar constructs for templates. Also, the script and template classes should be available in the Java API and should be able to parse and render (build) themselves in XContent as well as being able to be serialized between nodes.
</description><key id="75236233">11091</key><summary>Improve request structure for scripts and templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T14:13:47Z</created><updated>2015-05-29T15:58:08Z</updated><resolved>2015-05-29T15:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-11T14:22:21Z" id="100924302">Can you deprecate rather than remove the
{
  "script": " name/body",
  "lang": " whatever"
}
style?

Or maybe offer the new style in 1.6 if you are dropping the old in 2.0?

Created from the discussion at #8393 (comment)
https://github.com/elastic/elasticsearch/pull/8393#issuecomment-96388815

The ScriptParameterParser added in 1.5 was an effort to make the parsing of
scripts in Elasticsearch consistent but did not go far enough. The way
scripts are defined in requests is still not one consistent method across
the codebase. We want scripts to be defined in an object containing the
script, params, lang, and type. This will be defined in the request in the
following way:

Inline Scripts:

..."script": {
    "inline": "doc[foo] + 1",
    "lang": "groovy",
    "params": {
        ...
    }
}
...

File Scripts:

..."script": {
    "file": "my_file_script",
    "lang": "groovy",
    "params": {
        ...
    }
}
...

Index Scripts:

..."script": {
    "id": "my_indexed_script",
    "lang": "groovy",
    "params": {
        ...
    }
}
...

Inline scripts can also be specified using the following short form, but
the language will be the default language and the params will be empty:

..."script": "doc[foo] + 1"
...

We should have similar constructs for templates. Also, the script and
template classes should be available in the Java API and should be able to
parse and render (build) themselves in XContent as well as being able to be
serialized between nodes.

—
Reply to this email directly or view it on GitHub
https://github.com/elastic/elasticsearch/issues/11091.
</comment><comment author="colings86" created="2015-05-11T14:29:32Z" id="100926206">@nik9000 yes, it is the intention to support both the current format and the new format in 1.6. Sorry, I should have mentioned that in the issue.
</comment><comment author="nik9000" created="2015-05-11T14:40:12Z" id="100928892">Thanks!

On Mon, May 11, 2015 at 10:29 AM, Colin Goodheart-Smithe &lt;
notifications@github.com&gt; wrote:

&gt; @nik9000 https://github.com/nik9000 yes, it is the intention to support
&gt; both the current format and the new format in 1.6. Sorry, I should have
&gt; mentioned that in the issue.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11091#issuecomment-100926206
&gt; .
</comment><comment author="colings86" created="2015-05-29T12:51:10Z" id="106794347">Backporting the changes to 1.6 proved far too complex to have the confidence in the change, so the plan is now to merge this to 2.0 only and support the old API in the REST layer so that stuff thats stored like transforms and templates continue to work, but to remove support for the old script API in the Java API and the wire format. When the old script API is used through the REST layer it will log a deprecation warning to the new deprecation logger
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentType.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>src/main/java/org/elasticsearch/script/AbstractScriptParser.java</file><file>src/main/java/org/elasticsearch/script/Script.java</file><file>src/main/java/org/elasticsearch/script/ScriptParameterParser.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/script/Template.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/InternalScriptedMetric.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsParseElement.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortBuilders.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>src/test/java/org/elasticsearch/action/update/UpdateRequestTests.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/NoMasterNodeTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryTest.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/routing/AliasRoutingTests.java</file><file>src/test/java/org/elasticsearch/script/CustomScriptContextTests.java</file><file>src/test/java/org/elasticsearch/script/GroovyScriptTests.java</file><file>src/test/java/org/elasticsearch/script/IndexLookupTests.java</file><file>src/test/java/org/elasticsearch/script/IndexedScriptTests.java</file><file>src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>src/test/java/org/elasticsearch/script/OnDiskScriptTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptFieldTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptParameterParserTest.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/EquivalenceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file><file>src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/search/stats/SearchStatsTests.java</file><file>src/test/java/org/elasticsearch/search/timeout/SearchTimeoutTests.java</file><file>src/test/java/org/elasticsearch/update/UpdateByNativeScriptTests.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Scripting: Unify script and template requests across codebase</comment></comments></commit></commits></item><item><title>Fix geo bounds aggregation when longitude is 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11090</link><project id="" key="" /><description>When the longitude is zero for a document, the left and right bounds do not get updated in the geo bounds aggregation which can cause the bounds to be returned with Infinite values for longitude

Closes #11085
</description><key id="75180423">11090</key><summary>Fix geo bounds aggregation when longitude is 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-11T10:56:52Z</created><updated>2015-05-11T12:46:49Z</updated><resolved>2015-05-11T12:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-11T12:29:40Z" id="100891219">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reopen  #11037</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11089</link><project id="" key="" /><description>Please reopen  #11037
</description><key id="75178196">11089</key><summary>Reopen  #11037</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels /><created>2015-05-11T10:47:29Z</created><updated>2015-05-15T15:56:26Z</updated><resolved>2015-05-15T15:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>fix #11086 termvector missing paths on 1.5 branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11088</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/7530 introduced support for artificial documents

I submitted a PR to both es [master](https://github.com/elastic/elasticsearch/pull/8868/files) and [1.4](https://github.com/elastic/elasticsearch/pull/8869/files) branches but they are no longer reflected in the [1.5](https://github.com/elastic/elasticsearch/blob/v1.5.0/rest-api-spec/api/termvector.json) branch.
</description><key id="75162344">11088</key><summary>fix #11086 termvector missing paths on 1.5 branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-05-11T10:04:53Z</created><updated>2015-05-11T14:40:44Z</updated><resolved>2015-05-11T14:40:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>REST spec for termvector missing paths on 1.x branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11087</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/7530 introduced support for artificial documents

I submitted a PR to both es [master](https://github.com/elastic/elasticsearch/pull/8868/files) and [1.4](https://github.com/elastic/elasticsearch/pull/8869/files) branches but they are no longer reflected in the [1.5](https://github.com/elastic/elasticsearch/blob/v1.5.0/rest-api-spec/api/termvector.json) branch.

Closes #11086
</description><key id="75161706">11087</key><summary>REST spec for termvector missing paths on 1.x branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label></labels><created>2015-05-11T10:01:32Z</created><updated>2015-05-29T10:22:25Z</updated><resolved>2015-05-29T10:03:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-05-11T14:45:22Z" id="100930062">Left one comment, LGTM
</comment><comment author="clintongormley" created="2015-05-29T10:03:01Z" id="106763091">Merged into 1.5 and 1.x
</comment><comment author="Mpdreamz" created="2015-05-29T10:22:25Z" id="106766682">thanks @clintongormley!

Tying various loose ends before my holiday and this one totally slipped
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>termvector rest spec documentation for 1.5 out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11086</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/7530 introduced support for artificial documents

I submitted a PR to both es [master](https://github.com/elastic/elasticsearch/pull/8868/files) and [1.4](https://github.com/elastic/elasticsearch/pull/8869/files) branches but they are no longer reflected in the `1.5` branches.

I need to update the `1.x` and `1.5` branches with these changes as well
</description><key id="75158757">11086</key><summary>termvector rest spec documentation for 1.5 out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-05-11T09:49:27Z</created><updated>2015-05-29T10:07:01Z</updated><resolved>2015-05-29T10:07:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T10:07:00Z" id="106763626">Closed by #11087
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggs: geo_bounds don't like when the lat or lon is equal to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11085</link><project id="" key="" /><description>Here is a reproduction:

```
DELETE test 

PUT test 
{
  "mappings": {
    "test": {
      "properties": {
        "loc": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT test/test/1
{
  "loc": "1,0"
}

GET test/_search
{
  "aggs": {
    "loc_bounds": {
      "geo_bounds": {
        "field": "loc"
      }
    }
  }
}
```

which returns

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "_score": 1,
            "_source": {
               "loc": "1,0"
            }
         }
      ]
   },
   "aggregations": {
      "loc_bounds": {
         "bounds": {
            "top_left": {
               "lat": 1,
               "lon": "Infinity"
            },
            "bottom_right": {
               "lat": 1,
               "lon": "-Infinity"
            }
         }
      }
   }
}
```
</description><key id="75155649">11085</key><summary>Aggs: geo_bounds don't like when the lat or lon is equal to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2015-05-11T09:39:03Z</created><updated>2015-05-11T12:44:44Z</updated><resolved>2015-05-11T12:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java</file></files><comments><comment>Aggregations: Fix geo bounds aggregation when longitude is 0</comment></comments></commit></commits></item><item><title>Upgrade the dependency on t-digest (3.0 -&gt; 3.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11084</link><project id="" key="" /><description>Hi,

t-digest 3.1 has a minor incompatibility with the version 3.0 currently used by Elasticsearch, here is a patch fixing it.
</description><key id="75139728">11084</key><summary>Upgrade the dependency on t-digest (3.0 -&gt; 3.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ebourg</reporter><labels /><created>2015-05-11T08:42:54Z</created><updated>2015-05-11T09:35:54Z</updated><resolved>2015-05-11T09:30:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-11T09:06:49Z" id="100825500">Thanks @ebourg. FYI we already started discussing an upgrade to 3.1 and this causes failures due to changes in the CDF computation (see https://github.com/elastic/elasticsearch/issues/10216#issuecomment-90506029 in particular) that we have not taken time to investigate yet.
</comment><comment author="ebourg" created="2015-05-11T09:30:32Z" id="100832626">Thank you, and sorry for not better checking the existing issues.
</comment><comment author="jpountz" created="2015-05-11T09:35:54Z" id="100833367">No worries at all!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Validate snapshot via dry-run restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11083</link><project id="" key="" /><description>I know we do verification during the snapshot process (#7159), but if a snapshot is kept in a repo for an extended period (eg months/years) then it'd be nice to know we could periodically verify there has been no FS/other corruption that may cause restoration problems.

Therefore it'd be awesome if we had a dry-run/verification restore flag that verified the snapshots but obviously doesn't restore anything.
</description><key id="75079921">11083</key><summary>Validate snapshot via dry-run restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label></labels><created>2015-05-11T04:22:36Z</created><updated>2017-02-14T16:34:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T15:24:06Z" id="102431486">@imotov what do you think about this?
</comment><comment author="imotov" created="2015-05-15T15:33:26Z" id="102433581">I really like the idea. I think it will nicely complement #9183. I wouldn't make it a restore option though because this will imply that we verify that snapshot has compatible settings, metadata, etc., which is much more difficult to test in dry run. Instead, I would make it a separate snapshot-level `verify` command and maybe add an option to repository-level `verify` command to check checksum for all files in the repository.
</comment><comment author="mellieA" created="2017-01-25T04:41:21Z" id="275017617">Did this functionality ever make it to any release? I have another request for this enhancement. </comment><comment author="imotov" created="2017-01-26T14:38:07Z" id="275403426">@mellieA no, it wasn't implemented yet.</comment><comment author="mellieA" created="2017-02-14T16:34:42Z" id="279760022">I've been thinking about this, could we also print a list of associated files (verbose option) that restoring the snapshot is dependent on? </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes for #11044,  #11076, #11081 revoked #11038 (discuss and decline)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11082</link><project id="" key="" /><description>Summary:
1) elasticsearch shell script will be usable from Windows msys bash without changes for Unix (#11044)

2) POST /index/_analyze now will have additional ?alldata=true parameter to allow include flags, increment, payload, keyword info in returned token's JSON  (#11076)

3) PUT /index/_settings now will have ability to completly remove parameters and sub-trees from
settings with special ```"__REMOVE__"``` value  (#11081)
#11076, #11081 have corresponded tests
</description><key id="74936790">11082</key><summary>Fixes for #11044,  #11076, #11081 revoked #11038 (discuss and decline)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels /><created>2015-05-10T15:05:32Z</created><updated>2016-03-08T19:30:08Z</updated><resolved>2016-03-08T19:30:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T19:30:08Z" id="193933762">Sorry it has taken a while to look at this.  Most issues seem to have been dealt with already.  Btw, PRs should deal with one issue at a time, rather than combining unrelated changes

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow remove settings with PUT /index/_settings command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11081</link><project id="" key="" /><description>It's very usefull because in development cycle we usually have index with data and experiment with analzer config on it. So we usually want to remove some filters, parameters and so on from settings.
And in current implementation we cannot. 

In my fork i have fix it and will send cherry-picked branch to pull request.

Our solution is to add spectial keyword `"__REMOVE__"` for this special case.

Usage.
POST /myindex/_close
PUT /myindex/_settings 

``` javascript
{
     "settings" : {
           "analysis": {
                 "filter" : {
                      "very_deprecated_filter" : "__REMOVE__"
                 }
           }
     }
}
```

It  must completly remove "index.analysis.filter.very_deprecated_filter" and it's descendants for example "index.analysis.filter.very_deprecated_filter.type" from settings.

We choose from variants : 
1. `"some" : null`               -- not good it may be treated as existed setting with null value
2. `"some" : false`             -- not good for same reasones
3. `"some" : { "remove" : true}` -- more complex to handle and to read/write and can cause conflicts
4. `"some" : { "__remove__" : true }` --less conflictable but still unreadable
5. `"some" : "__REMOVE__"`  -- has some chance to conflict but not great, simple to read/write

SO `"some" :"__REMOVE__"` syntax is our team choise.
</description><key id="74932745">11081</key><summary>Allow remove settings with PUT /index/_settings command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels /><created>2015-05-10T14:41:21Z</created><updated>2015-05-15T20:05:35Z</updated><resolved>2015-05-15T14:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T14:50:02Z" id="102419534">Hi @comdiv 

Actually, we can probably support deleting settings with a `DELETE` request. But this is a bigger problem than just deleting settings. We also need to check for any references to the setting (eg deleting an analyzer that is in use by a field) and be able to reset the default value (eg when deleting `refresh_interval`).

This requires a much bigger settings rewrite, which we plan to do at some stage.  I'm going to close this ticket in favour of https://github.com/elastic/elasticsearch/issues/6732
</comment><comment author="comdiv" created="2015-05-15T20:05:35Z" id="102511169">Hm. Much wrong is how it is now in main trunk.
For example:
_settings 
{
   "x" : {
          "a" : "b"
    }
}

and then 

_settings
{
      "x" : null
}

what doest cause?

{
    "x": null,
   "x.a" : "b"
}

It's invalid config i think.  My opinion is that if someone decide to delete some part of configuration he do it on his own risk and must have valid tools to accomplish it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Something is wrong with template mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11080</link><project id="" key="" /><description>Hi,
   First of all, thank U for sharing the wonderful product ElasticSeach.
  These  days ,I have confronted some  confuses.I hope U can help me.
  When I execute the curl command as follows(I just wanna create an index):
  curl -XPUT 'http://localhost:9200/es4zq2/_mapping/netdepartment' -d '{"properties":{"MOBILEPHONE":{"analyzer": "ik","type": "string"},"MODIFYTIME": {"format": "dateOptionalTime","type": "date"},"trs_publish_month": {"format": "yyyy-MM","type": "date"},"DEPARTMENTID": {"type":"integer"},"CRTIME": {"format": "dateOptionalTime","type": "date"},"match_all": {"type": "object"},"DEPARTMENTDESC": {"analyzer": "ik","type": "string"},"ISVALIDITY": {"type": "integer"},"EMAIL": {"analyzer": "ik","type": "string"},"trs_global_version": {"type": "long"},"MODIFYUSER": {"analyzer": "ik","type": "string"},"CRUSER": {"analyzer":"ik","type": "string"},"SYSGROUPID": {"type": "integer"},"MODIFYFLAG": {"type": "integer"},"EVENTNOTIFY": {"type": "integer"},"DEPARTMENTORDER": {"type": "integer"},"DEPARTMENTTYPE": {"type": "integer"},"PRINCIPAL": {"analyzer": "ik","type": "string"},"trs_category_flag": {"type": "short"},"DEPARTMENTNAME": {"analyzer": "ik","type": "string"},"trs_publish_year": {"format": "yyyy","type": "date"},"trs_field_weight": {"type": "short"},"trs_global_category": {"type": "short"},"trs_publish_day": {"format": "yyyy-MM-dd","type": "date"},"PARENTDPMID": {"type": "integer"}}}'
But It fedbacked an error:
![1](https://cloud.githubusercontent.com/assets/5074423/7551226/ce7adbf4-f6b3-11e4-9102-9152d2a5398e.png)
as U can see,there is no field which is named title,dexcription,price,onSale and so on.
there is no template.
![2](https://cloud.githubusercontent.com/assets/5074423/7551227/e157a180-f6b3-11e4-86a0-9c9c401012c6.png)
there is no .json file in the mappings dir.
![3](https://cloud.githubusercontent.com/assets/5074423/7551228/e74e960c-f6b3-11e4-9dfc-0694078aa0f8.png)
PS:
I have put default.json on conf/mapping  many days ago. And then ,I deleted the mapping dir yestoday.and I have restart the elasticsearch.
default.json contains these fields(title,onSale,description...).
this is start info:
![4](https://cloud.githubusercontent.com/assets/5074423/7551230/ecc8c33c-f6b3-11e4-8e25-fe0c3d4d0b57.png)

Questions:
1.how  can I fix the error,why the error appeared?
2.If I delete the dymatic mapping file(json file in conf/mappings/index),does the mapping work on late index?
3.what can i do if I want to delete this template which is created by json file?
thank U very much.
</description><key id="74719171">11080</key><summary>Something is wrong with template mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leo650</reporter><labels /><created>2015-05-09T17:29:36Z</created><updated>2015-05-15T07:09:46Z</updated><resolved>2015-05-11T15:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-11T15:58:38Z" id="100956939">@leo650 we are using github to keep track of bug reports and feature requests. So, our mailing list and [discussion forums](https://discuss.elastic.co/) are much better place to ask questions like this. Saying that, you [missed the name of the type](http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) in your mapping. It should be:

```
curl -XPUT 'http://localhost:9200/es4zq2/_mapping/netdepartment' -d '{
    "netdepartment": {
        "properties": {
            "MOBILEPHONE": {
                "analyzer": "ik",
                "type": "string"
            },
            ..............
            "trs_publish_day": {
                "format": "yyyy-MM-dd",
                "type": "date"
            },
            "PARENTDPMID": {
                "type": "integer"
            }
        }
    }
}'
```

If you have any other questions, please don't hesitate to ask them on the mailing list of forum.
</comment><comment author="leo650" created="2015-05-15T07:08:45Z" id="102293907">@imotov  THX a lot.   BTW, I can put the mapping to ES without the name "netdepartment" in other ES Server . and ES feedback with "acknowledge:true"
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrading mappings which uses `index_name` and `path`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11079</link><project id="" key="" /><description>In 2.0, I think we can upgrade the mapping in a way that is not perfect, but will cover most use cases and help users to upgrade without reindexing.

`index_name` and `path` are used for two main purposes:
- the old way to do `copy_to`
- provide fieldname aliases, eg `tag` points to `tags`

We can't distinguish between these two use cases automatically, but we can handle the first use case gracefully, and the second use case is an easy change to make application side (ie just change all use of `tags` to `tag` in searches)

A mapping that looks like this (with `path` set to `just_name`):

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "path": "just_name", 
          "properties": {
            "first": {
              "type": "string",
              "index_name": "fullname"
            },
            "last": {
              "type": "string",
              "index_name": "fullname"
            }
          }
        }
      }
    }
  }
}
```

could be rewritten to:

```
{
  "mappings": {
    "test": {
      "properties": {
        "fullname": {
          "type": "string"
        },
        "name": {
          "type": "object",
          "properties": {
            "first": {
              "index": "no",
              "copy_to": "fullname"
            },
            "last": {
              "index": "no",
              "copy_to": "fullname"
            }
          }
        }
      }
    }
  }
}
```

The mapping for the new `fullname` field can just be the same as the mapping of the first field which uses `index_name` (without the `index_name`) setting.  The original field will not be indexed (or searchable).

In the case where `path`  is set to `full`, the same rules apply, but the new field uses the full path name, ie this:

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "path": "full", 
          "properties": {
            "first": {
              "type": "string",
              "index_name": "fullname"
            },
            "last": {
              "type": "string",
              "index_name": "fullname"
            }
          }
        }
      }
    }
  }
}
```

could be rewritten as:   

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "properties": {
            "first": {
              "index": "no",
              "copy_to": "name.fullname"
            },
            "last": {
              "index": "no",
              "copy_to": "name.fullname"
            },
            "fullname": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```
</description><key id="74689669">11079</key><summary>Upgrading mappings which uses `index_name` and `path`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2015-05-09T14:53:50Z</created><updated>2016-01-18T16:21:20Z</updated><resolved>2016-01-18T16:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-09T14:55:53Z" id="100500796">As of https://github.com/elastic/elasticsearch/pull/9570, we maintain the old `index_name` and `path` settings on old indices.  Should we do this mapping upgrade instead?  If so, in 2.0 or 3.0 (when we remove bwc support)?

Personally, I think we should do it for 2.0 and warn about the change in the migration plugin (#10214)
</comment><comment author="clintongormley" created="2016-01-18T16:21:20Z" id="172575748">Won't fix.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add boolean 'alldata' optional paramter for /{index}/_analyze that al…</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11078</link><project id="" key="" /><description>…lows return flags, increment, keyword and payload for tokens #11076 Fixed
</description><key id="74670316">11078</key><summary>Add boolean 'alldata' optional paramter for /{index}/_analyze that al…</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels /><created>2015-05-09T13:11:16Z</created><updated>2015-05-10T15:13:34Z</updated><resolved>2015-05-10T14:57:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="comdiv" created="2015-05-10T14:57:18Z" id="100652964">Close this one to upstream with my master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove XPostingsHighlighter in favour of Lucene's PostingsHighlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11077</link><project id="" key="" /><description>Our own fork of the lucene PostingsHighlighter is very hard to maintain and doesn't give us any added value at this point. In particular, it was introduced to support the `require_field_match` option and discrete per value highlighting, used in case one wants to highlight the whole content of a field, but get back one snippet per value. These two features won't make it into lucene the way I implemented them as they slow things down and shouldn't have been supported from day one on our end probably.

One other customization we had was support for a wider range of queries via custom rewrite etc. (yet another way to slow things down), which got added to lucene and works much much better than what we used to do (instead of or rewrite, terms are pulled out of the automata for multi term queries).

Removing our fork means the following in terms of features:
- dropped support for require_field_match: the postings highlighter will only highlight fields that were queried
- some custom es queries won't be supported anymore, meaning they won't be highlighted. The only one I found up until now is the `phrase_prefix`. Postings highlighter rewrites against an empty reader to avoid slow operations (like the ones that we were performing with the fork that we are removing here), thus the prefix will not be expanded to any term. What the postings highlighter does instead is pulling the automata out of multi term queries, but this is not supported at the moment with our `MultiPhrasePrefixQuery`.

Closes #10625
</description><key id="74664918">11077</key><summary>Remove XPostingsHighlighter in favour of Lucene's PostingsHighlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-09T12:43:21Z</created><updated>2016-01-07T08:03:35Z</updated><resolved>2015-05-15T19:00:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-09T13:03:28Z" id="100482266">this looks great Luca!, agreed that this fork is a challenge to maintain. Out of the features we removed, the one that users ask for (regardless of highlighting impl) is to be able to take multi value fields into account and highlight each one, its a big usability aspect. I would check if its possible to try and add it to Lucene posting highlighter itself in the future.
</comment><comment author="javanna" created="2015-05-09T13:21:26Z" id="100484136">&gt; the one that users ask for (regardless of highlighting impl) is to be able to take multi value fields into account and highlight each one, its a big usability aspect

I may have made it sound worse than it actually is. The postings highlighter is aware of multiple values, at least the way we use it, as we use a specific paragraph separator between values which the break iterator can detect. That doesn't work only when wanting to highlight the whole content of a field (setting `fragments_size` to `0`), as in that case we use a whole break iterator. We could split the resulting snippet ourselves based on where we find the paragraph separator, but it seems a bit of a hack since the original text might contain that same character.
</comment><comment author="javanna" created="2015-05-09T20:43:21Z" id="100543486">One other idea to address the "highlight the whole content, value per value" usecase could be to write a  break iterator to use instead of the `WholeBreakIterator`, which breaks only when it encounters the paragraph separator that we use between values...that said I'm not 100% comfortable with this as the field content can always contain that same character...
</comment><comment author="nik9000" created="2015-05-10T00:19:27Z" id="100564654">&gt; One other idea to address the "highlight the whole content, value per value" usecase could be to write a break iterator to replace the whole break iterator, which breaks only when it encounters the paragraph separator which we use between values...that said I'm not 100% comfortable with this as the field content can always contain that same character...

For what its worth I think the experimental highlighter supports this use case at roughly the same performance you'd get out of the postings highlighter. Its been a while since I looked at the other highlighters so I'm not 100% sure what the use case is - I'm just going from memory here.
</comment><comment author="jpountz" created="2015-05-12T09:50:52Z" id="101218288">LGTM, I think this is a great change. Let's make sure there is a Lucene ticket for the multi-value highlighting issue and reference it from here?
</comment><comment author="javanna" created="2015-05-12T10:03:00Z" id="101220569">@rmuir can you have a look too please?
</comment><comment author="rmuir" created="2015-05-12T10:04:57Z" id="101220954">paragraph separator is only a "recommended" thing to use, because the default java breakiterator will split on it already out of box (for typical highlighting use cases). If you want to do something atypical, use a different character that can't be in the data. Use U+0000 if you want.

```
/** 
 * Returns the logical separator between values for multi-valued fields.
 * The default value is a space character, which means passages can span across values,
 * but a subclass can override, for example with {@code U+2029 PARAGRAPH SEPARATOR (PS)}
 * if each value holds a discrete passage for highlighting.
 */
protected char getMultiValuedSeparator(String field) {
  return ' ';
}
```

In this patch (which is much simpler, thanks!), this method is actually only called by overridden code, not by lucene at all. So really @jpountz, there isn't anything for lucene "to fix" here. Just don't use 2029, use something else for whatever the strange use case is. Or don't call the method at all and do something different :)
</comment><comment author="javanna" created="2015-05-12T10:14:41Z" id="101222482">&gt; this method is actually only called by overridden code, not by lucene at all

right @rmuir that is because we need to load the content from `_source`. I tried to mimic what the lucene code does by using the same method though, so the behaviour (apart from loading from source) should be the same (apart from bugs that I may have added to it :)   )...

&gt; don't call the method at all and do something different 

Correct me if I'm wrong, I don't think we can do whatever we want as offsets etc. need to match the value loaded from stored fields. `loadFieldValues` really needs to return the whole content of a field (all values) and from then on we lose the distinction between the different values. I will look into using a different separator between values (that can't be in the data), which should solve this.
</comment><comment author="rmuir" created="2015-05-12T10:19:41Z" id="101223601">&gt; Correct if I'm wrong, I don't think we can do whatever we want as offsets etc. need to match the value loaded from stored fields. loadFieldValues really needs to return the whole content of a field (all values) and from then on we lose the distinction between the different values. I will look into using a different separator between values (that can't be in the data), which should solve this.

right, because Analyzer.getOffsetGap() is 1 by default, it should be one character. I recommend a control character like U+0000 or INFORMATION SEPARATOR X.

If we were programming in C, we would be using NUL terminated strings implicitly and without hesitation!
</comment><comment author="javanna" created="2015-05-13T08:38:16Z" id="101574479">I pushed another commit, highlighting one field value at a time works well now, the output is the same as before. Added a `CustomSeparatorBreakIterator` that can probably be contributed back to lucene as well, which we use to break on U+0000 when number_of_fragments is set to `0` (knowing that the same separator is used between different values).

Updated the migrate docs, I removed the above limitation and added another one that I found around highlighting match query with type set to `phrase_prefix`, which is not supported anymore.

@rmuir can you have a look? The break iterator tests will be a deja vu for you I believe :)
</comment><comment author="rmuir" created="2015-05-13T15:18:02Z" id="101712557">This looks great, I like the strategy for the breakiterator. That will be a nice one to put in lucene!
</comment><comment author="javanna" created="2015-05-15T19:03:25Z" id="102494447">This PR is marked breaking as it removes support for the `require_field_match` option when using the postings highlighter. This option is not supported by the lucene postings highlighter and the way it was supported in elasticsearch made the postings highlighter much slower and harder to maintain, so we decided to stick with the lucene implementation.
</comment><comment author="andreip" created="2016-01-07T08:03:35Z" id="169590573">do you guys think this change generated the issue I'm describing here #15560 ? @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIterator.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighterTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Highlighting: nuke XPostingsHighlighter</comment></comments></commit></commits></item><item><title>It's better to return all token attributes in _analyze response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11076</link><project id="" key="" /><description>It allows to make remote TokenStream with full context.
For now _analyze returns just few of token's data, while Lucene operates with many.
May be it can be optional for ex. /myindex/_analyze?alldata=true
</description><key id="74645440">11076</key><summary>It's better to return all token attributes in _analyze response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">comdiv</reporter><labels><label>:Analysis</label><label>enhancement</label></labels><created>2015-05-09T10:38:51Z</created><updated>2015-12-10T14:12:05Z</updated><resolved>2015-12-10T14:12:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-05-10T15:45:04Z" id="100658122">Hi, 
I develop https://github.com/johtani/elasticsearch-extended-analyze plugin.
I think this plugin will help you.
</comment><comment author="comdiv" created="2015-05-11T07:35:43Z" id="100798257">May be it's much .net - like style, but I think that posibility of full reflection support is a concern of core of elastic search not for plugins. It's my opinion. So i have done own implementation in ES-fork itself.
But your plugin force me to add rendering of custom attributes along with lucene-defined.

When it's something that doesn't match phrase "storage wrapper over lucene with standard analyzer tool set" - it's something for plugin. If it match - think it's something to extend ES.  

It's maybe kind of holywar. After .NET, Java shock me with many-of-many various implementation of base things as streams, buffers and other stuff with many-of-many co-dependencies in 3d party. It's not much good. For me - better - to have fully-functional and standartized API at core level and only specialized - super-perfomance, integration-aware, special-cased code in plugins. 

So I suggest include full token reflection to _analyze itself, while it's still bridging to information that lucene conatins and returns.
</comment><comment author="clintongormley" created="2015-05-15T15:01:27Z" id="102423590">Hi @comdiv 

We're trying to keep the Elasticsearch core small and manageable (see https://github.com/elastic/elasticsearch/issues/10368) so I agree completely with @johtani that these optional extras should be in a plugin.  @johtani nice work on the plugin - what do you think about making it an official plugin?
</comment><comment author="johtani" created="2015-05-15T15:50:24Z" id="102443481">@clintongormley Wow, it is OK for me. I'd like to hear our colleague's opinions
</comment><comment author="johtani" created="2015-06-12T08:08:26Z" id="111403577">Hi @comdiv ,

We discussed about this few weeks ago and we decided to move my plugin function to _analyze API.
I reopen this issue and I will make PR soon.
If you have any idea, please let me know on my PR.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java</file></files><comments><comment>Analysis: Add detail response support</comment></comments></commit></commits></item><item><title>bulk index connection timeout caused by readtimeout error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11075</link><project id="" key="" /><description>In my case,I am trying to use bulk indexing for a large amount of documents.

```
def bulk_index():
    chunk = []
    for count, doc in enumerate(doc_generator()):
        id = doc.get('_id')
        chunk.append({
            'index': {
                '_id': id
            }
        })
        chunk.append(doc)
        if (count + 1) % 10000 == 0:
            assert len(chunk) == 20000
            res = es.bulk(index='weibo', doc_type='user', body=chunk)
            assert res['errors'] is False
            chunk = []
    print 'docs total count: %s' % (count + 1)
```

```
Traceback (most recent call last):
  File "es_index.py", line 63, in &lt;module&gt;
    bulk_index()
  File "es_index.py", line 16, in _
    rv = func(*args, **kwargs)
  File "es_index.py", line 53, in bulk_index
    res = es.bulk(index='weibo', doc_type='user', body=chunk)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py", line 68, in _wrapped
    return func(*args, params=params, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/client/__init__.py", line 711, in bulk
    params=params, body=self._bulk_body(body))
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py", line 307, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py", line 79, in perform_request
    raise ConnectionTimeout('TIMEOUT', str(e), e)
elasticsearch.exceptions.ConnectionTimeout: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))
```

My `elasticsearch.yml`is just like:

```
index.analysis.analyzer.default.type : "ik"
index.store.type: mmapfs
indices.memory.index_buffer_size: 30%
index.translog.flush_threshold_ops: 50000
refresh_interval: 30s
```
</description><key id="74617279">11075</key><summary>bulk index connection timeout caused by readtimeout error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fantastao</reporter><labels /><created>2015-05-09T07:58:01Z</created><updated>2015-05-09T11:09:10Z</updated><resolved>2015-05-09T11:09:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-09T11:09:10Z" id="100465823">Hello,
We're moving to https://discuss.elastic.co for any technical troubleshooting issues that are not bugs, please join us there for future discussion on this topic.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove ability to set the value of meta fields inside `_source`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11074</link><project id="" key="" /><description>A few meta fields can currently be set within a document's source.
However, the recommended way to set meta fields like this is through
the api, and setting within the document can be a performance trap
(e.g. needing to find _id in order to route the document).

This change removes the ability to set meta fields within
a document source for 2.0+ indexes.

closes #11051
</description><key id="74612756">11074</key><summary>Remove ability to set the value of meta fields inside `_source`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-09T07:34:09Z</created><updated>2015-06-06T15:48:04Z</updated><resolved>2015-05-13T06:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-09T12:27:46Z" id="100473040">This looks great! I think we should also add a note about this to the migration guide and have a simple test that checks that specifying these fields still works on old indices?
</comment><comment author="rjernst" created="2015-05-13T06:07:43Z" id="101524548">@jpountz I pushed a commit adding a migration note and tests. I actually found while testing that only `_id` and `_parent` actually worked when specified in a document.  The others would result in ignoring the field, which I am testing for true "backcompat".
</comment><comment author="jpountz" created="2015-05-13T06:22:40Z" id="101527694">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file></files><comments><comment>Mappings: Enforce metadata fields are not passed in documents</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/InternalMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>src/main/java/org/elasticsearch/index/mapper/RootMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #11074 from rjernst/pr/include-in-object-removal</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/InternalMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>src/main/java/org/elasticsearch/index/mapper/RootMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Mappings: Remove ability to set meta fields inside documents</comment></comments></commit></commits></item><item><title>[Docs] Nested Filter Documentation References Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11073</link><project id="" key="" /><description>While reminding myself of the nested filter syntax, I noticed that it shows the `join` option by using a facet rather than an aggregation (it also has a stray quote at the bottom of the JSON object).

http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-filter.html#_join_option

``` json
{
    "query" : {
        "nested" : {
            "path" : "offers",
            "query" : {
                "match" : {
                    "offers.color" : "blue"
                }
            }
        }
    },
    "facets" : {
        "size" : {
            "terms" : {
                "field" : "offers.size"
            },
            "facet_filter" : {
                "nested" : {
                    "path" : "offers",
                    "query" : {
                        "match" : {
                            "offers.color" : "blue"
                        }
                    },
                    "join" : false
                }
            },
            "nested" : "offers"
        }
    }
}'
```
</description><key id="74536852">11073</key><summary>[Docs] Nested Filter Documentation References Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Nested Docs</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-05-09T00:09:14Z</created><updated>2015-05-15T14:24:02Z</updated><resolved>2015-05-15T14:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T14:24:02Z" id="102412706">@pickypg the `join` option is just for facets. For aggs we have `nested` and `reverse_nested` instead.  This page has been removed in master (along with facets) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't allow indices containing too-old segments to be opened</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11072</link><project id="" key="" /><description>When an index is introduced into the cluster via cluster upgrade, restore or as a dangled index the MetaDataIndexUpgradeService checks if this index can be upgraded to the current version. If upgrade is not possible, the newly upgraded cluster startup and restore process are aborted, the dangled index is imported as a closed index that cannot be open. This change doesn't actually checks the segment versions. Instead, it indiscriminately bans all old indices created with Elasticsearch older than v0.90. In 1.6 we will add an additional logic to upgrade API that will mark upgraded indices as "upgraded" using special setting and we will modify this process to accept such indices (see #11095).

Closes #10215
</description><key id="74512763">11072</key><summary>Don't allow indices containing too-old segments to be opened</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T22:17:10Z</created><updated>2015-06-06T15:38:37Z</updated><resolved>2015-05-20T03:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-09T05:57:21Z" id="100435148">Maybe I'm misunderstanding this, but it looks like this check is only on the ES version the index was created with, and not the lucene index?  While this will certainly block upgrades that are not possible, it would also fail indexes that were already upgraded (ie don't have any lucene 3.x segments)? As far as I understand, the created version is never modified (certainly not by merging or the upgrade api)?
</comment><comment author="imotov" created="2015-05-11T15:39:31Z" id="100949470">@rjernst great observation! Sorry, I forgot to mention this in the description. I have updated the description. 
</comment><comment author="kimchy" created="2015-05-14T08:30:20Z" id="101969824">@imotov the code looks ok, but just so I understand how we will work in 2.0. Effectively, any version that has created version before 0.90 will require going through 1.6 for an upgrade, right? 
</comment><comment author="imotov" created="2015-05-14T12:49:25Z" id="102027604">@kimchy correct, if a cluster contains at least one index that was created before 0.90 such cluster will have to be upgraded to v1.6 before it can be upgraded to 2.0.
</comment><comment author="rjernst" created="2015-05-15T17:37:39Z" id="102465576">This looks pretty good, I left a few comments.
</comment><comment author="imotov" created="2015-05-15T19:15:35Z" id="102497170">@kimchy, @rjernst I pushed changes addressing your comments.
</comment><comment author="rjernst" created="2015-05-15T19:26:49Z" id="102500475">LGTM
</comment><comment author="clintongormley" created="2015-05-15T20:36:19Z" id="102520924">&gt; @kimchy correct, if a cluster contains at least one index that was created before 0.90 such cluster will have to be upgraded to v1.6 before it can be upgraded to 2.0.

Or the index can be closed, and have the setting updated manually, without upgrading to 1.6.  The migration plugin https://github.com/elastic/elasticsearch-migration/ checks for minimum segment version and checks for the presense of this setting.  We could add a link to instructions about how to set it without upgrading
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/ShardUpgradeStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/TransportUpgradeStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsResponse.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeReallyOldIndexTest.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeTest.java</file></files><comments><comment>Core: refactor upgrade API to use transport and write minimum compatible version that the index was upgraded to</comment></comments></commit></commits></item><item><title>Transport exception processing BigDecimal type from groovy script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11071</link><project id="" key="" /><description>I'm using ElasticSearch 1.5.2.

I have a **groovy** script_file 
`disk_total_free_percent.groovy`:

```
def totalSize = doc['Machine.disk_total_size_(mb)'].value;
def freeSpace = doc['Machine.disk_total_free_space_(mb)'].value;
if(totalSize == 0) { 
    //none is free, since the disk has no size
    0.0 //percent
}else {
    freeSpace / totalSize   
}
```

When I execute a search with `script_fields` including the above script, I get an error from ES
`[indices:data/read/search[phase/fetch/id]]]; nested: IOException[Can't write type [class java.math.BigDecimal]];` 
This is generated by [this line of code in StreamOutput.java](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java#L402)

It's also interesting that the number of shards that fail with this exception varies (for the same query). Sometimes it's 1, sometimes 2 or 3.

---

For comparison, I installed the **javascript** scripting plugin and tried the same script (replacing `def` with `var`)
`disk_total_free_percent.js`:

```
var totalSize = doc['Machine.disk_total_size_(mb)'].value;
var freeSpace = doc['Machine.disk_total_free_space_(mb)'].value;
if(totalSize == 0) { 
    //none is free, since the disk has no size
    0.0 //percent
}else {
    freeSpace / totalSize   
}
```

And **it worked** without any exceptions. This makes me think it's a problem with the Groovy scripting module.
</description><key id="74506154">11071</key><summary>Transport exception processing BigDecimal type from groovy script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvido</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-05-08T21:43:50Z</created><updated>2016-08-24T16:57:02Z</updated><resolved>2015-07-16T16:31:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-15T14:18:41Z" id="102411664">@bradvido I've tried various combinations of mappings etc to replicate this problem on 1.5.2, and it all seems to work just fine.  Could you add a small recreation?  

For some reason Groovy is converting one of those values to BigDecimal (which we don't support), but I can't see why it is doing that.  I found this bug which may be related? http://jira.codehaus.org/browse/GROOVY-7238
</comment><comment author="bradvido" created="2015-05-15T14:28:42Z" id="102414068">I should mention I'm running ES on Windows -- maybe the bug is platform specific,

Also, in Groovy, [every literal number with a decimal point becomes a BigDecimal](http://docs.codehaus.org/display/GROOVY/Groovy+Math).  It  needs a 'd' identifier to be treated as a double.

So maybe, i should change the literal `0.0` to `0.0d` in my script?  I'll test when I have time to get back to this.
</comment><comment author="dakrone" created="2015-05-15T15:54:27Z" id="102444693">We automatically convert BigDecimal constants to floating-point, see here: https://github.com/elastic/elasticsearch/pull/6609
</comment><comment author="bradvido" created="2015-05-15T16:09:01Z" id="102449820">Hmm, then it shouldn't make a difference if i use literal decimals in my script.  I'll try to find a simple way so others can reproduce this.
</comment><comment author="fredrlx" created="2015-06-17T10:17:29Z" id="112745962">Hi! I've run into a similar issue (same "Can't write type [class java.math.BigDecimal]" exception on some shards). Our groovy script had this line:

```
s = round(s*100)/100; 
```

rewriting it to the following got rid of the exception:

```
s = round(s*100d)/100d; 
```
</comment><comment author="bradvido" created="2015-06-19T16:54:37Z" id="113573850">@fredrlx what version of ES &amp; Groovy are you using?  #6609 should have already take care of this, but I've seen similar results.
</comment><comment author="fredrlx" created="2015-06-22T08:40:29Z" id="114039962">@bradvido We were using ES 1.5.1 when we had this issue. How do I check what version of Groovy I have ? I'll update ES to a more recent version, but the issue was solved with the "d" postfix on constants.
</comment><comment author="clintongormley" created="2015-07-16T16:31:21Z" id="122012407">This should be fixed by #12288 - I'm going to close for now, but feel free to reopen if you see the same thing on 1.7.0 or later

thaks
</comment><comment author="bradvido" created="2015-07-16T16:33:44Z" id="122013036">Thanks. Will test on 1.7.0
</comment><comment author="mausch" created="2016-08-24T16:57:02Z" id="242135821">Just hit this issue in 1.7.2. I know this is an old issue and an old version of ES, just wanted to say for future reference that changing an expression from `xxx.sum(0)` to `xxx.sum(0d)` as suggested above did the trick.
Also, this only happened in a `script_fields` script. The same script did not crash as a `function_score`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improved Request Circuit Breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11070</link><project id="" key="" /><description>In certain circumstances, the request circuit breaker is not blocking requests that are individually fine, but holistically a problem. For example, if you have an aggregation on a very high-cardinality field _and_ you allow the `shard_size` to become `Integer.MAX_VALUE` (either directly, or indirectly by setting it to `0`), then you can create a lot of CPU and network congestion (this is documented behavior).

On a per-request basis, this may be caught and safely blocked. However, for requests that manage to sneak in under the request threshold, I have come across scenarios where I can have multiple in-flight requests that manage to crash the node that handles the request.

In particular, I have seen a client node forced into OOM conditions due parallel aggregations with a lot of shards:

```
/my-index/_search
{
  "aggs": {
    "agg-name":{
      "terms": {
        "field" : "high-cardinality"
        "size" : 10,
        "shard_size" : 0  // THIS IS THE CULPRIT!
      },
      "aggs": {
        // ...
      }
    }
}
```

In this case, an individual shard response was _only_ ~70 MB, but there were many shards. Worse, other aggregations were in-flight at the same time. Eventually the memory became too much, causing the client node (in this case) to drop out due to OOM. I suspect that a similar problem could surface if a data node were forced to handle the initial request.

This is certainly not an easy problem to catch, nor will the solution to it be easy, but hopefully we can figure something out to combat the issue.
</description><key id="74458766">11070</key><summary>Improved Request Circuit Breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Circuit Breakers</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-05-08T18:52:14Z</created><updated>2016-09-08T09:03:59Z</updated><resolved>2016-09-08T09:03:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cfeio" created="2015-05-13T18:43:07Z" id="101772590">Could the circuit breaker be enhanced to set a threshold that any query which will be sent to more than X shards should be blocked?

We have a similar issue where we have some search requests that are hitting every shard and we would like to block such requests from executing.
</comment><comment author="ppf2" created="2015-09-30T08:28:36Z" id="144326596">Have a similar request where the end user asked for circuit breaker functionality at the node level, i.e limiting accumulative memory used by all queries on a node.
</comment><comment author="dakrone" created="2015-09-30T08:35:42Z" id="144327914">So, for the cardinality aggregation and request-level semantics, the circuit breaker (even though it's called "request breaker") has no notion of the request itself. Instead, it is part of the `BigArrays` class which is a generic class that chunks of byte arrays are requested from. This is nice from a programming level because they are decoupled, but not as nice from an end-user perspective because there is no notion of the actual search/agg request, just the byte array that was requested.

Now, multiple queries using `BigArrays` _should_ be using the same circuit breaker and thus already be at the node level, however, for things like @pickypg mentioned, the `terms` aggregation does not currently go through the request breaker (needs to be added!), which is why it can still kill a node.
</comment><comment author="pweerd" created="2016-03-04T11:30:32Z" id="192242977">Does someone know what the outlook for this feature is?
I really would like this feature, because currently same user requests blowup a client node. See
[aggregations-blowing-up-client-node](https://discuss.elastic.co/t/aggregations-blowing-up-client-node-oom/43500)
</comment><comment author="clintongormley" created="2016-09-08T09:03:58Z" id="245537530">Fixed by https://github.com/elastic/elasticsearch/pull/19394
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ban PathUtils.get (for now, until we fix the two remaining issues)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11069</link><project id="" key="" /><description>The idea is to ban what will only cause securityexceptions. Instead paths should really be resolved against the ones configured in Environment.

Once we fix #11065 and #11068, then maybe we can make the whole class private/package-private in .env package somewhere.

I fixed the remaining "bogus" use cases of the method in this PR so we are only left with the real issues.
</description><key id="74456934">11069</key><summary>Ban PathUtils.get (for now, until we fix the two remaining issues)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T18:44:52Z</created><updated>2015-05-15T15:57:33Z</updated><resolved>2015-05-11T12:52:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-08T18:47:58Z" id="100325144">I moved tmpdir stuff to Environment.tmpFile() as well. IMO, Files.createTempFile()/Directory() that do not take Path should be banned as well as a followup PR.

Two reasons for that:
1. it gets initialized in environment once, we ensure it is created and give permissions to it, and is not mutable (e.g. impacted by sysprop changes or whatever).
2. its consistent with just accessing all paths from environment.
</comment><comment author="rjernst" created="2015-05-08T19:31:13Z" id="100335260">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/main/java/org/elasticsearch/common/io/PathUtils.java</file><file>src/main/java/org/elasticsearch/env/ESFileStore.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file></files><comments><comment>Merge pull request #11069 from rmuir/ban_pathutils</comment></comments></commit></commits></item><item><title>Fix FSRepository location configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11068</link><project id="" key="" /><description>As pointed out by Lee in #11065. We might want to do a similar thing as what happens there, but today it is not always going to work correctly (depending where its configured to be), as it might not have permissions.
</description><key id="74454612">11068</key><summary>Fix FSRepository location configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-05-08T18:35:58Z</created><updated>2015-05-21T16:07:17Z</updated><resolved>2015-05-21T04:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/main/java/org/elasticsearch/common/io/PathUtils.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/repositories/RepositoryBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/env/EnvironmentTests.java</file><file>src/test/java/org/elasticsearch/index/IndexWithShadowReplicasTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Snapshot/Restore: fix FSRepository location configuration</comment></comments></commit></commits></item><item><title>`require_field_match` now defaults to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11067</link><project id="" key="" /><description>The default `false` for `require_field_match` is a bit odd and confusing for users, given that field names get ignored by default and every field gets highlighted if it contains terms extracted out of the query, regardless of which fields were queried. Changed the default to `true`, it can always be customized per request.

Closes #10627
</description><key id="74449305">11067</key><summary>`require_field_match` now defaults to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T18:17:53Z</created><updated>2015-06-06T15:43:39Z</updated><resolved>2015-05-15T19:39:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-05-08T19:06:50Z" id="100330124">+1

I'm so glad this is built in and doesn't have to happen 4 times....
</comment><comment author="johtani" created="2015-05-09T13:46:42Z" id="100488221">@javanna I left some comment.
</comment><comment author="javanna" created="2015-05-12T10:04:39Z" id="101220922">@clintongormley you ok with this change?
</comment><comment author="rmuir" created="2015-05-15T12:09:31Z" id="102382274">looks good
</comment><comment author="clintongormley" created="2015-05-15T19:12:34Z" id="102496616">@javanna +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Highlighting: require_field_match set to true by default</comment></comments></commit></commits></item><item><title>Scripting: API to check executability and existence of script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11066</link><project id="" key="" /><description>This is driven by Kibana scripted fields, https://github.com/elastic/kibana/issues/3797 but would be useful for many situations.

It would be nice if ElasticSearch had an API that could be called for validating scripts:

The client would provide a dynamic script or the name of a script file in `config/scripts` as well as a "test query" if the client wan't to validate that the script executes successfully against the query.

ES would validate:
1. If the ES config settings allow execution of the script 
2. If the script is a file, it would check that the file exists.
3. That the script is executable in the test query provided. Note that if there is a way to validate script (syntax etc. that should always be done).
</description><key id="74442579">11066</key><summary>Scripting: API to check executability and existence of script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvido</reporter><labels /><created>2015-05-08T17:50:25Z</created><updated>2016-10-06T00:13:25Z</updated><resolved>2015-05-15T14:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-05-08T21:22:05Z" id="100372279">My question is what is the difference in expectation of output between this and just running a test query against a script right now, and checking what errors are returned?
</comment><comment author="bradvido" created="2015-05-08T21:26:28Z" id="100373054">Running the test and processing exceptions would work, but it leaves something to be desired. It doesn't seem very clean to me and programatically determining the actual problem from the ES exception is difficult.

Also, an API method to validate scripts won't generate errors in the ES log that could be confused with actual errors that need action taken.  

It's a "check" as opposed to an "attempt and see if it fails".
</comment><comment author="clintongormley" created="2015-05-15T14:03:58Z" id="102407323">Hi @bradvido 

Really there is no way to check the script properly without executing it with a real query, with real params, and against real docs, ie: execute a search

However, the new structured exceptions make debugging the problem a whole lot easier than before. For instance, the following buggy script:

```
GET /_search
{
  "script_fields": {
    "FIELD": {
      "script": "foo+bar",
      "lang": "expression"
    }
  }
}
```

returns the much more readable exception:

```
{
   "error": {
      "root_cause": [
         {
            "type": "expression_script_compilation_exception",
            "reason": "Unknown variable [foo] in expression"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 1,
            "index": "test123",
            "node": "v8K5ojNHQ6e24vQTtrrDPg",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\n  \"script_fields\": {\n    \"FIELD\": {\n      \"script\": \"foo+bar\",\n      \"lang\": \"expression\"\n    }\n  }\n}\n]",
               "line": 6,
               "col": 5,
               "caused_by": {
                  "type": "expression_script_compilation_exception",
                  "reason": "Unknown variable [foo] in expression"
               }
            }
         }
      ]
   },
   "status": 400
}
```

Also, we already have the `_validate/query` API, which does pretty much what you're asking for, eg:

```
GET /_validate/query?explain
{
  "query": {
    "function_score": {
      "script_score": {
        "lang": "expression",
        "script": "foo"
      }
    }
  }
}
```

returns:

```
{
   "valid": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "test123",
         "valid": false,
         "error": "[test123] script_score the script could not be loaded; org.elasticsearch.script.expression.ExpressionScriptCompilationException: Unknown variable [foo] in expression"
      }
   ]
}
```
</comment><comment author="benbenwilde" created="2016-10-05T23:56:35Z" id="251833008">This is no good. I want to run a script during reindex, but I want to run it asynchronously. But I'm stuck with the version where there is a bug with the reindex task going away when it finishes (or fails). So essentially I need to at least validate that the script file exists before running. So I try to validate using a search or something but it seems to run in a different context throwing random errors. So I'm currently resorting to running the search and seeing which kind of error it returns to identify if the script exists or not. It's really not a great solution. 
</comment><comment author="nik9000" created="2016-10-06T00:13:25Z" id="251835318">That sucks! You are very likely better off running reindex with a size of 1
synchronously and then kicking it off asynchronously.

At this point even if no matter the outcome of this issue you'll get the
fix for your async status  problem first because that is in 5.0. Any new
feature would only be available in subsequent versions anyway.

On Oct 5, 2016 7:56 PM, "benbenwilde" notifications@github.com wrote:

&gt; This is no good. I want to run a script during reindex, but I want to run
&gt; it asynchronously. But I'm stuck with the version where there is a bug with
&gt; the reindex task going away when it finishes (or fails). So essentially I
&gt; need to at least validate that the script file exists before running. So I
&gt; try to validate using a search or something but it seems to run in a
&gt; different context throwing random errors. So I'm currently resorting to
&gt; running the search and seeing which kind of error it returns to identify if
&gt; the script exists or not. It's really not a great solution.
&gt; 
&gt; —
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11066#issuecomment-251833008,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLon6T1S-vSoQaSG_nK0I1XwOi5Duxks5qxDk3gaJpZM4EUBtm
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `path.shared_data`, change `index.data_path` to be relative</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11065</link><project id="" key="" /><description>This allows `path.shared_data` to be added to the security manager while
still allowing a custom `data_path` for indices using shadow replicas.

For example, configuring `path.shared_data: /tmp/foo`, then created an
index with:

```
POST /myindex
{
  "index": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "data_path": "bar/baz",
    "shadow_replicas": true
  }
}
```

The index will then reside in `/tmp/foo/bar/baz`.

`path.shared_data` defaults to `${path.home}/data` if not specified.
</description><key id="74435382">11065</key><summary>Add `path.shared_data`, change `index.data_path` to be relative</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T17:25:34Z</created><updated>2015-06-01T22:35:12Z</updated><resolved>2015-05-08T20:16:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-08T17:26:20Z" id="100304166">@rjernst @rmuir I'm curious what you guys think of this, it complicates the path reasoning for shadow replicas, but allows us to use the security manager when custom data paths are used.
</comment><comment author="rmuir" created="2015-05-08T17:31:27Z" id="100305701">Is the only use case for shared_data where it is a prefix and data directories are relative paths?
</comment><comment author="dakrone" created="2015-05-08T17:35:30Z" id="100307101">@rmuir yes, that is the only use.

Though, in the future, I guess I could see the snapshot &amp; restore API using `path.shared_data` for the location of snapshots, but that's really out of the scope of this and I don't think we have any current plans to change it.
</comment><comment author="rmuir" created="2015-05-08T17:35:40Z" id="100307136">In other words, i'd rather us avoid the confusing name if we can:

Today path.data is an array, and its resolved against CWD ("."). I think what you want (unless i'm missing a use case), is the ability to configure that X (where X is CWD today). This doesnt need to be anything we even store as an instance or getter in Environment, its just used once.

To me, this would be less confusing and more contained.
</comment><comment author="dakrone" created="2015-05-08T17:40:16Z" id="100308016">&gt; I think what you want (unless i'm missing a use case), is the ability to configure that X (where X is CWD today).

If I am understanding your correctly, this would mean that someone that had `path.data: /var/elasticsearch/data` and wanted to use a custom index path of `/data/myindex` would need to configure `X` as `/` because that is in the common root of both paths?

Would this mean that `/` would have to be given access in the security manager then?
</comment><comment author="rmuir" created="2015-05-08T17:44:24Z" id="100309255">I think i am missing something with custom index paths. What are these and how are they used?

I am going to ban PathUtils.get locally to get an understanding here.
</comment><comment author="dakrone" created="2015-05-08T17:49:36Z" id="100310447">Custom index paths are used for specifying where an index should be allocated, specifically for indices using shared filesystems.

So, if you had a shared FS mounted on `/mnt/sharedfs/data`, currently you would create an index with these settings:

```
POST /myindex
{
  "index": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "data_path": "/mnt/sharedfs/data/indices",
    "shadow_replicas": true
  }
}
```

It is outside the regular `path.data` setting because we need both a shared part (the index data) and a non-shared part (shard and index state that is per-node and should not be visible to all nodes). So `/mnt/sharedfs/data/indices` is used for the Lucene index itself and `path.data` is used for all the non-shared state files.
</comment><comment author="rmuir" created="2015-05-08T17:57:05Z" id="100312279">&gt; Custom index paths are used for specifying where an index should be allocated, specifically for indices using shared filesystems.

specifically for, or only for shared filesystems? I agree, this issue needs to be fixed! I just want to think about the easiest way, and how to make it less confusing (for example perhaps it should be documented and checked that this custom index path is a relative one just to prevent scary access control errors).
</comment><comment author="dakrone" created="2015-05-08T18:00:45Z" id="100313254">&gt; specifically for, or only for shared filesystems?

I think we should make this _only_ for shared filesystems, we can enforce `index.shared_filesystem==true`

&gt; perhaps it should be documented and checked that this custom index path is a relative one just to prevent scary access control errors

Yes, +1 to this, once we nail down the exact semantics I am all for adding validation with more helpful messages for possible misuses
</comment><comment author="rmuir" created="2015-05-08T18:13:41Z" id="100317887">&gt; I think we should make this only for shared filesystems, we can enforce index.shared_filesystem==true

+1, and if we do that i wish the name in the index API reflected that (e.g. shared_path or something). But you can't have it all.

I can see how this scheme works fine on unix systems, but how does this scheme work on windows systems? i only think of windows mounts as stuff like H: or `\\server\export`, but maybe my knowledge is out of date.

As far as sharing this path with snapshot, i dont think we should do that. To me they are different and we may have to think about them differently (as far as things like defaults, permissions, etc).  So i'd rather have a separate snapshot path for that in environment in a later PR, and give the name of this Path a good clear obvious one.
</comment><comment author="dakrone" created="2015-05-08T20:16:34Z" id="100348574">Actually, now that I think about this, I am not sure we can easily do this in a backwards compatible way. I will think about it some more and re-open or open a new PR.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesCustomDataPathIT.java</file><file>core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Add `path.shared_data`</comment></comments></commit></commits></item><item><title>Unify SearchResponse and BroadcastOperationResponse code around shards header</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11064</link><project id="" key="" /><description>Different responses hold the shards header, search, count, flush etc. The code was duplicated in two different places, centralized in RestActions.
It turns out that only the search response printed out the status field before the reason, which was added to all other broadcast responses too.
</description><key id="74418723">11064</key><summary>Unify SearchResponse and BroadcastOperationResponse code around shards header</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T16:28:56Z</created><updated>2015-05-30T10:51:54Z</updated><resolved>2015-05-09T13:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-08T16:31:25Z" id="100287859">this is great!, LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file></files><comments><comment>Java api: unify SearchResponse and BroadcastOperationResponse code around shards header</comment></comments></commit></commits></item><item><title>Remove duplicated buildAsBytes and corresponding toString methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11063</link><project id="" key="" /><description>We have some builders, specifically query builders, `SearchSourceBuilder`, `QuerySourceBuilder` and `SuggestBuilder`, that implement `ToXContent` and also allow to build their content as bytes by simply creating a `BytesReference` that holds their json (or yaml etc.) content (`buildAsBytes` methods). They can also print out their content through `toString`. Made sure that those common methods are in one single place and reused where needed.

Also, merged `QueryBuilder` and `BaseQueryBuilder` and made `QueryBuilder` an abstract class instead of an interface.
</description><key id="74412206">11063</key><summary>Remove duplicated buildAsBytes and corresponding toString methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T16:08:54Z</created><updated>2015-06-07T10:34:25Z</updated><resolved>2015-05-13T14:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-13T09:11:03Z" id="101583280">@jpountz can you have a look please?
</comment><comment author="jpountz" created="2015-05-13T10:03:08Z" id="101606235">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/QuerySourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/ToXContentToBytes.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BaseQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/weight/WeightBuilder.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>Java api: remove duplicated buildAsBytes and corresponding toString methods</comment></comments></commit></commits></item><item><title>Running into IllegalAccessError on Virtuozzo VPS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11062</link><project id="" key="" /><description>log does not show any entry (no errors) but a search results gives the message:
**IllegalAccessError[tried to access method .&lt;init&gt;()V from class org.elasticsearch.common.hppc.IntObjectOpenHashMap$ValuesContainer]**

_also the following (other error but with that it's still running)_
2015-05-08 15:03:25,583][DEBUG][action.admin.cluster.node.stats] [Cypher] failed to execute on node [dB-NuNvWR1ynNyc9aHxHOw]
java.lang.NullPointerException
    at org.elasticsearch.monitor.fs.SigarFsProbe.stats(SigarFsProbe.java:83)
    at org.elasticsearch.monitor.fs.FsService$FsStatsCache.refresh(FsService.java:56)
    at org.elasticsearch.monitor.fs.FsService$FsStatsCache.refresh(FsService.java:49)
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
    at org.elasticsearch.monitor.fs.FsService.stats(FsService.java:46)
    at org.elasticsearch.node.service.NodeService.stats(NodeService.java:156)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:96)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:141)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

sometimes it returns to work after a few minutes, sometimes I have to restart es. I do not know how to reproduce the error - only that is happening since the beginning of the week on 2 vservers.
</description><key id="74409770">11062</key><summary>Running into IllegalAccessError on Virtuozzo VPS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wodka</reporter><labels><label>feedback_needed</label></labels><created>2015-05-08T15:58:44Z</created><updated>2015-05-15T14:29:38Z</updated><resolved>2015-05-15T14:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T17:28:50Z" id="100305031">Hi @wodka 

What OS are you running on, and what version of Elasticsearch?  I'm guessing that sigar doesn't yet support whatever you're using.  Try removing `lib/sigar` and restarting Elasticsearch, and see if the problem goes away.
</comment><comment author="wodka" created="2015-05-08T23:46:44Z" id="100395172">Hi, 

can you tell me how to remove lib/sigar?

OS: Ubuntu 14.04.2 LTS
Kernel: 3.13.0-042stab106.6 #1 SMP Mon Apr 20 14:48:47 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux
Java(TM) SE Runtime Environment (build 1.8.0_40-b25)
Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)

Elasticsearch Version:

``` json
{
  "status" : 200,
  "name" : "Skinhead",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

I cannot tell which version of virtuozzo (running on hosteurope.de infrastructure)
</comment><comment author="imotov" created="2015-05-09T02:19:17Z" id="100412557">@wodka: just delete the `sigar` directory or move it somewhere else from the `lib` directory where it is residing. On Ubuntu it should be in `/usr/share/elasticsearch/lib/sigar`.
</comment><comment author="wodka" created="2015-05-09T17:31:00Z" id="100520442">Hi, this seems to solve the problem, the strange thing is that it was working prior to this week... 

thx anyway!

should I keep the ticket open?
</comment><comment author="clintongormley" created="2015-05-15T14:29:36Z" id="102414284">Closing in favour of #11034
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ThreadPool: make sure no leaking threads are left behind in case of initialization failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11061</link><project id="" key="" /><description>Our ThreadPool constructor creates a couple of threads (scheduler and timer) which might not get shut down if the initialization of a node fails. A guice error might occur for example, which causes the InternalNode constructor to throw an exception. In this case the two threads are left behind, which is not a big problem when running es standalone as the error will be intercepted and the jvm will be stopped as a whole. It can become more of a problem though when running es in embedded mode, as we'll end up with lingering threads or testing an handling of initialization failures.

Closes #9107
</description><key id="74408760">11061</key><summary>ThreadPool: make sure no leaking threads are left behind in case of initialization failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T15:54:21Z</created><updated>2015-05-15T13:49:55Z</updated><resolved>2015-05-08T21:08:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-08T16:01:59Z" id="100280963">I like it, LGTM
</comment><comment author="jpountz" created="2015-05-08T19:21:47Z" id="100332723">Left some suggestions but I like the change too!
</comment><comment author="imotov" created="2015-05-08T20:00:33Z" id="100341867">@jpountz I pushed some changes. Could you take another look? Thanks!
</comment><comment author="jpountz" created="2015-05-08T20:10:33Z" id="100345317">LGTM, only one minor comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Propagate headers &amp; contexts to sub-requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11060</link><project id="" key="" /><description>Whenever a query parser (or any other component) issues another
request as part of a request, the headers and the context has to
be supplied as well.

In order to do this, the `SearchContext` has to have those headers
available, which in turn means, the shard level request needs to
copy those from the original `SearchRequest`

This commit introduces two new interface to supply the needed methods
to work with context and headers.

Closes #10979
</description><key id="74393403">11060</key><summary>Propagate headers &amp; contexts to sub-requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T15:06:24Z</created><updated>2015-06-07T18:15:51Z</updated><resolved>2015-05-18T13:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-08T15:08:00Z" id="100265484">@uboness can you take a look, so we can validate this approach

The PR is not yet finished completely, needs some cleaning up and moving packages around.
</comment><comment author="uboness" created="2015-05-08T15:21:11Z" id="100271065">@spinscale sure... Will do it today 
</comment><comment author="spinscale" created="2015-05-15T16:30:17Z" id="102453550">@uboness handled your review comments, removed the suggester features due to the latest changes by areek, also changed the tests to check for more requests and if they contain headers (indexing, refresh)
</comment><comment author="spinscale" created="2015-05-18T09:05:27Z" id="102982161">@martijnvg @uboness thx for the comments, incorporated
</comment><comment author="uboness" created="2015-05-18T09:12:51Z" id="102984864">LGTM on my side... @jaymode wanna have a final review?
</comment><comment author="martijnvg" created="2015-05-18T10:04:58Z" id="103005211">LGTM from my side too.
</comment><comment author="jaymode" created="2015-05-18T11:07:10Z" id="103018929">LGTM as well
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Order by sum aggregation throws error in elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11059</link><project id="" key="" /><description>I am trying to sort the aggregated result by applying another aggregation that does the summing and then applying order by descending to that sum.

if I try like below, the aggregation result get sorted by doc count.

```
"order": {
    "revrsenestedowners": "desc"
}
```

Below code explains the problem am facing. (field names are changed just to illustrate the problem)

"machines" is my nested object but the "owners" is not nested and it belongs to parent object.

I need to get the top 10 machines name by owners machine count (require sum as the owners object are list and can have more than one value).

```
{
  "query": {
    "range": {
      "createdDate": {
        "gte": "2015-04-28T00:00:00",
        "lte": "2015-05-01T23:59:59"
      }
    }
  },
  "aggs": {
    "nestedagg": {
      "nested": {
        "path": "machines"
      },
      "aggs": {
        "terms": {
          "terms": {
            "field": "machines.machineName",
            "size": 10,
              "order": {
                  "sumowners": "desc"
               }
          },
          "aggs": {
            "revrsenestedowners": {
              "reverse_nested": {},
              "aggs": {
                "sumowners": {
                  "sum": {
                    "field": "owners.machinesCount"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

I require the sum ordering and not the doc count ordering.
for it to work I may require something like :

```
"order": {
    "revrsenestedowners.sumowners": "desc"
}
```

Is there a way to achieve what I'm looking for.

Or Is this the limitation with elastic search? or a bug?

I'm stuck and really appreciate any help
</description><key id="74368991">11059</key><summary>Order by sum aggregation throws error in elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yaskamal</reporter><labels /><created>2015-05-08T13:43:54Z</created><updated>2015-05-08T14:23:40Z</updated><resolved>2015-05-08T14:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T14:20:25Z" id="100245572">As explained in the documentation for `order`, the syntax is `revrsenestedowners &gt; sumowners.value`.

See   http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-order
</comment><comment author="yaskamal" created="2015-05-08T14:23:40Z" id="100246199">Thanks. I got it. 
Sorry couldn't get to the correct document. Thanks for pointing to right document.
Appreciate your help.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use System.nanoTime for elapsed time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11058</link><project id="" key="" /><description>I'm running a "power loss" ES test and noticed this strange log line from ES after reboot:

```
crashnode: [2015-05-08 05:10:57,381][DEBUG][index.gateway            ] [crashnode] [logs][4] recovery completed from [shard_gateway], took [4h]
```

The [4h] is completely wrong ... the recovery only took a minute or two.  Digging into it, I noticed that for some reason, on my Fedora Linux box, System.currentTimeMillis suddenly jumps 4 hours into the future, thus messing up this log line.  This is reproducible...

I think when we want to measure elapsed time we should use System.nanoTime, unless it's in some crazy hot spot (nanoTime is somewhat more costly).  currentTimeMillis pulls from the local clock and timezone, which can easily change... whereas nanoTime, on modern JVM/OS/hardware, should only monotonically increase.

When I changed RecoveryState.time to use nanoTime instead, the wrong [4h] output is fixed.

We use System.currentTimeMillis in a number of places in ES, and unfortunately I was only able to fix a subset of them in this PR because in the other places the timestamp is part of the API and I'm not sure if someone "relies" on that timestamp having come from currentTimeMillis...
</description><key id="74363082">11058</key><summary>Use System.nanoTime for elapsed time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T13:24:55Z</created><updated>2015-05-11T15:25:10Z</updated><resolved>2015-05-11T15:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-08T14:51:28Z" id="100260237">&gt; whereas nanoTime, on modern JVM/OS/hardware, should only monotonically increase

There's a bug for this http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6458294 also, so we should make sure we don't rely on it monotonically increasing anywhere in the code.
</comment><comment author="dakrone" created="2015-05-08T15:05:49Z" id="100264855">Left some minor comments, but this looks good.

That `TODO` looks really dangerous though... I think we should fix that in a separate issue?
</comment><comment author="mikemccand" created="2015-05-08T15:52:31Z" id="100278561">&gt; There's a bug for this http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6458294 also, so we should make sure we don't rely on it monotonically increasing anywhere in the code.

Yeah, timestamps are really messy :)  In general we should never rely on them for anything "important".

Since I'm replacing System.currentTimeMillis() here, ES should already not be relying on a monotonic clock... nanoTime should only be an improvement since it "tries harder" to be monotonic (JVM falls back to currentTimeMillis if monotonic clock isn't available).
</comment><comment author="mikemccand" created="2015-05-08T15:55:11Z" id="100279475">&gt; That TODO looks really dangerous though... I think we should fix that in a separate issue?

The TODO is really scary ... if clock jumps backwards by an hour suddenly writing legacy checksums can spin in a busy loop for an hour before finishing!

But: I think this code may be dead in master?  Do we really ever call LegacyChecksums.writeChecksums after opening an ancient index?
</comment><comment author="mikemccand" created="2015-05-08T15:57:49Z" id="100280122">Really I'd love to ban the System.currentTimeMillis API (Rob's idea) but in this PR I've only addressed maybe 25% of our usage because we expose the timestamps through public APIs in the other ~75% cases.
</comment><comment author="mikemccand" created="2015-05-08T15:59:13Z" id="100280401">Thanks for the feedback @dakrone, I pushed another commit folding it in.
</comment><comment author="dakrone" created="2015-05-08T16:06:34Z" id="100281970">LGTM

About whether we write the checksums, it looks like we do call it when restoring an older version of a shard from a snapshot as well as when we clean files at the end of phase1 recovery.

Maybe we can just use `lastVersion + 1`? Looping to set it to `currentTimeMillis` seems really arbitrary.
</comment><comment author="mikemccand" created="2015-05-08T19:34:10Z" id="100335816">&gt; Maybe we can just use lastVersion + 1? Looping to set it to currentTimeMillis seems really arbitrary.

OK, I just pushed commit to use Math.max(lastVersion+1, System.currentTimeMillis())
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/index/TrackingConcurrentMergeScheduler.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/StopWatch.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/Stopwatch.java</file><file>src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/GlobalOrdinalsBuilder.java</file><file>src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Use System.nanoTime when measuring elapsed time</comment></comments></commit></commits></item><item><title>Unify query_string parameters parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11057</link><project id="" key="" /><description>There currently are small differences between search api and count, exists, validate query, explain api when it comes to reading query_string parameters.  `analyze_wildcard`, `lowercase_expanded_terms` and `lenient` are only read by the search api and ignored by all other mentioned apis. Unified code to fix this and make sure it doesn't happen again. Also shared some code when it comes to printing out the query as part of SearchSourceBuilder conversion to ToXContent.
</description><key id="74361954">11057</key><summary>Unify query_string parameters parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T13:21:16Z</created><updated>2015-05-30T10:53:58Z</updated><resolved>2015-05-11T09:57:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-11T07:01:29Z" id="100790164">LGTM in general, but we may need to adapt the rest spec and add tests for the new APIs supporting the parameters?
</comment><comment author="javanna" created="2015-05-11T09:11:47Z" id="100827452">Thanks @spinscale I pushed a new commit. Updated REST spec (including parameters that were already support but not listed) and added REST tests. Tests were completely missing for count api and search_exists, so I added some basic generic tests too.
</comment><comment author="spinscale" created="2015-05-11T09:26:56Z" id="100831511">nice, ++ for more tests

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] documented missing query_string parameters for count, exists, search &amp; validate_query</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/support/QuerySourceBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file></files><comments><comment>REST: Unify query_string parameters parsing</comment></comments></commit></commits></item><item><title>Transport: read/write support for list of strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11056</link><project id="" key="" /><description>Add support for reading and writng string lists to existing StreamInput and StreamOutput class.
We will need this often when adding streaming support to queries, so it makes sense to add this to main branch also.
</description><key id="74347481">11056</key><summary>Transport: read/write support for list of strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T12:26:10Z</created><updated>2015-05-21T10:00:44Z</updated><resolved>2015-05-08T12:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-08T12:49:27Z" id="100222911">LGTM
</comment><comment author="cbuescher" created="2015-05-08T12:52:34Z" id="100223509">thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>Transport: remove support for reading/writing list of strings, use arrays instead</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>Merge pull request #11056 from cbuescher/feature/read-write-stringlist</comment></comments></commit></commits></item><item><title>Defining a template for geo_points messes up the full_name of other fields in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11055</link><project id="" key="" /><description>For a template like this one

```
PUT _template/test 
{
  "template": "test-*",
  "mappings": {
    "document": {
      "dynamic_templates": [
        {
          "geoFields": {
            "match": "geo*",
            "mapping": {
              "type": "geo_point"
            }
          }
        }
      ]
    }
  }
}
```

Using this document

```
PUT test-1/document/1
{
  "geo":[
    {
      "lon": 12,
      "lat": 34
    },
    {
      "lon": 13,
      "lat": 33
    }],
  "foo": "foo1"
}
```

Will generate a weird `full_name` for field `foo` (`GET test-1/document/_mapping/field/foo,geo`):

```
{
   "test-1": {
      "mappings": {
         "document": {
            "geo": {
               "full_name": "geo",
               "mapping": {
                  "geo": {
                     "type": "geo_point"
                  }
               }
            },
            "foo": {
               "full_name": "geo.foo",
               "mapping": {
                  "foo": {
                     "type": "string"
                  }
               }
            }
         }
      }
   }
}
```

The same **doesn't** happen when the document used to create the index is not using an array of `geo_point`s:

```
PUT test-1/document/1
{
  "geo": {
    "lon": 12,
    "lat": 34
  },
  "foo": "foo1"
}
```

the resulting mapping being:

```
{
   "test-1": {
      "mappings": {
         "document": {
            "geo": {
               "full_name": "geo",
               "mapping": {
                  "geo": {
                     "type": "geo_point"
                  }
               }
            },
            "foo": {
               "full_name": "foo",
               "mapping": {
                  "foo": {
                     "type": "string"
                  }
               }
            }
         }
      }
   }
}
```
</description><key id="74335550">11055</key><summary>Defining a template for geo_points messes up the full_name of other fields in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-05-08T11:42:06Z</created><updated>2015-05-15T00:27:45Z</updated><resolved>2015-05-15T00:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2015-05-08T11:42:56Z" id="100205312">Tested with ES 1.5.2 and 1.4.4.
</comment><comment author="rjernst" created="2015-05-15T00:27:04Z" id="102208639">I was able to reproduce this, but only on 1.x.  I believe it was fixed by one of the many mapping refactorings in master. I'm going to close this because I don't think it is worth the effort to track down which mapping refactoring fixed the issue and try to backport (as these refactorings have been complicated and rely on each other).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Confirm: possible to model openinghours using Geo Polygon Filter?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11054</link><project id="" key="" /><description>Some years ago I had a conversation on the Solr userlist to use Solr geo-functionality for modelling POI openinghours. This got some nice debate going with endresult being: it's possible with some hacking. A couple of years later (2013) it was possible out of the box with [solr](http://lucene.472066.n3.nabble.com/Modeling-openinghours-using-multipoints-td4025336.html) 

Partially based on that Chris Hostetter (a.k.a. Hossman) did the following [presentation](https://people.apache.org/~hossman/spatial-for-non-spatial-meetup-20130117/) in 2013, which nicely outlines several usecases (all sharing a temporal component) using geo-polygon intersections, etc.  

Question: 
1. are all these geo polygon intersection/overlap related queries possible with Elasticsearch as well? Geo Polygon Filter seems the way go to, but like to know the support for the above. 
2. Is there a way to define x/y instead of lat/long for ease-of-use? I.e.: getting the geo part out of geo-spatial? 
</description><key id="74312720">11054</key><summary>Confirm: possible to model openinghours using Geo Polygon Filter?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gebrits</reporter><labels /><created>2015-05-08T10:18:10Z</created><updated>2015-05-09T00:15:14Z</updated><resolved>2015-05-09T00:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-09T00:15:14Z" id="100397421">@gebrits I think this would be a great topic for a conversation on [elasticsearch user list](https://groups.google.com/forum/?fromgroups#%21forum/elasticsearch) or our new [discussion forums](http://discuss.elastic.co) since we are using github issues to keep track of bugs and feature requests. I just want to mention that while using geo polygons for filtering working hours is undeniably awesome, you might want to also consider simpler solutions such as using [nested documents](http://stackoverflow.com/a/7118625/783043).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Limit multi fields to relevant settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11053</link><project id="" key="" /><description>Multi fields are more limited than regular fields. They cannot be object or nested types. There are also other settings like `copy_to` or `include_in_all` (any others?) that should not work. We should enforce this by rejecting these settings when parsed within a multi field.
</description><key id="74267185">11053</key><summary>Limit multi fields to relevant settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-05-08T07:35:42Z</created><updated>2017-03-22T05:09:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T08:02:56Z" id="100143137">+1
</comment><comment author="clintongormley" created="2016-01-18T16:20:48Z" id="172575622">`copy_to` no longer works but `include_in_all`, `fields` and probably others do work - for clarity these should throw an exception
</comment><comment author="paregos" created="2017-03-22T05:07:04Z" id="288299899">It looks like copy_to and include_in_all no longer work, is there a list of other fields that shouldn't work? 
Sorry if I misunderstood your comment but did you mean if the propName is fields within a multiField it should be rejected like if the propName is include_in_all within a multiField?

Relevant file - https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/TypeParsers.java</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove file based index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11052</link><project id="" key="" /><description>As a follow up to #10870, this removes support for
index templates on disk. It also removes a missed
place still allowing disk based mappings.
</description><key id="74247262">11052</key><summary>Remove file based index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Index Templates</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T06:12:44Z</created><updated>2015-08-28T02:21:30Z</updated><resolved>2015-05-11T19:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-08T08:07:42Z" id="100144251">LGTM. Can you add a note the the migration guide?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/index/mapper/FileBasedMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Settings: Remove file based index templates</comment></comments></commit></commits></item><item><title>Enforce meta fields are set through the api, not inside a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11051</link><project id="" key="" /><description>Some meta fields are currently allowed to be set within a document's source:
- `_all`
- `_id`
- `_parent`
- `_routing`
- `_timestamp`
- `_ttl`

In #6730 the ability to set a custom path for `_id` and `_routing` was removed. We should go one step further and not allow setting any meta fields within a document. These are special fields that the system uses for special purposes. There should be one way to set them, through the api.
</description><key id="74176551">11051</key><summary>Enforce meta fields are set through the api, not inside a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-08T00:37:57Z</created><updated>2015-06-06T16:00:40Z</updated><resolved>2015-05-13T06:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T07:56:34Z" id="100140043">+1
</comment><comment author="jpountz" created="2015-05-08T08:06:21Z" id="100144044">+1!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/InternalMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>src/main/java/org/elasticsearch/index/mapper/RootMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Mappings: Remove ability to set meta fields inside documents</comment></comments></commit></commits></item><item><title>Truncate log messages at 10,000 characters by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11050</link><project id="" key="" /><description /><key id="74141020">11050</key><summary>Truncate log messages at 10,000 characters by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T21:54:14Z</created><updated>2015-06-01T22:35:10Z</updated><resolved>2015-05-08T16:10:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-07T21:56:58Z" id="100032528">Happy to change this to 10,000 also if people think this is too lenient.
</comment><comment author="rjernst" created="2015-05-07T22:12:47Z" id="100035533">+1
</comment><comment author="clintongormley" created="2015-05-08T09:06:23Z" id="100164905">10,000 sounds reasonable, plus it is easy to change for users who want to do so

+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"key_as_string" keys in the percentile aggregation response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11049</link><project id="" key="" /><description>I'm using version  "1.5.1"

By default, my percentile aggregations are returning like this, with "as_string" keys:

```
"aggregations": {
      "load_time_outlier": {
         "values" : {
            "1.0": 15,
            "1.0_as_string": "15",
            "5.0": 20,
            "5.0_as_string": "20",
            "25.0": 23,
            "25.0_as_string": "23",
            "50.0": 25,
            "50.0_as_string": "25",
            "75.0": 29,
            "75.0_as_string": "29",
            "95.0": 60,
            "95.0_as_string": "60",
            "99.0": 150,
            "99.0_as_string": "150",
         }
      }
   }
}
```

But the docs say:

"By default, the percentile metric will generate a range of percentiles: [ 1, 5, 25, 50, 75, 95, 99 ]. The response will look like this:"

```

"aggregations": {
      "load_time_outlier": {
         "values" : {
            "1.0": 15,
            "5.0": 20,
            "25.0": 23,
            "50.0": 25,
            "75.0": 29,
            "95.0": 60,
            "99.0": 150
         }
      }
   }
}
```
- http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html

I don't believe I'm passing any format options, but it's possible the elasticsearch-ruby driver is without my knowledge. 

I'm thinking this might be a bug. Seems similar to https://github.com/elastic/elasticsearch/issues/6655

If it's not a bug, is there a way to disable the "as_strings" keys from returning in the percentile aggregation response? 

Thanks. 
</description><key id="74133301">11049</key><summary>"key_as_string" keys in the percentile aggregation response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattruzicka</reporter><labels /><created>2015-05-07T21:26:05Z</created><updated>2015-05-08T04:22:09Z</updated><resolved>2015-05-08T04:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2015-05-08T04:22:07Z" id="100097466">Hey @mattruzicka this has been fixed in 1.5.2.

See https://github.com/elastic/elasticsearch/pull/10571 for details.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Overriding path.home with bin/elasticsearch causes ClassNotFoundExceptions </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11048</link><project id="" key="" /><description>With the addition of security manager, we now restrict reads/writes to anything within known paths. When path.home is overriden, `bin/elasticsearch` does not know about it. Instead the script uses the base dir of wherever `bin/elasticsearch` is located, and sets up the classpaths based on this. When the security policy is created with the overridden path.home, it does not know about this other path, and so any classes loaded after security manager is installed will fail to load.

We have a couple possible fixes:
- Don't allow overriding path.home. To do this properly, we need to check it in the startup scripts, or bootstrap (it is a little weird to be checking this outside environment, but maybe do it with a helper method in Environment? `Environment.validateSettings` or something?)
- Pass bin and lib as extra settings from the startup script. This might be a bit of a pain to fix for all the tests the create nodes (or maybe not, we set path.home now in all nodes).
- Pass a somehow smartly named "other home" that is where the software actually exists (just one setting instead of two for bin/lib). 
</description><key id="74130961">11048</key><summary>Overriding path.home with bin/elasticsearch causes ClassNotFoundExceptions </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-05-07T21:18:38Z</created><updated>2016-01-18T19:28:40Z</updated><resolved>2016-01-18T19:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-07T21:20:39Z" id="100022406">@tlrx @rmuir Interested in your thoughts here.
</comment><comment author="rmuir" created="2015-05-07T21:40:23Z" id="100027477">What is the use case? Can we please start underengineering this stuff instead of overengineering it. All paths need to be in environment... very simple :)
</comment><comment author="tlrx" created="2015-05-08T07:05:00Z" id="100128503">@rjernst not sure to understand the use case, can you please give an exemple (env var, jvm option, command line)? thanks :)
</comment><comment author="rjernst" created="2015-05-08T07:13:12Z" id="100129360">@tlrx The "use case" is specifying path.home to relocate data/logs/etc without specifying path.data, path.logs, etc. This used to work, but no longer does. The first question is do we want to support this at all?

Here is how to reproduce (from within elasticsearch installation dir):

```
mkdir myhome
cp -R config myhome/
bin/elasticsearch -Des.path.home=myhome
```
</comment><comment author="clintongormley" created="2016-01-18T16:18:05Z" id="172574947">@rjernst note your recreation isn't correct as it just copies the contents of the `config` dir to `myhome/` - that's not going to work.  If you copy `lib/`, `bin/`, `config/`, and `plugins/` there then it will work correctly.

I don't think we want to support reconfiguring `path.home`, but  `./bin/elasticsearch` sets this value automatically today.  Not sure that we actually need to prevent overriding it. If you do override it, and point to somewhere that is missing what you need, then things error anyway.

I think we can just close this.
</comment><comment author="rjernst" created="2016-01-18T19:28:40Z" id="172629031">Well, they error in a way that is not obvious what went wrong. But I guess this is advanced enough (since most people override specific dirs like `path.conf`, etc) that it's not worth doing any fancy checking.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Document the `index.shared_filesystem.recover_on_any_node` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11047</link><project id="" key="" /><description>Relates to #10960

When backporting to 1.x, I'll remove the blurb about the security manager.
</description><key id="74087294">11047</key><summary>[DOCS] Document the `index.shared_filesystem.recover_on_any_node` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>docs</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T18:43:40Z</created><updated>2016-02-29T16:37:22Z</updated><resolved>2015-06-03T10:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-03T10:37:44Z" id="108293969">Merged into master and into 1.x, without the reference to the security manager.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Document the `index.shared_filesystem.recover_on_any_node` setting</comment></comments></commit></commits></item><item><title>Add monitoring for inconsistent doc count between primary and replica shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11046</link><project id="" key="" /><description>Hi I'm running ES 1.5.2.

While indexing, a node got disconnected (See exception bellow) and the primary and replica shards got out of sync.

The _count and query APIs (hits.total) constantly alternated between 2 values. I noticed this because I physically ran a query in sense. By using preference=primary, the right doc count was always returned.

I was wondering maybe there could be a statistic or monitoring value that can maybe set the index state to Yellow when the counts are off between primary and replica shards so we can see it in Marvel/prefered monitoring tool?

Note: Using stunnel to add SSL. haven't evaluated Shield yet.

[2015-05-07 12:04:12,419][DEBUG][action.admin.indices.stats] [MYSERVER 01 (10.0.0.xx6)] [myindex-20150101][3], node[g2kwLV_RA3uDjoZBrPnL2q], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2e939ede]
org.elasticsearch.transport.SendRequestTransportException: [MYSERVER 04 (10.0.0.xx9)][inet[/127.0.0.1:9703]][indices:monitor/stats[s]]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:286)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:249)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:183)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:151)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:71)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
    at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:77)
    at org.elasticsearch.client.FilterClient$IndicesAdmin.execute(FilterClient.java:120)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient$IndicesAdmin.execute(BaseRestHandler.java:149)
    at org.elasticsearch.client.support.AbstractIndicesAdminClient.stats(AbstractIndicesAdminClient.java:524)
    at org.elasticsearch.rest.action.admin.indices.stats.RestIndicesStatsAction.handleRequest(RestIndicesStatsAction.java:104)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:53)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:225)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:170)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:329)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [MYSERVER 04 (10.0.0.xx9)][inet[/127.0.0.1:9703]] Node not connected
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:936)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:629)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:276)
    ... 55 more
</description><key id="74075323">11046</key><summary>Add monitoring for inconsistent doc count between primary and replica shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javadevmtl</reporter><labels /><created>2015-05-07T18:04:14Z</created><updated>2015-05-08T07:45:50Z</updated><resolved>2015-05-08T07:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T07:45:50Z" id="100137769">Hi @javadevmtl 

Comparing the count on the replica and primary would only be useful if we know that there are no changes in progress.  The only way Elasticsearch would know that is if the index were marked as read-only, so I don't think this is a statistic we can add to Marvel.

Instead, I think we should focus on fixing known issues such as https://github.com/elastic/elasticsearch/issues/7572
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove mapper listeners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11045</link><project id="" key="" /><description>The mapper listener concept is only now used as a callback to the
MapperService when new fields are added. This change removes the
listeners, instead storing a link to the mapper service in
each doc mapper.

There is also a small refactoring of MergeResult to eliminate a nested callback to the doc mapper.
</description><key id="74061691">11045</key><summary>Remove mapper listeners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T17:22:47Z</created><updated>2015-06-08T08:58:03Z</updated><resolved>2015-05-07T17:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T17:40:07Z" id="99954583">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adapt /bin/elasticsearch to launch under windows (msys) bash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11044</link><project id="" key="" /><description>Base OS for our team is Windows but we build cross-platform .net/mono applications so main host for our shell scripts is bash for windows (msys) not cmd.exe. So it's not much comfortable when bash scripts in bin are adapted only for Unix
</description><key id="74042627">11044</key><summary>Adapt /bin/elasticsearch to launch under windows (msys) bash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels /><created>2015-05-07T16:18:34Z</created><updated>2015-05-08T08:23:30Z</updated><resolved>2015-05-07T16:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="comdiv" created="2015-05-07T16:29:54Z" id="99930007">closed in d50d37c (comdiv fork)
</comment><comment author="clintongormley" created="2015-05-08T07:32:18Z" id="100133072">Hi @comdiv 

Did you mean to close this, or would you like to send this as a PR?
</comment><comment author="comdiv" created="2015-05-08T08:23:30Z" id="100150276">I close it my branch. Don't know you policy well. Here it's PR ).
What to do further?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Holt-Winters to moving_avg aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11043</link><project id="" key="" /><description>Adds Holt-Winters model (aka triple exponential) to `moving_avg` aggregation.  Holt-Winters has some finicky properties, so there were general changes to the agg to help make life easier:
- Better `settings` hash parsing, such that proper SearchParsePhaseExceptions are thrown if the datatype is wrong, out of bounds, etc
- Models can now implement `hasNext()` to determine if the model is capable of producing a new value.  This is important for HW, which has a "cold start" phase at the beginning where it is incapable of producing values.
- Simplified logic in the actual `reduce()` method
</description><key id="74042547">11043</key><summary>Add Holt-Winters to moving_avg aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T16:18:17Z</created><updated>2015-05-28T18:17:27Z</updated><resolved>2015-05-27T18:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-11T14:55:19Z" id="100934754">@polyfractal left some comments but it looks pretty good
</comment><comment author="polyfractal" created="2015-05-15T17:14:16Z" id="102461165">@colings86 All tidied up :)
</comment><comment author="colings86" created="2015-05-18T12:42:13Z" id="103047207">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelParserMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/TransportMovAvgModelModule.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Merge pull request #11043 from polyfractal/feature/aggs_holtwinters</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelParserMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/TransportMovAvgModelModule.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Aggregations: Add Holt-Winters model to `moving_avg` pipeline aggregation</comment></comments></commit></commits></item><item><title>Make it possible to configure missing values.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11042</link><project id="" key="" /><description>Most aggregations (terms, histogram, stats, percentiles, geohash-grid) now
support a new `missing` option which defines the value to consider when a
field does not have a value. This can be handy if you eg. want a terms
aggregation to handle the same way documents that have "N/A" or no value
for a `tag` field.

This works in a very similar way to the `missing` option on the `sort`
element.

One known issue is that this option sometimes cannot make the right decision
in the unmapped case: it needs to replace all values with the `missing` value
but might not know what kind of values source should be produced (numerics,
strings, geo points?). For this reason, we might want to add an `unmapped_type`
option in the future like we did for sorting.

Related to #5324
</description><key id="74042202">11042</key><summary>Make it possible to configure missing values.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T16:17:11Z</created><updated>2015-09-02T07:31:06Z</updated><resolved>2015-05-15T14:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T16:23:04Z" id="99927072">While the API proposal here is different from the one proposed on #5324, I think it could address most use-cases and even be more generic. For instance, in some cases you might want to have a dedicated bucket for documents that miss a value and all that you would have to do would be to pass a value which doesn't exist in the index (eg. `_missing`, but the choice is free). In other cases however it might make sense to put documents that miss a value into an existing bucket, I think a good example of that would be the `N/A` value for a tag field: documents that don't have a value for the tag field and documents that have this value should really be treated the same.

Also I like that we would have a consistent behaviour in all aggregations that support this parameter (ie. all aggregations that work on top of a field or script but `missing`), which would be consistent with sorting as well.
</comment><comment author="clintongormley" created="2015-05-07T16:35:19Z" id="99931873">Nice work!
</comment><comment author="jpountz" created="2015-05-11T08:09:01Z" id="100807697">Thanks @clintongormley for helping fix the docs, I pushed a new commit.
</comment><comment author="colings86" created="2015-05-11T15:20:50Z" id="100941645">@jpountz left a couple of minor comments
</comment><comment author="colings86" created="2015-05-14T11:24:50Z" id="102007724">LGTM
</comment><comment author="mrfelton" created="2015-07-16T08:56:37Z" id="121885433">Can't get this working for the life of me. Is this in 1.6? I can't find any documentation on this feature at https://www.elastic.co/ 

&gt; Parse Failure [Unknown key for a VALUE_STRING in [campaign_term]: [missing].]]

``` json
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "_type:Subscription",
          "analyze_wildcard": true
        }
      }
    }
  },
  "aggs": {
    "2": {
      "date_histogram": {
        "field": "date",
        "interval": "1M",
        "pre_zone_adjust_large_interval": true,
        "min_doc_count": 1
      },
      "aggs": {
        "campaign_term": {
          "terms": {
            "field": "context.campaign.term",
            "size": 0,
            "missing": "hr-openers"
          }
        }
      }
    }
  }
}
```
</comment><comment author="colings86" created="2015-07-16T09:40:46Z" id="121903033">@mrfelton this will be available from 2.0 onwards. The documentation for it is availble on the master branch of the docs. There is a new section for each agg called 'Missing Values'. For example: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-metrics-avg-aggregation.html#_missing_value
</comment><comment author="GrahamHannington" created="2015-08-24T14:47:07Z" id="134231010">In the meantime, before 2.0, and with apologies if this has already been covered: can you specify a script in the Kibana "JSON input" field that dynamically replaces a missing field value with zero? (And can someone point me to detailed documentation of what can be specified in that field? My Google-fu has failed there, too.)
</comment><comment author="GrahamHannington" created="2015-08-25T07:54:22Z" id="134514267">Suppose I have an Elasticsearch document with no "grade" field; the "grade" field is missing.

Suppose I have another document with a "grade" field explicitly specified as `null`:

```
"grade": null
```

Will the new-for-2.0 `missing` option apply to both documents?
</comment><comment author="jpountz" created="2015-08-25T09:07:32Z" id="134534582">Missing and `null` will be considered the same by default, unless you configure a [`null_value`](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html) in your mappings.

Regarding scripting, you can indeed do that in 1.x by running the aggregation of a script (likely with a bit performance/memory usage hit) that would check whether the list of values is empty.
</comment><comment author="GrahamHannington" created="2015-08-26T09:41:30Z" id="134924995">Thanks, @jpountz . Re:

&gt; you can indeed [replace a null/missing field value with zero] in 1.x by running the aggregation of a script

Could you please either spoonfeed me (cringe, sorry) the appropriate contents of the Kibana **JSON Input** field, or point me to detailed documentation for specifying the contents of that field? I can write "if _x_ is null, then set _x_ to 0" in a few programming languages, but I lack the experience and detailed documentation I need to do that in this context (such as the surrounding JSON, the specific syntax and variable names).
</comment><comment author="clintongormley" created="2015-08-26T11:00:51Z" id="134948723">@GrahamHannington 

Not sure about the Kibana side, but here's an example (with groovy dynamic scripting) which will replace missing values with -1:

```
DELETE t

POST t/t/
{
  "num": 1
}
POST t/t/
{
  "num": 2
}
POST t/t/
{

}

GET t/_search?size=0
{
  "aggs": {
    "nums": {
      "histogram": {
        "interval": 1,
        "script": "doc['num'][0] == null ? -1 : doc['num'].value"
      }
    }
  }
}
```

You could use the `expression` language instead, but be aware that it doesn't support nulls, so you can't distinguish null from 0.  If zeroes aren't important you can do:

```
GET t/_search?size=0
{
  "aggs": {
    "nums": {
      "histogram": {
        "interval": 1,
        "script": "doc['num'].value || -1",
        "lang": "expression"
      }
    }
  }
}
```
</comment><comment author="GrahamHannington" created="2015-08-31T04:20:44Z" id="136248770">Thanks again, @jpountz .

I need the average (`avg`) aggregation to include documents with null or missing field values in its count, and treat those null or missing field values as zero. Otherwise, I get (what I consider to be) skewed averages.

For example, suppose I have the following five Elasticsearch documents, where T_n_ is a timestamp value, and grade is the name of a field on which I want to perform an average calculation:

| Timestamp | grade |
| --- | --- |
| T1 | null or missing |
| T2 | 10 |
| T3 | null or missing |
| T4 | 10 |
| T5 | null or missing |

Currently, when I use an average aggregration in a visualization, a bucket that includes T1 - T5 shows the average grade as 10:

(10 + 10) / 2 = 10

(that is, it skips the documents with null or missing grade)

whereas I want it to show 4 (to include the documents with null or missing grade, and treat grade as zero):

(0 + 10 + 0 + 10 + 0) / 5 = 4

However, I have so far been unable to trap null field values via the Kibana **JSON Input** field.

I suspect (I could be wrong) that what is happening is that Kibana (more specifically, Elasticsearch; but I'm doing all of this through the Kibana user interface) skips the documents with null or missing field values, and so those documents never "reach" the **JSON Input** field value.

I can use the following **JSON Input** field value to override the values of fields that are present (say, replace 10 with 20):

```
{ "script": "10 ? 20 : _value" }
```

but the following has no effect:

```
{ "script": "null ? 20 : _value" }
```

Similarly, neither does this, possibly unfaithfully transcribed from your suggestion (much appreciated, thank you):

```
{ "script" : "doc['a'][0] == null ? 0 : doc['a'].value" }
```

I'd appreciate some more advice here. I'd like to have a workaround (before 2.0 arrives) for these skewed averages that doesn't involve re-loading the (currently, deliberately "sparse") data with explicit zero field values. Even if that workaround involves a performance hit on large data sets (as I imagine this script-based would; so far, I've only tested it on very small indices).
</comment><comment author="jpountz" created="2015-09-02T07:31:06Z" id="136961447">Unfortunately, this can't be done today because Kibana requires you to configure a field and then merges the agg definition with the value in the json input, which makes elasticsearch run the script on every value instead of every document.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/MissingValues.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/MissingValueTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/support/MissingValuesTests.java</file></files><comments><comment>Merge pull request #11042 from jpountz/feature/aggs_missing</comment></comments></commit></commits></item><item><title>add parameter to flush api for synced flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11041</link><project id="" key="" /><description>This adds a boolean parameter "synced" to the flush api. if set, the flush will be synced flush instead of a normal flush.
I am yet unsure if this is the right way to go. We have two oddities when we use the flush api for synced flush:
- the flush actions in SyncedFlushService cannot run in the FLUSH threadpool anymore, else we get deadlocks
- the result counting will be off. flush api assumes there will be one response per shard and counts success and failures for each of them. synced flush response is a summary for a primary and all its copies. as is for a synced flush we now only count for each primary if the synced flush succeeded on any copy or on none instead of counting a result for each shard. 

I am inclined to make this a dedicated api after all but am not super passionate. If you think these drawbacks are ok then we can leave it this way. 
</description><key id="74004333">11041</key><summary>add parameter to flush api for synced flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>WIP</label></labels><created>2015-05-07T14:23:32Z</created><updated>2015-05-11T17:12:53Z</updated><resolved>2015-05-11T17:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-11T15:16:44Z" id="100940408">should this be reviewed? @brwe 
</comment><comment author="brwe" created="2015-05-11T17:12:51Z" id="100981547">no, I don't like it after all. Having an independent api might be more code but having a detailed response might be worth it? I opened #11098 instead. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Something may wrong when update the weight's of a doc to change the score of the result of complete suggestion when the doc size is large.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11040</link><project id="" key="" /><description>Here is a mapping of index

```
{
    "settings": {
        "analysis": {
            "analyzer": {
                "name_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": [ "standard", "lowercase" ]
                }
            }
        }
    },
    "mappings": {
        "doc": {
            "properties": {
                "suggest": {
                    "type": "completion",
                    "analyzer": "name_analyzer",
                    "payloads": true
                }
            }
        }
    }
}
```

Insert a doc : 

```
$ curl -XPUT localhost:9200/sample/doc/1 -d '{
"suggest" : {
 "output" : "1" ,
 "input" : [ "info" ],
 "weight" : 100
 }
}'
{"_index":"sample","_type":"doc","_id":"1","_version":1,"created":true}
```

And then , I try to query this doc, get result as follow : 

```
$ curl -X POST 'localhost:9200/sample/_suggest?pretty' -d '{
"sample-suggest" : {
"text" : "in",
"completion" : {
"field" : "suggest"
}}}'
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "sample-suggest" : [ {
    "text" : "in",
    "offset" : 0,
    "length" : 2,
    "options" : [ {
      "text" : "1",
      "score" : 100.0  # &lt;&lt; this score equals the weight I had settted
    } ]
  } ]
}
```

And then, I update this doc's weight  from 100 to 3

```
$ curl -XPUT localhost:9200/sample/doc/1 -d '{
"suggest" : {
 "output" : "1" ,
 "input" : [ "info" ],
 "weight" : 3
 }
}'
{"_index":"sample","_type":"doc","_id":"1","_version":2,"created":false}
```

Let's check this doc again : 

```
$ curl http://localhost:9200/sample/doc/1
{"_index":"sample","_type":"doc","_id":"1","_version":2,"found":true,"_source":{
"suggest" : {
 "output" : "1" ,
 "input" : [ "info" ],
 "weight" : 3
 }
}}
```

It had also updated.

And then, I request a suggestion again.

```
$ cur-X POST 'localhost:9200/sample/_suggest?pretty' -d '{
"sample-suggest" : {
"text" : "in",
"completion" : {
"field" : "suggest"
}}}'
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "sample-suggest" : [ {
    "text" : "in",
    "offset" : 0,
    "length" : 2,
    "options" : [ {
      "text" : "1",
      "score" : 3.0
    } ]
  } ]
}
```

the score updated from 100 to 3.

---

However, my index with 38M docs(with size about 64G Byte). When I update the doc as below.
I can also curl this doc, and found the weight had been changed, however, when I use the suggestion, It always give me the old score back.

I'm not sure is there something wrong when I  was trying to update those docs. Any replies are appreciated.
</description><key id="73992321">11040</key><summary>Something may wrong when update the weight's of a doc to change the score of the result of complete suggestion when the doc size is large.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuikns</reporter><labels /><created>2015-05-07T13:43:55Z</created><updated>2015-05-08T07:47:33Z</updated><resolved>2015-05-08T07:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T07:17:56Z" id="100130013">Hi @yuikns 

The completion suggester currently doesn't take deletions into account:

&gt; The suggest data structure might not reflect deletes on documents immediately. You may need to do an Optimize for that. You can call optimize with the only_expunge_deletes=true to only cater for deletes or alternatively call a Merge operation.

An update is really a delete+index-new-doc, so the deletions hang around until they are garbage collected by the merge process.  The new suggester (https://github.com/elastic/elasticsearch/issues/10746) will take deletions into account
</comment><comment author="yuikns" created="2015-05-08T07:41:25Z" id="100136033">@clintongormley  Thanks for your reply.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Developer path to elasticsearch.jar in elasticsearch.in.bat #11038</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11039</link><project id="" key="" /><description>See #11038 and #11044 
</description><key id="73988652">11039</key><summary>Developer path to elasticsearch.jar in elasticsearch.in.bat #11038</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels /><created>2015-05-07T13:31:08Z</created><updated>2015-05-10T14:57:48Z</updated><resolved>2015-05-10T14:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="comdiv" created="2015-05-10T14:57:47Z" id="100652993">Remove as it was declined by project leads
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix path to elasticsearch jar in elasticsearch.in.bat for developer proposes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11038</link><project id="" key="" /><description>Now elasticsearch.in.bat assumes that 
elasticsearch-\* is placed in 
%ES_HOME%/lib/elasticsearch-*.jar;
but after usual 
mvn clean package -DskipTests elasticsearch is compiled not  to  %ES_HOME%/lib/ but %ES_HOME%/
so it's more friendly to add both paths to elasticsearch.in.bat
</description><key id="73986791">11038</key><summary>Fix path to elasticsearch jar in elasticsearch.in.bat for developer proposes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">comdiv</reporter><labels><label>discuss</label></labels><created>2015-05-07T13:26:21Z</created><updated>2015-05-08T10:23:34Z</updated><resolved>2015-05-07T13:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T07:40:39Z" id="100135544">@comdiv On my system, it is compiled to `./target/`, and the packages placed into `./target/releases`.  Most developers would run elasticsearch through the IDE, or by unpacking the zip.  

`./bin/elasticsearch` is intended for the packaged version - I'm not sure I want to extra paths to it.
</comment><comment author="comdiv" created="2015-05-08T08:24:40Z" id="100150830">It doesn't break usual usage. But allow to launch elastic search process for quick checking.
</comment><comment author="rjernst" created="2015-05-08T08:41:47Z" id="100155311">You can use `mvn exec:exec` to do a quick check of actually running elasticsearch. Note this requires first running `mvn compile`.
</comment><comment author="comdiv" created="2015-05-08T10:23:34Z" id="100184990">thx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES return different result every time for more like this query for same document searched </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11037</link><project id="" key="" /><description>I have some 16 text documents with enough content  in each.
step 1: I index them with a string type field named "content" with store =false and TermVectorOption.WithPositions.
Now i make a request for more like this query as below 

{
 "query": {
    "more_like_this" : {
        "fields" : ["content"],
       "ids" : ["AU0uWRRxsLZwpQ7lzURn"],
        "min_term_freq": 1,
        "min_doc_freq": 1
    }
    }
}

step 2: I get the result and i note down the documents returned with score.

Step 3: Now I delete whole index by issuing below command

http://local:9200/test 

returning 

{
    "acknowledged": true
}

which confirms whole index is deleted

Now again i take same 16 documents and index again resulting every time it index same documents with different sequence and step 2 I get different result and score for same document I searched in step 1.

Is this a bug or am i missing anything to set?
</description><key id="73973478">11037</key><summary>ES return different result every time for more like this query for same document searched </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels><label>feedback_needed</label></labels><created>2015-05-07T12:42:01Z</created><updated>2015-05-15T15:55:55Z</updated><resolved>2015-05-08T07:06:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T07:06:11Z" id="100128635">Hi @dobariya 

I'm assuming that you're indexing these documents without specifying an ID, which means that each doc gets an autogenerated ID, and is assigned to a different shard each time that you reindex.  This means that the term frequencies are different for each reindex, which gives you the different results.

See http://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html for a fuller explanation.

Please feel free to reopen if my assumptions are incorrect.
</comment><comment author="dobariya" created="2015-05-11T10:25:29Z" id="100859423">Thanks @clintongormley  for the reply.
Yes i can understand the logic behind auto generated Id.
Is it required to index documents in sequence with a sequence id?because am indexing documents with parallel thread running for indexing and i used sequence id but its still not working.If i remove parallel thread indexing with sequence id input then it works well.
Is it mandatory to index documents in sequence with sequence id ?

For eg:

if i index documents in the following sequence id it works
1,2,3,4,5,6

but if i index in the following way then it does not work
2,6,4,1,3,5

if its mandatory to follow first method then it's a burden to index in sequence which will affect speed.I need to index 1 million documents in an hour or 2 which i can only achieve with parallel threads running and if documents needs to index in sequence to achieve the subjected solution then its of no use i can use elastic 

Please help me suggest how to index the documents so that i can get same relevancy for every time i re index  
</comment><comment author="clintongormley" created="2015-05-15T15:55:55Z" id="102444994">@dobariya Either (a) have one shard or (b) have enough data.  Term statistics will even out the more data you have..

As I said, read this: http://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html

It explains it all
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GET /pattern-*/_mapping/field/* kills the server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11036</link><project id="" key="" /><description># server

  "version" : {
    "number" : "1.4.4",
    "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
    "build_timestamp" : "2015-02-19T13:05:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
#1. put a template with some type=nested properties

$ curl -X PUT localhost:9200/_template/salesorder --data-binary @salesorder.mapping
{"acknowledged":true}
#2. add some documents (1681 documents in 96 salesorder-\* indexes)

$ for i in data*; do curl localhost:9200/_bulk --data-binary @$i; done
$ curl 2&gt;/dev/null localhost:9200/_aliases?format=yaml|grep ^salesorder-|wc -l
96
#3. run this request (used by kibana4 metadata discovery)

$ curl 'localhost:9200/salesorder-*/_mapping/field/*?ignore_unavailable=false&amp;allow_no_indices=false&amp;include_defaults=true'

Server immediately becomes irresponsive, java process rss size hits the roof and after some time this starts showing up in the log:

[2015-05-07 05:06:11,806][DEBUG][action.admin.indices.mapping.get] [Soldier X] failed to execute [org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsIndexRequest@3844bde7]
java.lang.OutOfMemoryError: Java heap space
</description><key id="73972785">11036</key><summary>GET /pattern-*/_mapping/field/* kills the server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zde</reporter><labels /><created>2015-05-07T12:40:01Z</created><updated>2015-05-07T13:19:56Z</updated><resolved>2015-05-07T13:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zde" created="2015-05-07T13:19:55Z" id="99861311">dupe of https://github.com/elastic/kibana/issues/1540
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to HPPC 0.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11035</link><project id="" key="" /><description>This patch updates a lot of HPPC-related code after API changes done in 0.7.0. The changes are not merely cosmetic, but also functional:

1) hash container ordering is randomized (it'll be reproducible for a given test seed to simplify debugging); for rationale, read https://github.com/carrotsearch/hppc/wiki/Scatter-Or-Hash
2) the 'allocated' array is gone from associative containers in preference for a sentinel key (zero or null); should result in speed improvements and better cache utilization.
3) the JAR has been split into two -- one contains "esoteric" classes, for example maps that take a byte or a floating point type as a key. They are used in ES (but I think they should be replaced with other, more appropriate containers -- by normalizing floating point types to an int or long and by simply creating a fixed table[] for byte-&gt;X mappings).

I fixed a few tests, but can't guarantee no side-effects.
</description><key id="73959696">11035</key><summary>Upgrade to HPPC 0.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T12:01:57Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-05-13T12:07:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-13T08:40:05Z" id="101575424">LGTM in general @jpountz can you take another look?
</comment><comment author="jpountz" created="2015-05-13T10:45:47Z" id="101621560">The change looks good to me too but I have some tests failures:

```
Tests with failures:
  - org.elasticsearch.versioning.SimpleVersioningTests.testForce
  - org.elasticsearch.index.translog.BufferedTranslogTests.testLocationComparison
  - org.elasticsearch.aliases.IndexAliasesTests.testIndicesGetAliases
```

I'll dig into it, we might be relying on iteration order somewhere...
</comment><comment author="dweiss" created="2015-05-13T11:21:29Z" id="101631305">Interesting, I ran the test suite before filing the PR and it passed for me. Might be randomization (or subsequent changes?).
</comment><comment author="jpountz" created="2015-05-13T11:25:12Z" id="101631746">I think I found these ones because I ran with `-Dtests.slow=true`. I don't think it can be subsequent changes as I did not rebase.
</comment><comment author="dweiss" created="2015-05-13T11:48:38Z" id="101637294">Ah, yes, I think I ran with the defaults. Also, take a look at my various comments on the patch if you haven't already done so, Adrien. There's one case where I added sorting for tests to reproduce (and it seems to make sense to do it for users as well), but it should be reviewed I guess.
</comment><comment author="jpountz" created="2015-05-13T11:50:04Z" id="101637470">@dweiss Yep, I saw this one. The IndexAliasesTests failure is actually similar as it depends on the order of the list of aliases.
</comment><comment author="jpountz" created="2015-05-13T12:08:29Z" id="101642962">Thanks @dweiss!
</comment><comment author="dweiss" created="2015-05-13T12:09:48Z" id="101643319">You're welcome!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/main/java/org/elasticsearch/common/ContextHolder.java</file><file>src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java</file><file>src/main/java/org/elasticsearch/common/collect/ImmutableOpenLongMap.java</file><file>src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recyclers.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java</file><file>src/main/java/org/elasticsearch/common/util/BytesRefHash.java</file><file>src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDateAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDoubleAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericFloatAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericIntegerAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericLongAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataStats.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestFielddataAction.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregator.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/controller/SearchPhaseController.java</file><file>src/main/java/org/elasticsearch/search/dfs/AggregatedDfs.java</file><file>src/main/java/org/elasticsearch/search/dfs/DfsPhase.java</file><file>src/main/java/org/elasticsearch/search/dfs/DfsSearchResult.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionStats.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/transport/TransportMessage.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/common/hppc/HppcMapsTests.java</file><file>src/test/java/org/elasticsearch/common/util/BytesRefHashTests.java</file><file>src/test/java/org/elasticsearch/common/util/LongHashTests.java</file><file>src/test/java/org/elasticsearch/common/util/LongObjectHashMapTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/CombiTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/EquivalenceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/MetaDataTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlusTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/DuelScrollTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file></files><comments><comment>Updating to HPPC-0.7.1</comment></comments></commit></commits></item><item><title>Remove libsigar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11034</link><project id="" key="" /><description>Supposedly ES works if its not available, hence, its unnecessary.

Having uunecessary JNI dependencies is too much.
</description><key id="73929157">11034</key><summary>Remove libsigar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T10:16:24Z</created><updated>2015-07-08T16:01:59Z</updated><resolved>2015-07-08T16:01:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-07T14:01:51Z" id="99878099">+1 in general 
</comment><comment author="dakrone" created="2015-05-07T15:21:10Z" id="99907882">+1
</comment><comment author="kimchy" created="2015-05-07T15:40:20Z" id="99914744">the fact that it works without mainly comes for the cases where there isn't an OS sigar supports, or its running embedded and someone doesn't wish to distribute sigar.

My main concern with sigar is that the project has stagnated. There hasn't been a new release since 1.6.4 which happened a few years ago... .

It has been very handy to use. Single API call to get the status on 100 node cluster. Many monitoring tools (bigdesk, ElasticHQ, ...) for ES rely on it (since its part of the API), and the ones I have found handy are basic ones, just with an opportunity to see the state across the cluster easily. I've mainly used cpu (os and process), swap usage, and in extreme cases network and fs statistics.
</comment><comment author="lukas-vlcek" created="2015-05-07T17:07:55Z" id="99940537">Do I understand this correctly that you are considering dropping REST APIs (cluster, admin...) that expose info based on Sigar lib? This would be unfortunate IMO. Not because I am Bigdesk maintainer but because getting this info directly from ES is extremely valuable feature. It is one of the reasons why we are using ES in our team.
</comment><comment author="s1monw" created="2015-06-01T18:27:47Z" id="107663924">&gt; Do I understand this correctly that you are considering dropping REST APIs (cluster, admin...) that expose info based on Sigar lib? This would be unfortunate IMO. Not because I am Bigdesk maintainer but because getting this info directly from ES is extremely valuable feature. It is one of the reasons why we are using ES in our team.

this is about removing the native library not the stats it's providing. We can in the meanwhile get several of them from other sources. We might not be able to provide a 1 to 1 replacement but a native lib that is basically unmaintained for more than half a decade is not really an option.
</comment><comment author="tlrx" created="2015-07-08T16:01:58Z" id="119635721">I'm closing this one: sigar has been removed in #12010 and the REST APIs have been updated to expose stats &amp; information that can be collected through JMX management beans (see #12043, #12049, #12053). Network Stats has been removed because there's no simple replacement (see #12054).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Infra for deprecation logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11033</link><project id="" key="" /><description>Add support for a specific deprecation logging that can be used to turn on in order to notify users of a specific feature, flag, setting, parameter, ... being deprecated.

 The deprecation logger logs with a "deprecation." prefix logger (or "org.elasticsearch.deprecation." if full name is used), and outputs the logging to a dedicated deprecation log file.

Deprecation logging are logged under the DEBUG category. The idea is not to enabled them by default (under WARN or ERROR) when running embedded in another application.

By default they are turned off (INFO), in order to turn it on, the "deprecation" category need to be set to DEBUG. This can be set in the logging file or using the cluster update settings API.
</description><key id="73929150">11033</key><summary>Infra for deprecation logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Logging</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T10:16:20Z</created><updated>2015-06-06T18:02:54Z</updated><resolved>2015-05-26T15:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T11:42:19Z" id="99829443">It looks good, my only concern is that this new code is not used anywhere. Maybe it wouldn't be too hard to make ParseField use it?
</comment><comment author="kimchy" created="2015-05-07T11:43:39Z" id="99829598">@jpountz ahh, yea, thats my next step, I just wanted to get feedback on the approach, I can create an example usage
</comment><comment author="rjernst" created="2015-05-07T18:36:07Z" id="99971723">Can we make this logger easier to grab/create? I'm thinking e.g. in meta field mappers (which are not components) and I dont think we should need to pass this down through all the layers form the mappings module (as we do not for other loggers right now). Perhaps something like ESLoggerFactory, or maybe just another method on that? Maybe too the logger could just fail to initialize if the log level is set incorrectly for it?
</comment><comment author="spinscale" created="2015-05-20T12:53:17Z" id="103874012">@rjernst I dont think the logger should fail to initialize, because we can change the log levels using the cluster update settings API

Would it be sufficient in your use-case to just have another constructor for the `DeprecatedLogger` hat allows to be called like this?

``` java
DeprecationLogger deprecationLogger = new DeprecationLogger(this.getClass().getName());
```
</comment><comment author="spinscale" created="2015-05-22T06:56:52Z" id="104541300">created a follow up PR that adds documentation, tests and the possibility to use the `ESLoggerFactory` to obtain a logger. PR is at #11285 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java</file><file>core/src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java</file></files><comments><comment>ResourceWatcher: Rename settings to prevent watcher clash</comment></comments></commit><commit><files /><comments><comment>Documentation: Fix elasticsearch documentation build</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/component/AbstractComponent.java</file><file>src/main/java/org/elasticsearch/common/logging/DeprecationLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/ESLoggerFactory.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerFactory.java</file><file>src/main/java/org/elasticsearch/index/AbstractIndexComponent.java</file><file>src/main/java/org/elasticsearch/index/shard/AbstractIndexShardComponent.java</file><file>src/test/java/org/elasticsearch/common/logging/jdk/JDKESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file></files><comments><comment>Infra for deprecation logging</comment></comments></commit></commits></item><item><title>Fix NPE in PendingDelete#toString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11032</link><project id="" key="" /><description /><key id="73920435">11032</key><summary>Fix NPE in PendingDelete#toString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T09:44:10Z</created><updated>2015-05-29T17:05:35Z</updated><resolved>2015-05-07T10:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T09:51:42Z" id="99798466">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file></files><comments><comment>Fix NPE in PendingDelete#toString</comment><comment>closes #11032</comment></comments></commit></commits></item><item><title>Elastic Search Mapping issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11031</link><project id="" key="" /><description>curl -XPUT 10.206.72.108:9200/_template/demotest -d '{

```
"template": "logstash-jmeter-results-*",
"settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index.refresh_interval": "5s"
},
"mappings": {
    "logs": {
        "properties": {
             "elapsed": {
                "type": "integer"
            },
            "label": {
                "type": "string"
            },
            "responseCode": {
                "type": "integer"
            },
            "threadName": {
                "type": "string"
            },
            "success": {
                "type": "boolean"
            },
            "bytes": {
                "type": "integer"
            },
            "responseMessage": {
                "type": "string"
            },
            "Latency": {
                "type": "integer"
            }, 
            "dataType": {
                "type": "string"
            }    
        }
    }
}
```

}'

I am unable to take STAT from kibana to show min,max ,count for the integer fields

Elastic Search shows the following error

csearch.action.search.SearchRequest@1495de2a]
org.elasticsearch.search.SearchParseException: [logstash-jmeter-results-2015.05.07][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"facets":{"stats":{"statistical":{"field":"Latency"},"facet_filter":{"fquery":{"query":{"filtered":{"query":{"bool":{"should":[{"query_string":{"query":"_"}}]}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"from":1430989905359,"to":1430990205359}}}]}}}}}}},"stats__":{"statistical":{"field":"Latency"},"facet_filter":{"fquery":{"query":{"filtered":{"query":{"bool":{"should":[{"query_string":{"query":"*"}}]}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"from":1430989905359,"to":1430990205359}}}]}}}}}}}},"size":0}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:664)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:515)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:487)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:256)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [stats]: field [Latency] isn't a number field, but a string
        at org.elasticsearch.search.facet.statistical.StatisticalFacetParser.parse(StatisticalFacetParser.java:132)
        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:648)
        ... 9 more
</description><key id="73911527">11031</key><summary>Elastic Search Mapping issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sugansan</reporter><labels /><created>2015-05-07T09:18:31Z</created><updated>2015-05-08T05:56:11Z</updated><resolved>2015-05-07T22:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-07T22:37:49Z" id="100039550">The error is this:

&gt; Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [stats]: field [Latency] isn't a number field, but a string

This indicates the mappings are not what you think (perhaps you pass in `Latency` in a json string and that template has not always existed?). Please join us on https://discuss.elastic.co/ for troubleshooting help, we reserve github for confirmed bugs and feature requests.
</comment><comment author="sugansan" created="2015-05-08T05:56:11Z" id="100108840">Yes. But I already added Latency field to mapping template. This  is resolved . There was a conflict between my old indexing templates. I deleted the old ones and renamed the index pattern. Then my problem got resolved.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `percent_terms_to_match`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11030</link><project id="" key="" /><description>Users should use `minimum_should_match` instead.
</description><key id="73901438">11030</key><summary>Remove `percent_terms_to_match`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T08:45:21Z</created><updated>2015-06-06T16:01:50Z</updated><resolved>2015-05-07T12:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T09:43:36Z" id="99796399">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisTests.java</file></files><comments><comment>More Like This: remove percent_terms_to_match</comment></comments></commit></commits></item><item><title>Aggregation to allow users to perform simple arithmetic operations on histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11029</link><project id="" key="" /><description>This would be a parent reducer which takes as input multiple metrics inside a histogram. The reduce operation would, for each bucket, present the value of each of these metrics to a script whose result would be the aggregation's output for that bucket.

An example use cases for this would be to subtract two series and output the result.
</description><key id="73899736">11029</key><summary>Aggregation to allow users to perform simple arithmetic operations on histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T08:40:56Z</created><updated>2015-06-12T08:32:49Z</updated><resolved>2015-06-12T08:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/seriesarithmetic/SeriesArithmeticBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/seriesarithmetic/SeriesArithmeticParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/seriesarithmetic/SeriesArithmeticPipelineAggregator.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SeriesArithmeticTests.java</file></files><comments><comment>Aggregations: allow users to perform simple arithmetic operations on histogram aggregations</comment></comments></commit></commits></item><item><title>Remove the `top_children` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11028</link><project id="" key="" /><description>Relates to #11022
</description><key id="73887892">11028</key><summary>Remove the `top_children` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T08:03:14Z</created><updated>2015-06-06T17:40:50Z</updated><resolved>2015-05-10T14:42:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-07T15:28:36Z" id="99910257">LGTM
</comment><comment author="jpountz" created="2015-05-08T19:36:35Z" id="100336214">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove traverse functions from Mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11027</link><project id="" key="" /><description>The mapper listener abstractions for object and field mappers are used
to notify the mapper service of new fields, as well as collect
all object and field mappers through a set of traversal functions.

This change removes the traversal functions in favor of simple
iteration over subfields of a mapper.

Note there is a single test which fails currently, for a seemingly impossible reason.
`RecoveryWhileUnderLoadTests.recoverWhileRelocating` fails with a NPE when trying to get the thread local parse context inside DocumentParser....
</description><key id="73860615">11027</key><summary>Remove traverse functions from Mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-07T06:19:26Z</created><updated>2015-06-08T08:58:14Z</updated><resolved>2015-05-07T15:52:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-07T06:23:14Z" id="99735528">FWIW, this is the stack trace of the NPE:

```
java.lang.NullPointerException
        at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:78)
        at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:71)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:343)
        at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:492)
        at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:487)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.prepareIndexOperationOnPrimary(TransportShardReplicationOperationAction.java:1041)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.executeIndexRequestOnPrimary(TransportShardReplicationOperationAction.java:1071)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:169)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:568)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:434)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
```
</comment><comment author="rjernst" created="2015-05-07T06:44:24Z" id="99743354">Thanks @s1monw for the fix for the NPE! This was fixed in 00e9654.
</comment><comment author="jpountz" created="2015-05-07T07:10:30Z" id="99751220">LGTM, I like it much better!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/ObjectMapperListener.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file></files><comments><comment>Merge pull request #11027 from rjernst/pr/mapper-subfields</comment></comments></commit></commits></item><item><title>Feature to sort in completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11026</link><project id="" key="" /><description>Is it possible to sort the completion suggester results?? I am not finding any way out.
</description><key id="73846344">11026</key><summary>Feature to sort in completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">violaivy</reporter><labels /><created>2015-05-07T05:14:00Z</created><updated>2015-05-07T19:58:47Z</updated><resolved>2015-05-07T19:58:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-05-07T19:58:46Z" id="99997021">it is not possible to sort results (other than index-time weights) in completion suggester at the moment. Have a look at https://github.com/elastic/elasticsearch/issues/10746, it proposes query-time sorting with 'boosts'.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fielddata_fields query string parameter ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11025</link><project id="" key="" /><description>Specifying `fielddata_fields` on the query string seems to be ignored, although the [rest spec](https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/api/search.json#L45) lists it as a valid query parameter.

```
PUT /twitter/tweet/1
{
  "user": "gregmarzouka"
}
```

```
GET /twitter/_search?fielddata_fields=user
```

``` json
"hits": {
  "total": 1,
  "max_score": 1,
  "hits": [
     {
        "_index": "twitter",
        "_type": "tweet",
        "_id": "1",
        "_score": 1,
        "_source": {
           "user": "gregmarzouka"
        }
     }
  ]
}
```

Specifying it as part of the request body works as expected:

```
GET /twitter/_search
{
  "fielddata_fields": ["user"]
}
```

``` json
"hits": {
  "total": 1,
  "max_score": 1,
  "hits": [
     {
        "_index": "twitter",
        "_type": "tweet",
        "_id": "1",
        "_score": 1,
        "_source": {
           "user": "gregmarzouka"
        },
        "fields": {
           "user": [
              "gregmarzouka"
           ]
        }
     }
  ]
}
```
</description><key id="73812131">11025</key><summary>fielddata_fields query string parameter ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Search</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-05-07T02:38:36Z</created><updated>2015-05-27T15:00:32Z</updated><resolved>2015-05-27T15:00:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file></files><comments><comment>Search fix: fielddata_fields query string parameter was ignored.</comment><comment>The RestSearchAction did not parse the fielddata_fields parameter. Added test case and missing parser code.</comment></comments></commit></commits></item><item><title>any field that has email as the value always gets analyzed even though the raw mappings explicitly says not to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11024</link><project id="" key="" /><description>Hi. 

I am having this weird problem where my "email" field mapping looks like this: 

```
"email": {

    "type": "string",
    "norms": {
        "enabled": false
    },
    "fields": {
        "raw": {
            "type": "string",
            "index": "not_analyzed",
            "ignore_above": 256
        }
    }
}
```

but whenever I do analyze even on the raw field, it analyzes them

```
curl -XGET 'localhost:9201/284b7db0f42911e4ad921a72c26f06e1-master-index/_analyze?field=email.raw&amp;pretty' -d 'abc@pqr.com';
{
  "tokens" : [ {
    "token" : "abc",
    "start_offset" : 0,
    "end_offset" : 3,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  }, {
    "token" : "pqr.com",
    "start_offset" : 4,
    "end_offset" : 11,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  } ]
}
```

I am not sure if this is a bug or works as designed? Any insights will be appreciated !
</description><key id="73721770">11024</key><summary>any field that has email as the value always gets analyzed even though the raw mappings explicitly says not to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shivangshah</reporter><labels /><created>2015-05-06T20:05:44Z</created><updated>2015-05-06T20:10:08Z</updated><resolved>2015-05-06T20:10:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shivangshah" created="2015-05-06T20:10:07Z" id="99590282">nevermind .. closing this .. As soon as I pressed submit to this issue i realized it was a nested field .. DUH ! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix grammar and typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11023</link><project id="" key="" /><description>Added commas, capitalized "JSON" and "API", capitalized titles, etc.
</description><key id="73702288">11023</key><summary>Fix grammar and typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">josephwolnskipn</reporter><labels /><created>2015-05-06T18:35:48Z</created><updated>2015-05-07T19:51:04Z</updated><resolved>2015-05-07T19:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-07T19:51:02Z" id="99994899">Nice, thanks @josephwolnskipn - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix grammar and typos in percolate</comment></comments></commit></commits></item><item><title>Deprecate the `top_children` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11022</link><project id="" key="" /><description>Deprecate the `top_children` query in favour of the `has_child` query.

The `top_children` query isn't always faster than the `has_child` query and the `top_children` query is often inaccurate. The total hits and any aggregations in the same search request will likely be off. The `has_child` query performance has improved over time and will improve more when #6511 lands.
</description><key id="73691577">11022</key><summary>Deprecate the `top_children` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-05-06T17:56:05Z</created><updated>2015-06-06T17:40:55Z</updated><resolved>2015-05-07T07:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-06T18:22:19Z" id="99560147">LGTM
</comment><comment author="jpountz" created="2015-05-06T19:51:48Z" id="99585736">Do we want to have it removed in 2.0 or only deprecated?
</comment><comment author="martijnvg" created="2015-05-06T19:53:02Z" id="99585947">@jpountz My plan was to remove `top_children` from master when this PR gets merged.
</comment><comment author="jpountz" created="2015-05-06T19:53:22Z" id="99586016">Awesome :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Increase coverage of BaseQueryTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11021</link><project id="" key="" /><description>We are creating random queries for testing purposes in BaseQueryTestCase and discussed using more repetitions of the basic test cases (e.g. serialization, fromXContent parsing) for better coverage if the query has more options. Instead of using @Repeat annotation this change uses outer loop in each individual test method to recreate new test queries to archive this.
</description><key id="73680963">11021</key><summary>Query Refactoring: Increase coverage of BaseQueryTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-06T17:14:27Z</created><updated>2015-05-07T12:24:02Z</updated><resolved>2015-05-07T09:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-06T20:21:28Z" id="99594442">left a tiny comment, LGTM though
</comment><comment author="cbuescher" created="2015-05-07T09:03:55Z" id="99784609">@javanna thanks, will push this with the minor adjustment on the feature branch then.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file></files><comments><comment>Query Refactoring: Increase coverage of BaseQueryTestCase</comment></comments></commit></commits></item><item><title>Deep structured Document has issue when using term query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11020</link><project id="" key="" /><description>I created two documents. One with deep structure and the other just flat string document:
curl -XPOST http://localhost:9200/test/test/ -d '
{
  "foo" : {"bar":"val_WILL_NOT_SHOW", "txt" : "val_THIS_EITHER"}
}'

curl -XPOST http://localhost:9200/test/test/ -d '
{
  "action" : "login"
}'

When I query  them, the structured document CANNOT be returned:
curl -XGET 'http://localhost:9200/test/test/_count' -d '
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "foo.bar": "val_WILL_NOT_SHOW"
          }
        }
      ]
    }
  }
}'

The flat string document works fine:
curl -XGET 'http://localhost:9200/test/test/_count' -d '
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "action": "login"
          }
        }
      ]
    }
  }
}'
</description><key id="73679673">11020</key><summary>Deep structured Document has issue when using term query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">toyangxia</reporter><labels /><created>2015-05-06T17:08:46Z</created><updated>2015-05-06T20:20:14Z</updated><resolved>2015-05-06T20:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T20:20:13Z" id="99594045">I suspect your issue is not related to deep nesting but to the fact that you are using the `term` query on an `analyzed` string. You should either use the `match` query instead or index your string fields as `not_analyzed`. This reproduction works for me:

```
DELETE test 

PUT test 
{
  "mappings": {
    "test": {
      "properties": {
        "action": {
          "type": "string",
          "index": "not_analyzed"
        },
        "foo": {
          "properties": {
            "bar": {
              "type": "string",
              "index": "not_analyzed"
            },
            "txt": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

POST test/test
{
  "foo" : {"bar":"val_WILL_NOT_SHOW", "txt" : "val_THIS_EITHER"}
}

POST test/test
{
  "action" : "login"
}

GET /test/test/_count
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "foo.bar": "val_WILL_NOT_SHOW"
          }
        }
      ]
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TTL deletion failures are not logged</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11019</link><project id="" key="" /><description>The `IndicesTTLService` uses a bulk request to perform the deletions of expired documents. Sometimes these deletions could fail, but the service does not log or provide any indication that the deletions are failing.

The `onResponse` method could check to see if the response had any failures and if so log the failures with the details. Additionally, the `onFailure` method looks like it will just swallow the Throwable that caused the failure.
</description><key id="73678227">11019</key><summary>TTL deletion failures are not logged</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Logging</label><label>bug</label><label>enhancement</label></labels><created>2015-05-06T17:00:23Z</created><updated>2015-05-22T15:41:15Z</updated><resolved>2015-05-22T15:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file></files><comments><comment>Logging: Add logging for failed TTL purges</comment></comments></commit></commits></item><item><title>[TEST] don't check shard counter if there are still write operations …</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11018</link><project id="" key="" /><description>…ongoing after test

We added a check to see if the counter for write operations on IndexShard always reaches 0 after a test.
The assumption here is that after a test there are no ongoing write operations. 
However, tests that use ttl or rivers might trigger write operations after the test also.
Test that have ongoing write operations after the test and inherit from
ElasticsearchIntegrationTest should override beforeIndexDeletion() to avoid failures.
</description><key id="73671410">11018</key><summary>[TEST] don't check shard counter if there are still write operations …</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T16:30:39Z</created><updated>2015-05-07T04:33:03Z</updated><resolved>2015-05-07T04:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-06T18:46:52Z" id="99570822">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/river/RiverTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Merge pull request #11018 from brwe/counter-test</comment></comments></commit></commits></item><item><title>Partial shard failures rendered as non-structured exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11017</link><project id="" key="" /><description>```
DELETE * 

PUT test/test/1
{
  "foz": "bar"
}

GET _search?sort=foo
```

Rendered as a structured exception:

```
{
   "error": {
      "root_cause": [
         {
            "type": "search_parse_exception",
            "reason": "No mapping found for [foo] in order to sort on"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test",
            "node": "v04p8T35S1ySNdsF9GTl7w",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\"sort\":[{\"foo\":{}}]}]",
               "caused_by": {
                  "type": "search_parse_exception",
                  "reason": "No mapping found for [foo] in order to sort on"
               }
            }
         }
      ]
   },
   "status": 400
}
```

However, if one shard succeeds, then exceptions are no longer structured:

```
PUT test2/test/1
{
  "foo": "bar"
}

GET _search?sort=foo
```

Renders as:

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 5,
      "failed": 5,
      "failures": [
         {
            "index": "test",
            "shard": 0,
            "status": 400,
            "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\":[{\"foo\":{}}]}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
         },
         {
            "index": "test",
            "shard": 1,
            "status": 400,
            "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\":[{\"foo\":{}}]}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
         },
         {
            "index": "test",
            "shard": 2,
            "status": 400,
            "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\":[{\"foo\":{}}]}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
         },
         {
            "index": "test",
            "shard": 3,
            "status": 400,
            "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\":[{\"foo\":{}}]}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
         },
         {
            "index": "test",
            "shard": 4,
            "status": 400,
            "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\":[{\"foo\":{}}]}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
         }
      ]
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": [
         {
            "_index": "test2",
            "_type": "test",
            "_id": "1",
            "_score": null,
            "_source": {
               "foo": "bar"
            },
            "sort": [
               "bar"
            ]
         }
      ]
   }
}
```

Similarly, errors in bulk APIs are not structured:

```
PUT test/test/_bulk
{"index": {"_id": 1 }}
{}
{"index": {"_id": 1 ,"_version": 2}}
{}
```

Returns:

```
{
   "took": 60,
   "errors": true,
   "items": [
      {
         "index": {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "_version": 1,
            "_shards": {
               "total": 2,
               "successful": 1,
               "failed": 0
            },
            "status": 201
         }
      },
      {
         "index": {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "status": 409,
            "error": "[test][3][test][1]: version conflict, current [1], provided [2]"
         }
      }
   ]
}
```

Curiously, `_msearch` doesn't return the reason if all shards failed:

```
GET _msearch
{"index": "test"}
{"sort": "foo"}
{"index": "_all"}
{"sort": "foo"}
```

Returns:

```
{
   "responses": [
      {
         "error": "SearchPhaseExecutionException[all shards failed]"
      },
      {
         "took": 6,
         "timed_out": false,
         "_shards": {
            "total": 10,
            "successful": 5,
            "failed": 5,
            "failures": [
               {
                  "index": "test",
                  "shard": 0,
                  "status": 400,
                  "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\": \"foo\"}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
               },
               {
                  "index": "test",
                  "shard": 1,
                  "status": 400,
                  "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\": \"foo\"}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
               },
               {
                  "index": "test",
                  "shard": 2,
                  "status": 400,
                  "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\": \"foo\"}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
               },
               {
                  "index": "test",
                  "shard": 3,
                  "status": 400,
                  "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\": \"foo\"}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
               },
               {
                  "index": "test",
                  "shard": 4,
                  "status": 400,
                  "reason": "RemoteTransportException[[Frigga][inet[/192.168.2.183:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[Failed to parse source [{\"sort\": \"foo\"}]]; nested: SearchParseException[No mapping found for [foo] in order to sort on]; "
               }
            ]
         },
         "hits": {
            "total": 1,
            "max_score": null,
            "hits": [
               {
                  "_index": "test2",
                  "_type": "test",
                  "_id": "1",
                  "_score": null,
                  "_source": {
                     "foo": "bar"
                  },
                  "sort": [
                     "bar"
                  ]
               }
            ]
         }
      }
   ]
}
```
</description><key id="73665222">11017</key><summary>Partial shard failures rendered as non-structured exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label></labels><created>2015-05-06T16:05:33Z</created><updated>2015-05-18T08:02:23Z</updated><resolved>2015-05-18T08:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/action/ActionWriteResponse.java</file><file>src/main/java/org/elasticsearch/action/ShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/fieldstats/RestFieldStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/snapshots/Snapshot.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java</file></files><comments><comment>Implement toXContent on ShardOpertionFailureException</comment></comments></commit></commits></item><item><title>RPM/DEB packages: change permissions/ownership of config and bin plugins dirs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11016</link><project id="" key="" /><description>When installing elasticsearch with a RPM or DEB package, the configuration directory `/etc/elasticsearch` should be owned by user `root` and group `elasticsearch` with `750` permissions (`root` has full access, read/list access for users of `elasticsearch` group and nothing for others users)

When installing a plugin that contains configuration files like `/etc/elasticsearch/awesome-plugin/`, the plugin directory should be owned by user `root` and group `elasticsearch` with `750` permissions.

When installing a plugin that contains bin/ files like `/usr/share/elasticsearch/bin/awesome-plugin/`, the plugin directory and all its files should be owned by user `root` and group `elasticsearch` with `750` permissions (`root` has read/write/execute access, read/execute access for users of `elasticsearch` group and nothing for others users).

We also need to test how RPM/DEB packages handle permissions/ownership changes when upgrading a package.
</description><key id="73629601">11016</key><summary>RPM/DEB packages: change permissions/ownership of config and bin plugins dirs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0</label></labels><created>2015-05-06T13:57:07Z</created><updated>2015-12-16T10:58:23Z</updated><resolved>2015-10-14T14:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-06T16:08:38Z" id="99524970">@tlrx Why is root ownership needed? Is this to make sure the ES process (which is run as a user in the elasticsearch group) cannot modify these files? It just seems weird that someone trying to configure e.g. elasticsearch.yml would need to run as root/sudo? But maybe it is just weird to me. :)
</comment><comment author="rmuir" created="2015-05-06T16:10:23Z" id="99525419">&gt; It just seems weird that someone trying to configure e.g. elasticsearch.yml would need to run as root/sudo?

I don't think its wierd to require root/sudo to change configuration files in /etc at all.
</comment><comment author="nik9000" created="2015-05-06T16:19:11Z" id="99527489">&gt; I don't think its wierd to require root/sudo to change configuration files in /etc at all.

+1

/etc/elasticsearch/\* is owned by root with 444. World readable because why not and not world writable because its written by puppet.
</comment><comment author="tlrx" created="2015-05-06T20:27:02Z" id="99597087">@rjernst yes, that's the idea. There was a debate about this but I think it makes sense, specially when elasticsearch is installed with a rpm/deb package (where rpm/dpkg commands often require to be executed as root)
</comment><comment author="rjernst" created="2015-05-06T21:01:54Z" id="99609084">Ok thanks. LGTM
</comment><comment author="ael-code" created="2015-05-13T15:03:53Z" id="101706309">I've installed Elasticsearch 1.5.2 on debian jessy through official elasticsearch repo.
First time I've launched `systemctl start elasticsearch` the bootstrap failed because of denied permission to create `/usr/share/elasticsearch/data` folder.

Error log:

```
May 13 10:02:59 debian8VM elasticsearch[792]: - ElasticsearchIllegalStateException[Failed to created node environment]
May 13 10:02:59 debian8VM elasticsearch[792]: AccessDeniedException[/usr/share/elasticsearch/data]
May 13 10:02:59 debian8VM elasticsearch[792]: org.elasticsearch.ElasticsearchIllegalStateException: Failed to created node environment
May 13 10:02:59 debian8VM elasticsearch[792]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:162)
May 13 10:02:59 debian8VM elasticsearch[792]: at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
May 13 10:02:59 debian8VM elasticsearch[792]: at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
May 13 10:02:59 debian8VM elasticsearch[792]: at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
May 13 10:02:59 debian8VM elasticsearch[792]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
May 13 10:02:59 debian8VM elasticsearch[792]: Caused by: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/data
```

folder permissions just after package installation:

```
ls -lah /usr/share/elasticsearch
drwxr-xr-x   5 root          root          4.0K May 13 06:04 .
drwxr-xr-x 116 root          root          4.0K May 13 06:04 ..
drwxr-xr-x   2 root          root          4.0K May 13 06:04 bin
drwxr-xr-x   3 root          root          4.0K May 13 06:04 lib
-rw-r--r--   1 root          root           150 Apr 27 04:22 NOTICE.txt
drwxr-xr-x   2 elasticsearch elasticsearch 4.0K May 13 06:04 plugins
-rw-r--r--   1 root          root          8.3K Apr 27 04:22 README.textile
```

to solve the issue I've manually created the fodler:
`drwxr-xr-x   3 elasticsearch elasticsearch 4.0K May 13 10:14 data`

Do I need to open new issue or it is related to this one?
</comment><comment author="tlrx" created="2015-05-13T16:25:15Z" id="101736460">@ael-code thanks for reporting. I think you hit the same issue as #10938 which will be fixed with #10725.
</comment><comment author="javanna" created="2015-10-14T14:12:46Z" id="148062612">Implemented via #14017 , #14048 and  #14088
</comment><comment author="carnalim" created="2015-12-15T16:56:31Z" id="164826214">I dont think this issue is closed, on the latest build I just experienced this problem: 

```
From the install: [root@ip-10-94-146-130 bin]# ./plugin install file:///home/jmblnvr/shield/license-2.1.0.zip
-&gt; Installing from file:/home/jmblnvr/shield/license-2.1.0.zip...
Trying file:/home/jmblnvr/shield/license-2.1.0.zip ...
Downloading .DONE
Verifying file:/home/jmblnvr/shield/license-2.1.0.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed license into /usr/share/elasticsearch/plugins/license
[root@ip-10-94-146-130 bin]# service elasticsearch restart
Stopping elasticsearch:                                    [FAILED]
Starting elasticsearch: Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/plugins/license/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.newByteChannel(Files.java:407)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
    at java.nio.file.Files.newInputStream(Files.java:152)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:302)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
                                                           [FAILED]
```

From Var Log:

```
[root@ip-10-94-146-130 elasticsearch]# cat elasticsearch.log
[2015-12-15 11:44:24,525][INFO ][node                     ] [Valtorr] stopping ...
[2015-12-15 11:44:24,639][INFO ][node                     ] [Valtorr] stopped
[2015-12-15 11:44:24,641][INFO ][node                     ] [Valtorr] closing ...
[2015-12-15 11:44:24,647][INFO ][node                     ] [Valtorr] closed
[2015-12-15 11:44:26,097][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
[2015-12-15 11:44:26,314][INFO ][node                     ] [Agon] version[2.1.0], pid[21703], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-12-15 11:44:26,314][INFO ][node                     ] [Agon] initializing ...
[2015-12-15 11:44:26,318][ERROR][bootstrap                ] Exception
java.lang.IllegalStateException: Unable to initialize plugins
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:111)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/plugins/license/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.newByteChannel(Files.java:407)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
    at java.nio.file.Files.newInputStream(Files.java:152)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:302)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
    ... 6 more
```

Fixed with:
     chown -R root:elasticsearch /usr/share/elasticsearch
</comment><comment author="clintongormley" created="2015-12-16T10:58:23Z" id="165066809">I've just tried this on a fresh install and it works correctly.  I wonder if you had an existing directory with incorrect ownership?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java</file></files><comments><comment>Plugins: plugin script to set proper plugin bin dir attributes</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java</file></files><comments><comment>Plugins: plugin script to set proper plugin config dir attributes</comment></comments></commit><commit><files /><comments><comment>Packaging: change permissions/ownership of config dir</comment></comments></commit></commits></item><item><title>Adding Count Bucket Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11015</link><project id="" key="" /><description>Closes #11008
</description><key id="73625648">11015</key><summary>Adding Count Bucket Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-05-06T13:46:04Z</created><updated>2015-06-06T17:47:19Z</updated><resolved>2015-05-06T14:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T14:27:39Z" id="99488619">Should it be called `value_count` if we want to be consistent with aggs?
</comment><comment author="colings86" created="2015-05-06T14:48:52Z" id="99498028">The use case for this feels very weak and not useful at all so this PR will be closed until we have a more compelling use case for adding it in. Most of the applications of this aggregation would also be covered by finding the length of the buckets array in the original aggregation
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove Codehaus repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11014</link><project id="" key="" /><description>Codehaus announced they are shutting down their services: https://www.codehaus.org/

We should remove their repository from our pom as it could cause some errors an useless HTTP calls.

Related to #10939 

I think this should go to at least 1.5, 1.x and master.
</description><key id="73625323">11014</key><summary>Remove Codehaus repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T13:44:45Z</created><updated>2015-05-06T16:02:48Z</updated><resolved>2015-05-06T16:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-06T13:48:55Z" id="99466708">@dadoonet are we sure we are not pulling anything from it? did you clean your local repo and ran it without it to double check?
</comment><comment author="dadoonet" created="2015-05-06T13:50:19Z" id="99467640">@kimchy We should not depend on a SNAPSHOT library. But you're right I'm going to run this test to make sure of this. 
</comment><comment author="dadoonet" created="2015-05-06T14:16:08Z" id="99483554">@kimchy It looks good to me:

```
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
```
</comment><comment author="kimchy" created="2015-05-06T15:28:31Z" id="99514294">@dadoonet cool, I just wanted to double check, thanks for doing it!
</comment><comment author="kimchy" created="2015-05-06T15:48:01Z" id="99519269">sorry, forgot, LGTM
</comment><comment author="dadoonet" created="2015-05-06T16:02:15Z" id="99523555">Closed with 234716a1ef4f671c4d5928dff3d7d60c8152c784 in master, 93cf608dcbbfce7db88857a8a788c5f9de888649 in 1.x and 7bcf792b15f27954940e322a3b4b0d1383adf0a1 in 1.5
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding Sum Bucket Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11013</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find sum of the values  of a specified sub-metric (or the doc_count).

Closes #11007
</description><key id="73618460">11013</key><summary>Adding Sum Bucket Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T13:22:47Z</created><updated>2015-06-06T17:48:07Z</updated><resolved>2015-05-06T14:50:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T14:26:59Z" id="99487902">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/ReducerBuilders.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketReducer.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/SumBucketTests.java</file></files><comments><comment>Merge pull request #11013 from colings86/feature/11007</comment></comments></commit></commits></item><item><title>Input Json is not properly validated while indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11012</link><project id="" key="" /><description>Elasticsearch does not seem to consistently validate the syntaxt of the input json.

For instance the following (lacking a closing '}' ) returns 400:

&gt; curl -v -XPUT 'http://localhost:9200/a/data/1' -d '{"t" : {"a" : "b"}'

But the following (same thing but with a bunch of extra '}' ) goes through, and it is even able to fetch it back:

curl -v -XPUT 'http://localhost:9200/a/data/1' -d '{"t" : {"a" : "b"}}}}}}}}}}}}}}'

I tried this on 1.4.3 and 1.5.2. Both show the same behavior.
</description><key id="73608700">11012</key><summary>Input Json is not properly validated while indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehrdad-hassanabadi</reporter><labels /><created>2015-05-06T12:51:43Z</created><updated>2015-05-06T12:52:49Z</updated><resolved>2015-05-06T12:52:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-06T12:52:49Z" id="99440385">Closing as duplicate of #2315
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make modifying operations durable by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11011</link><project id="" key="" /><description>This commit makes create, update and delete operations on an index durable
by default. The user has the option to opt out to use async translog flushes
on a per-index basis by settings `index.translog.durability=request`.

Initial benchmarks running on SSDs have show that indexing is about 7% - 10% slower
with bulk indexing compared to async translog flushes. This change is orthogonal to
the transaction log sync interval and will only sync the transaction log if the operation
has not yet been concurrently synced. Ie. if multiple indexing requests are submitted and
one operations sync call already persists the operations of others only one sync call is executed.

Relates to #10933
</description><key id="73601864">11011</key><summary>Make modifying operations durable by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>PITA</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T12:29:13Z</created><updated>2015-05-25T16:12:41Z</updated><resolved>2015-05-07T08:25:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-06T19:14:38Z" id="99578142">@rmuir I pushed a new commit and replied to your comment
</comment><comment author="kimchy" created="2015-05-06T19:22:28Z" id="99579611">@s1monw +1, looks great. As a follow up to this change, we need to think about an index level setting for it (or maybe adapt the "sync" config to do this automatically when set to 0), and have more benchmarks on more adventurous environments so we can help explain the tradeoffs
</comment><comment author="s1monw" created="2015-05-06T19:38:09Z" id="99582683">@kimchy after thinking about this I think we should go only with an index level setting and drop the entire per request thing. The reason is that with per-request settings you need to change an application to change the value while with index level settings we can do this via the API which is more flexible and safer, opinions?
</comment><comment author="kimchy" created="2015-05-06T19:39:04Z" id="99583001">@s1monw that was my reasoning towards having an index level setting for it, I tend to prefer it, we can always add per request later if needed
</comment><comment author="s1monw" created="2015-05-06T19:43:06Z" id="99584000">ok moving towards index level setting
</comment><comment author="rmuir" created="2015-05-06T20:43:28Z" id="99603481">thanks for improving the loop with asserts/comments!
</comment><comment author="s1monw" created="2015-05-06T20:49:10Z" id="99604964">@kimchy I think it's ready now with an index level setting
</comment><comment author="kimchy" created="2015-05-06T23:25:32Z" id="99642601">@s1monw this looks great
</comment><comment author="jpountz" created="2015-05-07T08:57:31Z" id="99781340">Should we document this new setting and add a note to the resiliency status page?
</comment><comment author="s1monw" created="2015-05-07T09:04:10Z" id="99784704">@jpountz I don't think we are done yet. I want to open a documentation issue once we are setted with all thte things - the more important issue where we fix the translog corruption is still coming
</comment><comment author="jpountz" created="2015-05-07T09:05:20Z" id="99785108">Sounds great. Just wanted to make sure it doesn't get forgotten. :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/io/stream/ByteBufferStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngineFactory.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamInput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReader.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/ImmutableTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReaderBase.java</file><file>src/main/java/org/elasticsearch/index/translog/MultiSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/TruncatedTranslogException.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>src/test/java/org/elasticsearch/transport/TransportMessageTests.java</file></files><comments><comment>Merge pull request #11143 from elastic/feature/translog_checkpoints</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/io/stream/ByteBufferStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamInput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReader.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/ImmutableTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReaderBase.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/TruncatedTranslogException.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>src/test/java/org/elasticsearch/transport/TransportMessageTests.java</file></files><comments><comment>Add translog checkpoints to prevent translog corruption</comment></comments></commit></commits></item><item><title>Adding Average Bucket Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11010</link><project id="" key="" /><description>Also includes changes to the other bucket metric aggregations to share code

Closes #11006 
</description><key id="73594906">11010</key><summary>Adding Average Bucket Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T11:55:27Z</created><updated>2015-05-06T13:05:19Z</updated><resolved>2015-05-06T12:59:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T12:04:36Z" id="99424961">This looks good. I just left a comment about a method name that confused me, I think we also should have javadocs on the new abstractions that you added.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation to calculate the cardinality of a metric in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11009</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find the cardinality of the values of a specified sub-metric (or the doc_count).
</description><key id="73577256">11009</key><summary>Aggregation to calculate the cardinality of a metric in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-05-06T10:01:30Z</created><updated>2015-06-06T17:48:29Z</updated><resolved>2015-05-06T14:52:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-06T14:52:54Z" id="99500020">The use case for this feels very weak and not useful at all so this PR will be closed until we have a more compelling use case for adding it in.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation to calculate the number of buckets in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11008</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and return the number of buckets in that aggregation.
</description><key id="73576884">11008</key><summary>Aggregation to calculate the number of buckets in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-05-06T09:59:34Z</created><updated>2016-01-07T12:39:02Z</updated><resolved>2015-05-06T14:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-06T14:52:30Z" id="99499822">The use case for this feels very weak and not useful at all so this PR will be closed until we have a more compelling use case for adding it in. Most of the applications of this aggregation would also be covered by finding the length of the buckets array in the original aggregation
</comment><comment author="acarstoiu" created="2016-01-07T12:39:02Z" id="169652369">Here is a simple use case: you don't want to get all the buckets over the network, you just want their count. You could simply instruct _Elasticsearch_ to drop the results of basic aggregations and keep the pipeline aggs - see #15823.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation to calculate the sum of the values of the buckets in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11007</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find sum of the values  of a specified sub-metric (or the doc_count).
</description><key id="73576597">11007</key><summary>Aggregation to calculate the sum of the values of the buckets in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T09:57:45Z</created><updated>2015-05-06T14:50:15Z</updated><resolved>2015-05-06T14:50:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/ReducerBuilders.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/sum/SumBucketReducer.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/SumBucketTests.java</file></files><comments><comment>Aggregations: Adding Sum Bucket Aggregation</comment></comments></commit></commits></item><item><title>Aggregation to calculate the (mean) average value of the buckets in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11006</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find the (mean) average  value of a specified sub-metric (or the doc_count). 
</description><key id="73576467">11006</key><summary>Aggregation to calculate the (mean) average value of the buckets in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T09:56:47Z</created><updated>2015-05-06T12:59:41Z</updated><resolved>2015-05-06T12:59:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/ReducerBuilders.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/BucketMetricsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/BucketMetricsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/BucketMetricsReducer.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/avg/AvgBucketBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/avg/AvgBucketParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/avg/AvgBucketReducer.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/max/MaxBucketBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/max/MaxBucketParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/max/MaxBucketReducer.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/min/MinBucketBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/min/MinBucketParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/bucketmetrics/min/MinBucketReducer.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/AvgBucketTests.java</file></files><comments><comment>Aggregations: Adding Average Bucket Aggregation</comment></comments></commit></commits></item><item><title>Refactors SpanTermQueryBuilder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11005</link><project id="" key="" /><description>Refactors SpanTermQueryBuilder analogous to TermQueryBuilder. Factors common code between the two into separate classes.

Relates to #10217 

This goes against the feature/query-refactoring branch.
</description><key id="73570955">11005</key><summary>Refactors SpanTermQueryBuilder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label></labels><created>2015-05-06T09:31:40Z</created><updated>2015-05-18T07:48:36Z</updated><resolved>2015-05-18T07:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-07T16:04:47Z" id="99922971">Left some super minor comments, I like the way the similarities in TermQuery and SpanTermQuery are factored out here but still I'd like to hear what @javanna or @dakrone think about this, especially the use of generics.
</comment><comment author="MaineC" created="2015-05-08T07:29:59Z" id="100132682">@cbuescher Thanks for your comments - changed the code accordingly.
</comment><comment author="cbuescher" created="2015-05-08T08:10:23Z" id="100145020">Thanks, LGTM, but lets see if anybody else has an opinion.
</comment><comment author="javanna" created="2015-05-08T12:03:04Z" id="100211373">left some comments
</comment><comment author="MaineC" created="2015-05-11T09:47:28Z" id="100837048">@javanna Thanks for your comments. Tried to incorporate them in the changes.
</comment><comment author="javanna" created="2015-05-13T09:18:34Z" id="101585994">left a very minor comment, LGTM though, please push ;)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11005 from MaineC/feature/span-term-query-refactoring</comment></comments></commit></commits></item><item><title>can't recover index from snapshot to multi-node cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11004</link><project id="" key="" /><description>I had one node "cluster" running 1.3.4 made a snapshot and tried to recover on 3 data node cluster (version 1.5.2).

I copied snapshot to master node of new cluster and run recovery, most of the indexes perfectly fine, but some had this issue:
Index has 2 primary shards one of them is recovered on master node, second one is trying to recover in any but master node, and it keeps looping between to other nodes trying to initialize it but it never goes to master where the recovery files are.

what I notice is that recovery data of that index is transferred to one of non master nodes, but only first shard documents are there second shard documents are not transferred.

logs keeps outputting same message which I think is related to this #9433 

error logs from non master node:

```
[2015-05-06 08:57:24,192][WARN ][indices.cluster          ] [esdn0001.dev.localdomain] [[phoenix_basket_20140910][0]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [phoenix_basket_20140910][0] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:162)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [phoenix_basket_20140910][0] restore failed
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:135)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:109)
        ... 3 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [phoenix_basket_20140910][0] failed to restore snapshot [snapshot_1]
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:164)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:126)
        ... 4 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [phoenix_basket_20140910][0] failed to read shard snapshot file
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$Context.loadSnapshot(BlobStoreIndexShardRepository.java:318)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:710)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:162)
        ... 5 more
Caused by: java.io.FileNotFoundException: /var/log/elasticsearch/snapshots/indices/phoenix_basket_20140910/0/snapshot-snapshot_1 (No such file or directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146)
        at org.elasticsearch.common.blobstore.fs.FsBlobContainer.openInput(FsBlobContainer.java:87)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$Context.loadSnapshot(BlobStoreIndexShardRepository.java:315)
        ... 7 more

```

If you need any additional data let me know.

work around would be copy snapshot to all nodes then it recovers it correctly
</description><key id="73570542">11004</key><summary>can't recover index from snapshot to multi-node cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">minde-eagleeye</reporter><labels /><created>2015-05-06T09:29:32Z</created><updated>2015-05-06T15:19:48Z</updated><resolved>2015-05-06T15:19:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-05-06T15:04:11Z" id="99504771">@minde-eagleeye "The path specified in the location parameter should point to the same location in the shared filesystem and be accessible on **all data and master nodes**". Please see http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository for more information. Please use our mailing list or forums at http://discuss.elastic.co if you have any additional questions.
</comment><comment author="minde-eagleeye" created="2015-05-06T15:19:47Z" id="99510764">@imotov Thank you I missed the "**shared filesystem**" bit
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the MLT API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11003</link><project id="" key="" /><description>Removes the More Like This API, users should now use the More Like This query.
The MLT API tests were converted to their query equivalent. Also some clean
ups in MLT tests.

Closes #10736
</description><key id="73566771">11003</key><summary>Remove the MLT API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T09:15:53Z</created><updated>2015-06-06T16:02:11Z</updated><resolved>2015-05-06T16:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T12:23:37Z" id="99429213">LGTM I like the diff stats :)
</comment><comment author="s1monw" created="2015-05-06T12:25:44Z" id="99430373">:punch: nice!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/package-info.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/mlt/RestMoreLikeThisAction.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/mlt/MoreLikeThisRequestTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/morelikethis/XMoreLikeThisTests.java</file><file>src/test/java/org/elasticsearch/search/morelikethis/ItemSerializationTests.java</file><file>src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisTests.java</file></files><comments><comment>More Like This: removal of the MLT API</comment></comments></commit></commits></item><item><title>`numeric_resolution` should only apply to dates provided as numbers.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11002</link><project id="" key="" /><description>Close #10995
</description><key id="73564273">11002</key><summary>`numeric_resolution` should only apply to dates provided as numbers.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T09:03:24Z</created><updated>2015-06-07T18:34:16Z</updated><resolved>2015-05-07T07:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T09:05:51Z" id="99390257">Not adding version numbers as this bug has not been released yet.
</comment><comment author="rjernst" created="2015-05-06T15:59:00Z" id="99522653">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Merge pull request #11002 from jpountz/fix/numeric_resolution_string_date</comment></comments></commit></commits></item><item><title>Respect http_proxy environment variable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11001</link><project id="" key="" /><description>I am trying to install elasticsearch in a corporate environment where all the machines are behind a proxy. The plugin installer does not seem to respect the `http_proxy` variable which is very different from virtually all other programs.  I've tried many iterations of smth like `bin/plugin -Dproxy_host=http://user:pass@host -D proxy_port=8080` , `bin/plugin -Dproxy_host=user:pass@host -D proxy_port=8080`, `bin/plugin -Dproxy_host=http://user:pass@host:8080 -D proxy_port=`, but none of them seem to work.

It'd be _so_ much easier if it just respected the environment variable just like everybody else does.
</description><key id="73564199">11001</key><summary>Respect http_proxy environment variable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">muelli</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2015-05-06T09:02:57Z</created><updated>2016-02-13T11:28:28Z</updated><resolved>2016-02-13T11:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-06T16:11:06Z" id="99525603">@muelli the `plugin` script does support proxies, see #7150 and the [plugin documentation](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-plugins.html#_proxy_settings). You need to specify `proxyHost` and `proxyPort` (no underscore).

You can also use it from an environment variable via `JAVA_OPTS`.
</comment><comment author="muelli" created="2015-05-06T21:59:14Z" id="99621433">Fair enough. So far, I haven't been able to make it work with the corporate proxy which requires authentication.

Again: If it would respect the `http_proxy` environment variable, just like everybody else, my life would be a tiny bit happier.
</comment><comment author="muelli" created="2015-07-06T08:26:19Z" id="118773293">FWIW: I can't make it work with proxies requiring authentication.  I've tried `bin/plugin -i jdbc -DproxyHost=http://username:password@proxy.acme.corp -D8080`.  But it fails with "failed to download out of all possible locations..."

However, `wget`, `curl`, and friends work just fine.  So my (annoying) workaround is to download the plugin first and then use a file:// URI to install the plugin.
</comment><comment author="spinscale" created="2015-07-08T08:27:19Z" id="119491080">@muelli I guess the `-D8080` is just a typo on your side and you meant `proxyPort`?
</comment><comment author="muelli" created="2015-07-08T08:37:30Z" id="119497468">eh. yes. sorry.  I couldn't copy and paste easily as it was on a different machine.
</comment><comment author="spinscale" created="2015-07-08T08:38:59Z" id="119498515">all right, I will take a look, write some tests and also it makes sense to add a CLI option instead of having to set properties.. thx for the report!
</comment><comment author="spinscale" created="2015-07-10T12:47:30Z" id="120401708">@muelli any chance you can test this in your environment maybe?
</comment><comment author="muelli" created="2015-07-10T13:05:37Z" id="120405552">hm. I guess I need to compile Elasticsearch with that patch.  I've never done that and I have other priorities atm, so it might take a while.  Or is there a binary available that I can replace my bin/plugin with?

Also, I noted that the proposed change does not respect the `http_proxy` environment variable.  Is there a specific reason other than not having gotten around to do it?  That environment variable is surely a standard variable on GNU/Linux system and respecting it dramatically increases the compatibility with those systems.
</comment><comment author="muelli" created="2015-09-14T10:01:43Z" id="140025846">I don't think that https://github.com/spinscale/elasticsearch/commit/9fa5324a752119076628abcceb27c5aab6dd4606 closes this bug, because this bug report is about respecting `http_proxy` environment variable. That commit does not achieve that.
</comment><comment author="hulu1522" created="2016-02-11T23:32:42Z" id="183103933">I agree... the plugin script should honor any `https_proxy` or `http_proxy` environment settings automatically.  That is why it's there for programs to see.
</comment><comment author="dadoonet" created="2016-02-12T12:02:24Z" id="183296352">@muelli @hulu1522 Thanks for the feedback. I opened #16633 to discuss about it.
</comment><comment author="clintongormley" created="2016-02-13T11:28:28Z" id="183646694">Closing in favour of #16633
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation for Index Modules &gt; Store does not explain all options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11000</link><project id="" key="" /><description>The documentation at http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-indices.html#throttling mentions that the settings for `indices.store.throttle.type` are `merge`, `none`, `all` and refers to http://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html for further detail. However, the referenced page does not explain all options, most notably `all` is missing. The referenced page also continues to say 

```
(...) it can be set as well using the index.store.throttle.type, and index.store.throttle.max_bytes_per_sec.  
The default value for the type is `node` (...)
```

adding yet another unexplained option (`node`).
</description><key id="73551875">11000</key><summary>Documentation for Index Modules &gt; Store does not explain all options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Xylakant</reporter><labels /><created>2015-05-06T08:15:47Z</created><updated>2015-05-08T07:54:28Z</updated><resolved>2015-05-08T07:54:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-07T19:36:41Z" id="99991371">I've improved the docs - hope this helps.
</comment><comment author="Xylakant" created="2015-05-07T19:43:43Z" id="99993377">I'm still confused: What does `indices.store.throttle.type: none` do?

And what are the valid values for `index.store.throttle.type` and what do they do? `node` is mentions, but the exact implications are unclear to me.
</comment><comment author="clintongormley" created="2015-05-07T19:48:16Z" id="99994305">`none` turns off throttling.  `index.store.throttle.type` can be set to `none`, `merge`, `all`, or if not set then it uses the node level setting (not an index level setting).

note: these settings have been removed from the docs in master, because we don't think you should be setting them :)
</comment><comment author="Xylakant" created="2015-05-07T19:53:28Z" id="99995493">what's the replacement on the node level
</comment><comment author="clintongormley" created="2015-05-07T19:54:39Z" id="99995770">What do you mean?
</comment><comment author="Xylakant" created="2015-05-07T20:29:53Z" id="100009606">sorry, I misread your previous comment. So none turns off store level throttling as in "unlimited"?
</comment><comment author="clintongormley" created="2015-05-08T07:54:25Z" id="100139798">yes, "no throttling" :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tighten up script security more</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10999</link><project id="" key="" /><description>- make the special codebase /_untrusted instead of /groovy/script,
  so that e.g. scripting plugins can use this if they want.
- give _untrusted its own section in policy file, we do this by
  giving it a separate file (as to not have to muck around with
  codebase URLs and break plugins and stuff).
- protect ES and lucene packages and only grant the ones we need
  to scripts. (yes its 6 and too many, but better than 500)
- other minor cleanups
</description><key id="73551278">10999</key><summary>Tighten up script security more</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T08:13:18Z</created><updated>2015-06-08T12:58:09Z</updated><resolved>2015-05-08T14:09:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-08T14:09:01Z" id="100242553">Wont fix. Problem is these scripting apis in es need some maturity before i can lock them down. They need to be simpler with more encapsulation, and non scripting code should not be using them. Otherwise the security would be equally complicated, and i dont want that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation: Mention RPM repo does not work with older distributions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10998</link><project id="" key="" /><description>Getting this to work would be a lot of work (creating two different
repositories, having another GPG key, integrating this into our build).

Closes #6498
</description><key id="73546903">10998</key><summary>Documentation: Mention RPM repo does not work with older distributions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label></labels><created>2015-05-06T07:51:40Z</created><updated>2015-05-07T06:20:58Z</updated><resolved>2015-05-07T06:20:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-06T16:21:02Z" id="99528047">+1
</comment><comment author="tlrx" created="2015-05-06T20:19:29Z" id="99593764">+1 with Centos =&gt; CentOS :) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Load fielddata on behalf of scripts.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10997</link><project id="" key="" /><description>If we have to do the one-time loading of fieldata, it requires
more permissions than groovy scripts currently have (zero). This
is because of RamUsageEstimator reflection and so on in PagedBytes.

GroovySecurityTests only test a numeric field, so add a string field
to the test (so pagedbytes fielddata gets created etc).
</description><key id="73509246">10997</key><summary>Load fielddata on behalf of scripts.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Fielddata</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-06T04:19:17Z</created><updated>2015-06-07T12:47:58Z</updated><resolved>2015-05-06T04:26:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-06T04:24:20Z" id="99317619">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/lookup/LeafDocLookup.java</file><file>src/test/java/org/elasticsearch/script/GroovySecurityTests.java</file></files><comments><comment>Merge pull request #10997 from rmuir/groovy_better</comment></comments></commit></commits></item><item><title>copy_to does not work with multi fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10996</link><project id="" key="" /><description>This previously worked, but was broken with document parser refactoring in #10802. We could fix this in a simple way, by handling multi fields in the doc parsing as well (so the multi field can be checked for a copyTo). 

However, the mapper attachment plugin currently relies on the ability to generate the values for multifields separately (not just copying the original value). My current thought is the mapper attachment should basically attach these values to the parser context, so that when multi fields are handled in the doc parser, it finds the value there for whatever subfields are present.
</description><key id="73454403">10996</key><summary>copy_to does not work with multi fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label></labels><created>2015-05-05T22:43:57Z</created><updated>2015-05-08T07:36:42Z</updated><resolved>2015-05-08T07:36:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="konradkonrad" created="2015-05-07T09:11:47Z" id="99786689">Is this related to #8483 or would your proposal tackle that specific problem, too?
</comment><comment author="rjernst" created="2015-05-07T16:28:33Z" id="99929137">@konradkonrad I do not think this is related. This bug was introduced in a very recent refactoring, only on master, and only happens when copy_to is used _within_ a multi field (which it doesn't look like that user does).
</comment><comment author="clintongormley" created="2015-05-08T07:31:16Z" id="100132946">@rjernst I'm not sure that it ever makes sense to use `copy_to` in a multi-field sub-field?  It can be used on the main field only, to the same effect.  There is similar confusion with using `include_in_all` on sub-fields.  Users think that the tokens from the sub-fields end up being added to `_all`, when really it is just the string value of the main field which is passed to `_all`.
</comment><comment author="rjernst" created="2015-05-08T07:36:13Z" id="100133842">@clintongormley Great! Then I will find a way to restrict multi fields to `copy_to` cannot be specified. I've created #11053 to track that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Date Bug When Using numeric_resolution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10995</link><project id="" key="" /><description>When I switch numeric_resolution to use seconds on a date field and input a date as "yyyy-MM-ddThh:mm:ssZ" it appears to calculate the incorrect number of milliseconds stored internally.

As an example, using the date "2015-04-28T04:02:07Z"  I was seeing the following number of milliseconds stored internally:

Using milliseconds: 1.4301...E12
Using seconds: 1.4301...E15

I would expect these to be the same since they are both ultimately stored as milliseconds.
</description><key id="73431580">10995</key><summary>Mapping: Date Bug When Using numeric_resolution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-05-05T21:07:27Z</created><updated>2015-05-07T07:39:59Z</updated><resolved>2015-05-07T07:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-06T08:53:22Z" id="99386353">Oh, this is a bad bug indeed. Numeric resolution should only be applied to dates provided as numbers, not as formatted dates.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Mappings: `numeric_resolution` should only apply to dates provided as numbers.</comment></comments></commit></commits></item><item><title>Minor TimeZone Fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10994</link><project id="" key="" /><description>Changed DateMethodFunctionValues to use UTC instead of GMT.  Should not affect what is currently there, but is correct and consistent.
</description><key id="73430386">10994</key><summary>Minor TimeZone Fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T21:00:09Z</created><updated>2015-06-08T15:44:08Z</updated><resolved>2015-05-05T21:35:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-05T21:16:16Z" id="99224572">LGTM
</comment><comment author="jdconrad" created="2015-05-05T21:35:51Z" id="99229882">@rjernst Thank you for the review.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java</file></files><comments><comment>Scripting: Minor TimeZone Fix</comment></comments></commit></commits></item><item><title>Use buffered translog type also when sync is set to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10993</link><project id="" key="" /><description>When settings sync to 0, we benefit from using the buffered type, no need to change to simple, since we get a chance to fsync multiple operations (for that single operation) and not have to sync for the other ones before returning each one
</description><key id="73388226">10993</key><summary>Use buffered translog type also when sync is set to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T18:18:26Z</created><updated>2015-05-29T18:05:48Z</updated><resolved>2015-05-05T21:06:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-05T19:17:01Z" id="99189644">LGTM
</comment><comment author="jpountz" created="2015-05-05T20:55:37Z" id="99219233">With that change in, I'm wondering if it would still make sense to support `TranslogType.Type.SIMPLE`? It would only be used if a user explicitely opts in for it using the `index.translog.fs.type` setting.
</comment><comment author="kimchy" created="2015-05-05T21:04:55Z" id="99221216">@jpountz yea, there is a case for it with making sure each operation ends up written to disk even with higher sync interval. In the long term, we might not need it if we change how things work, but I suggest we keep it for now
</comment><comment author="jpountz" created="2015-05-05T21:05:47Z" id="99221368">OK, just wanted to check. Thanks for the explanation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file></files><comments><comment>Merge pull request #10993 from kimchy/sync_interval_not_to_control_type</comment></comments></commit></commits></item><item><title>Query templates don't play nice with Expression script queries requiring parametrs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10992</link><project id="" key="" /><description>A query template containing a clause like this:

```
                                         "script": {
                                            "script": "doc['myfield'].value &lt; param1 - 1000*60*60*24*5",
                                            "lang": "expression",
                                            "params": {
                                              "param1": "{{foo}}"
                                            }
                                          }
```

Doesn't work, executing it with a numeric constant as a parameter will throw saying the parameter has to be numeric (and it is).
</description><key id="73373239">10992</key><summary>Query templates don't play nice with Expression script queries requiring parametrs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Search Templates</label><label>bug</label></labels><created>2015-05-05T17:17:59Z</created><updated>2016-04-14T12:21:00Z</updated><resolved>2015-06-03T13:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-06T08:26:38Z" id="99377264">I think the problem is query templates can really only deal with string parameters, because in order to fill in template parameters can only be described in strings. I think what we need to do is "pass through" the type, so running your query template with this should work:

```
GET /_search
{
    "query": {
        "template": {
            "query": ..., 
            "params" : {
                "foo" : 10
            }
        }
    }
}
```

However, at a quick glance it doesn't seem like mustache has this ability. @uboness any ideas?
</comment><comment author="clintongormley" created="2015-05-08T12:07:27Z" id="100212571">This works as long as the template is saved as a file, eg: save this as `config/scripts/test.mustache`:

```
{
  "script_fields": {
    "FIELD": {
      "script": "param + 123",
      "lang": "expression",
      "params": {
        "param": {{param}}
      }
    }
  }
}
```

Then this request works:

```
GET _search/template
{
  "template": "test",
  "params": {
    "param": 10
  }
}
```

It also works when storing the template in .scripts index as a string:

```
POST _search/template/x
{
  "template": "{  \"script_fields\": {    \"FIELD\": {      \"script\": \"param + 123\",      \"lang\": \"expression\",      \"params\": {        \"param\": {{param}}      }    }  }}"
}
```

However, passing the same script as an inline string doesn't:

```
GET _search/template
{
  "template": "{  \"script_fields\": {    \"FIELD\": {      \"script\": \"param + 123\",      \"lang\": \"expression\",      \"params\": {        \"param\": {{param}}      }    }  }}",
  "params": {
    "param": 1
  }
}
```

Throws this exception:

```
Failed to parse template]; nested: JsonParseException[Unexpected character ('{' (code 123)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name
```
</comment><comment author="clintongormley" created="2015-05-08T12:12:11Z" id="100213270">@MaineC is this related to #8393?
</comment><comment author="colings86" created="2015-06-03T13:50:12Z" id="108427901">@clintongormley I just tested your steps on master and the new script API logic has resolved the issue, inline, file and index templates work with your template content using both the new API and the old one.
</comment><comment author="gkozyryatskyy" created="2016-04-07T12:55:38Z" id="206881726">@colings86 
This 

`curl -XPOST localhost:9200/_scripts/mustache/scr1 -d '{"template" : { "size" : {{size}} }}'`

still throws an exception

`{
   "error":{
      "root_cause":[
         {
            "type":"illegal_argument_exception",
            "reason":"failed to parse template script"
         }
      ],
      "type":"illegal_argument_exception",
      "reason":"failed to parse template script",
      "caused_by":{
         "type":"json_parse_exception",
         "reason":"Unexpected character ('{' (code 123)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@8133db3; line: 1, column: 28]"
      }
   },
   "status":400
}
`

elasticsearch 2.2.0
</comment><comment author="clintongormley" created="2016-04-07T16:19:34Z" id="206977893">@gkozyryatskyy your exception has nothing to do with this issue. You are passing invalid JSON, so you need to pass your template as a string instead.
</comment><comment author="gkozyryatskyy" created="2016-04-14T12:21:00Z" id="209911906">@clintongormley Thx for you reply. I understand the problem, but is there any other api to pass templates with out json validation? Now Im using java api PutIndexedScriptRequestBuilder.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10991</link><project id="" key="" /><description /><key id="73368905">10991</key><summary>Fix typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">aqnouch</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-05-05T16:58:03Z</created><updated>2015-06-19T15:45:36Z</updated><resolved>2015-06-19T15:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-07T18:50:50Z" id="99976873">Hi @maqnouch 

Thanks for the PR. Please note my comments above, and push a fixed version? And please could I ask you to sign the CLA?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="aqnouch" created="2015-05-07T20:21:50Z" id="100005918">done thank you for your reply.
</comment><comment author="clintongormley" created="2015-05-08T07:53:57Z" id="100139744">@maqnouch please push a new commit once you have addressed my comments above.
</comment><comment author="clintongormley" created="2015-06-19T15:45:34Z" id="113553522">no update here. closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove double exception handling that causes false replica failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10990</link><project id="" key="" /><description>we already fail the shard in the `onFailure` method if the replica
operation barfs. This additional check has been added lately that
bypasses the clusterstate observer which causes replicas to fail
if the mappings are not yet present.
</description><key id="73355098">10990</key><summary>Remove double exception handling that causes false replica failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T16:01:49Z</created><updated>2015-06-07T18:40:25Z</updated><resolved>2015-05-05T16:05:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-05T16:02:17Z" id="99125377">@jpountz @brwe  ^^
</comment><comment author="jpountz" created="2015-05-05T16:02:47Z" id="99125481">LGTM
</comment><comment author="brwe" created="2015-05-05T16:03:10Z" id="99125561">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure JNA is fully loaded when its available, but don't fail its not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10989</link><project id="" key="" /><description>Note: core tests will still fail if JNA/sigar is e.g. misconfigured today. This is intentional.

But we don't need to make plugins tests fail, or even fail at all at runtime if stuff is not available.
</description><key id="73352815">10989</key><summary>Ensure JNA is fully loaded when its available, but don't fail its not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T15:52:21Z</created><updated>2015-06-08T13:13:20Z</updated><resolved>2015-05-05T16:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-05T16:00:24Z" id="99124372">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file></files><comments><comment>Merge pull request #10989 from rmuir/jna_optional</comment></comments></commit></commits></item><item><title>Remove Translog interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10988</link><project id="" key="" /><description>We only have one implementation of this interface which makes not much
sense. This commit removes the abstraction.
</description><key id="73348843">10988</key><summary>Remove Translog interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T15:36:09Z</created><updated>2015-06-06T16:06:13Z</updated><resolved>2015-05-05T15:50:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-05T15:39:06Z" id="99119500">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IllegalStateException on indexing with completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10987</link><project id="" key="" /><description>I'm getting a `java.lang.IllegalStateException: from state (0) already had transitions added` exception if i try to index certain documents with a mapping that has fields with `"type": "completion"`.

This happens at least in Version 1.5.1 and 1.5.2 and seems to work correctly up to 1.3.9.

First i create a new index:
`curl -XPUT localhost:9200/ses_firma/`
Then i add the mapping with:
`curl -s -XPUT localhost:9200/ses_firma/_mapping/firma --data-binary @MappingFirma.txt`

```
{
    "firma": {
        "_source": {
            "enabled": false
        },
        "_id": {
            "path": "oid",
            "store": true
        },
        "properties": {
            "adressen": {
                "properties": {
                    "ort": {
                        "type": "string"
                    },
                    "postleitzahl": {
                        "type": "string"
                    },
                    "strasse": {
                        "type": "string"
                    },
                    "bundesland": {
                        "include_in_all": false,
                        "properties": {
                            "name": {
                                "include_in_all": false,
                                "type": "string"
                            }
                        }
                    },
                    "postfach": {
                        "type": "string"
                    },
                    "land": {
                        "include_in_all": false,
                        "properties": {
                            "iso": {
                                "include_in_all": false,
                                "type": "string"
                            },
                            "archiv": {
                                "type": "boolean"
                            },
                            "name": {
                                "include_in_all": false,
                                "type": "string"
                            }
                        }
                    },
                    "oid": {
                        "type": "string"
                    },
                    "postfachplz": {
                        "type": "string"
                    }
                }
            },
            "bemerkung": {
                "type": "string",
                "fields": {
                    "suggest": {
                        "max_input_length": 50,
                        "payloads": false,
                        "analyzer": "simple",
                        "context": {
                            "type_context": {
                                "path": "_type",
                                "default": [
                                    "*"
                                    ,
                                    "firma"
                                ],
                                "type": "category"
                            }
                        },
                        "preserve_position_increments": true,
                        "type": "completion",
                        "preserve_separators": true
                    }
                }
            },
            "standardadresse": {
                "properties": {
                    "ort": {
                        "type": "string"
                    },
                    "postleitzahl": {
                        "type": "string"
                    },
                    "strasse": {
                        "type": "string"
                    },
                    "bundesland": {
                        "include_in_all": false,
                        "properties": {
                            "name": {
                                "include_in_all": false,
                                "type": "string"
                            }
                        }
                    },
                    "postfach": {
                        "type": "string"
                    },
                    "land": {
                        "include_in_all": false,
                        "properties": {
                            "iso": {
                                "include_in_all": false,
                                "type": "string"
                            },
                            "archiv": {
                                "type": "boolean"
                            },
                            "name": {
                                "include_in_all": false,
                                "type": "string"
                            }
                        }
                    },
                    "oid": {
                        "type": "string"
                    },
                    "postfachplz": {
                        "type": "string"
                    }
                }
            },
            "abkuerzung": {
                "type": "string",
                "fields": {
                    "suggest": {
                        "max_input_length": 50,
                        "payloads": false,
                        "analyzer": "simple",
                        "context": {
                            "type_context": {
                                "path": "_type",
                                "default": [
                                    "*"
                                    ,
                                    "firma"
                                ],
                                "type": "category"
                            }
                        },
                        "preserve_position_increments": true,
                        "type": "completion",
                        "preserve_separators": true
                    }
                }
            },
            "telefonnummern": {
                "properties": {
                    "nummer": {
                        "type": "string"
                    }
                }
            },
            "oid": {
                "type": "string"
            },
            "email": {
                "type": "string",
                "fields": {
                    "suggest": {
                        "max_input_length": 50,
                        "payloads": false,
                        "analyzer": "keyword",
                        "context": {
                            "type_context": {
                                "path": "_type",
                                "default": [
                                    "*"
                                    ,
                                    "firma"
                                ],
                                "type": "category"
                            }
                        },
                        "preserve_position_increments": true,
                        "type": "completion",
                        "preserve_separators": true
                    }
                }
            },
            "firmenname": {
                "type": "string",
                "fields": {
                    "suggest": {
                        "max_input_length": 50,
                        "payloads": false,
                        "analyzer": "simple",
                        "context": {
                            "type_context": {
                                "path": "_type",
                                "default": [
                                    "*"
                                    ,
                                    "firma"
                                ],
                                "type": "category"
                            }
                        },
                        "preserve_position_increments": true,
                        "type": "completion",
                        "preserve_separators": true
                    }
                }
            }
        }
    }
}
```

Then indexing the following documents:
`curl -s -XPOST localhost:9200/_bulk --data-binary @BulkError.txt`

```
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Alfred Reiter Bau GmbH","abkuerzung":"ROTTMEIER","email":"","adressen":[{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"}],"standardadresse":{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"},"telefonnummern":[{"nummer":"08751/5171"},{"nummer":"08751/9400"},{"nummer":"0170/7369223"},{"nummer":"08751/9400"},{"nummer":"0170/2847356 Alfred"}],"bemerkung":"info@reiter-bau.de","oid":"40d50149-aafa-4727-9c9b-0b95ae41d4bb"}
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Volkswagen Bankdirect","abkuerzung":"VOLKSWAGEN","email":"","adressen":[{"strasse":"Gifthorner Str. 57","postleitzahl":"38112","ort":"Braunschweig","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Niedersachsen"},"oid":"60672aff-dbb4-4291-9b3f-fd2ebc177cb6"}],"standardadresse":{"strasse":"Gifthorner Str. 57","postleitzahl":"38112","ort":"Braunschweig","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Niedersachsen"},"oid":"60672aff-dbb4-4291-9b3f-fd2ebc177cb6"},"telefonnummern":[{"nummer":"0531/2121732"},{"nummer":"0531/2122836"},{"nummer":"0531/2122880"}],"bemerkung":"VOLKSWAGEN","oid":"2c1dc58b-e5a2-4513-aea6-1327714b07b8"}
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Die Bayerische","abkuerzung":"BBV","email":"","adressen":[{"strasse":"Thomas-Dehler-Str. 25","postleitzahl":"81737","ort":"München","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"1482071b-c049-4802-8a40-e37aba535317"}],"standardadresse":{"strasse":"Thomas-Dehler-Str. 25","postleitzahl":"81737","ort":"München","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"1482071b-c049-4802-8a40-e37aba535317"},"telefonnummern":[{"nummer":"Kfz 089/6787-2222"},{"nummer":"089/6787-0"},{"nummer":"089/6787-9150"}],"bemerkung":"BBV","oid":"2786c191-b882-4c2d-a61f-1f962f74cdcd"}
```

and i get the aforementioned error on all three documents. There are documents that work correctly but i can cut out all content (e. g. "firmenname":"") from all fields of these documents from above apart from the oid field and i nevertheless get the exception.

Here is the call stack for one of the errors from the log file:

```
[2015-05-05 11:05:32,818][DEBUG][action.bulk              ] [Amina Synge] [ses_firma][0] failed to execute bulk item (index) index {[ses_firma][firma][40d50149-aafa-4727-9c9b-0b95ae41d4bb], source[{"firmenname":"Alfred","abkuerzung":"","email":"","adressen":[{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"}],"standardadresse":{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"},"telefonnummern":[{"nummer":"08751/5171"},{"nummer":"08751/9400"},{"nummer":"0170/7369223"},{"nummer":"08751/9400"},{"nummer":"0170/2847356 Alfred"}],"bemerkung":"","oid":"40d50149-aafa-4727-9c9b-0b95ae41d4bb"}
]}
org.elasticsearch.index.engine.IndexFailedEngineException: [ses_firma][0] Index failed for [firma#40d50149-aafa-4727-9c9b-0b95ae41d4bb]
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:368)
    at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:498)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:149)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalStateException: from state (0) already had transitions added
    at org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:158)
    at org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester.replaceSep(XAnalyzingSuggester.java:302)
    at org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester.toFiniteStrings(XAnalyzingSuggester.java:932)
    at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider.toFiniteStrings(AnalyzingCompletionLookupProvider.java:371)
    at org.elasticsearch.search.suggest.completion.CompletionTokenStream.incrementToken(CompletionTokenStream.java:63)
    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:618)
    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:241)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:465)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1526)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1252)
    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:431)
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
    ... 8 more
```

I tried this on three different machines also with a completely new installed Elasticsearch instance.

Best Regards,
Markus Dütting
</description><key id="73345067">10987</key><summary>IllegalStateException on indexing with completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">Duetting</reporter><labels><label>:Suggesters</label><label>bug</label></labels><created>2015-05-05T15:25:25Z</created><updated>2015-05-14T20:49:42Z</updated><resolved>2015-05-14T20:49:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-05-14T02:20:02Z" id="101882678">Hi @Duetting,

Thanks for reporting this. It seems the reason for this error is because the field `email` has an empty string. To avoid this error, you could omit the empty string completion fields from indexing.
Ideally, this should be handled by elasticsearch, I have opened a PR for this (https://github.com/elastic/elasticsearch/pull/11158).

Indexing the following works around this issue:

```
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Alfred Reiter Bau GmbH","abkuerzung":"ROTTMEIER","adressen":[{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"}],"standardadresse":{"strasse":"Salvatorbergstr. 21","postleitzahl":"84048","ort":"Mainburg","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"452d44ff-b34d-4f40-b5be-5169b2621960"},"telefonnummern":[{"nummer":"08751/5171"},{"nummer":"08751/9400"},{"nummer":"0170/7369223"},{"nummer":"08751/9400"},{"nummer":"0170/2847356 Alfred"}],"bemerkung":"info@reiter-bau.de","oid":"40d50149-aafa-4727-9c9b-0b95ae41d4bb"}
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Volkswagen Bankdirect","abkuerzung":"VOLKSWAGEN","adressen":[{"strasse":"Gifthorner Str. 57","postleitzahl":"38112","ort":"Braunschweig","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Niedersachsen"},"oid":"60672aff-dbb4-4291-9b3f-fd2ebc177cb6"}],"standardadresse":{"strasse":"Gifthorner Str. 57","postleitzahl":"38112","ort":"Braunschweig","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Niedersachsen"},"oid":"60672aff-dbb4-4291-9b3f-fd2ebc177cb6"},"telefonnummern":[{"nummer":"0531/2121732"},{"nummer":"0531/2122836"},{"nummer":"0531/2122880"}],"bemerkung":"VOLKSWAGEN","oid":"2c1dc58b-e5a2-4513-aea6-1327714b07b8"}
{"index":{"_index":"ses_firma","_type":"firma","_version_type":"external_gte","_version":3}}
{"firmenname":"Die Bayerische","abkuerzung":"BBV","adressen":[{"strasse":"Thomas-Dehler-Str. 25","postleitzahl":"81737","ort":"München","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"1482071b-c049-4802-8a40-e37aba535317"}],"standardadresse":{"strasse":"Thomas-Dehler-Str. 25","postleitzahl":"81737","ort":"München","land":{"name":"Deutschland","iso":"DEU","archiv":false},"bundesland":{"name":"Bayern"},"oid":"1482071b-c049-4802-8a40-e37aba535317"},"telefonnummern":[{"nummer":"Kfz 089/6787-2222"},{"nummer":"089/6787-0"},{"nummer":"089/6787-9150"}],"bemerkung":"BBV","oid":"2786c191-b882-4c2d-a61f-1f962f74cdcd"}

```

I confirmed that the empty string values do work up to v1.3.9. I will have to dig deeper to figure out what has changed in the meantime.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchTests.java</file></files><comments><comment>Ensure empty completion entries are never indexed</comment></comments></commit></commits></item><item><title>Add pid file to Environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10986</link><project id="" key="" /><description>This commit adds the path of the PID file to the Environment. It also add it to the Security Manager since the PID file is deleted by a shutdown hook when the JVM is exited.
</description><key id="73344022">10986</key><summary>Add pid file to Environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T15:20:37Z</created><updated>2015-06-08T13:13:44Z</updated><resolved>2015-05-05T18:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-05T15:22:01Z" id="99113437">@rmuir Can you have a look please? I spotted it when testing #10725. Thanks
</comment><comment author="rmuir" created="2015-05-05T15:24:49Z" id="99114060">Hey this is great. is there anyway we coudl add a unit test in SecurityTests? (just create a fake pid file in tests temp dir like the other tests and assert it has permissions)
</comment><comment author="tlrx" created="2015-05-05T16:23:23Z" id="99131318">@rmuir I updated the code after your last comment
</comment><comment author="rmuir" created="2015-05-05T16:30:48Z" id="99133005">Looks good. When pushing, can we add a small javadoc for the Environment.pidFile() that mentions it returns null if no pid file is configured? Other paths do not return null, but here callers need to check.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove filter parsers.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10985</link><project id="" key="" /><description>This commit makes queries and filters parsed the same way using the
QueryParser abstraction. This allowed to remove duplicate code that we had
for similar queries/filters such as `range`, `prefix` or `term`.
</description><key id="73323282">10985</key><summary>Remove filter parsers.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T14:02:24Z</created><updated>2015-06-06T17:13:33Z</updated><resolved>2015-05-07T18:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-05T14:25:08Z" id="99094048">For the record, the main challenges here were to deal with filters that behave differently from their equivalent query:
- `bool` has a default min_should_match of 0 in queries and 1 in filters that have `should` clauses
- `terms` parses as a `boolean` query when used as a query and a `terms` query otherwise

The query DSL is backward compatible, existing queries should keep working. Actually, it even allows to eg. use the `script` query by itself without wrapping it into a `constant_score` or `filtered_query`.

Regarding documentation, I moved query definitions right under `query-dsl` while it used to be splitted into `queries` and `filters`.
</comment><comment author="jpountz" created="2015-05-05T14:27:16Z" id="99094712">The diff is very large due to the fact that we have a lot of code that manipulates queries/filters but not very interesting on average, it might be easier to just ask questions. :-)
</comment><comment author="clintongormley" created="2015-05-06T14:14:17Z" id="99482371">In the `constant_score` query, I think we should deprecate the `query` parameter, because anything under there will be treated as a `filter`.
</comment><comment author="jpountz" created="2015-05-07T07:39:35Z" id="99756876">I pushed more commits to address comments.

I'm also adding the `breaking` label because users of the Java API will need to change their code to use `QueryBuilders` instead of `FilterBuilders`. For users of the REST API however, this change is backward compatible.
</comment><comment author="javanna" created="2015-05-07T12:26:49Z" id="99844587">All good on my end, thanks for updating @jpountz . I don't have enough lucene knowledge though to review the lucene aspects here.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: Fix `bool` parsing.</comment></comments></commit><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/Alias.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java</file><file>src/main/java/org/elasticsearch/common/ParseField.java</file><file>src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>src/main/java/org/elasticsearch/common/lucene/index/FreqTermsEnum.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAlias.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BytesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/FilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterParserFactory.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedFilter.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedQuery.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFlag.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java</file><file>src/main/java/org/elasticsearch/index/query/support/XContentStructure.java</file><file>src/main/java/org/elasticsearch/index/search/FieldDataTermsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/NumericRangeFieldDataFilter.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SubSearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java</file><file>src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java</file><file>src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/test/java/org/elasticsearch/action/admin/HotThreadsTest.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/SpecificMasterNodesTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/MatchAllDocsFilterTests.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/explain/ExplainActionTests.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/guice/IndexQueryParserModuleTests.java</file><file>src/test/java/org/elasticsearch/index/query/guice/MyJsonFilterParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPlugin2Tests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/PluginJsonFilterParser.java</file><file>src/test/java/org/elasticsearch/index/search/FieldDataTermsFilterTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/script/GroovyScriptTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/EquivalenceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FilterTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/NestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java</file></files><comments><comment>Merge pull request #10985 from jpountz/enhancement/remove_filters</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/FilterParser.java</file></files><comments><comment>removed //norelease comment, FilterParser will go away with #10985</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/AndFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BaseFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BaseFilterParserTemp.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BytesFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterWrappingFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/LimitFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperFilterParser.java</file><file>src/test/java/org/elasticsearch/index/query/guice/MyJsonFilterParser.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/PluginJsonFilterParser.java</file></files><comments><comment>Revert "Filter refactoring: Introduce toFilter() and fromXContent() in FilterBuilders and FilterParsers"</comment></comments></commit></commits></item><item><title>JDK9EA has buggy locale support when running with security manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10984</link><project id="" key="" /><description>this test fails with security manager enabled since yesterday 

```
mvn test -Pdev -Dtests.seed=2DCB147CE811B0B6 -Dtests.class=org.elasticsearch.index.mapper.date.SimpleDateMappingTests -Dtests.method="testLocale" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server" -Dtests.locale=es_AR -Dtests.timezone=Atlantic/Faeroe
```

likely related to changes pushed yesterady maybe this one https://github.com/elastic/elasticsearch/pull/10965 @rmuir @rjernst can you take a look...

here is a CI failure for this http://build-us-00.elastic.co/job/es_core_master_metal/9112/
</description><key id="73315602">10984</key><summary>JDK9EA has buggy locale support when running with security manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>adoptme</label><label>bug</label><label>jvm bug</label><label>v5.4.4</label></labels><created>2015-05-05T13:30:09Z</created><updated>2017-06-27T10:28:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-05T13:45:00Z" id="99082140">if it only happens on JDK9EA, its likely just a bug in that early access release.
</comment><comment author="s1monw" created="2015-05-05T14:54:24Z" id="99103791">I pushed a assume for those tests
</comment><comment author="dakrone" created="2016-09-27T14:30:34Z" id="249882207">There have been a lot of JDK 9 EA releases since this was pushed, we should check to see if the assumeFalse can be removed so I'm marking this as adoptme
</comment><comment author="bleskes" created="2016-12-06T11:25:16Z" id="265126518">@dakrone I see you added a blocker label. Was that intended?</comment><comment author="dakrone" created="2016-12-06T15:30:49Z" id="265179855">@bleskes whoops I don't think so, I'll remove it.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/count/simple/SimpleCountTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/search/simple/SimpleSearchTests.java</file></files><comments><comment>[TEST] Mute local tests with Java9EA</comment></comments></commit></commits></item><item><title>Let HTTPS work correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10983</link><project id="" key="" /><description>Currently pluginmanager init disables https verification _jvm-wide_ for any HttpsURLConnections. This is seriously screwed-up, and makes https completely useless here. It does this without any comment or justification as to why.

If you don't want the cert check, just use http. or configure this trust stuff in your jvm differently. But we should let HTTPS just work correctly by default.

cc: @jaymode 
</description><key id="73312856">10983</key><summary>Let HTTPS work correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T13:18:53Z</created><updated>2015-06-07T18:40:39Z</updated><resolved>2015-05-05T21:47:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-05-05T13:26:39Z" id="99077196">LGTM but lets get another review before merging
</comment><comment author="s1monw" created="2015-05-05T13:31:24Z" id="99078040">+1 
</comment><comment author="nik9000" created="2015-05-05T13:31:36Z" id="99078089">Do I count? +1

_maybe_ a parameter that disables it like  `--insecure`
</comment><comment author="rmuir" created="2015-05-05T13:46:54Z" id="99082616">@nik9000 if you want to break https in the jvm, you can configure this in the jvm configuration. I think for any esoteric cases that need it, we should just recommend that and not have options to do it here.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Merge pull request #10983 from rmuir/no_check_certificate</comment></comments></commit></commits></item><item><title>Deprecate the More-Like-This API in favour of the MLT query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10982</link><project id="" key="" /><description>Deprecates the More Like This API until it gets removed in 2.0. Users are
encouraged to use the More Like This query.

Relates #10736 
</description><key id="73312650">10982</key><summary>Deprecate the More-Like-This API in favour of the MLT query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-05-05T13:18:00Z</created><updated>2015-05-30T10:47:39Z</updated><resolved>2015-05-06T09:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-05T14:47:37Z" id="99101438">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>support HTTP/2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10981</link><project id="" key="" /><description>Elastic have plane add support (or maybe plugin for) protocol HTTP/2.0? 
Like in https://github.com/grpc/grpc(used HTTP/2.0) or may be with grpc
</description><key id="73309620">10981</key><summary>support HTTP/2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antonikonovalov</reporter><labels><label>:Network</label><label>high hanging fruit</label></labels><created>2015-05-05T13:02:30Z</created><updated>2016-09-27T22:44:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T14:54:54Z" id="249889399">This is now possible since we have moved to Netty 4, since Netty supports HTTPP/2.0 in 4.1+ /cc @jasontedor 
</comment><comment author="jasontedor" created="2016-09-27T22:44:34Z" id="250020921">To be clear, Netty 4 supports HTTP/2 but there is effort required to make Elasticsearch use it, and ensure that it works properly.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>API: Add response filtering with `filter_path` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10980</link><project id="" key="" /><description>This change adds a new "_path" parameter that can be used to filter and reduce the responses returned by the REST API of elasticsearch.

For example, returning only the shards that failed to be optimized:

```
curl -XPOST 'localhost:9200/beer/_optimize?_path=_shards.failed'
{"_shards":{"failed":0}}%
```

It supports multiple filters (separated by a comma):

```
curl -XGET 'localhost:9200/_mapping?pretty&amp;_path=*.mappings.*.properties.name,*.mappings.*.properties.title'
```

It also supports the YAML response format. Here it returns only the `_id` field of a newly indexed document:

```
curl -XPOST 'localhost:9200/library/book?_path=_id' -d '---hello:\n  world: 1\n'

---
_id: "AU0j64-b-stVfkvus5-A"
```

It also supports wildcards. Here it returns only the host name of every nodes in the cluster:

```
curl -XGET 'http://localhost:9200/_nodes/stats?_path=nodes.*.host*'
{"nodes":{"lvJHed8uQQu4brS-SXKsNA":{"host":"portable"}}}
```

And "**" can be used to include sub fields without knowing the exact path. Here it returns only the Lucene version of every segment:

```
curl 'http://localhost:9200/_segments?pretty&amp;_path=indices.**.version'
{
  "indices" : {
    "beer" : {
      "shards" : {
        "0" : [ {
          "segments" : {
            "_0" : {
              "version" : "5.2.0"
            },
            "_1" : {
              "version" : "5.2.0"
            }
          }
        } ]
      }
    }
  }
}
```

Note that elasticsearch sometimes returns directly the raw value of a field, like the "_source" field. If you want to filter the response that include _source fields like in Search or Get responses , you should consider using the already existing "fields" parameter:

```
 curl -XGET 'localhost:9200/beer/tasty/3672?pretty&amp;fields=category'
 {
   "_index" : "beer",
   "_type" : "tasty",
   "_id" : "3672",
   "_version" : 1,
   "found" : true,
   "fields" : {
     "category" : [ "German Lager" ]
   }
 }
```

The "_path" parameter can be used to further reduce the result:

```
curl -XGET 'localhost:9200/beer/tasty/3672?pretty&amp;fields=category&amp;_path=fields.category'
{
  "fields" : {
    "category" : [ "German Lager" ]
  }
}

```

Closes #7401
</description><key id="73300356">10980</key><summary>API: Add response filtering with `filter_path` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:REST</label><label>feature</label><label>release highlight</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T12:18:56Z</created><updated>2016-03-08T10:15:07Z</updated><resolved>2015-05-26T12:08:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-05T12:32:34Z" id="99066413">Note: 
- if the provided filter specified in `_path` does not match something, an empty response is returned
- if the API returns an error, the error is not filtered even if the `_path` is specified
</comment><comment author="clintongormley" created="2015-05-05T12:35:20Z" id="99066804">&gt; if the provided filter specified in _path does not match something, an empty response is returned

We should return an empty JSON object rather than an empty string, otherwise the client needs to check whether JSON has been returned.
</comment><comment author="tlrx" created="2015-05-05T12:47:25Z" id="99068937">@clintongormley thanks, I updated the code to reflect your last comment.
</comment><comment author="tlrx" created="2015-05-05T13:06:04Z" id="99073135">@spinscale Can you please have a look? Thanks
</comment><comment author="spinscale" created="2015-05-06T07:36:10Z" id="99355743">A couple of things

First, I really would like to see some simple benchmarking, so we have some ballpark numbers, how this affects performance. Especially because `write` operations that create sub-hierarchies also create new objects.

Minor thing: `FilteringJsonGenerator.parse()` - why not use an array instead of XContentFilter and `next` (like your own custom linked list).

One other thing, I know that jackson has some filtering capabiltiies built-in, I have no ideas if we can make use of them though, see [here](http://wiki.fasterxml.com/JacksonFeatureJsonFilter), [here](http://www.cowtowncoder.com/blog/archives/2011/09/entry_461.html) and [here](http://www.cowtowncoder.com/blog/archives/2011/02/entry_443.html)
</comment><comment author="tlrx" created="2015-05-06T07:55:24Z" id="99363193">&gt; One other thing, I know that jackson has some filtering capabiltiies built-in, I have no ideas if we can make use of them though, see here, here and here

That's the first thing we checked before starting working on this pull request. The current filtering capabilities of Jackson (version &lt;2.5.1) can be used only with jackson-databinding lib and apply only to POJOs but not when you use jackson to generate JSON in streaming mode. 

Jackson 2.6 will integrate an interesting filtering feature that will work with JSON streaming (see release note [here](https://github.com/FasterXML/jackson-core/blob/master/release-notes/VERSION#L22) and test class [here](https://github.com/FasterXML/jackson-core/blob/master/src/test/java/com/fasterxml/jackson/core/filter/JsonPointerGeneratorFilteringTest.java)). It currently uses [JSONPointer](https://tools.ietf.org/html/rfc6901) to filter properties but I think we will be able to implement our own [TokenFilter](https://github.com/FasterXML/jackson-core/blob/master/src/main/java/com/fasterxml/jackson/core/filter/TokenFilter.java) later, once version 2.6 is released.
</comment><comment author="tlrx" created="2015-05-07T11:03:04Z" id="99815387">&gt; First, I really would like to see some simple benchmarking, so we have some ballpark numbers, how this affects performance. Especially because write operations that create sub-hierarchies also create new objects.

I added the `FilteringJsonGeneratorBenchmark` and put some numbers [here](https://gist.github.com/tlrx/960ce767ffc084dee2d6) (not very readable - I apologize for that). 

There are no real surprise in the benchmark: 
- when writing a small number of documents (less than 100) with a small number of fields each(1-10), the filtering adds an overhead that makes it less efficient than what we have today
- when writing a larger amount of data (ie [large number of small docs](https://gist.github.com/tlrx/960ce767ffc084dee2d6#file-filteringjsongeneratorbenchmark-L64-L74), [small number of large docs](https://gist.github.com/tlrx/960ce767ffc084dee2d6#file-filteringjsongeneratorbenchmark-L240-L250)) there's a point where it becomes more efficient to check if fields must be included and skip them rather than writing more data. This point varies depending on the document size and the selectivity of the filter, but in this test it is in between 1-10%.

&gt; Minor thing: FilteringJsonGenerator.parse() - why not use an array instead of XContentFilter and next (like your own custom linked list).

There are room for improvements like this one (I'm also thinking of not creating FilterContext for sub fields when a parent field matches the end of a filter), but it might add complexity to the code and I'm not sure if it will be really more efficient.

I'm currently trying some of these improvement to see if they are pertinent or not.
</comment><comment author="tlrx" created="2015-05-07T14:50:27Z" id="99895694">Thinking of improvements again, we should be able to reuse the objects instead of creating new ones for every sub fields. I'll try to improve that.
</comment><comment author="clintongormley" created="2015-05-08T07:21:29Z" id="100130961">&gt; when writing a small number of documents (less than 100) with a small number of fields each(1-10), the filtering adds an overhead that makes it less efficient than what we have today

I assume that the code path (and performance) remains the same as today if no `_path` is specified?
</comment><comment author="tlrx" created="2015-05-08T07:45:49Z" id="100137765">@clintongormley yes. Branching is done [here](https://github.com/elastic/elasticsearch/pull/10980/files#diff-dea72b6ed4d9da9b9008b0e185d77887R77) where we fall back to the normal XContent generator.
</comment><comment author="spinscale" created="2015-05-08T16:05:09Z" id="100281573">added some absolutely minor comments...

First, this doesnt compile using mvn, due to the benchmark class and some locale, but not really an issue.

Few last things:
- the kind-of linkedlist data structure in the FilterContext, which could possibly be replaced by a simple array?
- This seems the only part, where we use `**` as 'greedy' wildcard in Elasticsearch, not sure if it is a problem, just to be aware
- No documentation so far?

Maybe we should also note somewhere to switch to the jackson filter features with the 2.6 release, I'd hate to forget that...

LGTM apart from that
</comment><comment author="rjernst" created="2015-05-08T16:57:14Z" id="100298225">I'm late to the party here, but I have a simple question. Why is the parameter `_path` and not `path`? Why use the underscore when we control all the url params anyways? And all the other examples I know of (eg `fields`, `pretty`, `human`) do not use underscore?
</comment><comment author="clintongormley" created="2015-05-08T17:00:37Z" id="100298880">@rjernst only because it works well with the existing `_source` filtering.  I kinda like the underscore because it is a "global" param (mind you, so is pretty).
</comment><comment author="clintongormley" created="2015-05-08T17:24:19Z" id="100303772">&gt; This seems the only part, where we use *\* as 'greedy' wildcard in Elasticsearch, not sure if it is a problem, just to be aware

Elsewhere we use `*` as a greedy wildcard, which I think is problematic.  We took the `**` syntax from ant, and I'd actually be tempted to reuse it elsewhere.
</comment><comment author="kimchy" created="2015-05-08T17:25:13Z" id="100303953">actually, since this is a "rest" level parameter, and for those, we have `pretty` and `human`, I think we should just call it `path`, or maybe be more explicit and call it `filter_path`
</comment><comment author="rjernst" created="2015-05-08T17:26:57Z" id="100304282">+1 on `filter_path`
</comment><comment author="kimchy" created="2015-05-09T21:53:24Z" id="100552354">here is another suggestion, calling it `filter_path_include`, in the future we might want to support `filter_path_exclude`. 

The reason I am raising it is that today, we support source level include/exclude when fetching source documents. With this infrastructure, if we eventually expand it to support include_s_/exclude_s_, we can also support zero copy (as in, not create a map of maps/lists representation of source) when someone asks for source includes/includes (we can just create a bytes based filtering generator, and copy structure from the parser right into it). 

I helped a fellow on IRC today where the loading and parsing of source include was the perf bottleneck (from 1ms it got up to 170ms for ~1000 docs). I think this will help a lot there eventually.
</comment><comment author="nik9000" created="2015-05-09T22:04:34Z" id="100553510">Sorry I just jumped into this late. It makes me think of jq. Nothing else
to add.
On May 9, 2015 5:53 PM, "Shay Banon" notifications@github.com wrote:

&gt; here is another suggestion, calling it filter_path_include, in the future
&gt; we might want to support filter_path_exclude.
&gt; 
&gt; The reason I am raising it is that today, we support source level
&gt; include/exclude when fetching source documents. With this infrastructure,
&gt; if we eventually expand it to support include_s_/exclude_s_, we can also
&gt; support zero copy (as in, not create a map of maps/lists representation of
&gt; source) when someone asks for source includes/includes (we can just create
&gt; a bytes based filtering generator, and copy structure from the parser right
&gt; into it).
&gt; 
&gt; I helped a fellow on IRC today where the loading and parsing of source
&gt; include was the perf bottleneck (from 1ms it got up to 170ms for ~1000
&gt; docs). I think this will help a lot there eventually.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/10980#issuecomment-100552354
&gt; .
</comment><comment author="clintongormley" created="2015-05-15T14:39:34Z" id="102416119">What about using a `-` prefix for excludes as in `"*.foo,-*.foo.bar"`. This is what is used by the (non-streaming) AntPath filter for Jackson https://github.com/Antibrumm/jackson-antpathfilter#usage
</comment><comment author="tlrx" created="2015-05-18T11:46:44Z" id="103030060">@kimchy now I'm reading your comment again, I think that's a neat idea :)

@clintongormley I like the `+/-` notation but this is not consistent with the `_source?_source_include=*.id&amp;_source_exclude=entities` notation... I think we should use the same notation (whatever it is) in both endpoints.
</comment><comment author="tlrx" created="2015-05-18T16:05:38Z" id="103114602">@spinscale I rebased and updated the code according to your last comments. Can you have a look and do some manual testing please? Thanks a lot :)

Note: latest benchmark numbers are [here](https://gist.github.com/tlrx/65d2a46e394b13aaa11e).
</comment><comment author="spinscale" created="2015-05-18T16:25:57Z" id="103118760">we may also not want to have manual testing but also add that parameter (plus tests) to our REST tests
</comment><comment author="tlrx" created="2015-05-19T10:17:25Z" id="103427742">@spinscale I just added the REST tests. The parameter is still named `_path` until we reach an agreement on the final name.
</comment><comment author="spinscale" created="2015-05-19T16:06:58Z" id="103568631">left one comment, apart from that it feels like we are close to getting this in, when we get the naming stuff resolved... /cc @clintongormley 
</comment><comment author="clintongormley" created="2015-05-20T18:31:27Z" id="103987762">The `_source` parameter is essentially a synonym for `_source_include`, so I'd go with `filter_path` for now.  We can always add `filter_path_include|exclude` later on.
</comment><comment author="tlrx" created="2015-05-20T18:39:51Z" id="103989932">@clintongormley sounds good to me, thanks
</comment><comment author="tlrx" created="2015-05-22T11:41:24Z" id="104637691">@spinscale Updated with your last comment :)
</comment><comment author="clintongormley" created="2015-05-25T10:06:16Z" id="105197283">@spinscale @kimchy does this need another review, or are we good to go?
</comment><comment author="spinscale" created="2015-05-26T06:18:24Z" id="105413112">LGTM on my side
</comment><comment author="clintongormley" created="2015-05-26T12:22:19Z" id="105504957">w00t
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Properly propagate the search request context &amp; headers to sub-requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10979</link><project id="" key="" /><description>Each transport request holds context and headers that represent the general context of the executed actions. There are actions that spawn other requests and we make sure that the context/headers of the original request is passed along to the sub-requests.

In the case of a search request, we have some cases where this context/headers passing doesn't happen and we need to fix that (we need to make sure the context &amp; headers are not lost at any point in the execution). The reason for this today is that some spawned request simply don't have access to the original request. For example, `TermsLookup` used by the external terms filter executes a search during the parsing of the query. It has no access to the original search request that was executed and therefore cannot pass along the context &amp; headers.

To solve that we can use the `SearchContext` and let it hold the search request context &amp; headers (maybe we can have the `SearchContext` extends `ContextHolder`). During the execution of a search, any phase that is part of the execution has access to the search context using the thread local based `SearchContext.current()`. We need to make sure, to set the request context &amp; headers on the `SearchContext` when it is created. From there on, we can extract these request context &amp; headers everywhere we need to and pass it to the sub-request (a la `TermsLookup`).

Other sub request we need to look at:
- `MoreLikeThisQuery`
- `GeoShapeFilter` and `GeoShapeQuery` with a indexed shape
- Indexed Scripts
- Phrase Suggester with a collate query or filter
- Search Templates with indexed templates
</description><key id="73293819">10979</key><summary>Properly propagate the search request context &amp; headers to sub-requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>:Search</label><label>:Search Templates</label><label>bug</label></labels><created>2015-05-05T11:47:23Z</created><updated>2015-05-18T13:23:34Z</updated><resolved>2015-05-18T13:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T11:48:26Z" id="99056444">@spinscale please could you take a look when you have a moment
</comment><comment author="spinscale" created="2015-05-06T14:49:56Z" id="99498266">checked the source, my current idea of implementing it is
- Make SearchContext implementing an interface that adheres to `ContextHolder`, propagate to `ShardSearchRequest`, maybe just changing `DefaultSearchContext` is enough, needs to be checked
- `ShardSearchRequest` also needs to implement that new interface, one of the two subclasses, `ShardSearchTransportRequest` already does, one needs to be extended `ShardSearchLocalRequest`
- Both classes have access to the original `SearchRequest` and thus can copy the context
- Make sure all of the above requests can make use of this infra
- Search for possible more requests that need this
- Copy headers need to be copied as well

Tricky: Testing this
</comment><comment author="uboness" created="2015-05-06T18:02:23Z" id="99554104">&gt; Later: Do headers need to be copied as well

while we're at it, I'd just copy the headers as well.. why not
</comment><comment author="spinscale" created="2015-05-08T07:56:06Z" id="100139991">Implementation note: Percolate-by-id must be supported as well
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/common/ContextAndHeaderHolder.java</file><file>src/main/java/org/elasticsearch/common/HasContext.java</file><file>src/main/java/org/elasticsearch/common/HasContextAndHeaders.java</file><file>src/main/java/org/elasticsearch/common/HasHeaders.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>src/main/java/org/elasticsearch/search/suggest/SuggestParseElement.java</file><file>src/main/java/org/elasticsearch/transport/TransportMessage.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportTests.java</file></files><comments><comment>Internal: Propagate headers &amp; contexts to sub-requests</comment></comments></commit></commits></item><item><title>Investigate cluster state signaling of index deletes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10978</link><project id="" key="" /><description>#9952 changed deletion of indices: Indices are now only deleted if the cluster state that signals the deletion comes from the same master that sent the previous cluster state (https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java#L107). This change was needed for the following scenario: A master with an empty cluster state is elected (for example because the node was restarted and data folder lost) but a data node misses the cluster state from that master that would contain the STATE_NOT_RECOVERED_BLOCK. In this case the node would delete all indices instead of importing them as dangling. see https://github.com/elastic/elasticsearch/pull/9952#issuecomment-97707128
</description><key id="73279254">10978</key><summary>Investigate cluster state signaling of index deletes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2015-05-05T10:47:24Z</created><updated>2015-06-18T12:23:57Z</updated><resolved>2015-06-16T11:49:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-06-16T11:49:02Z" id="112395774">Closing as we made a decision now, see #11665
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw exception if unrecognised params present in bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10977</link><project id="" key="" /><description>If additional elements (like `_source`) are present in the metadata line of the bulk request body, some arguments get ignored. This is an example where `_id` gets ignored (works fine without the `_source` there):

```
curl -XGET 'http://localhost:9200/?pretty'
#[200] (0.002s)
#{
#  "cluster_name": "elasticsearch",
#  "name": "Xemnu the Titan",
#  "status": 200,
#  "tagline": "You Know, for Search",
#  "version": {
#    "build_hash": "53bf0729d5cab01181e98d2de34c73a97fa2b1f8",
#    "build_snapshot": true,
#    "build_timestamp": "2015-05-05T10:11:47Z",
#    "lucene_version": "4.10.4",
#    "number": "1.6.0"
#  }
#}
curl -XPUT 'http://localhost:9200/test?pretty' -d ''
#[200] (0.095s)
#{
#  "acknowledged": true
#}
curl -XPOST 'http://localhost:9200/test/_bulk?pretty&amp;refresh=true' -d '{"index": {"_index": "test", "_type": "doc", "_source": {"hello": "world"}, "_id": 0}}
{"idx": 0}
'
#[200] (0.089s)
#{
#  "errors": false,
#  "items": [
#    {
#      "create": {
#        "_id": "AU0jlPM2pgX4WOPmtASc",
#        "_index": "test",
#        "_type": "doc",
#        "_version": 1,
#        "status": 201
#      }
#    }
#  ],
#  "took": 85
#}
```
</description><key id="73272360">10977</key><summary>Throw exception if unrecognised params present in bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">HonzaKral</reporter><labels><label>:Bulk</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-05-05T10:23:30Z</created><updated>2015-05-27T16:11:20Z</updated><resolved>2015-05-27T16:11:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T12:10:56Z" id="99062859">@HonzaKral we should throw an error there about unrecognised params, but the `_source` field doesn't belong in the metadata line.
</comment><comment author="HonzaKral" created="2015-05-05T12:13:47Z" id="99063364">Agreed, but it also caused the _id to be ignored, which is a bug
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file></files><comments><comment>Bulk: throw exception if unrecognized parameter in action/metadata line</comment></comments></commit></commits></item><item><title>Allow script language to be null when parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10976</link><project id="" key="" /><description>Closes #10926
</description><key id="73259470">10976</key><summary>Allow script language to be null when parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T09:26:13Z</created><updated>2015-05-07T08:52:27Z</updated><resolved>2015-05-07T08:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-07T08:43:42Z" id="99775576">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/script/ScriptParameterParser.java</file><file>src/test/java/org/elasticsearch/bwcompat/ScriptTransformBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/TransformOnIndexMapperIntegrationTest.java</file></files><comments><comment>Merge pull request #10976 from colings86/fix/10926</comment></comments></commit></commits></item><item><title>HttpServer: Support relative plugin paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10975</link><project id="" key="" /><description>When specifying relative paths on startup, handling plugin
paths failed due to recently added security fix.

The `HttpServer` now handles this correctly

In addition a new matcher has been added to easily check for a
status code of an HTTP response likes this

assertThat(response, hasStatus(OK));

Closes #10958
</description><key id="73258103">10975</key><summary>HttpServer: Support relative plugin paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T09:21:37Z</created><updated>2015-05-29T18:19:09Z</updated><resolved>2015-05-15T06:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-05T12:14:44Z" id="99063605">Why make things absolute? I really really dislike java code that makes paths absolute when its unnecessary. Seems its only necessary for that the little webserver paths.
</comment><comment author="spinscale" created="2015-05-05T12:30:30Z" id="99066146">My idea was to unify the code instead of having this one exception with the plugins.
I dont have a strong feeling about either. Do you think something can go wrong by using absolute paths or just not a clean approach?
</comment><comment author="rmuir" created="2015-05-05T12:39:25Z" id="99067395">I don't think we should add code to the codebase and justify the additional code with "well nothing can go wrong by adding it". Either it has a purpose or we don't add it.

If we want to do some processing up-front here, i cannot imagine a situation where i'd want to just do absolute+normalize, because its no correct "solution" to anything

On the other hand if the proposal was:

```
// these directories are heavily used, so reduce to canonical form for performance reasons 
// (resolving any symlinks, case normalization, and making absolute). 
Files.createDirectories(path); // ensure created
path = path.toRealPath(); // to canonical
```

Then i could see the advantages: we go full-throttle to the canonical file name for each path which can have a number of advantages and simplifications.
</comment><comment author="spinscale" created="2015-05-05T14:10:28Z" id="99089385">I actually rethought the whole implementation, and changed only the necessary part in `HttpServer` instead of changing the `Environment`, resulting in a much smaller change.

Thx for the helping comment!
</comment><comment author="rmuir" created="2015-05-05T14:12:26Z" id="99090014">+1 looks great.
</comment><comment author="spinscale" created="2015-05-10T15:32:07Z" id="100655960">just fyi, why this is not merged yet. Tests failed on windows, added a fix, but want to make sure this passes on windows before pushing
</comment><comment author="jaymode" created="2015-05-12T14:33:15Z" id="101302026">@spinscale I looked at the Windows issue and what I saw was that the code was trying to get a path with the first being relative `..\..\..\` and the second argument being absolute `C:\path\elasticsearch`. This causes an issue since Windows is always expected `C:` first. I worked around the issue with the following patch and tests pass on Windows 7 and my OS X.

```
diff --git a/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigTests.java b/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigTests.java
index 99fb23a..b80980b 100644
--- a/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigTests.java
+++ b/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigTests.java
@@ -40,35 +40,40 @@ import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasStatus;
 @ClusterScope(scope = SUITE, numDataNodes = 1)
 public class SitePluginRelativePathConfigTests extends ElasticsearchIntegrationTest {

+    final Path root = PathUtils.get(".").toAbsolutePath().getRoot();
+
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
-        // windows is not supported at the moment, see the assumeTrue statement in the test
-        if (Constants.WINDOWS) {
-            return super.nodeSettings(nodeOrdinal);
-        }

-        Path pluginDir = PathUtils.get(getRelativePath(PathUtils.get(".").toAbsolutePath()), getDataPath("/org/elasticsearch/plugins").toString());
+        String cwdToRoot = getRelativePath(PathUtils.get(".").toAbsolutePath());
+        Path pluginDir = PathUtils.get(cwdToRoot, relativizeToRootIfNecessary(getDataPath("/org/elasticsearch/plugins")).toString());
         boolean useRelativePath = randomBoolean();
         if (useRelativePath) {
             Path tempDir = createTempDir();
             pluginDir = PathUtils.get(tempDir.toString(), getRelativePath(tempDir), pluginDir.toString());
         }

+        pluginDir = pluginDir.toAbsolutePath();
         return settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put("path.plugins", pluginDir.toAbsolutePath())
+                .put("path.plugins", pluginDir)
                 .put("force.http.enabled", true)
                 .build();
     }

     @Test
     public void testThatRelativePathsDontAffectPlugins() throws Exception {
-        // the path is constructed like ..\..\..\C:\foo\bar, which does not work, thus skipping for now
-        assumeTrue("Absolute path inbetween paths are not supported on windows, skipping this test", !Constants.WINDOWS);
         HttpResponse response = httpClient().method("GET").path("/_plugin/dummy/").execute();
         assertThat(response, hasStatus(OK));
     }

+    private Path relativizeToRootIfNecessary(Path path) {
+        if (Constants.WINDOWS) {
+            return root.relativize(path);
+        }
+        return path;
+    }
+
     private String getRelativePath(Path path) {
         StringBuilder sb = new StringBuilder();
         for (int i = 0; i &lt; path.getNameCount(); i++) {
```
</comment><comment author="spinscale" created="2015-05-13T16:56:37Z" id="101746162">@jaymode incorporated review comments, thx for checking
</comment><comment author="jaymode" created="2015-05-13T17:28:05Z" id="101753003">tested again and all good on Windows. LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add testing for invalid XContent queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10974</link><project id="" key="" /><description>The query parsers already check for problems in XContent (e.g. missing fieldnames, wrong token types) in many places. We should investigate if it is possible to have some generic tests in the BaseQueryTestCase that check for expected exceptions and other things that happen when parsing invalid json.
</description><key id="73254223">10974</key><summary>Add testing for invalid XContent queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>test</label></labels><created>2015-05-05T09:06:28Z</created><updated>2016-09-06T11:51:12Z</updated><resolved>2016-09-06T11:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-19T10:34:20Z" id="149178410">@cbuescher did you have anything in mind around this? Would you want to pick it up?
</comment><comment author="cbuescher" created="2015-10-19T12:07:59Z" id="149196536">@javanna I think there are a few general checks we could do (like missing brackets, adding brackets) but I doubt that there are any other tests that are general enough for BaseQueryTestCase that produce predictable errors across all query parsers. Relabeled it and so this is not an issue that's concerning Query Refactoring, if thats okay.
</comment><comment author="clintongormley" created="2016-01-18T15:38:17Z" id="172563895">@cbuescher is this something that has been done as part of search refactoring, or still needs to be done?
</comment><comment author="cbuescher" created="2016-01-18T15:47:36Z" id="172566632">@clintongormley there are two general cases that have been added ( #14220, #14255). This kind of tests can alway be improved further, that's why I kept this open as a reminder to think about it again and see if tests can be extended even more, but we can close this if you like.
</comment><comment author="javanna" created="2016-09-06T11:51:12Z" id="244927132">I think we did enough here. We will improve this when we need to (aka when we find more bugs that our tests didn't catch). But I think we are in a very good place!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTests.java</file></files><comments><comment>Query DSL: Parsers should throw exception on unknown object</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java</file></files><comments><comment>Tests: Check exception from query parsers for unknown field</comment></comments></commit></commits></item><item><title>docs: Fixed typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10973</link><project id="" key="" /><description /><key id="73244288">10973</key><summary>docs: Fixed typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pborreli</reporter><labels /><created>2015-05-05T08:16:58Z</created><updated>2015-05-05T08:41:07Z</updated><resolved>2015-05-05T08:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T08:40:31Z" id="98995008">awesome - thanks @pborreli - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fixed typos</comment></comments></commit></commits></item><item><title>MovAvgTests fail reproducibly on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10972</link><project id="" key="" /><description>here is an example seed that works all the time for me

```
 mvn test -Pdev -Dtests.seed=1D9A36E8CF199668 -Dtests.class=org.elasticsearch.search.aggregations.reducers.moving.avg.MovAvgTests -Dtests.method="doubleSingleValuedField" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=787m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=sr -Dtests.timezone=America/Cambridge_Bay
```

see http://build-us-00.elastic.co/job/es_core_master_debian/5130/console
</description><key id="73241954">10972</key><summary>MovAvgTests fail reproducibly on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T08:03:53Z</created><updated>2015-06-07T18:40:49Z</updated><resolved>2015-05-05T16:31:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-05-05T16:31:48Z" id="99133263">Fixed in 4f651307034ddcd6c5fd7b9afb5e08b8180f57cd
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/aggregations/reducers/moving/avg/MovAvgTests.java</file></files><comments><comment>mute test in favor of #10972</comment></comments></commit></commits></item><item><title>Mapping: Improve date handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10971</link><project id="" key="" /><description>The current date mapping code treats unix timestamps differently from other date formats. We should unify this, even though this requires changing our defaults and requires the user to explicitely configure the unix timestamp usecase.

Today we parse dates as follows:

Mapped fields with a format (defaults to dateOptionalTime)
- If number, treat as epoch ms
- If string, try to parse with defined format(s)
- If it fails and is purely numeric, treat as epoch ms
- Else fail

Dynamic date detection
- If string, 
- and contains at least two `:`, `-`, or `/`
- and matches dynamic date formats (defaults to `dateOptionalTime || yyyy/MM/dd HH:mm:ss || yyyy/MM/dd` )
- then `date`, else `string`

There are a few issues which can surprise users:
- Joda dates are not strict, so `"1/1/1"` is detected as a date, and `"1"` would be interpreted as `0001-01-01 00:00:00`
- The distinction between numeric and string values is not always possible, eg query string params are always strings (`_timestamp`), a date in the `query_string` query is always a string, and even in the JSON body some languages can render a number as a string and vice versa
- Dates such as `2015.01.01` (german) or `20150101T000000` (iso8601) can never be detected dynamically
# Proposals

Make date parsing as unambiguous as possible.  Where there is ambiguity, it is because the user chose ambiguous options (which we can warn about in the docs).  
## For indices created in 2.0:
- Add two formats for parsing epoch: `epoch_ms` and `epoch_seconds` #11453
- Add strict Joda formats, where eg the year must have 4 digits  #6227
- Remove `numeric_resolution` (not needed with above)

For mapped `date` field:
- only check the specified formats, which default to `strictDateOptionalTime || epoch_ms`
- No distinction between numeric and string values for date fields - always parsed as strings (ie coerce from numeric)

For dynamic date detection:
- only check string values (don't coerce numerics)
- accept any formats except `epoch_ms` and `epoch_seconds`
- mapping should add just the matching format  (optionally append `epoch_ms`?)
## For indices created before 2.0:

We need to keep bwc on older indices, so we follow the same rules as specified at the beginning of this comment
## Query time

Typically users will always use the same format at index time - they don't mix epoch timestamps with formatted dates, which is why we should only parse the specified formats.

However, at query time it is quite possible that (eg) Kibana may query with epoch timestamps, even though the date field only accepts a formatted date.  Today, in the `range` query we accept a `format` parameter which is used to parse dates at query time.

There are two options to deal with this situation:
- Add a `format` parameter to the `term`, `terms`, `query_string`, and `simple_query_string` queries, and to the `range` aggregation
- Add a special format for epoch timestamps which is always recognised, eg `epoch_ms:123456789`
</description><key id="73238654">10971</key><summary>Mapping: Improve date handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>Meta</label></labels><created>2015-05-05T07:42:29Z</created><updated>2016-08-25T10:04:12Z</updated><resolved>2016-08-25T10:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="simianhacker" created="2015-06-08T20:21:18Z" id="110125698">Does this affect range queries as well? I just tried using Kibana 4 with ES 2.0 and I got the following error:

```
Jun 8, 2015 11:48 AM INFO ? ? Caused by: ElasticsearchParseException[failed to parse date field [1433788425964] with format [dateOptionalTime]]; nested: IllegalArgumentException[Invalid format: "1433788425964" is malformed at "5964"];
```

If I add `"format": "epoch_millis"` to the range query then everything works as expected.
</comment><comment author="spinscale" created="2015-06-09T07:15:06Z" id="110256034">@simianhacker yes it does. When creating a range query in kibana, do you always use a unix timestamp or just in this example?

I think it makes sense to build this BWC compatible.. will check it out
</comment><comment author="rashidkpc" created="2015-06-10T14:56:02Z" id="110784492">We always use a Unix timestamp because it has historically been accepted regardless of format. While we know the format from the mapping, there are differences between our date formatting lib and Joda.
</comment><comment author="kimchy" created="2015-06-12T13:37:29Z" id="111494915">I like this plan, at query time, I am leaning towards supporting the special format of `epoch_ms:12345`, its simpler to integrate and makes Kibana life simpler to provide it (for example).
</comment><comment author="clintongormley" created="2015-11-09T12:30:17Z" id="155051379">Also see https://github.com/elastic/elasticsearch/issues/14565 - Java8 prepends a plus sign to years &gt; 9999
</comment><comment author="jpountz" created="2016-08-24T15:02:23Z" id="242096292">Is there anything that remains to be done before closing this issue? Most items look implemented, except the `format` option on `query_string` and `simple_query_string`, which does not feel useful since dates have `epoch_millis` as a parser by default?
</comment><comment author="clintongormley" created="2016-08-25T10:04:07Z" id="242338158">@jpountz the reason for the special format was because dates are no longer required to understand epoch ms, so eg kibana (which always uses epoch ms) wouldn't work with a custom mapped field that doesn't specify `epoch_ms`.  That said, I haven't heard any complaints about this so far, so I think we can close the issue for now.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/count/simple/SimpleCountTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/DateBackwardsCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreTests.java</file></files><comments><comment>Dates: Be backwards compatible with pre 2.x indices</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/TimestampParsingException.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/count/simple/SimpleCountTests.java</file><file>src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file></files><comments><comment>Date Parsing: Add parsing for epoch and epoch in milliseconds</comment></comments></commit></commits></item><item><title>Bail if ES is run as root</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10970</link><project id="" key="" /><description /><key id="73216759">10970</key><summary>Bail if ES is run as root</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T05:39:42Z</created><updated>2015-06-08T13:14:03Z</updated><resolved>2015-05-06T00:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-05T11:52:23Z" id="99057858">LGTM
</comment><comment author="s1monw" created="2015-05-05T13:07:45Z" id="99073396">+1
</comment><comment author="markwalkom" created="2015-05-06T00:23:26Z" id="99270874">Awesome :D
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/common/jna/CLibrary.java</file><file>src/main/java/org/elasticsearch/common/jna/Natives.java</file></files><comments><comment>Merge pull request #10970 from rmuir/bad_idea</comment></comments></commit></commits></item><item><title>Run groovy scripts with no permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10969</link><project id="" key="" /><description>Scripts shouldn't really have side effects. 
There are a number of simple and complicated ways we can reduce their java permissions, this is the simplest one that I see.

This is not a sandbox, it just runs groovy scripts without priviledges since they should not need them.
A sandbox would be more, and involve API changes.
I did however revive the old sandbox test just as a simple start.
</description><key id="73207396">10969</key><summary>Run groovy scripts with no permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-05T04:45:55Z</created><updated>2015-06-08T13:14:52Z</updated><resolved>2015-05-05T18:36:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-05T13:07:21Z" id="99073334">LGTM this a great start
</comment><comment author="uschindler" created="2015-05-05T14:14:12Z" id="99090721">Phantastic. I know that you know what you know that you are doing! URL#getFile() is bad, but OK for those static checks...

To be sure: Is a equals enough, I would have expected that it should be startsWith("/groovy/script"), especially if one tries to define a class inside his script and maybe defines a custom package.
</comment><comment author="rmuir" created="2015-05-05T14:16:15Z" id="99091744">@uschindler yes, groovy always sets this strange "fake" URL. I think its just intended to make it easier to configure in a policy file. That would be hard for us (even if its preferred) because everything there must really be specified as a white-list. Here a black-list (simple check) is easier to reason about.
</comment><comment author="uschindler" created="2015-05-05T14:47:30Z" id="99101414">OK. Thanks for the info!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>src/test/java/org/elasticsearch/script/GroovySecurityTests.java</file></files><comments><comment>Merge pull request #10969 from rmuir/grooooooovy</comment></comments></commit></commits></item><item><title>[openbsd] shield doesn't work on OpenBSD, probably because of blowfish</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10968</link><project id="" key="" /><description>Hello,
I'm trying to learn elasticsearch and I tried shield plugin with ES 1.5.1 on OpenBSD 5.7 snapshot amd64.

```
# ./bin/elasticsearch -v     
Version: 1.5.1, Build: 5e38401/2015-04-09T13:41:35Z, JVM: 1.7.0_71
# find plugins/shield/           
plugins/shield/
plugins/shield/LICENSE.txt
plugins/shield/NOTICE.txt
plugins/shield/config
plugins/shield/commons-codec-1.10.jar
plugins/shield/automaton-1.11-8.jar
plugins/shield/unboundid-ldapsdk-2.3.8.jar
plugins/shield/elasticsearch-shield-1.2.1.jar
```

It could be related to blowfish implementation but I'm not sure. Let's compare shields hashed passwords and OpenBSD blowfish hashed passwords:

```
# cat config/shield/users                                                                                                                                                                                          
es_admin:$2a$10$PQHKemNIRLEz.qWIPNQSbe0y2Yp/B7bIfgdJtjJo5r9g8VBtMosk2
# 
I-search: 
# cat config/shield/users 
# grep jirib /etc/master.passwd  
jirib:$2b$10$m.........anonymized....:1000:1000:staff:0:0:jirib:/home/jirib:/bin/ksh
```

IIRC there was an issue with blowfish $2a passwords, here is what OpenBSD people wrote:

&gt; Solar found a bug in the OpenBSD implementation of bcrypt when hashing long passwords. The length is stored in an unsigned char type, which will overflow and wrap at 256. Although we consider the existence of affected hashes very rare, in order to differentiate hashes generated before and after the fix, we are introducing a new minor 'b'.
&gt; 
&gt; OpenBSD 5.5 (coming this spring) will accept and verify 'b' hashes, although it will still generate 'a' hashes. OpenBSD 5.6 (coming this fall) will change to generating 'b' hashes by default.
&gt; 
&gt; A future release of Solar's bcrypt code should also support 'b'. 

With shied plugin installed:

```
$  curl -u es_admin -XGET 'http://127.0.0.1:9200/'  
Enter host password for user 'es_admin':
{"error":"AuthenticationException[unable to authenticate user [es_admin] for REST request [/]]","status":401}
```

With shield plugin removed:

```
$  curl -XGET 'http://127.0.0.1:9200/'              
{
  "status" : 200,
  "name" : "Ringmaster",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.5.1",
    "build_hash" : "5e38401bc4e4388537a615569ac60925788e1cf4",
    "build_timestamp" : "2015-04-09T13:41:35Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

(I apologize but I could not manage to get some logging about auth issue.)

If you need more info I'd be pleased to provide it.
</description><key id="73143893">10968</key><summary>[openbsd] shield doesn't work on OpenBSD, probably because of blowfish</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jirib</reporter><labels /><created>2015-05-04T22:20:25Z</created><updated>2015-05-05T11:12:56Z</updated><resolved>2015-05-05T11:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T08:31:40Z" id="98993561">@jaymode could you take a look at this please
</comment><comment author="jaymode" created="2015-05-05T09:33:47Z" id="99009776">@jirib how did you install elasticsearch on OpenBSD? Is the config directory with the shield folder in it the same directory that contains your elasticsearch.yml?
</comment><comment author="jirib" created="2015-05-05T09:41:26Z" id="99011853">from OpenBSD ports/packages (http://cvsweb.openbsd.org/cgi-bin/cvsweb/ports/textproc/elasticsearch/). shield and license was under /usr/local/elasticsearch/plugins.

```
/etc/elasticsearch
/etc/elasticsearch/elasticsearch.in.sh
/etc/elasticsearch/elasticsearch.yml
/etc/elasticsearch/logging.yml
/usr/local/elasticsearch
/usr/local/elasticsearch/bin
/usr/local/elasticsearch/bin/plugin
/usr/local/elasticsearch/bin/elasticsearch
/usr/local/elasticsearch/lib
/usr/local/elasticsearch/lib/antlr-runtime-3.5.jar
/usr/local/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
/usr/local/elasticsearch/lib/asm-4.1.jar
/usr/local/elasticsearch/lib/asm-commons-4.1.jar
/usr/local/elasticsearch/lib/elasticsearch-1.5.1.jar
/usr/local/elasticsearch/lib/groovy-all-2.4.0.jar
/usr/local/elasticsearch/lib/jna-4.1.0.jar
/usr/local/elasticsearch/lib/jts-1.13.jar
/usr/local/elasticsearch/lib/log4j-1.2.17.jar
/usr/local/elasticsearch/lib/lucene-analyzers-common-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-core-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-expressions-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-grouping-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-highlighter-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-join-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-memory-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-misc-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-queries-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-queryparser-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-sandbox-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-spatial-4.10.4.jar
/usr/local/elasticsearch/lib/lucene-suggest-4.10.4.jar
/usr/local/elasticsearch/lib/spatial4j-0.4.1.jar
/usr/local/elasticsearch/data
/usr/local/elasticsearch/data/elasticsearch
/usr/local/elasticsearch/data/elasticsearch/nodes
/usr/local/elasticsearch/data/elasticsearch/nodes/0
/usr/local/elasticsearch/data/elasticsearch/nodes/0/node.lock
/usr/local/elasticsearch/plugins
/usr/local/elasticsearch/plugins/syslog
/usr/local/elasticsearch/plugins/syslog/elasticsearch-syslog-1.4.0.3.jar
/usr/local/elasticsearch/config
/usr/local/elasticsearch/config/shield
/usr/local/elasticsearch/config/shield/logging.yml
/usr/local/elasticsearch/config/shield/role_mapping.yml
/usr/local/elasticsearch/config/shield/roles.yml
/usr/local/elasticsearch/config/shield/users
/usr/local/elasticsearch/config/shield/users_roles
```
</comment><comment author="jaymode" created="2015-05-05T09:47:15Z" id="99013086">You will need to move the `/usr/local/elasticsearch/config/shield` to `/etc/elasticsearch/shield`. Can you please confirm that this works?

We do not mention OpenBSD, but have some documentation around this issue when installing elasticsearch that the configuration path may need to be set when using the plugin and esusers commands[1]; we're still working on improving this too.

[1] http://www.elastic.co/guide/en/shield/current/getting-started.html#_configuring_your_environment
</comment><comment author="jirib" created="2015-05-05T10:20:24Z" id="99021039">Works, thank you. Thus NOTABUG.

I have also couple of unrelated questions:

On OpenBSD we do not tell people to setup env vars but daemons set this env vars for themselves.
- handling JAVA_HOME: we use something like this JAVA_HOME="$(/usr/local/bin/javaPathHelper -h elasticsearch)"
- handling ES_INCLUDE: we have the file in /etc/elasticsearch/elasticsearch.in.sh

Do you have any recommendation for us while not telling people to define shell env vars?
Maybe we could for ES_INCLUDE do symlink from /usr/local/elasticsearch/bin/elasticsearch.in.sh -&gt; /etc/elasticsearch/elasticsearch.in.sh ? Or could you add ES_INCLUDE file search also for /etc/elasticsearch?

We could define JAVA_HOME in ES_INCLUDE file? Thus 'bin/plugin' or 'bin/shield/esusers' would work.

If we would define 'ES_JAVA_OPTS="-Des.path.conf=/etc/elasticsearch"' in ES_INCLUDE file would be plugins' configuration located in /etc/elasticsearch (like you told me to mv 'conf/shield' to '/etc/elasticsearch'...)?

Thank you for help and advices.
</comment><comment author="jirib" created="2015-05-05T10:27:07Z" id="99024075">&gt; We could define JAVA_HOME in ES_INCLUDE file? Thus 'bin/plugin' or 'bin/shield/esusers' would work.

Do this "bins" work independently or do they inherit elasticsearch configuration? I mean as we define '-Des.config=/etc/elasticsearch/elasticsearch.yml' thus elasticsearch instance knows about its 'path.config' from yml file, but would know 'path.config' also 'bin/plugin' and/or 'bin/shield/esusers'?
</comment><comment author="jirib" created="2015-05-05T11:06:13Z" id="99033879">FYI I did following modication and 'bin/plugin' and 'bin/shield/esusers' work out of the box (without JAVA_HOME in env) and after shield plugin installation I have 'shield' conf directory in /etc/elasticsearch.

```
# diff -uNp /usr/local/share/examples/elasticsearch/                                                                                                                             [5/1813]
elasticsearch.in.sh    elasticsearch.yml      logging.yml            
cal/share/examples/elasticsearch/elasticsearch.in.sh /etc/elasticsearch/elasticsearch.in.sh                                                                                            &lt;
--- /usr/local/share/examples/elasticsearch/elasticsearch.in.sh Fri May  1 04:37:07 2015
+++ /etc/elasticsearch/elasticsearch.in.sh      Tue May  5 12:56:53 2015
@@ -1,5 +1,8 @@
 #!/bin/sh

+JAVA_HOME="$(/usr/local/bin/javaPathHelper -h elasticsearch)"
+ES_JAVA_OPTS="-Des.path.conf=/etc/elasticsearch"
+
 ES_CLASSPATH=$ES_CLASSPATH:$ES_HOME/lib/elasticsearch-1.5.1.jar:$ES_HOME/lib/*:$ES_HOME/lib/sigar/*

 if [ "x$ES_MIN_MEM" = "x" ]; then
@@ -64,4 +67,4 @@ JAVA_OPTS="$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError"
 JAVA_OPTS="$JAVA_OPTS -XX:+DisableExplicitGC"

 # Ensure UTF-8 encoding by default (e.g. filenames)
-JAVA_OPTS="$JAVA_OPTS -Dfile.encoding=UTF-8"
\ No newline at end of file
+JAVA_OPTS="$JAVA_OPTS -Dfile.encoding=UTF-8"
# ls -l bin/elasticsearch.in.sh                                                                                                                                                         
lrwxr-xr-x  1 root  wheel  38 May  5 12:55 bin/elasticsearch.in.sh -&gt; /etc/elasticsearch/elasticsearch.in.sh
```
</comment><comment author="jirib" created="2015-05-05T11:07:28Z" id="99034222">Ah, not really, I also modified 'bin/plugin' to use ES_INCLUDE too.

```
# If an include wasn't specified in the environment, then search for one...
if [ "x$ES_INCLUDE" = "x" ]; then
    # Locations (in order) to use when searching for an include file.
    for include in /usr/share/elasticsearch/elasticsearch.in.sh \
                   /usr/local/share/elasticsearch/elasticsearch.in.sh \
                   /opt/elasticsearch/elasticsearch.in.sh \
                   ~/.elasticsearch.in.sh \
                   /etc/elasticsearch/elasticsearch.in.sh \
                   $ES_HOME/bin/elasticsearch.in.sh \
                   "`dirname "$0"`"/elasticsearch.in.sh; do
        if [ -r "$include" ]; then
            . "$include"
            break
        fi
    done
# ...otherwise, source the specified include.
elif [ -r "$ES_INCLUDE" ]; then
    . "$ES_INCLUDE"
fi
```
</comment><comment author="jaymode" created="2015-05-05T11:12:56Z" id="99035177">@jirib thanks for the updates on it working and your testing with the scripts and configuration settings.

@tlrx has been working on a lot of our packaging and scripts recently. In other OS's we are usually reading configuration variables from /etc/sysconfig/elasticsearch or /etc/default/elasticsearch; not sure if the equivalent exists in OpenBSD.

That said, I think this discussion may be better continued on the mailing list https://groups.google.com/forum/#!forum/elasticsearch or the new discussion forums https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log Storm on Bad Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10967</link><project id="" key="" /><description>I am seeing massive log spam on my servers with 1.5.2. The errors below are resulting in up to 10 GB log files for a given day. It seems like a pretty vicious logging loop. I will certainly delete this query as a temporary workaround but a single "bad" search should not allow this kind of storm to happen.

[2015-05-04 14:53:00,768][DEBUG][action.search.type       ] [White Fang] [probes][0], node[aZkN-qLtSHyUpn08VqAN6A], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@a5cc7e7] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [Ecstasy][inet[/192.168.102.135:9300]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.search.SearchParseException: [probes][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"filter":{"bool":{"must":[{"range":{"@timestamp":{"gt":"now-61s"}}},{"term":{"_type":"custom_win32_perfformatteddata_perfos_memory"}},{"script":{"lang":"expression","script":"doc['custom_win32_perfformatteddata_perfos_memory.AvailableMBytes'].value \* 1000000.0 / doc['custom_win32_perfformatteddata_perfos_memory.TotalPhysicalMemory'].value &lt; 0.1"}}]}},"size":10}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:721)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:291)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [custom_win32_perfformatteddata_perfos_memory.AvailableMBytes] used in expression does not exist in mappings
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:129)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:432)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter.&lt;init&gt;(ScriptFilterParser.java:135)
    at org.elasticsearch.index.query.ScriptFilterParser.parse(ScriptFilterParser.java:114)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:295)
    at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:705)
    ... 10 more
[2015-05-04 14:53:00,768][DEBUG][action.search.type       ] [White Fang] [issues-2015.05.01][1], node[aZkN-qLtSHyUpn08VqAN6A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@a5cc7e7] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [Ecstasy][inet[/192.168.102.135:9300]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.search.SearchParseException: [issues-2015.05.01][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"filter":{"bool":{"must":[{"range":{"@timestamp":{"gt":"now-61s"}}},{"term":{"_type":"custom_win32_perfformatteddata_perfos_memory"}},{"script":{"lang":"expression","script":"doc['custom_win32_perfformatteddata_perfos_memory.AvailableMBytes'].value \* 1000000.0 / doc['custom_win32_perfformatteddata_perfos_memory.TotalPhysicalMemory'].value &lt; 0.1"}}]}},"size":10}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:721)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:291)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [custom_win32_perfformatteddata_perfos_memory.AvailableMBytes] used in expression does not exist in mappings
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:129)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:432)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter.&lt;init&gt;(ScriptFilterParser.java:135)
    at org.elasticsearch.index.query.ScriptFilterParser.parse(ScriptFilterParser.java:114)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:295)
    at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:705)
    ... 10 more
[2015-05-04 14:53:00,768][DEBUG][action.search.type       ] [White Fang] [secrets][2], node[aZkN-qLtSHyUpn08VqAN6A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@a5cc7e7] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [Ecstasy][inet[/192.168.102.135:9300]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.search.SearchParseException: [secrets][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"filter":{"bool":{"must":[{"range":{"@timestamp":{"gt":"now-61s"}}},{"term":{"_type":"custom_win32_perfformatteddata_perfos_memory"}},{"script":{"lang":"expression","script":"doc['custom_win32_perfformatteddata_perfos_memory.AvailableMBytes'].value \* 1000000.0 / doc['custom_win32_perfformatteddata_perfos_memory.TotalPhysicalMemory'].value &lt; 0.1"}}]}},"size":10}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:721)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:291)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [custom_win32_perfformatteddata_perfos_memory.AvailableMBytes] used in expression does not exist in mappings
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:129)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:432)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter.&lt;init&gt;(ScriptFilterParser.java:135)
    at org.elasticsearch.index.query.ScriptFilterParser.parse(ScriptFilterParser.java:114)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:295)
    at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:705)
</description><key id="73140184">10967</key><summary>Log Storm on Bad Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rossbrower</reporter><labels /><created>2015-05-04T22:04:08Z</created><updated>2015-05-05T17:30:10Z</updated><resolved>2015-05-05T08:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T08:30:29Z" id="98992895">This error is logged once per shard (as the error may be different on each shard).  You can change the level of `action.search` logging from `DEBUG` to `INFO` to avoid having these bad queries logged.
</comment><comment author="rossbrower" created="2015-05-05T17:30:10Z" id="99152289">Fair enough, thanks for the quick reply! I would consider changing the defaults to INFO though as most users are probably not interested in or capable of making sense of this level of detail.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove local Lucene Spatial package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10966</link><project id="" key="" /><description>LUCENE-6422 - PackedQuadTree enhancement - was committed in Lucene 5.2 which is now integrated w/ ES 2.0. This eliminates the need to carry our own local lucene.spatial package. This commit removes the now unnecessary files.
</description><key id="73131215">10966</key><summary>Remove local Lucene Spatial package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T21:23:30Z</created><updated>2015-06-08T08:49:49Z</updated><resolved>2015-05-05T19:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-05T15:17:49Z" id="99112547">LGTM
</comment><comment author="nknize" created="2015-05-05T19:27:51Z" id="99196412">committed 0553001369510afe932559da9de737566d01e063
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>make testing better mimic reality for securitymanager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10965</link><project id="" key="" /><description>The two things are not the same today: so in tests we should use the same technique, and install policy etc the same way. But today tests do things differently than bin/elasticsearch, like use a java.security.policy system property. And this hides bugs!

Once its fixed to work the same way, we see the bugs, the java.io.tmpdir sysprop substitution does not work (#10925 this is some chicken/egg problem in the jvm). We don't use this method and instead dynamically generate it just like other paths in environment.

Second, today things like SSL are not going to work at all. That is because we don't have any grants for system permissions for jdk codesources (but when you set a sysprop, like tests were doing, it sucks in default files).

Finally, this has the advantage that now tests too, don't depend on the environment, like policy configurations on the machine that someone may have changed.
</description><key id="73122237">10965</key><summary>make testing better mimic reality for securitymanager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T20:35:49Z</created><updated>2015-06-08T13:15:02Z</updated><resolved>2015-05-05T00:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T21:08:53Z" id="98850298">good catch rob! LGTM
</comment><comment author="rmuir" created="2015-05-04T21:17:53Z" id="98851966">@rjernst @s1monw i pushed a new commit: https://github.com/rmuir/elasticsearch/commit/072b90296aa0cea3b684cce2cba56ef459096bdc
</comment><comment author="s1monw" created="2015-05-04T21:24:36Z" id="98853194">still LGTM
</comment><comment author="rjernst" created="2015-05-04T23:34:38Z" id="98884709">LGTM, I pushed a change to remove the flag for adding tmp dir permissions.
</comment><comment author="rmuir" created="2015-05-05T00:06:33Z" id="98888584">Thanks for that test improvement @rjernst 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTokenStreamTestCase.java</file><file>src/test/java/org/elasticsearch/test/SecurityBootstrap.java</file></files><comments><comment>Merge pull request #10965 from rmuir/lockdown4</comment></comments></commit></commits></item><item><title>Rename Moving Average models to their "common" names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10964</link><project id="" key="" /><description>Previously, we were using the "statistical", technically accurate name.  Instead, we should probably use the name that people are familiar with, e.g. "Holt Winters" instead of "triple exponential".  To that end:
- `single_exp` becomes `ewma` (exponentially weighted moving average)
- `double_exp` becomes `holt`
- Description still contains reference to the technical name, to facilitate anyone searching for it

When the `triple_exp` is added, it will be called `holt_winters`.
</description><key id="73108063">10964</key><summary>Rename Moving Average models to their "common" names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T19:33:37Z</created><updated>2015-06-08T08:44:08Z</updated><resolved>2015-05-06T13:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-06T08:32:43Z" id="99379720">Other than the typo @rjernst pointed out it LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/reducers/movavg/models/EwmaModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/movavg/models/HoltLinearModel.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/movavg/models/MovAvgModelModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/reducers/movavg/models/TransportMovAvgModelModule.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/moving/avg/MovAvgTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Merge pull request #10964 from polyfractal/feature/aggs_movavg_rename</comment></comments></commit></commits></item><item><title>Remove exitVM permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10963</link><project id="" key="" /><description>The tricky part here is just the windows handler for closing the console.

But its not so bad if we just have that handler properly shut down the instance instead of doing System.exit. 

There is also some general refactoring to Bootstrap. I'm sorry to mix this in, but it was too hard to reason about otherwise for me :)

Tested on linux, mac, and windows. also tested closing windows console worked.
</description><key id="73090909">10963</key><summary>Remove exitVM permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T18:24:22Z</created><updated>2015-06-08T13:15:12Z</updated><resolved>2015-05-04T18:56:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T18:46:27Z" id="98810475">left one minor comment...  LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>src/main/java/org/elasticsearch/bootstrap/ElasticsearchF.java</file></files><comments><comment>Merge pull request #10963 from rmuir/lockdown3</comment></comments></commit></commits></item><item><title>Remove JNI permissions, improve JNI testing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10962</link><project id="" key="" /><description>We don't need to allow JNI loadLibrary permissions since this is easy to do up-front first (which we mostly do already). However, the testing around some of this is suboptimal, if you run tests with 4 jvms, one will get mlockall'd or get console control handler installed (depending on which jvm runs NativesTests). Nothing checks that sigar is working or is not.

So instead explicitly force any JNI stuff to be loaded (e.g. sigar happens implicitly today during the node initialization, but that could easily break somehow during refactoring). Do this also when running tests, and test that sigar works, test that windows control handler gets installed on windows, etc. 
</description><key id="73071587">10962</key><summary>Remove JNI permissions, improve JNI testing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T17:01:22Z</created><updated>2015-06-08T13:15:53Z</updated><resolved>2015-05-04T17:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-04T17:01:32Z" id="98779219">I tested this on linux, os X, and windows.
</comment><comment author="jpountz" created="2015-05-04T17:15:01Z" id="98783985">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/main/java/org/elasticsearch/common/jna/Kernel32Library.java</file><file>src/test/java/org/elasticsearch/common/jna/NativesTests.java</file><file>src/test/java/org/elasticsearch/monitor/SigarTests.java</file><file>src/test/java/org/elasticsearch/test/SecurityHack.java</file></files><comments><comment>Merge pull request #10962 from rmuir/lockdown2</comment></comments></commit></commits></item><item><title>Supporting dots ('.') as separator for dynamic_date_formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10961</link><project id="" key="" /><description>dynamic_date_formats does don't support dots in mapping

**Steps to repro**

1- Modify default-mapping.json file and add

```
{
    "_default_" : {
        "dynamic_date_formats" : ["yyyy.MM.dd HH:mm"]
    }
}
```

2- Create new index and add a new document, without setting the mapping

```
{
    "my_date": "2012.03.08 12:30"
}
```

3- Verify that the my_date field is stored as string, and not as date

```
{
    "tryit_now": {
        "mappings": {
            "dateFormats": {
                "dynamic_date_formats": [
                    "yyyy.MM.dd HH:mm"
                ],
                "properties": {
                    "my_date": {
                        "type": "string"
                    }
                }
            }
        }
    }
}
```

According to the [DocumentParser.java](https://github.com/elastic/elasticsearch/blob/b4efb6c472715756f9920a653be70d3ce16767cc/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L466) we only allow ":", "-" and "/" for separator for dynamic dates.
</description><key id="73071061">10961</key><summary>Supporting dots ('.') as separator for dynamic_date_formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Dates</label><label>adoptme</label><label>bug</label></labels><created>2015-05-04T16:58:40Z</created><updated>2016-12-09T10:17:49Z</updated><resolved>2016-12-09T10:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2015-05-04T17:08:57Z" id="98782066">The workaround for this will be to refactor our mapping and add slashes to the dynamic format:

```
{
    "_default_" : {
        "dynamic_date_formats" : ["yyyy/MM/dd HH:mm"]
    }
}
```

Also, to use any of the [date formats](http://www.elastic.co/guide/en/elasticsearch/reference/1.3/mapping-date-format.html) suggested in our documentation.
</comment><comment author="clintongormley" created="2015-05-04T19:31:53Z" id="98826064">@gmoskovicz In what locales are periods used in dates by default?
</comment><comment author="gmoskovicz" created="2015-05-04T19:42:11Z" id="98828202">@clintongormley we have been seeing that some systems are using dot separator as the default for their dates.

A wat to hack this is also to add seconds to the date format, therefore it will contain 2 ":" and will overpass the restriction:

```
{
    "_default_" : {
        "dynamic_date_formats" : ["yyyy.MM.dd HH:mm:ss"]
    }
}
```

Nowadays the restriction is that the full pattern should contain at least 2 ":", two "-" or two "/" 
</comment><comment author="gmoskovicz" created="2015-05-04T19:43:13Z" id="98828646">But i am concerned also that there are not strict locales using periods in dates by default.
</comment><comment author="clintongormley" created="2015-05-04T19:47:26Z" id="98831104">The problem with using periods for date detection is that it could break floating point numbers, written as strings.  Also, I don't know of any locales that use periods by default (but that may just be ignorance).

We could reduce the requirement for "more than 1" colon, slash etc for date detection.  Or maybe this would cease to be an issue if we finally get around to merging https://github.com/elastic/elasticsearch/pull/6227

/cc @spinscale 
</comment><comment author="gmoskovicz" created="2015-05-04T19:51:42Z" id="98832179">According to [this](http://en.wikipedia.org/wiki/Date_format_by_country) there are some using dots as separator.
</comment><comment author="clintongormley" created="2016-12-09T10:17:49Z" id="265980011">Closing in favour of https://github.com/elastic/elasticsearch/issues/1694</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow shards on shared filesystems to be recovered on any node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10960</link><project id="" key="" /><description>Currently, when all copies of a shard are lost, we reach out to all
other nodes to see whether they have a copy of the data. For a shared
filesystem, though, we can assume that each node has a copy of the data
available, so return a state version of at least 0 for each node.

Fixes #10932
</description><key id="73066615">10960</key><summary>Allow shards on shared filesystems to be recovered on any node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T16:33:30Z</created><updated>2015-06-01T22:35:08Z</updated><resolved>2015-05-04T20:31:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-04T17:07:46Z" id="98781558">@dakrone I am not sure this is something that applies to all shadow replica shared storage systems, but only to ones that has auto mount logic assuming the shard is allocated on a node that it was not allocated on previously. I think there will be other cases where the mounts will he manually managed on node startup.

Maybe we can have a flag to control this behavior, and I would think we should optimize for the more conservative case, and have the behavior on this pull request disabled by default.
</comment><comment author="dakrone" created="2015-05-04T17:15:59Z" id="98784154">@kimchy ++, I think a flag is a good idea and off by default (to be safe), I'll add that.
</comment><comment author="dakrone" created="2015-05-04T17:28:23Z" id="98786678">Pushed another commit adding the flag and defaulting it to false.
</comment><comment author="kimchy" created="2015-05-04T17:55:21Z" id="98796363">left a small comment around the naming of the setting, other than that, LGTM
</comment><comment author="kimchy" created="2015-05-04T20:48:07Z" id="98845933">@dakrone I see you pushed it, but forgot to mentio, I think we should document this? specifically, explain that if this is enabled, auto mounting of shards should somehow be provided
</comment><comment author="dakrone" created="2015-05-04T20:49:40Z" id="98846203">@kimchy I am planning on opening up separate PRs for the documentation, one for master once I figure out how to enable paths for the security manager, and one for 1.x without any references to the security manager
</comment><comment author="kimchy" created="2015-05-04T20:50:16Z" id="98846340">@dakrone sounds good :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Document the `index.shared_filesystem.recover_on_any_node` setting</comment></comments></commit></commits></item><item><title>Eliminate APIs that depend on local files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10959</link><project id="" key="" /><description>We have a number of APIs that rely on the user placing files on every node. For example:
- File scripts must exist in the scripts dir on every node
- Some analyzer filters allow to specify their settings in local files, like a stopwords list or synonyms rules

Requiring to place a file on every node is error prone, and this issue is to discuss how we can lock down all APIs like this, and track the effort.

Last year, we added "indexed scripts", which is a special `.script` index that exists on every node. The index is very simple: it just stores the contents of the script, and can be looked up by the "document" id. I wonder if we could generalize this to a "resources" index, which could have scripts, token filter setup, and anything else we might need in the future.  Even plugins could be forced to use this, so we could remove the ability for plugins to read from local disk altogether? As far as security, I think the resources index should have the ability to be "locked" (metadata block?), which is essentially the same as disabling updating for indexed scripts today.
</description><key id="73062745">10959</key><summary>Eliminate APIs that depend on local files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Settings</label><label>discuss</label><label>Meta</label></labels><created>2015-05-04T16:16:55Z</created><updated>2017-04-12T07:19:46Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T06:19:52Z" id="98965984">It would be awesome to centralise resource management like this.  A couple of issues:
- we'd have to make sure that the "resources" index recovers before all other indices, because they may depend on it (eg for analysis setup)
- corruption or unavailability of this index could impact the rest of your cluster
- some plugins need to be setup before the cluster has started (eg discovery) (but typically these don't require resources other than config)
- how do you lock the resources index without also providing an API to unlock it? 
</comment><comment author="javanna" created="2015-05-05T08:24:56Z" id="98991523">How could we address scripting languages that don't support sandboxing? I guess locking/unlocking the resources index should not be exposed via api, otherwise anybody can use it and do whatever they can? Or am I missing something?
</comment><comment author="rjernst" created="2015-05-06T06:06:41Z" id="99335868">&gt; &gt; how do you lock the resources index without also providing an API to unlock it? 
&gt; 
&gt; How could we address scripting languages that don't support sandboxing? I guess locking/unlocking the resources index should not be exposed via api, otherwise anybody can use it and do whatever they can? 

I don't think this is any different than locking APIs like creating indexes or updating mappings. The "resources" api should be able to have a block on modifications. This is the same as e.g. blocking other metadata actions (and yes I view this as metadata, even though the underlying impl would be in a special index. I'm not sure what indexes scripts are classified as today).
</comment><comment author="tsullivan" created="2017-04-12T07:19:46Z" id="293495094">Would this possibly alleviate the current pain point requirement to close and re-open the index to re-read the synonym database?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem with _site plugin using 1.5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10958</link><project id="" key="" /><description>Hi after the upgrade from 1.5.1 to 1.5.2 the site plugins break. I keep getting 404 for all plugins. I do not have the most common setup, but it is not strange as well. I have my plugins in a separate folder and that does not seem to work. I use a script to startup a node:

```
#!/bin/bash

NODE=idmc

CURRENT_PROJECT=$(pwd)
CONFIG=$CURRENT_PROJECT/config
DATA=$CURRENT_PROJECT/data
LOGS=$CURRENT_PROJECT/logs
PLUGINS=$CURRENT_PROJECT/../../plugins/

BASH_ES_OPTS="-Des.config=$CONFIG/elasticsearch.yml -Des.path.conf=$CONFIG -Des.path.data=$DATA -Des.path.logs=$LOGS -Des.path.plugins=$PLUGINS"

ELASTICSEARCH=$HOME/javalibs/elasticsearch/elasticsearch-1.5.2

$ELASTICSEARCH/bin/elasticsearch $BASH_ES_OPTS
```

If I change the ELASTICSEARCH parameter to 1.5.1 it works. Any problem with the -Des.path.plugins and the security fix for 1.5.2?

Thanks Jettro
</description><key id="73039538">10958</key><summary>Problem with _site plugin using 1.5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jettro</reporter><labels><label>:Plugins</label></labels><created>2015-05-04T14:41:29Z</created><updated>2015-05-15T06:47:12Z</updated><resolved>2015-05-15T06:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T15:22:13Z" id="98750789">Hi @jettro 

Setting `PLUGINS` to an absolute path should fix this.

@spinscale perhaps we should be be converting the plugin path to absolute before we use it?
</comment><comment author="jettro" created="2015-05-05T16:49:56Z" id="99138143">Sorry for the delay, did not have the right laptop with me to try it out.
But I change the configuration to an absolute path and the result is the
same. Going to try to create a test to show the problem, but since elastic
has become kind of big I am not sure I can do it soon :-)

Thanks, so far,
Jettro

On Mon, May 4, 2015 at 5:23 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @jettro https://github.com/jettro
&gt; 
&gt; Setting PLUGINS to an absolute path should fix this.
&gt; 
&gt; @spinscale https://github.com/spinscale perhaps we should be be
&gt; converting the plugin path to absolute before we use it?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10958#issuecomment-98750789
&gt; .

## 

Jettro Coenradie
http://www.gridshore.nl
</comment><comment author="spinscale" created="2015-05-05T16:52:56Z" id="99139316">@jettro can you just paste the configuration you used with an absolute path, that failed as well? I am happy to do the rest.
</comment><comment author="jettro" created="2015-05-05T17:02:00Z" id="99141869">Went over all the steps once more and I now have a working configuration.

PLUGINS=/Users/jettrocoenradie/javalibs/elasticsearch/plugins

BASH_ES_OPTS="-Des.config=$CONFIG/elasticsearch.yml -Des.path.conf=$CONFIG
-Des.path.data=$DATA -Des.path.logs=$LOGS -Des.path.plugins=$PLUGINS"

ELASTICSEARCH=$HOME/javalibs/elasticsearch/elasticsearch-1.5.2

Usually I have the following line:
PLUGINS=$CURRENT_PROJECT/../../plugins

That fails, most likely due to the ../../ in there

But the exact path for PLUGINS does work.

regards Jettro
</comment><comment author="spinscale" created="2015-05-05T21:03:09Z" id="99220873">thanks for the reproduction. the PR referenced in here will fix that problem.

right now the easiest workaround is to use absolute paths... or call some specific function to get the absolute path before passing to elasticsearch, like

```
abspath () { case "$1" in /*)printf "%s\n" "$1";; *)printf "%s\n" "$PWD/$1";; esac; }
```
</comment><comment author="jorge80" created="2015-05-06T21:25:49Z" id="99614218">tried this under cygwin with above
eg.
CURRENT_PROJECT=$(pwd)
CONFIG=$CURRENT_PROJECT/config
bin/elasticsearch "-Des.config=$CONFIG/elasticsearch.yml"
{1.5.1}: Setup Failed ...
- FailedToResolveConfigException[Failed to resolve config path [/elk/elasticsearch/config/elasticsearch.yml], tried file path [\elk\elasticsearch\config\elasticsearch.yml], path file [C:\cygwin\elk\elasticsearch\config\elk\elasticsearch\config\elasticsearch.yml], and classpath]
  org.elasticsearch.env.FailedToResolveConfigException: Failed to resolve config path [/elk/elasticsearch/config/elasticsearch.yml], tried file path [\elk\elasticsearch\config\elasticsearch.yml], path file [C:\cygwin\elk\elasticsearch\config\elk\elasticsearch\config\elasticsearch.yml], and classpath
      at org.elasticsearch.env.Environment.resolveConfig(Environment.java:213)
      at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:68)
      at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:112)
      at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:183)
      at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)

-- this path exist "/elk/elasticsearch/config/elasticsearch.yml" 
</comment><comment author="jorge80" created="2015-05-06T21:30:38Z" id="99615473">and plugins simply not loading

CURRENT_PROJECT=$(pwd)

7ep@pc /elk/elasticsearch
$ PLUGINS=$CURRENT_PROJECT/elasticsearch/plugins

7ep@pc /elk/elasticsearch
$ bin/elasticsearch " -Des.path.plugins=$PLUGINS"
[2015-05-06 23:28:40,784][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-06 23:28:40,964][INFO ][node                     ] [Fasaud] version[1.5.1], pid[2956], build[5e38401/2015-04-09T13:41:35Z]
[2015-05-06 23:28:40,964][INFO ][node                     ] [Fasaud] initializing ...
[2015-05-06 23:28:40,974][INFO ][plugins                  ] [Fasaud] loaded [], sites []
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/test/java/org/elasticsearch/plugins/ResponseHeaderPluginTests.java</file><file>src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigTests.java</file><file>src/test/java/org/elasticsearch/plugins/SitePluginTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchMatchers.java</file></files><comments><comment>HttpServer: Support relative plugin paths in configuration</comment></comments></commit></commits></item><item><title>Update geo-distance-range-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10957</link><project id="" key="" /><description>missing comma
</description><key id="73035138">10957</key><summary>Update geo-distance-range-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">altitude</reporter><labels /><created>2015-05-04T14:29:41Z</created><updated>2015-05-04T15:18:18Z</updated><resolved>2015-05-04T15:18:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T15:18:06Z" id="98748282">thanks @altitude - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update geo-distance-range-filter.asciidoc</comment></comments></commit></commits></item><item><title>Update geo-distance-range-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10956</link><project id="" key="" /><description>missing comma
</description><key id="73034358">10956</key><summary>Update geo-distance-range-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">altitude</reporter><labels /><created>2015-05-04T14:26:00Z</created><updated>2015-05-04T14:28:52Z</updated><resolved>2015-05-04T14:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Centralize admin implementations and action execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10955</link><project id="" key="" /><description>This change removes the multiple implementations of different admin interfaces and centralizes it with AbstractClient. It also makes sure _all_ executions of actions now go through a single AbstractClient#execute method, taking care of copying headers and wrapping listener.
This also has the side benefit of removing all the code around differnet possible clients, and removes quite a bit of code (most of the + code is actually removal of generics and such).

This change also changes how TransportClient is constructed, requiring a Builder to create it, its a breaking change and its noted in the migration guide.

Yea another step towards simplifying the action infra and making it simpler...
</description><key id="73033812">10955</key><summary>Centralize admin implementations and action execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Java API</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T14:23:55Z</created><updated>2015-06-06T16:06:54Z</updated><resolved>2015-05-04T21:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-04T15:34:31Z" id="98754824">very nice @kimchy ! left a couple of minor comments
</comment><comment author="kimchy" created="2015-05-04T16:07:57Z" id="98767555">@javanna answered them, thanks for the review!
</comment><comment author="s1monw" created="2015-05-04T21:31:44Z" id="98854461">this one is huge +1 lets get it in
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the ability to synchronously inject data before startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10954</link><project id="" key="" /><description>As I was trying to create a docker image for an ElasticSearch instance, I wanted to inject some data directly into the image.

Sadly, it seems there is no way of injecting data when the service is not running : I had to create a script that starts the service in background, polls the server with cURL requests until they succeed then stops the service.

All of this seems fragile but I couldn't find a more satisfying solution. What would it take to allow for offline-injection of data in a synchronous manner ?
</description><key id="73033549">10954</key><summary>Add the ability to synchronously inject data before startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ereOn</reporter><labels /><created>2015-05-04T14:22:58Z</created><updated>2015-05-06T20:15:08Z</updated><resolved>2015-05-04T15:15:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T15:15:03Z" id="98747041">Just copy a pre-prepared `data` dir into your new image.
</comment><comment author="ereOn" created="2015-05-06T20:01:50Z" id="99588748">@clintongormley So much for dicussion I see :)

Is there a stable API to generate a customized data folder or does one have to fire up an instance, fill it and just copy the result ?
</comment><comment author="jpountz" created="2015-05-06T20:15:08Z" id="99592930">@ereOn There is no way to do it without starting a node. Starting a node, indexing some data and copying the data directory will work indeed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove shutdownHooks permission</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10953</link><project id="" key="" /><description>This is easy and obvious to remove. We just have to move one of our shutdown hooks before security init.
</description><key id="73033440">10953</key><summary>Remove shutdownHooks permission</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T14:22:28Z</created><updated>2015-06-08T13:16:12Z</updated><resolved>2015-05-04T14:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T14:28:25Z" id="98726626">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file></files><comments><comment>Merge pull request #10953 from rmuir/lockdown</comment></comments></commit></commits></item><item><title>API for locating unrecovered shard copies and their state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10952</link><project id="" key="" /><description>During shard recovery, Elasticsearch reaches out to all the nodes to find out which nodes hold copies of the shard. It then chooses one copy and tries to recover it.  Recovery may fail (eg if there is corruption).

It would be good to have an API which exposes this information, eg:
- which nodes hold copies of the shard
- how recent is each copy (ie which one is likely to be chosen)
- has recovery of that copy failed (and the reason for that)
</description><key id="73011290">10952</key><summary>API for locating unrecovered shard copies and their state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Stats</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T12:45:16Z</created><updated>2015-07-16T22:38:54Z</updated><resolved>2015-07-16T22:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-22T09:01:47Z" id="104581864">I think this would be awesome to have. We already have most of the infrastructure to get this information. We can also make it relatively fast with our new async shard fetching classes `org.elasticsearch.gateway.AsyncShardFetch` together with `TransportNodesListGatewayStartedShards`  We might need to enhance the response information a bit here but it can give use all it was asked for here in a fast way. We don't need to read store file metadata at all. @areek do you want to give this a try, we spoke about getting more into the core of the system and this seems like a rather easy start?
</comment><comment author="clintongormley" created="2015-05-28T18:01:08Z" id="106534078">@areek what information are you able to expose?  I'm thinking that maybe we should add a new API like:

```
GET /{indices}/_shard_copies
```
</comment><comment author="areek" created="2015-06-08T22:42:39Z" id="110164605">Hey @clintongormley,

The API:

```
GET /{indices}/_unassigned_shards

GET /_unassigned_shards
```

(action name is up for debate)

The response will report the following on all copies of unassigned shards for specified indices:
- node id of the host node
- shard version of the copy
- exception (if any) that has been encountered when trying to open the shard index. (will catch if the shard index has been corrupted, etc.)

``` json
{
  "indices": {
    "&lt;index&gt;": {
      "shards": {
        "&lt;shard_id&gt;": [
          {
            "node": "UcZRcveHSjyfiN0rHalDsQ",
            "version": 6,
            "exception": {..}
          },
          {
            ....
          }
        ],
        "&lt;shard_id&gt;": [
          ....
        ]
      }
    }
  }
}
```

Thoughts? 
PR: #11545
</comment><comment author="bleskes" created="2015-06-09T19:27:11Z" id="110475480">nice. I would call this "_list_stores" or something similar (the unassigned name means shards for which we have no store available or are throttled for being assigned). Question-- what is the exception? I presume it's a corruption marker? If so call it corruption? 
</comment><comment author="s1monw" created="2015-06-09T19:30:59Z" id="110477506">@bleskes there is a PR linked #11545
</comment><comment author="areek" created="2015-06-10T03:42:59Z" id="110576558">@bleskes thanks for the suggestion. "_list_stores" seem to imply that the API will list all the shard stores for the indices rather than only list stores of the copies of the currently unassigned shards? But I can not think of a better name though :). The exception is thrown when trying to open the shard index, can be caused by corruption marker or if the segment infos file can not be read. Maybe it still makes sense to call it corruption?
</comment><comment author="bleskes" created="2015-06-10T07:45:36Z" id="110639465">@areek I looked at the implementation and I'm not happy with `_list_stores` either, mainly because we use the `TransportNodesListGatewayStartedShards` and not the `TransportNodesListShardStoreMetaData`  (which is good, but the naming will confusing). I wonder if we should just rename `TransportNodesListGatewayStartedShards` to `TransportNodesListShardStores`  . Will think about this some more.

re: exception vs corruption - I think we should call it corruption and also make `Store#canOpenIndex` leave a corruption marker (as another change). If we can't open an index, for what ever reason, I think it is safe to mark it as such. @s1monw thoughts?

In general it fills to mee better to have API list unassigned shards by default but have a parameter to make list everything the nodes have . Should be very simple to implement and will give us an alternative to telling customers to grep their disks.
</comment><comment author="s1monw" created="2015-06-11T10:15:25Z" id="111077250">&gt; re: exception vs corruption - I think we should call it corruption and also make Store#canOpenIndex leave a corruption marker (as another change). If we can't open an index, for what ever reason, I think it is safe to mark it as such. @s1monw thoughts?

well we can also have exceptions if there is no index there for instance which is not a corruption?
</comment><comment author="areek" created="2015-06-12T05:32:17Z" id="111365831">We could expose this as a `_list_stores` or `_list_shard_stores` API that has a query parameter `state` to pass in the shard routing states to filter out shards. By default, it will only report for the shards that are `unassigned`.

```
# lists all shards in unassigned state
GET /_list_stores

# lists all {index} shards
GET /{index}/_list_stores?state=_all
```

Thoughts?
</comment><comment author="bleskes" created="2015-06-12T06:59:43Z" id="111385277">&gt; well we can also have exceptions if there is no index there for instance which is not a corruption?

I hear you see and see the distinction but I think it's OK to call this corruption (index is gone, is it distinguishable from a missing segment_n file?). All I was going at is that we need another name then a generic exception because currrently it's confusing - it's hard to telll if the operation has failed (ES/API exception) or the shard is bad (and I know we are adding a `failures` section, I think that's still confusing). 

&gt; ```
&gt; GET /{index}/_list_stores?state=_all
&gt; ```

`_list_shard_stores` is maybe even better. I would suggest `shards=_all`  and the default would be `shards=_unassigned` . @clintongormley how do you feel about these namings?
</comment><comment author="clintongormley" created="2015-06-12T16:01:06Z" id="111535711">I don't like including `_list` because that's implied with the GET. We have a `_search_shards` end point.  I'm wondering if this should just be:

```
GET {index}/_shards
```

The `state` filter should use `unassigned`, `relocating`, `started` etc, rather than `_unassigned`, and it should accept a comma-separated list.

I think the default should actually be `all`, instead of `unassigned`. Of course the response should include the shard state.

Do we need to filter by node?  I was considering whether this should be a `_node` API, but leaning towards no.  We could add a `nodes` param to allow filtering on a node spec, perhaps?
</comment><comment author="areek" created="2015-06-12T17:29:32Z" id="111567412">&gt; I'm wondering if this should just be:
&gt; 
&gt; ```
&gt; GET {index}/_shards
&gt; ```
&gt; 
&gt; The state filter should use unassigned, relocating, started etc, rather than _unassigned, and it should accept a comma-separated list.
&gt; I think the default should actually be all, instead of unassigned. Of course the response should include the shard state.

IMO it makes the purpose of the API clear. Locate all shards and their state meta-data for some index/indices.
The response format can be:

``` json
{
  "indices": {
    "&lt;index&gt;": {
      "shards": {
        "&lt;shard_id&gt;": [
          {
            "node": "UcZRcveHSjyfiN0rHalDsQ",
            "version": 6,
            "state": UNASSIGNED | STARTED | INITIALIZING | RELOCATING
            "exception": {..}
          },
          {
            ....
          }
        ],
        "&lt;shard_id&gt;": [
          ....
        ]
      }
    }
  }
}
```

Thoughts?

&gt; Do we need to filter by node? I was considering whether this should be a _node API, but leaning towards no. We could add a nodes param to allow filtering on a node spec, perhaps?

We currently filter by `indices` not `nodes`. Though possible, I think adding a `nodes` param would be confusing. The API should filter on either `indices` or `nodes` but not both.

&gt; well we can also have exceptions if there is no index there for instance which is not a corruption?

@s1monw currently this is indicated by version being `-1` for the shard in the response, should it be reported under exception instead? I can't think of a way to distinguish between shards that have failed in a node (no ShardStateMetaData) and shards that never existed. Thoughts?
</comment><comment author="clintongormley" created="2015-06-12T18:01:23Z" id="111576028">&gt; ```
&gt; "state": UNASSIGNED | STARTED | INITIALIZING | RELOCATING
&gt; ```

lgtm

&gt; We currently filter by indices not nodes. Though possible, I think adding a nodes param would be confusing. The API should filter on either indices or nodes but not both.

Nodes and indices are two ways of looking at the same data.  eg we have indices stats, and we have nodes-indices stats (which show index stats per node).  But agreed that it is a nice to have, most of the time we're interested in viewing this data by index.
</comment><comment author="bleskes" created="2015-06-15T15:34:20Z" id="112111491">&gt; GET {index}/_shards

My concern with just shards is that it's not clear we're going to disk and reporting what we found. It has nothing to do with the current state of shards as far as the cluster is concerned (at list as implemented now) - we don't care if shards are relocating, available for search etc. We need a name the reflect this (or change the API to do more in the future). Maybe something like

```
GET {indices}/_shards

returns:

{
   "index": {
       "1": {
              "routing": [{ ... extracted from the cluster state ... }]
              "stores": [{ ... this is where we put the current output ... }]
        }
  }
```

That said, something simple like `{indices}/_stores` (or something else that gives you what we return now) has my preference, assuming we can find a good name.

&gt; We currently filter by indices not nodes. Though possible, I think adding a nodes param would be confusing. The API should filter on either indices or nodes but not both.

+1  . If we need a node oriented API, we can add that as well later on.

&gt; `"node": "UcZRcveHSjyfiN0rHalDsQ",`

Can we use a DiscoveryNode instead of just an id? id's are hard to resolve and don't mean much on their own (they are random).

&gt; `"state": UNASSIGNED | STARTED | INITIALIZING | RELOCATING`

These are shard routings values. Things like initializing and relocating are a bit confusing imho. I think a simpler message is whether the store is "used" or not. I would use `"state": "unassigned|assigned"` .

&gt; I can't think of a way to distinguish between shards that have failed in a node (no ShardStateMetaData) and shards that never existed

I confused here - shard that never existed will not have an entry right? shards where the ShardStateMetaData is corrupted will only have an exception (or corruption ) and shard where ShardStateMetaData is OK but the lucene index is bad will have both. I feel like I miss something...
</comment><comment author="clintongormley" created="2015-06-17T20:40:55Z" id="112943516">Rethinking this API a bit.  When would you need it?  The typical use case is: I have unassigned primary shards and I want to know if copies of the shard exist somewhere in my cluster and whether they can recover or not. Replicas don't matter because they can recover from the primary.

So what I want to see are a list or shards which could become the primary, where they are, and if there has been a problem trying to open the shard already.  So perhaps we can drop the assigned shards completely and make this an API for unassigned primaries only.

What about:

```
GET {index}/_primary_candidates
```

And we can drop the `state` key as well.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterShardHealth.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>core/src/main/java/org/elasticsearch/client/Requests.java</file><file>core/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/store/Store.java</file><file>core/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/shards/RestIndicesShardStoresAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesShardStoreRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesShardStoreResponseTest.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>A new `_shard_stores` API provides store information for shard copies of indices.</comment><comment>Store information reports on which nodes shard copies exist, the shard</comment><comment>copy version, indicating how recent they are, and any exceptions</comment><comment>encountered while opening the shard index or from earlier engine failure.</comment></comments></commit></commits></item><item><title>Allocation of replicas failed since 1.4.4 -&gt; 1.4.5 upgrade (works again after upgrading more nodes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10951</link><project id="" key="" /><description>Since upgrading a single node in a 10 node cluster from 1.4.4 to 1.4.5 we get unassigned replicas for all new indices created after the "010" node got upgraded.

Example _cat/shards/xxx below:

```
index                          shard prirep state        docs  store ip            node
logstash-tst-endeca-2015.05.03 0     p      STARTED    214976 71.5mb 10.98.252.118 shd-logsearch-db-010.esaccdata10
logstash-tst-endeca-2015.05.03 0     r      UNASSIGNED
logstash-tst-endeca-2015.05.03 1     p      STARTED    214879 72.7mb 10.98.252.118 shd-logsearch-db-010.esaccdata10
logstash-tst-endeca-2015.05.03 1     r      UNASSIGNED
logstash-tst-endeca-2015.05.03 2     p      STARTED    214957 70.9mb 10.98.252.118 shd-logsearch-db-010.esaccdata10
logstash-tst-endeca-2015.05.03 2     r      UNASSIGNED
```

Changing number of replicas to any other number (even 0 and back to 1) does not fix this.

Rerouting shards is not allowed because of `NO(target node version [1.4.4] is older than source node version [1.4.5])`

I have never seen this behaviour. Many times we upgraded rolling over the span of multiple days, with new indices created at every day rollover.

I already checked the changelog of 1.4.5 but could not find anything related. Still I believe this is a bug.

Full output of reroute command:

```
{"error":"RemoteTransportException[[shd-logsearch-db-007.esaccmaster07
inet[/10.98.252.198:9300]
cluster:admin/reroute]]; nested: ElasticsearchIllegalArgumentException[[move_allocation] can't move [logstash-tst-endeca-2015.05.03
0], from [shd-logsearch-db-010.esaccdata10
NP3t4GAkRl2emR07pZw97A
shd-logsearch-db-010.bolcom.net
inet[/10.98.252.118:9300]]{datacenter=ams5, zone=shd-logsearch-db-010, master=false}, to [shd-logsearch-db-007.esaccdata07
IHd_NcH4TVCcD7uibZzX6w
shd-logsearch-db-007.bolcom.net
inet[/10.98.252.115:9300]]{datacenter=ams4, zone=shd-logsearch-db-007, master=false}, since its not allowed, reason: [
YES(shard is not allocated to same node or host)
YES(node passes include/exclude/require filters)
YES(shard is primary)
YES(below shard recovery limit of [2])
YES(allocation disabling is ignored)
YES(allocation disabling is ignored)
YES(node meets awareness requirements)
YES(total shard limit disabled: [-1] &lt;= 0)
NO(target node version [1.4.4] is older than source node version [1.4.5])
YES(enough disk for shard on node, free: [3.5tb])
YES(no snapshots are currently running)]]; ","status":400}
```
</description><key id="72992164">10951</key><summary>Allocation of replicas failed since 1.4.4 -&gt; 1.4.5 upgrade (works again after upgrading more nodes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rtoma</reporter><labels /><created>2015-05-04T11:06:19Z</created><updated>2015-07-13T19:32:13Z</updated><resolved>2015-05-04T14:53:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rtoma" created="2015-05-04T11:24:22Z" id="98681052">Browsed https://github.com/elastic/elasticsearch/blob/v1.4.5/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/NodeVersionAllocationDecider.java#L70

Lucene was changed between 1.4.4 and 1.4.5 (Upgrade to Lucene 4.10.4 bugfix release #9960).
Even though its 'just' a bugfix release, the AllocationDecider does not allow recovery:

```
/* we can allocate if we can recover from a node that is younger or on the same version
* if the primary is already running on a newer version that won't work due to possible
* differences in the lucene index format etc.*/
```
</comment><comment author="rtoma" created="2015-05-04T11:25:45Z" id="98681197">We upgraded a 2nd node and now replicas are started...

Maybe its worth it to improve the release notes text and warn about the impact of an upgrade lucene version?
</comment><comment author="s1monw" created="2015-05-04T12:28:45Z" id="98692597">this has nothing todo with Lucene. We are not forward compatible so we can't recover from a newer version. Thanks for clarifying this issue anyway
</comment><comment author="clintongormley" created="2015-05-04T14:47:52Z" id="98737231">Reopening: I think this at least requires a doc update.
</comment><comment author="jessegonzalez" created="2015-07-13T19:32:13Z" id="121032281">This issue bit me, and had me scratching my head for a while. I could figure out what may have caused 8 out of 10 shards to be "successful" with 0 "failed".
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update rolling upgrade</comment></comments></commit></commits></item><item><title>Add SecureString for sensitive settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10950</link><project id="" key="" /><description>In #10918 we are adding the ability to prompt in a terminal for sensitive settings so they do not need to be stored in a configuration file or specified as a system property. These settings however are stored as a `String` in the settings.

To better protect these sensitive settings, we should add a `SecureString` class that owns the underlying char array and provides a method to clear this settings. There should be a single consumer of the sensitive settings and it should be the responsibility of that consumer to clear this value.
</description><key id="72988624">10950</key><summary>Add SecureString for sensitive settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>enhancement</label></labels><created>2015-05-04T10:42:21Z</created><updated>2016-07-11T17:49:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Wait for mappings to be available on the primary before indexing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10949</link><project id="" key="" /><description>In some cases it might happen that a mapping which is already available on the
master node is not available yet on the node that holds the primary shard.
This commit changes indexing on the primary shard so that if a dynamic update
is triggered then the index operation is re-tried until required mappings are
available locally (using cluster state observing).
</description><key id="72979397">10949</key><summary>Wait for mappings to be available on the primary before indexing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-05-04T10:02:54Z</created><updated>2015-06-08T08:59:59Z</updated><resolved>2015-05-06T14:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-05T08:14:09Z" id="98989888">left 2 comments LGTM in general
</comment><comment author="jpountz" created="2015-05-06T12:55:05Z" id="99442005">@s1monw I initially liked the idea of reusing IllegalIndexShardStateException but this one is more about shards being eg. not started yet, while in this case the shard is started and working, it just happens to not have the latest version of the cluster state. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/test/java/org/elasticsearch/action/support/replication/ShardReplicationOperationTests.java</file><file>src/test/java/org/elasticsearch/indices/state/RareClusterStateTests.java</file></files><comments><comment>Merge pull request #10949 from jpountz/fix/wait_for_mappings_on_primary</comment></comments></commit></commits></item><item><title>Function score query fails with field_value_factor on not (yet) existing field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10948</link><project id="" key="" /><description>Hi,

My ES function score query contains several different functions based on a set of different fields. I'm having problems with field_value_factor on not existing fields. 
I'm not sure if that's expected behavior or a bug in ES. 
On the elasticsearch Google group, nobody had an idea, so any help is appreciated.

A minimal example on how to reproduce the problem:

Example document:

```
curl -XPOST "http://localhost:9200/blog/page/1?pretty" -d '
{
  "author": "tobias",
  "content": "Great",
  "views": 1
}'
```

A search request, that boosts based on the function score looks like that:

```
curl -XPOST "http://localhost:9200/blog/_search" -d '
{
  "query": {
    "function_score": {
      "functions": [
        {
          "field_value_factor": {
            "field": "views",
            "modifier": "log2p"
          }
        }
      ]
    }
  }
}'
```

Everything is fine until this point.
When I change the "field" param to "likes", the requests fails:

```
curl -XPOST "http://localhost:9200/blog/_search" -d '
{
  "query": {
    "function_score": {
      "functions": [
        {
          "field_value_factor": {
            "field": "likes",
            "modifier": "log2p"
          }
        }
      ]
    }
  }
}'
```

-&gt; `ElasticsearchException[Unable to find a field mapper for field [likes]`

"likes" doesn't exist yet in the "page" document type, but it might exist in the future. Or there could be another type "page2", where the "likes" field exists, and I need to query both "page" and "page2" in 1 request.

What I tried, is to put an exist filter beforehand, however, the error remains the same.

```
curl -XPOST "http://localhost:9200/blog/_search" -d '
{
    "query": {
        "function_score": {
            "functions": [
                {
                    "filter": {
                        "exists": {
                            "field": "likes"
                        }
                    },
                    "field_value_factor": {
                        "field": "likes"
                    }
                }
            ]
        }
    }
}'
```

-&gt; `ElasticsearchException[Unable to find a field mapper for field [likes]`

Is this a bug or expected bahavior?
I would expect ES to just skip the function, in case the "exists" filter evaluates to False or the field doesn't exist. 

The error originates from `FieldValueFactorFunctionParser.java` (line 89), in `FieldValueFactorFunctionParser.parse`. 
Is there really a need to check for the existence of all fields during parsing? 

With other function types, there is no problem, so apparently this parsing is not performed. E.g. the following works without an error:

```
curl -XPOST "http://localhost:9200/blog/_search" -d '
{
  "query": {
    "function_score": {
      "functions": [
        {
            "filter": {
                "range": {"likes": {"from": 1}}
            }, 
            "weight": 1.5
        }
      ]
    }
  }
}'
```

Any ideas?

Please let me know, if you need further information.

Best regards,
Tobias
</description><key id="72977060">10948</key><summary>Function score query fails with field_value_factor on not (yet) existing field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">tschubotz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-05-04T09:54:39Z</created><updated>2015-09-01T09:05:30Z</updated><resolved>2015-09-01T09:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-06T00:14:50Z" id="99268597">The "working" example you have isn't really the same; it is doing a filter, not a function score, in which case the field not existing in the mappings means "match no docs".  But for `field_value_factor` (and other functions that try to take the value of a field), if the field does not exist in mappings at all, then the only option is to produce a fake value for each doc (we can do this, but it is trappy and arbitrary).  IMO this does need some more standard behavior throughout ES apis.  For example, we should have the ability to be lenient (warn or ignore) for missing fields, but in those cases we need to have:
1. Well defined behavior
2. A standard way to choose what the behavior should be (on #9521 it was discussed for only queries, but function scores are only sort of queries, maybe it should be the same though).
3. Default to error on missing field (otherwise this is trappy for new users who e.g. mispell a field name in a query)
</comment><comment author="clintongormley" created="2015-05-08T09:40:54Z" id="100174491">In this particular case, we could probably handle this by adding a `missing` parameter, which would be used if a document doesn't have a value, or if the type doesn't have the field mapped.
</comment><comment author="clintongormley" created="2015-05-08T17:48:47Z" id="100310233">https://github.com/elastic/elasticsearch/pull/10845 has already added a `missing` value to `field_value_factor` but it doesn't work for an index which is missing the field in its mapping.

Perhaps the way to go here is to extend support for missing mappings with the suggestion here: https://github.com/elastic/elasticsearch/pull/9521#issuecomment-73662480
</comment><comment author="clintongormley" created="2015-05-22T09:06:35Z" id="104583726">After discussing this in fix it friday, we think that the new `missing` parameter should be extended to apply to types where the field itself is not mapped.
</comment><comment author="andrestc" created="2015-08-23T00:28:48Z" id="133767128">I will take a shot at this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>can't create index start with "logstash-"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10947</link><project id="" key="" /><description>I used to use elasticsearch &amp; logstash to manage log, but today I found the logstash related index can't be created.

Then I try to create an index with "logstah-" prefix manually, it failed:

```
curl -XPUT localhost:9200/logstash-a

{
    "error":"MapperParsingException[mapping [“date”]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [“store” : true]]; ","status":400
}
```

If I create a index with other prefix, it works:

```
curl -XPUT localhost:9200/logstash_a

{
    "acknowledged":true
}
```

Any idea on it? Thanks.
</description><key id="72963202">10947</key><summary>can't create index start with "logstash-"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zitang</reporter><labels /><created>2015-05-04T08:46:24Z</created><updated>2015-05-04T12:12:08Z</updated><resolved>2015-05-04T12:12:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-04T10:30:49Z" id="98671654">I just tested it on 1.5.1 and everything is working.
I think you have a bad index template somewhere?

So when elasticsearch tries to apply it it fails with this error?

Any chance you can share all the information? How you reproduce that?
</comment><comment author="zitang" created="2015-05-04T11:28:18Z" id="98681572">sorry, I've found the reason,
I've ran a script to create some index templates, one template is conflict with logstash template, both templates are:

"template" : "logstash-*",

After delete this template, it works.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST: Calling `_anything` returns confusing error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10946</link><project id="" key="" /><description>Using any non reserved string starting with an underscore results in an `IllegalArgumentException`, maybe we should add a more useful error message here like `unknown request` instead of `no feature for name`. The 4xx HTTP is fine already.

```
# curl 'localhost:9200/_siohgjoidfhjfihfg?pretty'
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "No feature for name [_siohgjoidfhjfihfg]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "No feature for name [_siohgjoidfhjfihfg]"
  },
  "status" : 400
}
```
</description><key id="72939199">10946</key><summary>REST: Calling `_anything` returns confusing error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-05-04T07:01:08Z</created><updated>2016-09-27T19:32:28Z</updated><resolved>2016-09-27T19:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-04T07:51:40Z" id="98616459">This happens because such a request gets interpreted as a get index request with no index nor type, and the expression that starts with '_' is seen as a potential feature to filter out the output (e.g. `_aliases` etc.). This is odd, I agree we should fix it.
</comment><comment author="s1monw" created="2015-05-04T08:00:16Z" id="98619293">at least you can read the error now :)
</comment><comment author="javanna" created="2015-05-04T08:02:08Z" id="98619947">&gt; at least you can read the error now :)

right, that's some beautiful structured output :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java</file></files><comments><comment>Clean up confusing error message on unhandled endpoint</comment></comments></commit></commits></item><item><title>elasticsearch 1.5.2. under cygwin environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10945</link><project id="" key="" /><description>elasticsearch 1.5.2 not loading installed plugins under cygwin x86 environment
Cygwin Package Information
Package                               Version            Status
_autorebase                           001002-1           OK
_update-info-dir                      01382-1            OK
alternatives                          1.3.30c-10         OK
base-cygwin                           3.8-1              OK
base-files                            4.2-3              OK
bash                                  4.3.33-1          OK
binutils                              2.25-1             OK
bzip2                                 1.0.6-2            OK
ca-certificates                       2.3-1              OK
coreutils                             8.23-4             OK
crypt                                 1.2-1              OK
csih                                  0.9.8-6            OK
curl                                  7.42.1-1           OK
cygrunsrv                             1.62-1             OK
cygutils                              1.4.14-1           OK
cygutils-extra                        1.4.14-1           OK
cygwin                                2.0.1-1            OK
dash                                  0.5.8-3            OK
diffutils                             3.3-2              OK
editrights                            1.03-1             OK
file                                  5.22-1             OK
findutils                             4.5.12-1           OK
gawk                                  4.1.1-1            OK
getent                                2.18.90-4          OK
git                                   2.1.1-1            OK
grep                                  2.21-2             OK
groff                                 1.22.3-1           OK
gzip                                  1.6-1              OK
hostname                              3.13-1             OK
info                                  5.2-3              OK
ipc-utils                             1.0-1              OK
less                                  458-2              OK
libargp                               20110921-2         OK
libattr1                              2.4.46-1           OK
libblkid1                             2.25.2-2           OK
libbz2_1                              1.0.6-2            OK
libcatgets1                           1.1-2              OK
libcharset1                           1.14-3             OK
libcom_err2                           1.42.12-2          OK
libcurl4                              7.42.1-1           OK
libdb4.8                              4.8.30-1           OK
libedit0                              20130712-1         OK
libexpat1                             2.1.0-3            OK
libffi6                               3.2.1-1            OK
libgcc1                               4.9.2-3            OK
libgdbm4                              1.8.3-20           OK
libgmp10                              6.0.0a-2           OK
libgssapi_krb5_2                      1.13.1-1           OK
libiconv                              1.14-3             OK
libiconv-devel                        1.14-3             OK
libiconv2                             1.14-3             OK
libidn11                              1.29-1             OK
libintl-devel                         0.19.4-1           OK
libintl8                              0.19.4-1           OK
libk5crypto3                          1.13.1-1           OK
libkrb5_3                             1.13.1-1           OK
libkrb5support0                       1.13.1-1           OK
liblzma5                              5.0.8-1            OK
libmetalink3                          0.1.2-1            OK
libmpfr4                              3.1.2-2            OK
libncursesw10                         5.9-20150404-1     OK
libopenldap2_4_2                      2.4.40-2           OK
libopenssl100                         1.0.2a-1           OK
libp11-kit0                           0.20.7-1           OK
libpcre1                              8.37-1             OK
libpipeline1                          1.4.0-1            OK
libpopt0                              1.16-1             OK
libreadline7                          6.3.8-1            OK
libsasl2_3                            2.1.26-9           OK
libsigsegv2                           2.10-1             OK
libsmartcols1                         2.25.2-2           OK
libsqlite3_0                          3.8.9-1            OK
libssh2_1                             1.5.0-1            OK
libssp0                               4.9.2-3            OK
libstdc++6                            4.9.2-3            OK
libtasn1_6                            4.4-1              OK
libuuid-devel                         2.25.2-2           OK
libuuid1                              2.25.2-2           OK
libxml2                               2.9.2-1            OK
login                                 1.11-1             OK
lynx                                  2.8.7-1            OK
man-db                                2.7.1-1            OK
mintty                                1.1.3-1            OK
mksh                                  50f-1              OK
openssh                               6.8p1-1            OK
openssl                               1.0.2a-1           OK
p11-kit                               0.20.7-1           OK
p11-kit-trust                         0.20.7-1           OK
perl                                  5.14.4-3           OK
perl-Archive-Zip                      1.46-1             OK
perl-B-Generate                       1.50-1             OK
perl-Capture-Tiny                     0.28-1             OK
perl-Compress-Bzip2                   2.22-1             OK
perl-Config-Tiny                      2.22-1             OK
perl-CPAN-Reporter                    1.2011-1           OK
perl-Data-Alias                       1.18-2             OK
perl-Devel-Autoflush                  0.06-1             OK
perl-Digest-HMAC                      1.03-4             OK
perl-Digest-SHA                       5.95-1             OK
perl-Encode-Locale                    1.04-1             OK
perl-Error                            0.17023-1          OK
perl-File-Copy-Recursive              0.38-4             OK
perl-File-HomeDir                     1.00-2             OK
perl-File-Listing                     6.04-4             OK
perl-File-pushd                       1.009-1            OK
perl-HTML-Parser                      3.71-2             OK
perl-HTTP-Cookies                     6.01-4             OK
perl-HTTP-Daemon                      6.01-4             OK
perl-HTTP-Date                        6.02-4             OK
perl-HTTP-Message                     6.06-2             OK
perl-HTTP-Negotiate                   6.01-4             OK
perl-IO-CaptureOutput                 1.1104-1           OK
perl-IO-Socket-INET6                  2.72-1             OK
perl-IO-String                        1.08-4             OK
perl-IO-Tty                           1.12-1             OK
perl-IPC-Cmd                          0.92-1             OK
perl-IPC-Run                          0.94-1             OK
perl-libwww-perl                      6.13-1             OK
perl-LWP-MediaTypes                   6.02-4             OK
perl-Module-ScanDeps                  1.18-1             OK
perl-Module-Signature                 0.73-2             OK
perl-Net-DNS                          0.83-1             OK
perl-Net-HTTP                         6.07-1             OK
perl-Net-IP                           1.26-2             OK
perl-PadWalker                        2.0-1              OK
perl-PAR-Dist                         0.49-2             OK
perl-Pod-Coverage                     0.23-2             OK
perl-Pod-Escapes                      1.07-1             OK
perl-Pod-Simple                       3.30-1             OK
perl-Probe-Perl                       0.03-2             OK
perl-Proc-ProcessTable                0.51-1             OK
perl-Socket6                          0.25-1             OK
perl-Tee                              0.14-4             OK
perl-Term-ReadLine-Gnu                1.26-1             OK
perl-Term-ReadLine-Perl               1.0303-4           OK
perl-TermReadKey                      2.32-1             OK
perl-Test-NoWarnings                  1.04-3             OK
perl-Test-Pod                         1.48-2             OK
perl-Test-Pod-Coverage                1.10-1             OK
perl-Test-Reporter                    1.60-2             OK
perl-Test-Reporter-Transport-Metabase 1.999008-4         OK
perl-Test-Simple                      1.001014-1         OK
perl-URI                              1.67-1             OK
perl-WWW-RobotRules                   6.02-4             OK
perl-XML-LibXML                       2.0118-1           OK
perl-XML-NamespaceSupport             1.11-4             OK
perl-XML-Parser                       2.44-1             OK
perl-XML-SAX                          0.99-4             OK
perl-YAML                             1.14-1             OK
perl_autorebase                       5.14.4-3           OK
perl_base                             5.14.4-3           OK
perl_vendor                           5.14.4-1           OK
popt                                  1.16-1             OK
python                                2.7.8-1            OK
rebase                                4.4.1-1            OK
rsync                                 3.1.1-1            OK
run                                   1.3.3-1            OK
sed                                   4.2.2-3            OK
tar                                   1.27.1-1           OK
tcsh                                  6.18.01-6          OK
terminfo                              5.9-20150404-1     OK
tzcode                                2014j-1            OK
unzip                                 6.0-14             OK
util-linux                            2.25.2-2           OK
vim                                   7.4.692-1          OK
vim-common                            7.4.692-1          OK
vim-minimal                           7.4.692-1          OK
which                                 2.20-2             OK
xxd                                   7.4.692-1          OK
xz                                    5.0.8-1            OK
zlib-devel                            1.2.8-3            OK
zlib0                                 1.2.8-3            OK
$ bin/elasticsearch
[2015-05-04 08:31:40,295][WARN ][bootstrap                ] jvm uses the client                                                                                      vm, make sure to run `java` with the server vm for best performance by adding `-                                                                                     server` to the command line
[2015-05-04 08:31:40,395][INFO ][node                     ] [test] version[1.5.2                                                                                     ], pid[5192], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-04 08:31:40,395][INFO ][node                     ] [test] initializing                                                                                      ...
[2015-05-04 08:31:40,405][INFO ][plugins                  ] [test] loaded [], si                                                                                     tes []
[2015-05-04 08:31:44,335][INFO ][node                     ] [test] initialized
[2015-05-04 08:31:44,395][INFO ][node                     ] [test] starting ...
[2015-05-04 08:31:44,545][INFO ][transport                ] [test] bound_address                                                                                      {inet[/127.0.0.1:9300]}, publish_address {inet[/127.0.0.1:9300]}
## [2015-05-04 08:31:45,005][INFO ][discovery                ] [test] ela/pWkhcy3GR                                 

tested scenarios:
- tried with installed x86 jdk1.7.0_79,jre6,jre7,jdk6u45
- tried with modified conf.files with defined paths to plugiins
- tried with all permissions on whole folders
</description><key id="72934882">10945</key><summary>elasticsearch 1.5.2. under cygwin environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jorge80</reporter><labels><label>feedback_needed</label></labels><created>2015-05-04T06:34:23Z</created><updated>2016-01-18T15:32:27Z</updated><resolved>2016-01-18T15:32:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-04T06:39:36Z" id="98598671">Where are the plugins installed?

It should check `$PWD/plugins` or whatever you have specified `path.plugins` as in your config.
</comment><comment author="jorge80" created="2015-05-04T06:42:50Z" id="98599345">folder structure is follwoing:
bin
config
data
lib
plugins &lt; installed eg. bigdesk plugin
- this is not working for me to directly specify path.plugins in conf.file
</comment><comment author="clintongormley" created="2015-05-04T13:29:24Z" id="98704655">Try:

```
./bin/elasticsearch --path.plugins /abs/path/to/plugins/dir
```

Also: 

```
ls -lf plugins/ plugins/*
```
</comment><comment author="jorge80" created="2015-05-04T18:36:17Z" id="98807408">tried all combinations this is not working for me still
for u its working?

---

&gt; Od: Clinton Gormley notifications@github.com
&gt; Komu: "elastic/elasticsearch" elasticsearch@noreply.github.com
&gt; Datum: 04.05.2015 15:30
&gt; Předmět: Re: [elasticsearch] elasticsearch 1.5.2. under cygwin environment (#10945)
&gt; 
&gt; Try:
&gt; 
&gt;    ./bin/elasticsearch --path.plugins /abs/path/to/plugins/dir
&gt; 
&gt; Also: 
&gt; 
&gt;    ls -lf plugins/ plugins/*
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/10945#issuecomment-98704655
</comment><comment author="clintongormley" created="2015-05-05T07:37:10Z" id="98983198">@jbrablec "it doesn't work" doesn't help much.  eg what was the output of the `ls` command that I asked for above?

If you set the `--path.plugins` to an absolute path, what path do you see when you do `GET _nodes`?
</comment><comment author="tlrx" created="2015-05-05T08:17:19Z" id="98990576">@jbrablec As far as I know the plugin manager doesn't work with Cygwin. So I guess you manually unziped the BigDesk plugin?

If so, the directory layout must look like this:

```
plugins/
plugins/bigdesk/
plugins/bigdesk/_site/
plugins/bigdesk
plugins/bigdesk/_site
plugins/bigdesk/_site/bigdesk-1.0.0.jpg
plugins/bigdesk/_site/bigdesk-2.0.0-SNAPSHOT.jpg
plugins/bigdesk/_site/css
plugins/bigdesk/_site/images
plugins/bigdesk/_site/index.html
plugins/bigdesk/_site/js
plugins/bigdesk/_site/LICENSE
plugins/bigdesk/_site/NOTICE
...
```
</comment><comment author="jorge80" created="2015-05-05T08:35:04Z" id="98994203"> ./bin/ela
sticsearch --path.plugins /cygdrive/c/cygwin/elk/elasticsearch/plugins/bigdesk
[2015-05-05 10:17:24,266][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 10:17:24,616][INFO ][node                     ] [Vixen] version[1.5.2], pid[1716], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 10:17:24,616][INFO ][node                     ] [Vixen] initializing ...
[2015-05-05 10:17:24,646][INFO ][plugins                  ] [Vixen] loaded [], sites []
 
 
pc /elk/elasticsearch
$ ls -lf plugins/ plugins/*
plugins/:
.  ..  bigdesk  elasticsearch-head

plugins/bigdesk:
.  ..  .git  .gitignore  bigdesk-1.0.0.jpg  bigdesk-2.0.0-SNAPSHOT.jpg  css  images  index.html  js  LICENSE  NOTICE  README.md

plugins/elasticsearch-head:
.   .git        .jshintrc  elasticsearch-head.sublime-project  grunt_fileSets.js  LICENCE       README.textile  test
..  .gitignore  dist       Gruntfile.js                        index.html         package.json  src
 
 
installed plugins this way:
via git:
git clone https://github.com/lukas-vlcek/bigdesk.git this will not do in folder structure _sites for this plugin
extracted from zip into folder plugins
created _sites folder manually as it was missing and moved content this is not working too

 /elk/elasticsearch
$ curl -XGET 'http://192.168.0.13:9200/_nodes'
{"cluster_name":"elasticsearch","nodes":{"6b3Z22ApRWST0Hkddyo8bQ":{"name":"Martin Gold","transport_address":"inet[/192.168.0.13:9300]","host":"pc","ip":"192.168.0.13","version":"1.5.2","build":"62ff986","http_address":"inet[/192.168.0.13:9200]","settings":{"name":"Martin Gold","path":{"logs":"C:/cygwin/elk/elasticsearch/logs","plugins":"/cygdrive/c/cygwin/elk/elasticsearch/plugins/","home":"C:\cygwin\elk\elasticsearch"},"cluster":{"name":"elasticsearch"},"client":{"type":"node"},"foreground":"yes"},"os":{"refresh_interval_in_millis":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Core(TM)2 Duo CPU P7450 @ 2.13GHz","mhz":2128,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size_in_bytes":-1},"mem":{"total_in_bytes":4260397056},"swap":{"total_in_bytes":8518897664}},"process":{"refresh_interval_in_millis":1000,"id":3952,"max_file_descriptors":-1,"mlockall":false},"jvm":{"pid":3952,"version":"1.8.0_45","vm_name":"Java HotSpot(TM) Client VM","vm_ver
 sion":"25.45-b02","vm_vendor":"Oracle Corporation","start_time_in_millis":1430814779392,"mem":{"heap_init_in_bytes":268435456,"heap_max_in_bytes":1060372480,"non_heap_init_in_bytes":163840,"non_heap_max_in_bytes":0,"direct_max_in_bytes":1060372480},"gc_collectors":["ParNew","ConcurrentMarkSweep"],"memory_pools":["Code Cache","Metaspace","Par Eden Space","Par Survivor Space","CMS Old Gen"]},"thread_pool":{"percolate":{"type":"fixed","min":2,"max":2,"queue_size":"1k"},"listener":{"type":"fixed","min":1,"max":1,"queue_size":-1},"index":{"type":"fixed","min":2,"max":2,"queue_size":"200"},"refresh":{"type":"scaling","min":1,"max":1,"keep_alive":"5m","queue_size":-1},"suggest":{"type":"fixed","min":2,"max":2,"queue_size":"1k"},"generic":{"type":"cached","keep_alive":"30s","queue_size":-1},"warmer":{"type":"scaling","min":1,"max":1,"keep_alive":"5m","queue_size":-1},"search":{"type":"fixed","min":6,"max":6,"queue_size":"1k"},"flush":{"type":"scaling","min":1,"max":1,"keep_alive":"5
 m","queue_size":-1},"optimize":{"type":"fixed","min":1,"max":1,"queue_size":-1},"management":{"type":"scaling","min":1,"max":5,"keep_alive":"5m","queue_size":-1},"get":{"type":"fixed","min":2,"max":2,"queue_size":"1k"},"merge":{"type":"scaling","min":1,"max":1,"keep_alive":"5m","queue_size":-1},"bulk":{"type":"fixed","min":2,"max":2,"queue_size":"50"},"snapshot":{"type":"scaling","min":1,"max":1,"keep_alive":"5m","queue_size":-1}},"network":{"refresh_interval_in_millis":5000,"primary_interface":{"address":"192.168.0.13","name":"eth10","mac_address":"00:24:D6:3A:20:FA"}},"transport":{"bound_address":"inet[/0:0:0:0:0:0:0:0:9300]","publish_address":"inet[/192.168.0.13:9300]","profiles":{}},"http":{"bound_address":"inet[/0:0:0:0:0:0:0:0:9200]","publish_address":"inet[/192.168.0.13:9200]","max_content_length_in_bytes":104857600},"plugins":[]}}}

@pc /elk/elasticsearch
$ ./bin/elasticsearch --path.plugins /cygdrive/c/cygwin/elk/elasticsearch/plugins/
[2015-05-05 10:33:00,112][WARN ][bootstrap ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 10:33:00,232][INFO ][node ] [Martin Gold] version[1.5.2], pid[3952], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 10:33:00,232][INFO ][node ] [Martin Gold] initializing ...
[2015-05-05 10:33:00,242][INFO ][plugins ] [Martin Gold] loaded [], sites []
[2015-05-05 10:33:04,381][INFO ][node ] [Martin Gold] initialized 
[2015-05-05 10:33:04,492][INFO ][node ] [Martin Gold] starting ...
[2015-05-05 10:33:04,786][INFO ][transport ] [Martin Gold] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.13:9300]}
[2015-05-05 10:33:05,080][INFO ][discovery ] [Martin Gold] elasticsearch/6b3Z22ApRWST0Hkddyo8bQ
[2015-05-05 10:33:08,903][INFO ][cluster.service ] [Martin Gold] new_master [Martin Gold][6b3Z22ApRWST0Hkddyo8bQ][pc][inet[/192.168.0.13:9300]], reason: zen-disco-join (elected_as_master)
[2015-05-05 10:33:09,070][INFO ][http ] [Martin Gold] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.13:9200]}
[2015-05-05 10:33:09,071][INFO ][node ] [Martin Gold] started
[2015-05-05 10:33:09,638][INFO ][gateway ] [Martin Gold] recovered [1] indices into cluster_state

still not loaded

 

---

&gt; Od: Clinton Gormley notifications@github.com
&gt; Komu: "elastic/elasticsearch" elasticsearch@noreply.github.com
&gt; Datum: 05.05.2015 09:38
&gt; Předmět: Re: [elasticsearch] elasticsearch 1.5.2. under cygwin environment (#10945)
&gt; 
&gt; @jbrablec https://github.com/jbrablec "it doesn't work" doesn't help much. eg what was the output of the ls command that I asked for above?
&gt; If you set the --path.plugins to an absolute path, what path do you see when you do GET _nodes?
&gt; —
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/10945#issuecomment-98983198.
</comment><comment author="jorge80" created="2015-05-05T08:42:39Z" id="98995395">yes did unziping as well, not working too + created same structure you posted and copied content which should be there..
plugin manager i get working via plugin.bat under windows cmd line , yes plugin script does not correctly for searching for JAVA_HOME under cygwin i think some modif to plugin  file are needed as did some to plugin.bat to load java correctly and then it works.

---

&gt; Od: Tanguy Leroux notifications@github.com
&gt; Komu: "elastic/elasticsearch" elasticsearch@noreply.github.com
&gt; Datum: 05.05.2015 10:18
&gt; Předmět: Re: [elasticsearch] elasticsearch 1.5.2. under cygwin environment (#10945)
&gt; 
&gt; @jbrablec https://github.com/jbrablec As far as I know the plugin manager doesn't work with Cygwin. So I guess you manually unziped the BigDesk plugin?
&gt; If so, the directory layout must look like this:
&gt; plugins/plugins/bigdesk/plugins/bigdesk/_site/plugins/bigdeskplugins/bigdesk/_siteplugins/bigdesk/_site/bigdesk-1.0.0.jpgplugins/bigdesk/_site/bigdesk-2.0.0-SNAPSHOT.jpgplugins/bigdesk/_site/cssplugins/bigdesk/_site/imagesplugins/bigdesk/_site/index.htmlplugins/bigdesk/_site/jsplugins/bigdesk/_site/LICENSEplugins/bigdesk/_site/NOTICE...—
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/10945#issuecomment-98990576.
</comment><comment author="tlrx" created="2015-05-05T11:01:07Z" id="99033011">&gt; plugins/bigdesk:
&gt; .  ..  .git  .gitignore  bigdesk-1.0.0.jpg  bigdesk-2.0.0-SNAPSHOT.jpg  css  images  index.html  js  LICENSE  NOTICE  README.md

This is not the expected directory structure

&gt; created _sites folder manually as it was missing and moved content this is not working too

It `_site` not `_sites`.

Also, what is your OS?

Once your plugin directory structure is good, you can try
 `./bin/elasticsearch --path.plugins C:/cygdrive/c/cygwin/elk/elasticsearch/plugins/` 
or  
`./bin/elasticsearch -Des.path.plugins="C:\\path\to\real\plugins\folder"`
</comment><comment author="jorge80" created="2015-05-05T14:31:07Z" id="99095599">It _site not _sites.
---my bad, with _site is not working too

OS
---Microsoft Windows 7 Enterprise
6.1.7601 Service Pack 1 7601 x64

---Microsoft Windows 7 Pro
6.1.7601 Service Pack 1 7601 x64

Once your plugin directory structure is good, you can try
./bin/elasticsearch --path.plugins C:/cygdrive/c/cygwin/elk/elasticsearch/plugins/

$ ./bin/elasticsearch --path.plugins C:/cygdrive/c/cygwin/elk/elasticsearch/plugins
[2015-05-05 16:23:56,134][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 16:23:56,243][INFO ][node                     ] [Kierrok] version[1.5.2], pid[2748], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 16:23:56,243][INFO ][node                     ] [Kierrok] initializing ...
[2015-05-05 16:23:56,259][INFO ][plugins                  ] [Kierrok] loaded [], sites []
[2015-05-05 16:24:00,440][INFO ][node                     ] [Kierrok] initialized
[2015-05-05 16:24:00,564][INFO ][node                     ] [Kierrok] starting ...
[2015-05-05 16:24:00,861][INFO ][transport                ] [Kierrok] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.13:9300]}
[2015-05-05 16:24:01,126][INFO ][discovery                ] [Kierrok] elasticsearch/QQGRJ6Y4SwKjpxueH2muHQ
[2015-05-05 16:24:04,964][INFO ][cluster.service          ] [Kierrok] new_master [Kierrok][QQGRJ6Y4SwKjpxueH2muHQ][pc][inet[/192.168.0.13:9300]], reason: zen-disco-join (elected_as_master)
[2015-05-05 16:24:05,198][INFO ][http                     ] [Kierrok] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.13:9200]}
[2015-05-05 16:24:05,198][INFO ][node                     ] [Kierrok] started
[2015-05-05 16:24:06,321][INFO ][gateway                  ] [Kierrok] recovered [1] indices into cluster_state

getting this
{"error":"ElasticsearchIllegalArgumentException[No feature for name [_site]]","status":400}

otherwise its ok on that port
{
  "status" : 200,
  "name" : "Kierrok",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}

./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins\bigdesk"

$ ./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins\bigdesk"
[2015-05-05 16:27:33,442][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 16:27:33,567][INFO ][node                     ] [Siena Blaze] version[1.5.2], pid[2540], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 16:27:33,567][INFO ][node                     ] [Siena Blaze] initializing ...
[2015-05-05 16:27:33,567][INFO ][plugins                  ] [Siena Blaze] loaded [], sites []
[2015-05-05 16:27:37,748][INFO ][node                     ] [Siena Blaze] initialized
[2015-05-05 16:27:37,873][INFO ][node                     ] [Siena Blaze] starting ...
[2015-05-05 16:27:38,154][INFO ][transport                ] [Siena Blaze] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.13:9300]}
[2015-05-05 16:27:38,419][INFO ][discovery                ] [Siena Blaze] elasticsearch/9dW0uxwJRKqbruKm7QK2QQ
[2015-05-05 16:27:42,272][INFO ][cluster.service          ] [Siena Blaze] new_master [Siena Blaze][9dW0uxwJRKqbruKm7QK2QQ][pc][inet[/192.168.0.13:9300]], reason: zen-disco-join (elected_as_master)
[2015-05-05 16:27:42,428][INFO ][http                     ] [Siena Blaze] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.13:9200]}
[2015-05-05 16:27:42,428][INFO ][node                     ] [Siena Blaze] started
[2015-05-05 16:27:42,990][INFO ][gateway                  ] [Siena Blaze] recovered [1] indices into cluster_state

./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins"
[2015-05-05 16:28:34,548][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 16:28:34,673][INFO ][node                     ] [Shooting Star] version[1.5.2], pid[332], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 16:28:34,673][INFO ][node                     ] [Shooting Star] initializing ...
[2015-05-05 16:28:34,673][INFO ][plugins                  ] [Shooting Star] loaded [], sites []
[2015-05-05 16:28:38,853][INFO ][node                     ] [Shooting Star] initialized
[2015-05-05 16:28:38,978][INFO ][node                     ] [Shooting Star] starting ...
[2015-05-05 16:28:39,259][INFO ][transport                ] [Shooting Star] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.13:9300]}
[2015-05-05 16:28:39,524][INFO ][discovery                ] [Shooting Star] elasticsearch/8XMZuq__SqmzQ2Ptxi52jQ
[2015-05-05 16:28:43,346][INFO ][cluster.service          ] [Shooting Star] new_master [Shooting Star][8XMZuq__SqmzQ2Ptxi52jQ][pc][inet[/192.168.0.13:9300]], reason: zen-disco-join (elected_as_master)
[2015-05-05 16:28:43,502][INFO ][http                     ] [Shooting Star] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.13:9200]}
[2015-05-05 16:28:43,502][INFO ][node                     ] [Shooting Star] started
[2015-05-05 16:28:44,064][INFO ][gateway                  ] [Shooting Star] recovered [1] indices into cluster_state

----not working too
----have this currently on

java -version
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b15)
Java HotSpot(TM) Client VM (build 25.45-b02, mixed mode)

 

---

&gt; Od: Tanguy Leroux notifications@github.com
&gt; Komu: "elastic/elasticsearch" elasticsearch@noreply.github.com
&gt; Datum: 05.05.2015 13:02
&gt; Předmět: Re: [elasticsearch] elasticsearch 1.5.2. under cygwin environment (#10945)
&gt; 
&gt; plugins/bigdesk:
&gt; .  ..  .git  .gitignore  bigdesk-1.0.0.jpg  bigdesk-2.0.0-SNAPSHOT.jpg  css  images  index.html  js  LICENSE  NOTICE  README.md
&gt; This is not the expected directory structure
&gt; created _sites folder manually as it was missing and moved content this is not working too
&gt; It _site not _sites.
&gt; Also, what is your OS?
&gt; Once your plugin directory structure is good, you can try
&gt;  ./bin/elasticsearch --path.plugins C:/cygdrive/c/cygwin/elk/elasticsearch/plugins/ 
&gt; or

./bin/elasticsearch -Des.path.plugins="C:\path\to\real\plugins\folder"
—
Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/10945#issuecomment-99033011.
</comment><comment author="tlrx" created="2015-05-05T14:34:49Z" id="99096919">&gt; ./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins\bigdesk"

My bad, can you try: ./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins" and also show us the "plugins" directory structure in detail? Just to check that it is ok.
</comment><comment author="jorge80" created="2015-05-05T14:54:43Z" id="99103876"> ./bin/elasticsearch -Des.
 
path.plugins="C:\cygwin\elk\elasticsearch\plugins"
[2015-05-05 16:39:19,666][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-05-05 16:39:19,796][INFO ][node                     ] [Hardshell] version[1.5.2], pid[2692], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-05 16:39:19,796][INFO ][node                     ] [Hardshell] initializing ...
[2015-05-05 16:39:19,806][INFO ][plugins                  ] [Hardshell] loaded [], sites []
[2015-05-05 16:39:24,150][INFO ][node                     ] [Hardshell] initialized
[2015-05-05 16:39:24,272][INFO ][node                     ] [Hardshell] starting ...
[2015-05-05 16:39:24,606][INFO ][transport                ] [Hardshell] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.13:9300]}
[2015-05-05 16:39:24,890][INFO ][discovery                ] [Hardshell] elasticsearch/JROP9wSfSEa5rsJNgQ5JZg
[2015-05-05 16:39:28,702][INFO ][cluster.service          ] [Hardshell] new_master [Hardshell][JROP9wSfSEa5rsJNgQ5JZg][pc][inet[/192.168.0.13:9300]], reason: zen-disco-join (elected_as_master)
[2015-05-05 16:39:28,867][INFO ][http                     ] [Hardshell] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.13:9200]}
[2015-05-05 16:39:28,868][INFO ][node                     ] [Hardshell] started
[2015-05-05 16:39:29,426][INFO ][gateway                  ] [Hardshell] recovered [1] indices into cluster_state

bigdesk
-rwxrwxrwx  1 Administrators None  1775  2. 5  22.39 NOTICE
-rwxrwxrwx  1 Administrators None 11358  2. 5  22.39 LICENSE
-rwxrwxrwx  1 Administrators None    17  2. 5  22.39 .gitignore
drwxrwxrwx+ 1 Administrators None     0  4. 5  20.16 .git
drwxr-xr-x+ 1 Administrators None     0  5. 5  16.19 _site
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 .
 
 
 
bigdesk/_site
$ ls -latr
celkem 232
-rwxrwxrwx  1 Administrators None  1775  2. 5  22.39 NOTICE
-rwxrwxrwx  1 Administrators None 11358  2. 5  22.39 LICENSE
-rwxrwxrwx  1 Administrators None  8507  2. 5  22.39 README.md
-rwxrwxrwx  1 Administrators None 93038  2. 5  22.39 bigdesk-2.0.0-SNAPSHOT.jpg
-rwxrwxrwx  1 Administrators None 91393  2. 5  22.39 bigdesk-1.0.0.jpg
-rwxrwxrwx  1 Administrators None  6435  2. 5  22.39 index.html
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 css
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 images
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 js
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 .
drwxrwxrwx+ 1 Administrators None     0  5. 5  16.19 ..

 
 

---

&gt; Od: Tanguy Leroux notifications@github.com
&gt; Komu: "elastic/elasticsearch" elasticsearch@noreply.github.com
&gt; Datum: 05.05.2015 16:35
&gt; Předmět: Re: [elasticsearch] elasticsearch 1.5.2. under cygwin environment (#10945)
&gt; 
&gt; ./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins\bigdesk"
&gt; My bad, can you try: ./bin/elasticsearch -Des.path.plugins="C:\cygwin\elk\elasticsearch\plugins" and also show us the "plugins" directory structure in detail? Just to check that it is ok.
&gt; —
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/10945#issuecomment-99096919.
</comment><comment author="jorge80" created="2015-05-06T12:24:11Z" id="99429566">should be related to this issue ? #10958 opened 2 days ago by jettro 
</comment><comment author="clintongormley" created="2016-01-18T15:32:27Z" id="172562294">This appears to work correctly on 2.1.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GClient and GNode not exists in 1.5 any more but the documentation is very misleading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10944</link><project id="" key="" /><description>http://www.elastic.co/guide/en/elasticsearch/client/groovy-api/current/client.html

Moved to Groovy extensions, so that this documentation should update 
</description><key id="72930837">10944</key><summary>GClient and GNode not exists in 1.5 any more but the documentation is very misleading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JamesLuoau</reporter><labels /><created>2015-05-04T06:04:03Z</created><updated>2015-05-04T13:22:52Z</updated><resolved>2015-05-04T13:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T13:22:51Z" id="98703347">Closing as duplicate of https://github.com/elastic/elasticsearch-groovy/issues/24
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix add node name to _cat/recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10943</link><project id="" key="" /><description>fix #8041
</description><key id="72914463">10943</key><summary>Fix add node name to _cat/recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephenfournier</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label></labels><created>2015-05-04T03:51:15Z</created><updated>2015-11-21T19:38:24Z</updated><resolved>2015-11-21T19:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-04T15:38:50Z" id="98757099">@Myll thanks for the PR! Unfortunately the tests are failing with this, see:

```
mvn test -Pdev -Dtests.seed=76787037B2A4AA85 -Dtests.class=org.elasticsearch.test.rest.Rest2Tests -Dtests.slow=true -Dtests.method="test {yaml=cat.recovery/10_basic/Test cat recovery output}" -Des.logger.level=ERROR -Dtests.heap.size=512m -Dtests.locale=sk -Dtests.timezone=Asia/Ulan_Bator
```

Can you fix the test in `rest-api-spec/test/cat.recovery/10_basic.yaml` to expect the correct output?
</comment><comment author="clintongormley" created="2015-11-21T19:38:24Z" id="158676777">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java API documentation - refresh API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10942</link><project id="" key="" /><description>There doesn't appear to be any documentation for the Java API on how to use the refresh API to say, force an index to refresh.

I can see code for using `client().admin().indices().prepareRefresh()` within the test suite but I'm unsure if this is the right/only way to initiate an index refresh.
</description><key id="72881394">10942</key><summary>Java API documentation - refresh API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2015-05-03T23:18:43Z</created><updated>2015-12-31T14:23:33Z</updated><resolved>2015-12-31T14:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-30T17:14:30Z" id="168036960">I'll add it shortly after #15713. 

Some examples that will be added to the guide soonish:

``` java
// Refresh all indices
client.admin().indices().prepareRefresh().get();

// Refresh one index
client.admin().indices().prepareRefresh("twitter").get();

// Refresh many indices
client.admin().indices().prepareRefresh("twitter", "company").get();
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add documentation for Java API refresh API</comment></comments></commit></commits></item><item><title>Java API documentation - no information on update-index-settings API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10941</link><project id="" key="" /><description>The Java API docs don't contain any information on how to programmatically change the index settings using the update-index-settings API with the Java API.  This would be useful for users who need to update their index settings on the fly from within their clients.  
</description><key id="72880910">10941</key><summary>Java API documentation - no information on update-index-settings API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2015-05-03T23:15:43Z</created><updated>2015-12-31T13:54:20Z</updated><resolved>2015-12-31T13:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add documentation for Java API update/get settings API</comment></comments></commit></commits></item><item><title>Automatically thread client based action listeners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10940</link><project id="" key="" /><description>Today, we rely on the user to set request listener threads to true when they are on the client side in order not to block the IO threads on heavy operations. This proves to be very trappy for users, and end up creating problems that are very hard to debug.
Instead, we can do the right thing, and automatically thread listeners that are used from the client when the client is a node client or a transport client.
This change also removes the ability to set request level listener threading, in the effort of simplifying the code path and reasoning around when something is threaded and when it is not.
</description><key id="72872437">10940</key><summary>Automatically thread client based action listeners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Java API</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-05-03T22:00:55Z</created><updated>2016-01-20T06:58:19Z</updated><resolved>2015-05-04T09:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-03T22:02:02Z" id="98549590">Note, there is some repetition in the code between the different clients (client, cluster and indices). I want to address it in a different pull request, I am personally leaning towards having a single client with a `adminClusterXXX` and `adminIndicesXXX` APIs, which will cleanup the code even more.
</comment><comment author="kimchy" created="2015-05-03T22:02:35Z" id="98549612">@spinscale would be great if you can have a look, this is what we discussed in your listener thread change on the transport client handler code
</comment><comment author="kimchy" created="2015-05-03T22:05:43Z" id="98549729">btw, this is a change that I would also love to have to simplify the transport handler response code (similar to the request handler changes I made), the fact that there isn't anymore a request level decision on which executor to use on the transport node proxy greatly help the future simplification there
</comment><comment author="s1monw" created="2015-05-04T07:31:10Z" id="98609543">I love this change - this is awesome. Can we have a simple test that ensures we are spawning a thread when we need it ie. create a dedicated transport client that will call the listener on it's network thread etc. 
</comment><comment author="kimchy" created="2015-05-04T08:18:11Z" id="98626757">@s1monw thanks for the review, addressed the comments
</comment><comment author="s1monw" created="2015-05-04T08:57:34Z" id="98641123">left one minor comment otherwise LGTM
</comment><comment author="spinscale" created="2015-05-04T09:03:52Z" id="98642127">nice and clean, awesome. LGTM
</comment><comment author="node" created="2016-01-20T06:58:19Z" id="173111896">I got the same question ,and then found this issue. So what should I do to fix this ?  Update my elasticsearch version to 2.0 ?  
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionRequest.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>src/main/java/org/elasticsearch/action/support/PlainListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/ThreadedActionListener.java</file><file>src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/delete/RestDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/put/RestPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/verify/RestVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/reroute/RestClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/shards/RestClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/close/RestCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/indices/RestIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/types/RestTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/open/RestOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/recovery/RestRecoveryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/get/RestGetIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/fieldstats/RestFieldStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestHeadAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/mlt/RestMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/script/RestPutIndexedScriptAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/rest/action/termvectors/RestMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>src/main/java/org/elasticsearch/river/RiversService.java</file><file>src/test/java/org/elasticsearch/action/ListenerActionTests.java</file><file>src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java</file><file>src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file></files><comments><comment>Automatically thread client based action listeners</comment><comment>Today, we rely on the user to set request listener threads to true when they are on the client side in order not to block the IO threads on heavy operations. This proves to be very trappy for users, and end up creating problems that are very hard to debug.</comment><comment>Instead, we can do the right thing, and automatically thread listeners that are used from the client when the client is a node client or a transport client.</comment><comment>This change also removes the ability to set request level listener threading, in the effort of simplifying the code path and reasoning around when something is threaded and when it is not.</comment><comment>closes #10940</comment></comments></commit></commits></item><item><title>Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10939</link><project id="" key="" /><description>I'm trying to package `elasticsearch` for [Alpine Linux](http://alpinelinux.org) &amp; am new to `maven`. I am building from a clean fakeroot &amp; perhaps this error is similar to [this issue with elasticsearch-river-couchdb](https://github.com/elastic/elasticsearch-river-couchdb/issues/83).

Error with version 1.5.2:

```
[ERROR] [ERROR] Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13
```

Error with today's master.zip:

```
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13
```

&amp; an extra `&lt;repository&gt;` should be configured in `pom.xml`
</description><key id="72823153">10939</key><summary>Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">itoffshore</reporter><labels /><created>2015-05-03T15:39:12Z</created><updated>2015-05-07T15:18:51Z</updated><resolved>2015-05-07T15:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-04T07:26:38Z" id="98608898">I found very strange that you have an issue with 1.5.2 which has been released.
Any chance you got a network issue or something similar?

Could you share a `pom.xml` which does not work so we can reproduce your issue?
</comment><comment author="itoffshore" created="2015-05-04T18:57:05Z" id="98812868">`pom.xml` from the [top level of the source](http://hastebin.com/dipebocabu.apache)

&amp; [debug log](http://hastebin.com/lizomepeki.coffee)

~~To do with [codehaus services being terminated](https://www.codehaus.org/termination.html) ?~~
</comment><comment author="itoffshore" created="2015-05-06T13:03:59Z" id="99448109">As the hastebin has expired here is the debug log:

```
&gt;&gt;&gt; elasticsearch: Unpacking /var/cache/distfiles/elasticsearch-1.5.2.tar.gz...
Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T11:57:37+00:00)
Maven home: /usr/share/java/maven-3.3.3
Java version: 1.7.0_79, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-1.7-openjdk/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "3.16.0-0.bpo.4-amd64", arch: "amd64", family: "unix"
[DEBUG] Created new class realm maven.api
[DEBUG] Importing foreign packages into class realm maven.api
[DEBUG]   Imported: javax.enterprise.inject.* &lt; plexus.core
[DEBUG]   Imported: javax.enterprise.util.* &lt; plexus.core
[DEBUG]   Imported: javax.inject.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.artifact &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.classrealm &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.cli &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.configuration &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.exception &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.execution &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.execution.scope &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.lifecycle &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.model &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.monitor &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.plugin &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.profiles &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.project &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.reporting &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.repository &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.rtinfo &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.settings &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.toolchain &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.usability &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.authentication &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.authorization &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.events &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.observers &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.proxy &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.repository &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.resource &lt; plexus.core
[DEBUG]   Imported: org.codehaus.classworlds &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.* &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.classworlds &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.component &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.configuration &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.container &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.context &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.lifecycle &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.logging &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.personality &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.Xpp3Dom &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlPullParser &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlPullParserException &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlSerializer &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.* &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.artifact &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.collection &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.deployment &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.graph &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.impl &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.installation &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.internal.impl &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.metadata &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.repository &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.resolution &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.spi &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.transfer &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.version &lt; plexus.core
[DEBUG]   Imported: org.slf4j.* &lt; plexus.core
[DEBUG]   Imported: org.slf4j.spi.* &lt; plexus.core
[DEBUG] Populating class realm maven.api
[INFO] Error stacktraces are turned on.
[DEBUG] Reading global settings from /usr/share/java/maven-3.3.3/conf/settings.xml
[DEBUG] Reading user settings from /home/stuart/.m2/settings.xml
[DEBUG] Reading global toolchains from /usr/share/java/maven-3.3.3/conf/toolchains.xml
[DEBUG] Reading user toolchains from /home/stuart/.m2/toolchains.xml
[DEBUG] Using local repository at /home/stuart/.m2/repository
[DEBUG] Using manager EnhancedLocalRepositoryManager with priority 10.0 for /home/stuart/.m2/repository
[INFO] Scanning for projects...
[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://repository.codehaus.org/
[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://repository.codehaus.org/
Downloading: http://repository.codehaus.org/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[DEBUG] Writing tracking file /home/stuart/.m2/repository/org/sonatype/oss/oss-parent/7/oss-parent-7.pom.lastUpdated
[DEBUG] Using transporter WagonTransporter with priority -1.0 for https://repo.maven.apache.org/maven2
[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for https://repo.maven.apache.org/maven2
Downloading: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[DEBUG] Writing tracking file /home/stuart/.m2/repository/org/sonatype/oss/oss-parent/7/oss-parent-7.pom.lastUpdated
[ERROR] [ERROR] Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13
 @ 
[ERROR] The build could not read 1 project -&gt; [Help 1]
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13

    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:422)
    at org.apache.maven.graph.DefaultGraphBuilder.collectProjects(DefaultGraphBuilder.java:419)
    at org.apache.maven.graph.DefaultGraphBuilder.getProjectsForMavenReactor(DefaultGraphBuilder.java:410)
    at org.apache.maven.graph.DefaultGraphBuilder.build(DefaultGraphBuilder.java:83)
    at org.apache.maven.DefaultMaven.buildGraph(DefaultMaven.java:491)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:219)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]   
[ERROR]   The project org.elasticsearch:elasticsearch:1.5.2 (/home/stuart/aports/testing/elasticsearch/src/elasticsearch-1.5.2/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM for org.elasticsearch:elasticsearch:1.5.2: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13 -&gt; [Help 2]
org.apache.maven.model.resolution.UnresolvableModelException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:197)
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:246)
    at org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:978)
    at org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:796)
    at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:325)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:469)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:438)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:401)
    at org.apache.maven.graph.DefaultGraphBuilder.collectProjects(DefaultGraphBuilder.java:419)
    at org.apache.maven.graph.DefaultGraphBuilder.getProjectsForMavenReactor(DefaultGraphBuilder.java:410)
    at org.apache.maven.graph.DefaultGraphBuilder.build(DefaultGraphBuilder.java:83)
    at org.apache.maven.DefaultMaven.buildGraph(DefaultMaven.java:491)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:219)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:444)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:246)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:223)
    at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveArtifact(DefaultRepositorySystem.java:294)
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:193)
    ... 25 more
Caused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed(ArtifactTransportListener.java:43)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:355)
    at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:67)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$DirectExecutor.execute(BasicRepositoryConnector.java:581)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector.get(BasicRepositoryConnector.java:249)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:520)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:421)
    ... 29 more
Caused by: org.apache.maven.wagon.TransferFailedException: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:1066)
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:960)
    at org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
    at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)
    at org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
    at org.eclipse.aether.transport.wagon.WagonTransporter$GetTaskRunner.run(WagonTransporter.java:560)
    at org.eclipse.aether.transport.wagon.WagonTransporter.execute(WagonTransporter.java:427)
    at org.eclipse.aether.transport.wagon.WagonTransporter.get(WagonTransporter.java:404)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$GetTaskRunner.runTask(BasicRepositoryConnector.java:447)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:350)
    ... 34 more
Caused by: javax.net.ssl.SSLException: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at sun.security.ssl.Alerts.getSSLException(Alerts.java:208)
    at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1904)
    at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1862)
    at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1845)
    at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1366)
    at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1343)
    at org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:275)
    at org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:254)
    at org.apache.maven.wagon.providers.http.httpclient.impl.conn.HttpClientConnectionOperator.connect(HttpClientConnectionOperator.java:123)
    at org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:318)
    at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:363)
    at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:219)
    at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:195)
    at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:86)
    at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExec.execute(RedirectExec.java:108)
    at org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)
    at org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:832)
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:983)
    ... 43 more
Caused by: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at sun.security.validator.PKIXValidator.&lt;init&gt;(PKIXValidator.java:90)
    at sun.security.validator.Validator.getInstance(Validator.java:179)
    at sun.security.ssl.X509TrustManagerImpl.getValidator(X509TrustManagerImpl.java:314)
    at sun.security.ssl.X509TrustManagerImpl.checkTrustedInit(X509TrustManagerImpl.java:173)
    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:186)
    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:126)
    at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1428)
    at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:209)
    at sun.security.ssl.Handshaker.processLoop(Handshaker.java:901)
    at sun.security.ssl.Handshaker.process_record(Handshaker.java:837)
    at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1023)
    at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1332)
    at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1359)
    ... 57 more
Caused by: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at java.security.cert.PKIXParameters.setTrustAnchors(PKIXParameters.java:200)
    at java.security.cert.PKIXParameters.&lt;init&gt;(PKIXParameters.java:120)
    at java.security.cert.PKIXBuilderParameters.&lt;init&gt;(PKIXBuilderParameters.java:104)
    at sun.security.validator.PKIXValidator.&lt;init&gt;(PKIXValidator.java:88)
    ... 69 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
&gt;&gt;&gt; ERROR: elasticsearch: all failed
&gt;&gt;&gt; elasticsearch: Uninstalling dependencies...
musl64 [~/aports/testing/elasticsearch]$ echo "testing connectivity"
testing connectivity
musl64 [~/aports/testing/elasticsearch]$ wget https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
--2015-05-06 12:56:40--  https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
Resolving repo.maven.apache.org... 23.235.43.215
Connecting to repo.maven.apache.org|23.235.43.215|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4824 (4.7K) [text/xml]
Saving to: ‘oss-parent-7.pom.1’

oss-parent-7.pom.1                     100%[============================================================================&gt;]   4.71K  --.-KB/s   in 0s     

2015-05-06 12:56:41 (745 MB/s) - ‘oss-parent-7.pom.1’ saved [4824/4824]
```

I have connectivity as you can see at the end (my build environment is an Alpine Linux LXC container with no proxy to configure) - I think the top level `pom.xml` just needs some repositories configured. I include `git` in the `fakeroot'.
</comment><comment author="itoffshore" created="2015-05-07T13:56:43Z" id="99877038">Building with the latest [pom.xml](https://github.com/elastic/elasticsearch/blob/master/pom.xml) in a clean `fakeroot` is still broken
</comment><comment author="dadoonet" created="2015-05-07T14:03:12Z" id="99878687">@itoffshore So you tried the master branch? Could run `mvn clean install -U -X -DskipTests` from master branch?
And share your logs.
</comment><comment author="itoffshore" created="2015-05-07T14:17:58Z" id="99882966">Also on [hastebin](http://hastebin.com/ocidijepuc.vala) where it may be a little easier to read.

```
Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T11:57:37+00:00)
Maven home: /usr/share/java/maven-3.3.3
Java version: 1.7.0_79, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-1.7-openjdk/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "3.16.0-0.bpo.4-amd64", arch: "amd64", family: "unix"
[DEBUG] Created new class realm maven.api
[DEBUG] Importing foreign packages into class realm maven.api
[DEBUG]   Imported: javax.enterprise.inject.* &lt; plexus.core
[DEBUG]   Imported: javax.enterprise.util.* &lt; plexus.core
[DEBUG]   Imported: javax.inject.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.artifact &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.classrealm &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.cli &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.configuration &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.exception &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.execution &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.execution.scope &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.lifecycle &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.model &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.monitor &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.plugin &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.profiles &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.project &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.reporting &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.repository &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.rtinfo &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.settings &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.toolchain &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.usability &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.* &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.authentication &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.authorization &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.events &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.observers &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.proxy &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.repository &lt; plexus.core
[DEBUG]   Imported: org.apache.maven.wagon.resource &lt; plexus.core
[DEBUG]   Imported: org.codehaus.classworlds &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.* &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.classworlds &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.component &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.configuration &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.container &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.context &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.lifecycle &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.logging &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.personality &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.Xpp3Dom &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlPullParser &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlPullParserException &lt; plexus.core
[DEBUG]   Imported: org.codehaus.plexus.util.xml.pull.XmlSerializer &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.* &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.artifact &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.collection &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.deployment &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.graph &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.impl &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.installation &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.internal.impl &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.metadata &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.repository &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.resolution &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.spi &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.transfer &lt; plexus.core
[DEBUG]   Imported: org.eclipse.aether.version &lt; plexus.core
[DEBUG]   Imported: org.slf4j.* &lt; plexus.core
[DEBUG]   Imported: org.slf4j.spi.* &lt; plexus.core
[DEBUG] Populating class realm maven.api
[INFO] Error stacktraces are turned on.
[DEBUG] Reading global settings from /usr/share/java/maven-3.3.3/conf/settings.xml
[DEBUG] Reading user settings from /home/stuart/.m2/settings.xml
[DEBUG] Reading global toolchains from /usr/share/java/maven-3.3.3/conf/toolchains.xml
[DEBUG] Reading user toolchains from /home/stuart/.m2/toolchains.xml
[DEBUG] Using local repository at /home/stuart/.m2/repository
[DEBUG] Using manager EnhancedLocalRepositoryManager with priority 10.0 for /home/stuart/.m2/repository
[INFO] Scanning for projects...
[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}
[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}
Downloading: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[DEBUG] Writing tracking file /home/stuart/.m2/repository/org/sonatype/oss/oss-parent/7/oss-parent-7.pom.lastUpdated
[DEBUG] Using transporter WagonTransporter with priority -1.0 for https://repo.maven.apache.org/maven2
[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for https://repo.maven.apache.org/maven2
Downloading: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[DEBUG] Writing tracking file /home/stuart/.m2/repository/org/sonatype/oss/oss-parent/7/oss-parent-7.pom.lastUpdated
[ERROR] [ERROR] Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13
 @ 
[ERROR] The build could not read 1 project -&gt; [Help 1]
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13

    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:422)
    at org.apache.maven.graph.DefaultGraphBuilder.collectProjects(DefaultGraphBuilder.java:419)
    at org.apache.maven.graph.DefaultGraphBuilder.getProjectsForMavenReactor(DefaultGraphBuilder.java:410)
    at org.apache.maven.graph.DefaultGraphBuilder.build(DefaultGraphBuilder.java:83)
    at org.apache.maven.DefaultMaven.buildGraph(DefaultMaven.java:491)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:219)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]   
[ERROR]   The project org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT (/home/stuart/aports/testing/elasticsearch/src/elasticsearch-master/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13 -&gt; [Help 2]
org.apache.maven.model.resolution.UnresolvableModelException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:197)
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:246)
    at org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:978)
    at org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:796)
    at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:325)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:469)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:438)
    at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:401)
    at org.apache.maven.graph.DefaultGraphBuilder.collectProjects(DefaultGraphBuilder.java:419)
    at org.apache.maven.graph.DefaultGraphBuilder.getProjectsForMavenReactor(DefaultGraphBuilder.java:410)
    at org.apache.maven.graph.DefaultGraphBuilder.build(DefaultGraphBuilder.java:83)
    at org.apache.maven.DefaultMaven.buildGraph(DefaultMaven.java:491)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:219)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:444)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:246)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:223)
    at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveArtifact(DefaultRepositorySystem.java:294)
    at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:193)
    ... 25 more
Caused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
    at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed(ArtifactTransportListener.java:43)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:355)
    at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:67)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$DirectExecutor.execute(BasicRepositoryConnector.java:581)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector.get(BasicRepositoryConnector.java:249)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:520)
    at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:421)
    ... 29 more
Caused by: java.lang.IllegalArgumentException: Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
    at java.net.URI.create(URI.java:859)
    at org.apache.maven.wagon.providers.http.httpclient.client.methods.HttpGet.&lt;init&gt;(HttpGet.java:69)
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:970)
    at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:960)
    at org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
    at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)
    at org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
    at org.eclipse.aether.transport.wagon.WagonTransporter$GetTaskRunner.run(WagonTransporter.java:560)
    at org.eclipse.aether.transport.wagon.WagonTransporter.execute(WagonTransporter.java:427)
    at org.eclipse.aether.transport.wagon.WagonTransporter.get(WagonTransporter.java:404)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$GetTaskRunner.runTask(BasicRepositoryConnector.java:447)
    at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:350)
    ... 34 more
Caused by: java.net.URISyntaxException: Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
    at java.net.URI$Parser.fail(URI.java:2829)
    at java.net.URI$Parser.checkChars(URI.java:3002)
    at java.net.URI$Parser.parseHierarchical(URI.java:3086)
    at java.net.URI$Parser.parse(URI.java:3034)
    at java.net.URI.&lt;init&gt;(URI.java:595)
    at java.net.URI.create(URI.java:857)
    ... 45 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
&gt;&gt;&gt; ERROR: elasticsearch: all failed
&gt;&gt;&gt; elasticsearch: Uninstalling dependencies...
musl64 [~/aports/testing/elasticsearch]$ 
```

I think my shell does not know `lucene.snapshot.revision` (`http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}` )
</comment><comment author="dadoonet" created="2015-05-07T14:23:29Z" id="99884650">Actually `lucene.snapshot.revision` is defined in https://github.com/elastic/elasticsearch/blob/master/pom.xml#L35.

I don't understand why in your case it's not.

Can you run `mvn help:effective-pom`?
</comment><comment author="itoffshore" created="2015-05-07T14:38:54Z" id="99890454">```
musl64 [~/aports/testing/elasticsearch/src/elasticsearch-master]$ mvn help:effective-pom
[INFO] Scanning for projects...
Downloading: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
Downloading: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[ERROR] [ERROR] Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13
 @ 
[ERROR] The build could not read 1 project -&gt; [Help 1]
[ERROR]   
[ERROR]   The project org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT (/home/stuart/aports/testing/elasticsearch/src/elasticsearch-master/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to lucene-snapshots (http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}): Illegal character in path at index 44: http://download.elastic.co/lucenesnapshots/${lucene.snapshot.revision}/org/sonatype/oss/oss-parent/7/oss-parent-7.pom and 'parent.relativePath' points at wrong local POM @ line 27, column 13 -&gt; [Help 2]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
```
</comment><comment author="dadoonet" created="2015-05-07T14:46:59Z" id="99894736">Can you replace in the `pom.xml` each `${lucene.snapshot.revision}` by `1677039` and run the same command again?
</comment><comment author="itoffshore" created="2015-05-07T15:00:33Z" id="99899868">Back to non resolvable parent pom:

```
musl64 [~/aports/testing/elasticsearch/src/elasticsearch-master]$ mvn help:effective-pom
[INFO] Scanning for projects...
Downloading: http://download.elastic.co/lucenesnapshots/1677039/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
Downloading: https://repo.maven.apache.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.pom
[ERROR] [ERROR] Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13
 @ 
[ERROR] The build could not read 1 project -&gt; [Help 1]
[ERROR]   
[ERROR]   The project org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT (/home/stuart/aports/testing/elasticsearch/src/elasticsearch-master/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM for org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT: Could not transfer artifact org.sonatype.oss:oss-parent:pom:7 from/to central (https://repo.maven.apache.org/maven2): java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and 'parent.relativePath' points at wrong local POM @ line 27, column 13 -&gt; [Help 2]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
```
</comment><comment author="dadoonet" created="2015-05-07T15:06:31Z" id="99901328">I see. It's definitely a Maven/Java installation issue. Have a look at this thread: http://stackoverflow.com/questions/6784463/error-trustanchors-parameter-must-be-non-empty

Though they explained it's an OpenJDK concern and you seems to use Oracle...
</comment><comment author="itoffshore" created="2015-05-07T15:17:47Z" id="99906468">Many thx for the pointer - I will package [ca-certificates-java](http://packages.ubuntu.com/trusty/ca-certificates-java) for [Alpine](http://alpinelinux.org) to fix this issue &amp; report back
</comment><comment author="dadoonet" created="2015-05-07T15:18:49Z" id="99906978">Closing for now as I don't think this is something we could fix on our end.
Feel free to reopen if you don't think so.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove Codehaus repository</comment></comments></commit></commits></item><item><title>Problems using debian package with new debian jessie</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10938</link><project id="" key="" /><description>Hi

no deep dive yet, just figured out that there is a problem with the repository when running with the freshly released debian jessie, which now uses systemd

```
root@debian-800-jessie:~# systemctl status elasticsearch
● elasticsearch.service - Starts and stops a single elasticsearch instance on this system
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled)
   Active: failed (Result: exit-code) since Sun 2015-05-03 13:17:43 GMT; 35min ago
     Docs: http://www.elasticsearch.org
  Process: 389 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR (code=exited, status=3)
 Main PID: 389 (code=exited, status=3)

May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:383)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createDirectory(Files.java:630)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createAndCheckIsDirectory(Files.java:734)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createDirectories(Files.java:720)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:105)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:160)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: ... 4 more
May 03 13:17:43 debian-800-jessie systemd[1]: elasticsearch.service: main process exited, code=exited, status=3/NOTIMPLEMENTED
May 03 13:17:43 debian-800-jessie systemd[1]: Unit elasticsearch.service entered failed state.
```

this is from a `journalctl` output and shows a wrong data path

```
May 03 13:17:42 debian-800-jessie elasticsearch[389]: Failed to configure logging...
May 03 13:17:42 debian-800-jessie elasticsearch[389]: org.elasticsearch.ElasticsearchException: Failed to load logging configuration
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:139)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:89)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:100)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:184)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.readAttributes(Files.java:1686)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.walkFileTree(Files.java:2602)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:123)
May 03 13:17:42 debian-800-jessie elasticsearch[389]: ... 4 more
May 03 13:17:43 debian-800-jessie elasticsearch[389]: log4j:WARN No appenders could be found for logger (node).
May 03 13:17:43 debian-800-jessie elasticsearch[389]: log4j:WARN Please initialize the log4j system properly.
May 03 13:17:43 debian-800-jessie elasticsearch[389]: log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
May 03 13:17:43 debian-800-jessie elasticsearch[389]: {1.5.2}: Initialization Failed ...
May 03 13:17:43 debian-800-jessie elasticsearch[389]: - ElasticsearchIllegalStateException[Failed to created node environment]
May 03 13:17:43 debian-800-jessie elasticsearch[389]: AccessDeniedException[/usr/share/elasticsearch/data]
May 03 13:17:43 debian-800-jessie elasticsearch[389]: org.elasticsearch.ElasticsearchIllegalStateException: Failed to created node environment
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:162)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: Caused by: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/data
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:383)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createDirectory(Files.java:630)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createAndCheckIsDirectory(Files.java:734)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at java.nio.file.Files.createDirectories(Files.java:720)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:105)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:160)
May 03 13:17:43 debian-800-jessie elasticsearch[389]: ... 4 more
```
</description><key id="72809216">10938</key><summary>Problems using debian package with new debian jessie</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label></labels><created>2015-05-03T13:54:11Z</created><updated>2015-05-27T14:53:55Z</updated><resolved>2015-05-27T14:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-05-03T14:07:37Z" id="98484658">@spinscale there are few issues with SytemD, can you please check if you can reproduce it with #10725 ? Thanks
</comment><comment author="spinscale" created="2015-05-04T07:16:51Z" id="98605524">yes, elasticsearch starts with that PR on jessie, let's close this one with the PR as well
</comment><comment author="spinscale" created="2015-05-27T14:53:54Z" id="105943546">Closing this one due to #10725 being closed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[1.5.2] Launcher script doesn't like unicode characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10937</link><project id="" key="" /><description>The script ./bin/elasticsearch does not work when there are unicode characters in the path:

```
/tmp » pwd
/tmp
------------------------------------------------------------
/tmp » tar zxvf elasticsearch-1.5.2.tar.gz                                                                                 
elasticsearch-1.5.2/config/logging.yml
[...]
elasticsearch-1.5.2/NOTICE.txt
------------------------------------------------------------
/tmp » mkdir dir_with_french_char_é                                                                                        
------------------------------------------------------------
/tmp » mv elasticsearch-1.5.2 dir_with_french_char_é                                                                       
------------------------------------------------------------
/tmp » ./dir_with_french_char_é/elasticsearch-1.5.2/bin/elasticsearch
Error: Could not find or load main class org.elasticsearch.bootstrap.Elasticsearch
------------------------------------------------------------
/tmp » mv dir_with_french_char_é dir_with_french_char_e
------------------------------------------------------------
/tmp » ./dir_with_french_char_e/elasticsearch-1.5.2/bin/elasticsearch
[2015-05-02 23:43:59,233][INFO ][node                     ] [Prometheus] version[1.5.2], pid[15437], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-02 23:43:59,234][INFO ][node                     ] [Prometheus] initializing ...
[2015-05-02 23:43:59,237][INFO ][plugins                  ] [Prometheus] loaded [], sites []
[2015-05-02 23:44:01,153][INFO ][node                     ] [Prometheus] initialized
[2015-05-02 23:44:01,153][INFO ][node                     ] [Prometheus] starting ...
[2015-05-02 23:44:01,224][INFO ][transport                ] [Prometheus] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.12:9300]}
[2015-05-02 23:44:01,234][INFO ][discovery                ] [Prometheus] elasticsearch/cVO5bJgySh-kySL-8sKoKA
```
</description><key id="72704443">10937</key><summary>[1.5.2] Launcher script doesn't like unicode characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">gribouille</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-05-02T21:51:04Z</created><updated>2016-01-18T14:34:40Z</updated><resolved>2016-01-18T14:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-03T07:45:07Z" id="98448635">Works fine for me, also with master. Can you show the results of 'env' or at least 'locale' (prints only the locale related ones)? It may be related. I strongly recommend using unicode (e.g. fr_FR.UTF-8) 

```
rmuir@beast:/data/t$ ./dir_with_french_char_é/elasticsearch-1.5.2/bin/elasticsearch
[2015-05-03 03:40:14,127][INFO ][node                     ] [Defensor] version[1.5.2], pid[11114], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-03 03:40:14,127][INFO ][node                     ] [Defensor] initializing ...
[2015-05-03 03:40:14,130][INFO ][plugins                  ] [Defensor] loaded [], sites []
```
</comment><comment author="gribouille" created="2015-05-03T18:25:37Z" id="98519780">My locale is configured for UTF-8:

```
~ » locale                                                                                                         hakim@aeon
LANG=fr_FR.UTF-8
LANGUAGE=
LC_CTYPE=fr_FR.UTF-8
LC_NUMERIC="fr_FR.UTF-8"
LC_TIME="fr_FR.UTF-8"
LC_COLLATE="fr_FR.UTF-8"
LC_MONETARY="fr_FR.UTF-8"
LC_MESSAGES="fr_FR.UTF-8"
LC_PAPER="fr_FR.UTF-8"
LC_NAME="fr_FR.UTF-8"
LC_ADDRESS="fr_FR.UTF-8"
LC_TELEPHONE="fr_FR.UTF-8"
LC_MEASUREMENT="fr_FR.UTF-8"
LC_IDENTIFICATION="fr_FR.UTF-8"
LC_ALL=
```
</comment><comment author="gribouille" created="2015-05-03T18:31:00Z" id="98520021">I have forgotten to say that to run elasticearch with the starter command works:

```
/tmp » /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/tmp/é/elasticsearch-1.5.2 -cp ":/tmp/é/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar:/tmp/é/elasticsearch-1.5.2/lib/*:/tmp/é/elasticsearch-1.5.2/lib/sigar/*" org.elasticsearch.bootstrap.Elasticsearch
[2015-05-03 20:27:40,761][INFO ][node                     ] [Asteroth] version[1.5.2], pid[4301], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-03 20:27:40,761][INFO ][node                     ] [Asteroth] initializing ...
[2015-05-03 20:27:40,764][INFO ][plugins                  ] [Asteroth] loaded [], sites []
[2015-05-03 20:27:42,736][INFO ][node                     ] [Asteroth] initialized
[2015-05-03 20:27:42,737][INFO ][node                     ] [Asteroth] starting ...
```
</comment><comment author="clintongormley" created="2015-05-04T13:10:13Z" id="98701135">When I try this on OSX, it works out of the box, with these locale settings:

```
LANG=
LC_COLLATE="C"
LC_CTYPE="UTF-8"
LC_MESSAGES="C"
LC_MONETARY="C"
LC_NUMERIC="C"
LC_TIME="C"
LC_ALL=
```

If I change `LC_CTYPE` to `C`, then I get the same error message: 

```
LC_CTYPE="C" ./fã/elasticsearch-1.5.2/bin/elasticsearch 
Error: Could not find or load main class org.elasticsearch.bootstrap.Elasticsearch
```

Does seem like a local locale problem...
</comment><comment author="clintongormley" created="2015-05-04T13:15:32Z" id="98702077">What OS are you running on?  Could you check this again, running `env` in the same shell where you see the failed start?
</comment><comment author="gribouille" created="2015-05-04T18:33:43Z" id="98806457">My config:

```
~ » env
LANG=fr_FR.UTF-8
TERM=xterm
SHELL=/bin/zsh
PAGER=less
LESS=-R
LC_CTYPE=fr_FR.UTF-8
LSCOLORS=Gxfxcxdxbxegedabagacad
_=/usr/bin/env
[...]
~ » uname -a
Linux aeon 3.16.0-0.bpo.4-amd64 #1 SMP Debian 3.16.7-ckt7-1~bpo70+1 (2015-04-07) x86_64 GNU/Linux
~ » cat /etc/debian_version
7.8
```

This command works with zsh or dash:

```
/tmp » /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/tmp/é/elasticsearch-1.5.2 -cp ":/tmp/é/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar:/tmp/é/elasticsearch-1.5.2/lib/*:/tmp/é/elasticsearch-1.5.2/lib/sigar/*" org.elasticsearch.bootstrap.Elasticsearch
[2015-05-03 20:27:40,761][INFO ][node                     ] [Asteroth] version[1.5.2], pid[4301], build[62ff986/2015-04-27T09:21:06Z]
[2015-05-03 20:27:40,761][INFO ][node                     ] [Asteroth] initializing ...
[2015-05-03 20:27:40,764][INFO ][plugins                  ] [Asteroth] loaded [], sites []
[2015-05-03 20:27:42,736][INFO ][node                     ] [Asteroth] initialized
[2015-05-03 20:27:42,737][INFO ][node                     ] [Asteroth] starting ...
```
</comment><comment author="clintongormley" created="2015-05-05T07:33:07Z" id="98982778">@tlrx could you take a look at this when you have a moment?
</comment><comment author="clintongormley" created="2016-01-18T14:34:40Z" id="172542172">A lot has changed since 1.5, and this is not reproducible in master - closing for now, please reopen if there is still an issue in 2.1 or above.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify securitymanager init</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10936</link><project id="" key="" /><description>Today we create a temp file, concatenate our template with some dynamically generated rules, and then set a sysprop to the new temp file.

A custom java.security.Policy is cleaner and easier to understand: we can avoid the sysprop, temp files, path escaping, etc.
</description><key id="72683045">10936</key><summary>Simplify securitymanager init</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-02T18:58:41Z</created><updated>2015-06-08T13:16:32Z</updated><resolved>2015-05-04T11:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-05-03T17:44:58Z" id="98514074">nice, much cleaner! I think that on a different change we should see how we can separate the test rules from the security.policy, since it will allow us to get read of create security manager permission?

LGTM
</comment><comment author="rmuir" created="2015-05-03T20:37:25Z" id="98541386">The whole idea is that the system can only do what its tested to do. If we want to do a bunch of untested shit, thats cool, but id rather stay out in that case.
So for better security, clean up the tests. No cheating.
</comment><comment author="kimchy" created="2015-05-03T20:38:57Z" id="98541544">I see, I thought it was needed in the tests only test the security manager feature itself...
</comment><comment author="rmuir" created="2015-05-03T20:45:47Z" id="98542010">I dont think creating an sm is particularly harmful.  as opposed to setting it. But there is some reason tests need it. Like many of the many permissions here that need to be cleaned up 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file></files><comments><comment>Merge pull request #10936 from rmuir/eight_point_three</comment></comments></commit></commits></item><item><title>Split brain problem in 2 node elasticsearch cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10935</link><project id="" key="" /><description>Version: 1.4. 
Say there are 2 nodes X and Y, both capable of becoming master. 
When network goes down, both nodes get disconnected from each other and assume the responsibility of master. 
When network is restored, they don't ping each other and form a cluster.

Elasticsearch service has to be restarted on any one of the nodes for them to form a cluster. Even after they form a cluster, all primary shards remain on one node ( on which the service was restarted ), and all replica shards are on the other node.
</description><key id="72585124">10935</key><summary>Split brain problem in 2 node elasticsearch cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gouravdhelaria</reporter><labels /><created>2015-05-02T04:46:26Z</created><updated>2015-05-04T05:03:58Z</updated><resolved>2015-05-04T05:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-05-02T05:02:41Z" id="98303243">Take a look through this page in the docs, in particular `minimum_master_nodes` -  http://www.elastic.co/guide/en/elasticsearch/guide/current/_important_configuration_changes.html. This sort of situation is also why we recommend having an uneven number of master eligible nodes.

This isn't a bug though, if you have other problems like this then please raise them on the mailing list - https://groups.google.com/forum/#!forum/elasticsearch
</comment><comment author="gouravdhelaria" created="2015-05-04T05:03:58Z" id="98589213">Moving to forum:
https://groups.google.com/forum/#!topic/elasticsearch/QavwaaYcSEY
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document and test custom analyzer `position_offset_gap`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10934</link><project id="" key="" /><description>Document and add a quick test for setting a `position_offset_gap` on a custom analyzer.  This functionality was added way back in 2012 in #1812 but I just recently discovered it and feel that it should not go undocumented.  
</description><key id="72585100">10934</key><summary>Document and test custom analyzer `position_offset_gap`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-02T04:45:47Z</created><updated>2015-06-06T19:05:51Z</updated><resolved>2015-05-04T15:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T12:35:31Z" id="98694791">++ I've always wondered what this option did :)
</comment><comment author="s1monw" created="2015-05-04T15:18:02Z" id="98748266">@rjernst can you review this please
</comment><comment author="rjernst" created="2015-05-04T15:58:44Z" id="98765261">Thanks @mattweber!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Merge pull request #10934 from mattweber/custom_analyzer_pos_offset_gap</comment></comments></commit></commits></item><item><title>Node crashes can cause data loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10933</link><project id="" key="" /><description>You're gonna hate me for this one. I apologize in advance.

On Elasticsearch 1.5.0 (Jepsen ecab97547123a0c88bb39ddd3ba3db873dccf251), `lein test :only elasticsearch.core-test/create-crash` does _not_ affect the network in any way, but instead kills a randomly selected subset of elasticsearch nodes and immediately restarts them, waits for all nodes to recover, then kills a new subset, and so on. As usual, we wait for all nodes to restart and for the cluster status to return green, plus some extra buffer time, before issuing a final read. This failure pattern induces the loss of inserted documents throughout the test: in one particular case, 10% of [acknowledged inserts](https://aphyr.com/media/es-create-crash.edn) did not appear in the final set.

![latency](https://cloud.githubusercontent.com/assets/3748/7439592/6d260578-f036-11e4-8f5d-00cf5c5a3060.png)

``` clj
{:valid? false,
 :lost
 "#{0..49 301..302 307 309 319..322 325 327 334 341 351 370 372 381 405 407 414 416 436 438 447 460..462 475 494 497 499 505 508 513 522..523 526 531 534 537 546 559 564 568 573 588 591 599 614 634 645..646 664 673 679 695 699 702 706 727..728 736 747 753 761 771 775 779 784 788 795 822 827 833 845 862 866 872 876 878 887 892 901 903 923..924 927..928 936 944 968 977..978 983 985 998 1000 1006 1014 1016 1028 1033 1035 1043 1045 1047 1049..1050 1066..1067 1074 1088 1091..1092 1105..1106 1109 1112 1114 1116 1131..1132 1140..1141 1144 1155 1157 1159 1166 1168 1174..1175 1178 1181 1187 1210 1218 1224 1227 1239 1243 1269..1270 1273 1275 1282 1286 1291..1292 1295 1298..1299 1301 1303 1310 1312 1319 1328..1329 1333 1339 1341..1343 1346..1347 1355 1357 1366 1368 1380 1385 1396 1400 1406 1411 1431 1434 1436 1438..1440 1448 1455 1461 1463 1466..1467 1470 1477 1485..1486 1499 1501 1509 1512..1514 1518 1520 1528 1533..1534 1536 1543 1546 1555 1559 1564..1565 1571 1575 1579 1581 1598 1600 1605 1607..1608 1639 1649 1660 1662 1670 1676 1689 1691 1699 1703 1728 1732 1735 1739 1741..1742 1745 1751 1756 1760 1767 1783..1784 1792 1809 1812 1815 1823 1825 1829 1842 1846 1852..1853 1856..1857 1860 1867 1870 1875 1884 1892 1900..1901 1910 1912 1925 1927 1934 1946..1947 1952 1955 1957 1967..1968 1970 1983 1990 1993 1995 1998 2007 2010 2015 2021 2027..2028 2037..2038 2041 2045..2046 2066 2074 2080 2082 2089 2092 2099..2100 2104 2108 2110 2117 2397..2398 2402 2404 2407..2408 2411 2413 2415 2421..2424 2428 2431 2433..2434 2436..2439 2442..2459 2461..2462 2469..2470 2472..2476 2478 2684 2687..2688 2691..2695 2697..2698 2700 2702..2706 2708 2710..2712 2714 2716 2720 2723..2724 2726 2728 2730 2732..2735 2738 2740..2742 2749..2750 2752 2757..2758 2761 2764 2772 2778 2782 2785 2788..2790 2793..2794 2798..2799 2801 2803 2805 2808 2811 2816 2819 2822 2826 2831 2833 2836 2838 2840..2841 2844 2848..2849 2852 2859 2863 2868 2870..2871 2874 2877 2879..2880 2883..2884 2886 2889..2890 2893 2899 2901 2905 2910..2911 2914..2915 2921 2923 2925..2926 2930 2935 2939..2940 2944 2946 2950 2954 2957..2959 2961..2964 2968..2971 2977..2978 2981..2983 2986 2989..2993 2995..2998 3000 3002..3007 3010..3016 3020 3022..3025 3027..3029 3036..3037 3040..3042 3045..3046 3049..3051 3054 3056..3058 3060 3062..3066 3068..3069 3073 3075 3078 3081..3083 3085 3087..3091 3093 3097..3098 3100 3103..3104 3106..3109 3111..3112 3114 3116..3119 3121 3123..3125 3127..3128 3130..3132 3134..3135 3138..3142 3144 3153..3155 3158..3159 3161 3164..3168 3170..3173 3175..3177 3179 3185..3186 3188..3191 3193 3197..3199 3202 3204 3208 3210..3211 3213 3217 3223 3229..3230 3233 3237..3238 3241 3243 3245..3246 3249 3255..3256 3262 3265 3267..3268 3273..3274 3276 3278..3280 3289..3290 3295 3299 3302..3303 3306..3307 3309 3312 3315..3316 3318..3319 3321 3324..3325 3328..3329 3333 3338 3342 3344 3348 3350..3351 3355 3357..3358 3360 3364..3366 3370..3371 3375 3384 3386..3388 3395..3396 3398 3401 3404 3407 3413 3420..3421 3423..3424 3426..3430 3432 3434..3438 3440 3444..3445 3450..3452 3456..3460 3462 3464 3466..3469 3473..3476 3480..3481 3483 3487 3491..3494 3499..3501 3504 3506..3508 3511..3514 3516..3519 3521..3522 3525..3529 3531..3532 3534 3536..3540 3544 3549..3550 3552..3553 3555..3557 3560 3562 3564..3565 3570..3571 3573..3575 3578..3579 3581..3583 3585..3587 3591 3594 3597 3599..3601 3605..3608 3611..3615 3617..3622 3624..3626 3628..3636 3638..3702 6154 6156 6174 6179 6181..6182 6186 6198 6201..6202 6211 6214 6216 6219..6220 6224..6225 6562..6567 6569 6571..6573 6575 6578..6579 6581 6583 6585..6587 6592 6594 6596..6601 6605..6606 6608..6611 6613 6615..6616 6618 6622..6625 6627 6629..6631 6634..6637 9887 9889 9896 9903 9905 9909 9912 9920 9923 9931 9943 9953..9954 9963 9968 9998 10003 10006 10008 10028 10038 10048 10077 10097 10099 10101 10111 10114..10115 10136..10138 10141 10143 10159 10162..10163 10166 10168..10169 10171 10194 10198..10199 10212 10218 10223 10225..10226 10228 10240 10248 10250 10253 10258 10268 10283..10284 10289 10294..10295 10305..10307 10309..10311 10315 10319 10322 10324 10326 10328 10330 10334..10335 10337 10339..10343 10345..10347 10351..10359 10361 10363..10365 10367 10370 10374 10377 10379 10381..10385 10387..10391 10394..10395 10397..10405 10642 10649 10653 10661 10664 10668..10669 10671 10674..10676 10681 10685 10687 10700..10718}",
 :recovered
 "#{2129 2333 2388 2390 2392..2395 2563 2643 2677 2680 2682..2683 4470 4472 4616 4635 4675..4682 4766 4864 4967 5024..5026 5038 5042..5045 5554 5556..5557 5696..5697 5749..5757 5850 5956 6063 6115..6116 6146 6148..6149 6151 6437 6541 6553..6554 6559 6561 11037 11136 11241 11291..11295}",
 :ok
 "#{289..300 303..306 308 310..318 323..324 326 328..333 335..338 340 343..346 348..350 352 354..359 361..363 365..368 371 373..375 377..380 383..387 389..395 397..400 402..404 408..409 411..413 418..423 425..429 431..433 435 437 439 441..443 445..446 448 450..453 455..458 463..464 466..469 471..473 476 478..479 481..487 489 491..493 495 498 500 502..504 507 510..512 515..517 519..520 524..525 528..530 533 535 538..540 542..545 547..548 550..552 554..558 560..563 565..567 569..572 574..587 589..590 592..598 600..613 615..633 635..644 647..663 665..672 674..678 680..694 696..698 700..701 703..705 707..726 729..735 737..746 748..752 754..760 762..770 772..774 776..778 780..783 785..787 789..793 796..798 814..818 820..821 823 825..826 828 830..831 834..836 838..842 844 846..849 851..853 855 857..859 861 863 865 867..868 870..871 873..875 879..880 882..884 886 888 890 893..896 898..900 904..906 908..912 914..915 917..920 922 926 930..931 933..935 937 939..941 943 945 947..950 952..956 958..967 969..976 979..982 984 986..997 999 1001..1005 1007..1013 1015 1017..1027 1029..1032 1034 1036..1042 1044 1046 1048 1051..1065 1068..1073 1075..1087 1089..1090 1093..1104 1107..1108 1110..1111 1113 1115 1117..1130 1133..1136 1138..1139 1142 1145..1148 1150..1154 1158 1160 1162..1164 1167 1169..1171 1173 1177 1179 1182..1185 1188..1190 1192..1194 1196..1200 1202..1205 1207..1208 1211..1213 1215..1217 1220..1223 1226 1228..1229 1231..1236 1238 1240..1242 1245..1247 1249..1250 1252..1254 1256..1258 1260..1261 1263..1267 1271 1276..1278 1280..1281 1283..1284 1287..1288 1290 1293..1294 1296 1300 1304..1307 1309 1311 1313..1314 1316..1318 1321..1326 1330 1332 1334..1337 1340 1345 1348 1350..1351 1353..1354 1356 1358..1365 1367 1369..1379 1381..1384 1386..1395 1397..1399 1401..1405 1407..1410 1412..1430 1432..1433 1435 1437 1441..1447 1449..1454 1456..1460 1462 1464..1465 1468..1469 1471..1476 1478..1484 1487..1498 1500 1502..1508 1510..1511 1515..1517 1519 1521..1527 1529..1532 1535 1537..1542 1544..1545 1547..1554 1556..1558 1560..1563 1566..1570 1572..1574 1576..1578 1580 1582..1597 1599 1601..1604 1606 1609..1617 1619 1621 1623 1625 1627..1629 1632..1633 1636..1637 1641..1643 1645 1647..1648 1651..1652 1654 1657..1658 1663..1664 1666..1667 1671..1672 1675 1678..1679 1681 1683..1684 1687 1690 1693 1695..1696 1701..1702 1706..1710 1713 1715..1717 1721 1723 1725..1726 1730 1733 1736 1738 1746 1748 1752..1753 1759 1763..1765 1768 1770..1772 1775..1776 1778 1780 1785 1788..1790 1794 1796 1798 1800..1801 1804..1805 1807 1811 1814 1818..1820 1826 1828 1832..1833 1836..1841 1843..1845 1847..1851 1854..1855 1858..1859 1861..1866 1868..1869 1871..1874 1876..1883 1885..1891 1893..1899 1902..1909 1911 1913..1924 1926 1928..1933 1935..1945 1948..1951 1953..1954 1956 1958..1966 1969 1971..1982 1984..1989 1991..1992 1994 1996..1997 1999..2006 2008..2009 2011..2014 2016..2020 2022..2026 2029..2036 2039..2040 2042..2044 2047..2065 2067..2073 2075..2079 2081 2083..2088 2090..2091 2093..2098 2101..2103 2105..2107 2109 2111..2116 2118..2126 2129 2333 2388 2390 2392..2395 2399..2401 2403 2405..2406 2409..2410 2412 2414 2416..2420 2425..2427 2429..2430 2432 2435 2440..2441 2460 2463..2468 2471 2477 2563 2643 2677 2680 2682..2683 2685..2686 2689..2690 2696 2699 2701 2707 2709 2713 2715 2717..2719 2721..2722 2725 2727 2729 2731 2736..2737 2746..2747 2753 2755 2760 2762 2765 2767..2769 2773 2775 2777 2783..2784 2796 2804 2810 2814..2815 2818 2820 2823 2828..2829 2837 2846..2847 2850 2855..2857 2861 2864 2867 2876 2892 2894 2896 2898 2902 2904 2907 2909 2917..2918 2920 2927 2931..2932 2936..2937 2943 2947 2951..2952 2956 2960 2965..2967 2972..2976 2979..2980 2984..2985 2987..2988 2994 2999 3001 3008..3009 3017..3019 3021 3026 3030..3035 3038..3039 3043..3044 3047..3048 3052..3053 3055 3059 3061 3067 3070..3072 3074 3076..3077 3079..3080 3084 3086 3092 3094..3096 3099 3101..3102 3105 3110 3113 3115 3120 3122 3126 3129 3133 3136..3137 3143 3145..3152 3156..3157 3160 3162..3163 3169 3174 3178 3180..3184 3187 3192 3194..3195 3205 3207 3215 3219 3221..3222 3225..3226 3236 3242 3247 3251..3252 3258 3260..3261 3272 3283 3285..3286 3292 3294 3296 3301 3311 3314 3320 3331 3334..3335 3339 3341 3345 3353 3361 3369 3374 3376..3377 3380..3382 3390 3394 3399 3402 3405 3408..3409 3412 3414..3415 3418..3419 3422 3425 3431 3433 3439 3441..3443 3446..3449 3453..3455 3461 3463 3465 3470..3472 3477..3479 3482 3484..3486 3488..3490 3495..3498 3502..3503 3505 3509..3510 3515 3520 3523..3524 3530 3533 3535 3541..3543 3545..3548 3551 3554 3558..3559 3561 3563 3566..3569 3572 3576..3577 3580 3584 3588..3590 3592..3593 3595..3596 3598 3602..3604 3609..3610 3616 3623 3627 3637 3946..3949 3952..4009 4012 4015..4016 4018 4020 4022..4024 4026 4029..4031 4034..4035 4037..4038 4040 4042..4043 4045..4047 4050..4052 4055..4058 4061..4062 4065..4067 4070..4071 4074..4076 4078 4080..4081 4084..4085 4087..4088 4090..4091 4093 4095 4097..4098 4100..4101 4104..4105 4107 4109..4110 4113 4115 4117..4118 4121..4123 4126 4128..4129 4131 4133..4135 4137..4138 4140 4142..4144 4147..4148 4150..4151 4154..4156 4159..4160 4162..4163 4165 4167..4168 4171..4174 4177..4179 4181 4183..4184 4187..4189 4191 4194..4195 4198..4200 4203 4205..4206 4208 4211..4213 4215..4216 4218 4220 4222..4223 4225..4470 4472 4551 4553 4616 4618 4623 4627 4631 4635 4675..4763 4766 4864 4967 5024..5026 5038 5042..5098 5100..5103 5105..5110 5112..5114 5116..5121 5123..5124 5126..5129 5131..5136 5138..5140 5142..5146 5148..5153 5155..5160 5162 5164..5169 5171..5175 5177..5179 5181..5185 5187 5189..5193 5195..5200 5202..5205 5207..5210 5212..5214 5216..5221 5223..5224 5226..5228 5230..5232 5234..5235 5237..5239 5241..5246 5248..5249 5251..5252 5254..5257 5259..5260 5262..5264 5266..5267 5269..5274 5277..5280 5282..5288 5290..5296 5298..5301 5303..5305 5307 5309..5554 5556..5557 5635..5636 5696..5697 5749..5846 5850 5956 6063 6115..6116 6146 6148..6149 6151..6153 6155 6157..6173 6175..6178 6180 6183..6185 6187..6197 6199..6200 6203..6210 6212..6213 6215 6217..6218 6221..6223 6226..6228 6437 6541 6553..6554 6559 6561 6568 6570 6574 6576..6577 6580 6582 6584 6588..6591 6593 6595 6602..6604 6607 6612 6614 6617 6619..6621 6626 6628 6632..6633 6880 6882..6883 6893..6945 6947..6953 6955..6958 6960..6965 6967..6969 6971..6977 6979..6981 6983..6986 6988..6991 6993..6996 6998..7001 7003..7006 7008..7010 7012..7015 7017..7021 7023..7024 7026..7030 7032..7034 7036..7038 7040..7041 7043..7046 7048..7050 7052..7053 7055..7056 7058..7060 7062 7064..7067 7069..7073 7075..7079 7081..7084 7086..7088 7090..7091 7093..7099 7101..7104 7106..7108 7110..7114 7116..7119 7121..7126 7128..7133 7135..7140 7142..7145 7147 7149..7153 7155..7395 7398..7399 7401 7403..7405 7408..7410 7413..7415 7419 7421..7422 7424..7427 7430 7432..7433 7436..7437 7440..7443 7446..7447 7449 7451 7453..7455 7459 7461..7462 7464..7465 7467..7468 7471..7473 7476..7477 7479..7482 7485..7486 7488..7489 7491 7493 7495..7496 7498..7500 7503..7505 7507..7508 7510..7511 7514..7515 7518 7520..7521 7524..7525 7528..7530 7533..7534 7536 7538 7540 7542..7543 7546..7547 7549..7550 7553..7556 7559 7561 7563..7566 7570..7571 7573 7576..7578 7580..7581 7584..7585 7587..7588 7590 7592..7595 7598..7599 7602..7604 7607..7612 7615..7887 7889..7890 7921..7923 7926 7929..7932 7934 7937..7939 7941..7942 7945 7947..7948 7951..7953 7956..7958 7961 7963..7964 7966 7968..7970 7973 7975..7976 7978 7980 7982..7984 7986..7987 7990 7992 7994 7996 7998..7999 8001 8003..8005 8008..8010 8012 8014 8017 8019..8020 8022..8023 8025..8026 8028 8030..8031 8033 8035 8037..8038 8040 8042 8044..8045 8047 8049..8050 8053..8055 8058..8059 8061 8063..8064 8066..8245 8248..8249 8251 8282..8284 8286 8288..8290 8292..8293 8295 8297..8299 8301 8304..8306 8308..8309 8312..8313 8315..8318 8321 8323..8325 8328..8330 8332 8334..8336 8338 8341..8342 8344 8346..8347 8349 8351..8352 8354 8356 8358..8359 8361 8363 8365..8366 8368 8370..8371 8374..8376 8378 8381..8382 8384 8386 8388..8389 8391 8393 8395 8397..8399 8402..8404 8407..8410 8413..8414 8416..8418 8420 8422 8424..8425 8428 8430..8432 8434..8614 8616..8620 8622..8626 8628..8631 8633..8636 8638..8640 8642..8643 8645..8649 8651..8655 8657..8661 8664..8668 8671..8676 8678..8681 8683..8688 8690..8694 8696..8699 8701..8704 8706..8709 8712..8715 8717..8720 8722..8725 8728..8732 8734..8739 8741 8743..8746 8748..8750 8752..8756 8758..8760 8762..8767 8769..8771 8773..8777 8779..8782 8784..8788 8790..8794 8796..8800 8802..8803 8805..8809 8811..8813 8815..8818 8820..8824 8826..9079 9081..9084 9086..9091 9093..9098 9100..9105 9107..9108 9110..9113 9115..9119 9121..9124 9126..9130 9132..9134 9136..9140 9142..9144 9146..9151 9153..9157 9159..9163 9165..9166 9168..9169 9171..9175 9177..9181 9183..9186 9188..9190 9192..9195 9197..9200 9202..9207 9209..9215 9217..9221 9223..9228 9230..9234 9236..9241 9243..9245 9247..9248 9250..9254 9256..9259 9261 9263..9268 9270..9272 9274..9277 9279..9283 9285..9289 9291..9293 9295..9564 9567..9569 9602 9605..9606 9608..9609 9612..9614 9617..9619 9622..9624 9628 9630..9633 9636..9637 9639..9640 9642 9644 9646..9647 9650..9652 9654 9656..9657 9659 9661 9663..9664 9666 9669..9671 9673..9674 9676..9677 9680..9681 9683 9685..9686 9688 9690 9692..9694 9696..9697 9699 9701 9704..9706 9710..9712 9714..9715 9717 9721..9722 9724..9725 9727 9729 9731 9733..9734 9737..9738 9740 9742 9744 9746 9748..9886 9888 9890..9895 9897..9902 9904 9906..9908 9910..9911 9913..9919 9921..9922 9924..9927 9929..9930 9933..9936 9938..9939 9941..9942 9944..9946 9948..9949 9951..9952 9955 9957..9960 9962 9964 9966..9967 9969..9971 9973..9976 9978 9980..9984 9986..9991 9993..9995 9997 10000..10002 10005 10007 10010..10013 10015..10017 10019..10022 10024..10026 10029..10033 10035 10037 10039..10040 10042..10045 10047 10049..10050 10052..10056 10058..10059 10061..10066 10068..10071 10073..10076 10079..10082 10084..10085 10087..10091 10093..10094 10096 10098 10102..10103 10105..10107 10109..10110 10113 10117..10122 10124..10127 10129..10130 10132..10134 10140 10142 10144..10158 10160..10161 10164..10165 10167 10170 10172..10193 10195..10197 10200..10211 10213..10217 10219..10222 10224 10227 10229..10239 10241..10247 10249 10251..10252 10254..10257 10259..10267 10269..10282 10285..10288 10290..10293 10296..10304 10308 10312..10314 10316..10318 10320..10321 10323 10325 10327 10329 10331..10333 10336 10338 10344 10348..10350 10360 10362 10366 10368..10369 10371..10373 10375..10376 10378 10380 10386 10392..10393 10396 10641 10644..10645 10648 10650..10652 10654..10660 10662..10663 10665..10667 10670 10672..10673 10677..10680 10682..10684 10686 10688..10699 10964 10966..10967 10969 10972..11035 11037 11136 11241 11291..11299}",
 :recovered-frac 37/5650,
 :unexpected-frac 0,
 :unexpected "#{}",
 :lost-frac 23/226,
 :ok-frac 1463/2825}
```

Is this actually a bug? It's not entirely clear to me what kind of crash schedules Elasticsearch should actually tolerate, and the docs seem pretty thin. https://www.elastic.co/products/elasticsearch says

&gt; Per-Operation Persistence
&gt; 
&gt; Elasticsearch puts your data safety first. Document changes are recorded in transaction logs on multiple nodes in the cluster to minimize the chance of any data loss.

But http://www.elastic.co/guide/en/elasticsearch/reference/1.3/index-modules-translog.html suggests that ES only fsyncs the transaction log every five seconds, so maybe there aren't supposed to be any guarantees around retaining the most recent 5 seconds of data? In that case, why advertise transaction logs as a persistence feature?

Maybe this is super naive of me, but I kinda envisioned "putting data safety first" meaning, well, flushing the transaction log _before_ acknowledging a write to the client. That's the [Postgres default](http://www.postgresql.org/docs/current/static/wal.html), and MySQL's default appears to be [write and flush the transaction log](https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit) for every commit as well. Zookeeper blocks writes for fsync, which makes sense, but Riak's [leveldb backend](http://docs.basho.com/riak/1.0.0/tutorials/choosing-a-backend/LevelDB/) and [bitcask backend](http://docs.basho.com/riak/1.0.0/tutorials/choosing-a-backend/Bitcask/#Configuring-Bitcask) default to not syncing at all, and Cassandra [fsyncs on a schedule](https://issues.apache.org/jira/browse/CASSANDRA-3950) rather than blocking writes as well.

Thoughts?
</description><key id="72569847">10933</key><summary>Node crashes can cause data loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">aphyr</reporter><labels><label>:Translog</label><label>v2.0.0-beta1</label></labels><created>2015-05-02T02:11:08Z</created><updated>2015-09-09T06:51:26Z</updated><resolved>2015-05-18T19:45:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ITWrangler" created="2015-05-02T05:33:36Z" id="98307930">This may not be helpful - and certainly not a technical answer - but personally I've never expected ES to have ACID like features, it's an indexer not a db, to that end I never use it as a primary store, simply an upstream service providing search services over data I've ingested from elsewhere.
</comment><comment author="kimchy" created="2015-05-02T09:50:08Z" id="98341903">The thought process we had is that the most common deployments of ES when its introduced to be used is next to a database, a replay-able upstream, or typically in a logging/metrics use case, where the default expected behavior of ES is to be semi geared towards faster insertion rate by not fsync'ing each operation (though one can configure ES to do so).

I find it similar to the (very early) decision to have by default 2 copies of the data (1 replica), compared to 3 (2 replicas). Most common use cases of ES won't expect a 3x increase in storage. Years ago, even 2x was a pain to explain as a default, since most people didn't expect such a system to have even a single additional copy of data (and was the cause of some blows thrown ES way that it doesn't use Lucene correctly and thats why there is extra cost of storage, yay).

Also, as you mentioned, other systems don't all default to fsync on each write, or block the result until an fsync happened. (on the other hand, there are known issues that need to be fixed regardless of the default, like #7572).

@dakrone / @bleskes lets make sure we run these tests regardless and see if nothing else was uncovered here, and update this issue

&gt; You're gonna hate me for this one. I apologize in advance.

Why? :), I personally think its a valid discussion to have, and reopened every once in a while to verify that the defaults chosen (sometimes on inception :) ) still make sense.
</comment><comment author="sorokod" created="2015-05-02T09:58:38Z" id="98342388">&gt; personally I've never expected ES to have ACID like features,

I don't think ES promises "ACID like" features, on the other hand they seem not to deliver on what they do promise. 
</comment><comment author="sdekock" created="2015-05-02T10:01:09Z" id="98342929">There are better options than fsync:

http://ayende.com/blog/164484/are-you-tripping-on-acid-i-think-you-forgot-something
http://ayende.com/blog/164673/the-difference-between-fsync-write-through-through-the-os-eyes
</comment><comment author="kimchy" created="2015-05-02T10:04:39Z" id="98344484">&gt; I don't think ES promises "ACID like" features, on the other hand they seem not to deliver on what they do promise.

It doesn't feel to me that we made a false promise? We didn't claim to have 0 data loss, and are very open around what works and what doesn't in our resiliency status page: http://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html.
</comment><comment author="sorokod" created="2015-05-02T10:28:48Z" id="98346606">The full quote from the ES website is:

&gt; Elasticsearch puts your data safety first. Document changes are recorded in transaction logs on multiple nodes in the cluster to minimize the chance of any data loss.

I guess that it's up to the reader to infer from the wording that `Pr(any data loss) &gt; 0` 
</comment><comment author="VorticonCmdr" created="2015-05-02T12:47:58Z" id="98353988">"minimize the chance of any data loss" doesn't sound like a 100% promise to me
</comment><comment author="sorokod" created="2015-05-02T13:16:55Z" id="98356432">Everyone can choose what is to be highlighted, the following is semantically equivalent:  

&gt; Document changes are recorded in transaction logs on multiple nodes in the cluster so that data loss occurs infrequently.
</comment><comment author="evantahler" created="2015-05-02T17:16:55Z" id="98380715">A question about this test:  Does the data loss occur as the shards are being moved/replicated/new master is being elected? If you replicate the shards across all active nodes before the test starts, does the data loss still occur?     
</comment><comment author="xorgy" created="2015-05-02T17:18:30Z" id="98380780">It's clearly a problem to acknowledge writes which can go on to fail, regardless of whether you expect data loss(something I find comical) or not. Though I suppose the schedules required to guarantee something like that could prove problematic in some general cases.

That said, even in a search indexing environment, would you want to lose track of even one document in your index? That sounds like a disaster, at least for the next person who has to find it.

To jump on the lexical analysis bandwagon, _putting data safety first_ would mean only acknowledging completed, synced writes; any other situation means that you're putting something other than data safety first, in this case probably write acknowledgement speed; which would be fine if we were frank about it, and even better if we had a strategy to avoid these silent failure cases, at least when we're loading up the database without any particular rush.
</comment><comment author="jfelectron" created="2015-05-02T17:21:21Z" id="98380910">Thanks @aphyr . ACID is beside the point. If you loose 10% of important data that you'd like your users to being to search for that's a breakdown in the contract implicit in an Acknowledged Insert.  If you're say an e-retailler are you cool with 10% of you inventory (random set and any given time) being unsearchable?
</comment><comment author="aphyr" created="2015-05-02T17:33:37Z" id="98381779">(I mean, keep in mind that the actual fraction lost is gonna depend on your failure schedule; jepsen tests are intentionally pathological haha)
</comment><comment author="gregoryyoung" created="2015-05-02T18:34:38Z" id="98384629">@sdekock you may be surprised but there are also circumstances where fsync out performs directio. In general though most people align fsync/directio/memmap sync in language even though they are different system calls.
</comment><comment author="adamcee" created="2015-05-03T22:38:21Z" id="98552487">FYI the link to the translog docs are version 1.3 Here are the Elasticsearch docs for version 1.5.  The one difference seems to be the transaction log flush threshold has increased.
http://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html
</comment><comment author="honzajde" created="2015-05-04T11:17:03Z" id="98679670">Btw. I'd like to ask (like total noob in this area), assuming same setup as in the original post, is there any mechanism after crash recovery that notifies you about possible data loss? Does every crash (one node let's say) mean potential data loss? Any standard way to recover from the ES logs only afterwards? Or do I need to put app logs together with the time of crash? I know its many questions - I said I am a noob:)
</comment><comment author="mdcallag" created="2015-05-04T14:46:31Z" id="98736950">I get why fsync-on-commit isn't the default and I am extremely naive about ES but I don't get why the defaults are 512 MB and 5 seconds and not something smaller, like commit at least once per second?
http://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html
</comment><comment author="geekpete" created="2015-05-05T23:44:16Z" id="99262893">What performance hit would ES take if it was changed to do it "the right way" ?

If the performance hit is small/acceptable, make it a default.
If it's not a small performance hit, make it an option eitherway.
</comment><comment author="mycrEEpy" created="2015-05-06T09:46:58Z" id="99401665">Does the test replicate all shards to all data nodes or only to a subset of nodes in the cluster (for example exactly the subset which will randomly crash)? In my understanding the index operation is synchronous and only returns with an OK if all shards received the document. Therefore if there is no network partition the whole cluster has to crash to loose documents this way if all nodes have a replica. Did i miss something?
</comment><comment author="yogirackspace" created="2015-05-17T00:52:31Z" id="102716375">If the Elastic Search is going to have random data loss which cannot be detected that poses huge doubt on Elastic Search as a viable technology for any major business use case. Even if we have a primary store and replicate data in Elastic Search and don't have a way to know that there is data loss it would still cause major concerns as we would never get to know when to when to reindex the data.  Could more details about the tests be shared. Are we talking an exceptional stress test scenario or should we take it that there is not really any guarantee around retaining the most recent 5 seconds of data. 

Also this document seems to be saying otherwise

https://www.elastic.co/guide/en/elasticsearch/guide/master/translog.html#img-xlog-pre-refresh

The purpose of the translog is to ensure that operations are not lost. This begs the question: how safe is the translog?

Writes to a file will not survive a reboot until the file has been fsync'ed to disk. By default, the translog is fsync'ed every 5 seconds. Potentially, we could lose 5 seconds worth of data—if the translog were the only mechanism that we had for dealing with failure.

Fortunately, the translog is only part of a much bigger system. Remember that an indexing request is considered successful only after it has completed on both the primary shard and all replica shards. Even if the node holding the primary shard were to suffer catastrophic failure, it would be unlikely to affect the nodes holding the replica shards at the same time.

While we could force the translog to fsync more frequently (at the cost of indexing performance), it is unlikely to provide more reliability.

If we are willing to take a performance loss what settings need to be tweaked to  fsync'ing each operation.
</comment><comment author="mycrEEpy" created="2015-05-17T11:23:26Z" id="102785498">@yogirackspace Check the linked PR above which exactly addresses your problem. It's labled for Elasticsearch 2.0
</comment><comment author="yogirackspace" created="2015-05-17T15:17:34Z" id="102814514">@mycrEEpy  Sorry the link is not obvious. Could you repost the link? In case you are referring to the link I gave as per document it is part of 1.4.0

https://www.elastic.co/guide/en/elasticsearch/guide/master/_elasticsearch_version.html
</comment><comment author="mycrEEpy" created="2015-05-17T16:04:23Z" id="102818197">@yogirackspace It's this PR https://github.com/elastic/elasticsearch/issues/11011
</comment><comment author="yogirackspace" created="2015-05-17T16:30:27Z" id="102820574">Thanks @mycrEEpy. Could it also be confirmed whether on a real time setup if the node holding the primary shard were to suffer catastrophic failure and gets restarted, in case the replica sets don't go down, the data would not be lost? Just want to be clear about data loss occurrence pattern.
</comment><comment author="clintongormley" created="2015-05-18T11:41:54Z" id="103027260">@yogirackspace re:

&gt; Could it also be confirmed whether on a real time setup if the node holding the primary shard were to suffer catastrophic failure and gets restarted, in case the replica sets don't go down, the data would not be lost? Just want to be clear about data loss occurrence pattern.

This is correct.  An indexing request goes through the following process:
- Written to the translog on the primary
- indexed on the primary
- written to the translog on each replica shard
- indexed on each replica shard
- once all replicas have responded, the request returns to the client

So as long as the replicas remain alive, the change will be persisted on the replica.
</comment><comment author="s1monw" created="2015-05-18T19:45:03Z" id="103188466">Hey @aphyr I decided to not hate you for this but instead overhaul a bit how our translog works as well as how we use it. Apparently there are different expectations on the durability aspects of elasticsearch as well as unclear understanding what an async commit / fsync means in terms of durability guarantees. Long story short here are two problems that cause the dataloss:
- currently the translog is buffered and is periodically written to disk. This would be fine if we knew exactly upto which offset it was written and fsynced since then the window would be exactly the interval configured to do an async flush. By default you'd then loose ~5 seconds of data at max. Yet that wasn't the case and the translog was very lenient when we encountered unexpected reads past EOF etc. This has been fixed in #11143 
- the other problem is the async flush itself, apparently folks expected a sync flush rather than some async process flushing data to disk. Fair enough! The main concern here was performance but after running some bulk indexing [benchmarks](http://benchmarks.elasticsearch.org) we decided that a ~7% perf hit on bulk is a good tradeoff for the durability guarantees we gained. When you look at single operation performance I personally think throughput is not so much a concern compared to durability so the perf hit will be higher but it's a good price to pay. I fixed this in #11011 introduces a durability mode that is by default set to `REQUEST` this means we are not fsyncing on every operation but on every Request for instance a bulk would only be synced once the entire bulk is processed.

Both fixes are only in master and targeted for `2.0` I think it's pretty unlikely to backport those at this point since the next major is reasonably close and the changes are pretty big. I am going to close this issue with target verion `2.0` if you have any question I am happy to answer them.
</comment><comment author="huntc" created="2015-08-26T10:55:15Z" id="134947522">&gt; This failure pattern induces the loss of inserted documents throughout the test: in one particular case, 10% of acknowledged inserts did not appear in the final set.

I think that what I'd like to see is ES only return OK when things are indeed OK. If things are not OK then allow that to push back to the client. I'm left wondering if part of this issue is related to ES accepting requests when it is not in a position to process them. WDYT?
</comment><comment author="clintongormley" created="2015-08-26T11:14:26Z" id="134950736">Let's say you have configured an index to have 2 replicas per shard.  The write `consistency` defaults to `quorum`, so the indexing request will only be accepted if at least the primary and one replica are live. So far so good.

However, let's say that the document is indexed on the primary, then the only live replica dies before it can be indexed on the replica.  In 1.x, the indexing request is returned as 200 OK (even though potentially the write on the primary could be lost if eg the primary is isolated). Any further indexing requests would time out until at least one replica shard is available again.

What has changed in 2.x is that indexing requests now return the number of shards that successfully indexed the document (and how many failed), which at least gives you the information to decide whether you want the write to be retried or not.

Note: the default number of replicas per shard is 1, and the quorum requirement is ignored if only 1 replica is configured.  Having just two copies of your data (ie the primary and the replica) is sufficient for most use cases, but to make things safer you need at least three (ie two replicas).
</comment><comment author="henrikno" created="2015-09-02T14:15:30Z" id="137096102">I'm still experiencing lost documents when restarting nodes in 2.0.0-beta1.
I'm doing a series of bulk insertions with WriteConsistencyLevel.ALL.
I check both BulkResponse.hasFailures and shardInfo.getFailed(), and if any of them fail I retry that bulk until it succeeds.
I run 3 nodes (minimum master nodes 2) on localhost. While indexing I stop them with Ctrl-C, wait a bit and start them again, wait, and then restart the next node.

```
Inserted documents: 200000
ES document _count = 191579
Checking documents
Documents Lost: 8421 (found by doing GET on each one)
```

On version 1.5.2 I also tried running with `index.gateway.local.sync: 0` and `index.translog.fs.type: simple` (according to #5594), but it did not help against lost documents on the "rolling restart"-scenario.
I also tried doing _flush before each bulk is considered success, which helped some because flush fails a lot when nodes go down, but it didn't guarantee no loss.

Test code: https://gist.github.com/henrikno/e0ebd6804cb62491343c

Might there be other things than fsyncing that causes this issue?
</comment><comment author="jasontedor" created="2015-09-08T12:57:28Z" id="138552142">@henrikno Thank you for bringing this issue to our attention. We approach these matters with the utmost seriousness. We are currently taking steps to reproduce, diagnose and resolve the issue that you report. If you have any additional information that you think will be helpful, please send it our way.
</comment><comment author="bleskes" created="2015-09-08T13:48:56Z" id="138565239">@henrikno another quick question to help us go in the same direction you did.  How long did you wait after starting a node and before killing the next one? Was the cluster fully recovered from the previous restart? (i.e., green)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/io/stream/ByteBufferStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngineFactory.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamInput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReader.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/ImmutableTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReaderBase.java</file><file>src/main/java/org/elasticsearch/index/translog/MultiSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/TruncatedTranslogException.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>src/test/java/org/elasticsearch/transport/TransportMessageTests.java</file></files><comments><comment>Merge pull request #11143 from elastic/feature/translog_checkpoints</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/io/stream/ByteBufferStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/PropertiesSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferedChecksumStreamInput.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReader.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/ChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/Checkpoint.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/ImmutableTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogReaderBase.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>src/main/java/org/elasticsearch/index/translog/TruncatedTranslogException.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>src/test/java/org/elasticsearch/transport/TransportMessageTests.java</file></files><comments><comment>Add translog checkpoints to prevent translog corruption</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/BufferingTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/SimpleTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogService.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Make modifying operations durable by default.</comment></comments></commit></commits></item><item><title>Primary shards with `index.shared_filesystem` should be able to be recovered by any node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10932</link><project id="" key="" /><description>When a cluster using `index.shared_filesystem` lose its copy of the primary shard, we currently don't allocate the shard because ES doesn't think the data exists anywhere else.

It would be nice if Elasticsearch knew that the index existed on shared storage and allowed the primary shard to be allocated to a different node in the cluster in the case that no node has a copy of the shard state.
</description><key id="72559145">10932</key><summary>Primary shards with `index.shared_filesystem` should be able to be recovered by any node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label></labels><created>2015-05-02T00:56:09Z</created><updated>2015-05-04T20:31:13Z</updated><resolved>2015-05-04T20:31:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T09:41:07Z" id="98652723">++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/index/IndexWithShadowReplicasTests.java</file></files><comments><comment>Allow shards on shared filesystems to be recovered on any node</comment></comments></commit></commits></item><item><title>Support multiple clusters in marvel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10931</link><project id="" key="" /><description>I came across a mailing list entry here (http://elasticsearch-users.115913.n3.nabble.com/Marvel-monitoring-multiple-ES-clusters-by-one-Monitoring-ES-cluster-td4053005.html) where you say that a single marvel instance isn't able to support monitoring multiple clusters.  I've had trouble but it still kindof works if you filter them down.  

I'm wondering when this is planned, if it's still planned, and if not, if is it possible to create separate per cluster indexes so that I can at least switch between cluster indexes?
</description><key id="72548895">10931</key><summary>Support multiple clusters in marvel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">clly</reporter><labels /><created>2015-05-01T23:42:02Z</created><updated>2015-05-21T12:07:19Z</updated><resolved>2015-05-21T12:07:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T12:24:58Z" id="98692190">Hi @clly 

I believe it is still planned, but for the moment you need separate marvel installs.  I'll leave this for @bleskes to comment on in more detail.
</comment><comment author="bleskes" created="2015-05-21T12:07:16Z" id="104248649">@clly it is high on the agenda to support multiple cluster in the next major version of  Marvel. However, we plan to rewrite Marvel to take use of the latest and best available today (most notably Kibana 4) which means it will take some time... 

The current UI kinda works if you know what you're doing but it can be very confusing and break subtly in some places, which why we say it's not supported.

I'm going to close this for now - we are tracking this and it will take time so I don't feel it's worth it to keep it open here.

Let me know if you have any question
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MapperParsingException unknown property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10930</link><project id="" key="" /><description>@message contains XML. If I parse the xml with xmltodict, attach that to the @fields field, and insert the resulting json, I get this type of error on only some of the records being inserted.
# Error

```
Failed to index document 9d876794b2d34d8947228ed88975f230: {u'status': 400, u'_type': u'AuditLog', u'_id': u'9d876794b2d34d8947228ed88975f230', u'error': u'MapperParsingException[failed to parse [@fields.BroadsoftDocument.command.agentUnavailableCode]]; nested: ElasticsearchIllegalArgumentException[unknown property [@nil]]; ', u'_index': u'bwauditlog-2015.04.23'}
```
# Failing document

``` javascript
{
  "_type": "AuditLog",
  "_id": "9d876794b2d34d8947228ed88975f230",
  "_source": {
    "@tags": [],
    "@fields": {
      "BroadsoftDocument": {
        "@protocol": "OCI",
        "userId": "foo",
        "command": {
          "@echo": "UserCallCenterModifyRequest",
          "@type": "UserCallCenterModifyRequest",
          "userId": "foo@simplesignal.com",
          "agentACDState": "Unavailable",
          "agentUnavailableCode": {
            "@nil": "true"
          } 
        } 
      },
      "hour": "10",
      "level": "User",
      "tzone": "EDT",
      "userid": "foo",
      "month": "04",
      "logtype": "Audit",
      "second": "12",
      "command": "UserCallCenterModifyRequest",
      "ms": "314",
      "year": "2015",
      "operation": "write3063895",
      "day": "23",
      "minute": "22"
    },
    "@timestamp": 1429777332,
    "@source": "BWLOG:///var/BSFT/spool/AS1/AuditLog2015.04.23-10.21.48.txt",
    "@message": "2015.04.23 10:22:12:314 EDT | Audit | write3063895 | foo | User | UserCallCenterModifyRequest\n\t&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;BroadsoftDocument protocol=\"OCI\" xmlns=\"C\"&gt;\n  &lt;userId xmlns=\"\"&gt;foo&lt;/userId&gt;\n  &lt;command echo=\"UserCallCenterModifyRequest\" xsi:type=\"UserCallCenterModifyRequest\" xmlns=\"\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n    &lt;userId&gt;foo@simplesignal.com&lt;/userId&gt;\n    &lt;agentACDState&gt;Unavailable&lt;/agent
ACDState&gt;\n    &lt;agentUnavailableCode xsi:nil=\"true\"/&gt;\n  &lt;/command&gt;\n&lt;/BroadsoftDocument&gt;\n\n",
    "@type": "BW-AuditLog"
  },
  "_index": "bwauditlog-2015.04.23"
} 
```
# Successful document

``` javascript
{
  "_type": "AuditLog",
  "_id": "02f0f017e4da853594e00b6b4245fc9e",
  "_source": {
    "@tags": [],
    "@fields": {
      "BroadsoftDocument": {
        "@protocol": "OCI",
        "userId": "bar",
        "command": {
          "@type": "UserPersonalPhoneListGetListRequest",
          "userId": "bar@simplesignal.com",
          "searchCriteriaUserPersonalPhoneListNumber": {
            "mode": "Equal To",
            "value": "8001",
            "isCaseInsensitive": "false"
          }
        }
      },
      "hour": "10",
      "level": "User",
      "tzone": "EDT",
      "userid": "bar", 
      "month": "04", 
      "logtype": "Audit", 
      "second": "11", 
      "command": "UserPersonalPhoneListGetListRequest", 
      "ms": "710", 
      "year": "2015", 
      "operation": "read167780490", 
      "day": "23", 
      "minute": "22"
    }, 
    "@timestamp": 1429777331, 
    "@source": "BWLOG:///var/BSFT/spool/AS1/AuditLog2015.04.23-10.21.48.txt", 
    "@message": "2015.04.23 10:22:11:710 EDT | Audit | read167780490 | bar | User | UserPersonalPhoneListGetListRequest\n\t&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;BroadsoftDocument protocol=\"OCI\" xmlns=\"C\"&gt;\n  &lt;userId xmlns=\"\"&gt;bar&lt;/userId&gt;\n  &lt;command xsi:type=\"UserPersonalPhoneListGetListRequest\" xmlns=\"\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n    &lt;userId&gt;bar@simplesignal.com&lt;/userId&gt;\n    &lt;searchCriteriaUserPersonalPhoneListNumber&gt;\n      &lt;mode&gt;Equal To&lt;/mode&gt;\n      &lt;value&gt;8001&lt;/value&gt;\n      &lt;isCaseInsensitive&gt;false&lt;/isCaseInsensitive&gt;\n    &lt;/searchCriteriaUserPersonalPhoneListNumber&gt;\n  &lt;/command&gt;\n&lt;/BroadsoftDocument&gt;\n\n", 
    "@type": "BW-AuditLog"
  }, 
  "_index": "bwauditlog-2015.04.23"
}
```
</description><key id="72538196">10930</key><summary>MapperParsingException unknown property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kputland</reporter><labels /><created>2015-05-01T22:25:16Z</created><updated>2015-05-04T12:23:15Z</updated><resolved>2015-05-04T12:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T12:23:13Z" id="98691968">You have previously indexed a document that used `agentUnavailableCode` as a scalar value, eg:

```
{ "agentUnavailableCode": "foo" }
```

Now you're trying to index a document where `agentUnavailableCode` is an object:

```
{ "agentUnavailableCode": { "@nil": "true" }}
```

Fields can't change type, so the first version that is indexed will win.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restructure Aggregation documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10929</link><project id="" key="" /><description>First pass at cleaning up the Aggregation documentation.
- Moved Aggregations out of Search and put it as a "top-level" chapter
- De-floated Buckets / Metrics / Reducers in the Aggregations section and made them real sections. 
- Moved individual buckets / metrics / reducers under their appropriate section
- Alphabetized everything ![emot-toot](https://cloud.githubusercontent.com/assets/1224228/7436542/b43c662e-f01c-11e4-9b9d-223e61d6c707.gif)
- Moved miscellaneous agg topics (caching, metadata, etc) to the end of the chapter
- Centralized some reducer content (`buckets_path` syntax, `_count`, `gap_policy`)
- Standardized the reducers a bit by adding a "agg in isolation" snippet, and a "parameters" table.  This wasn't strictly necessary, but I really think all our APIs should have a parameters table :)

Still more that _should_ be done (e.g. I think the Agg chapter intro is still too long / wordy / technical), but we can tackle content stuff later.

/cc @colings86 @jpountz @clintongormley 
</description><key id="72511753">10929</key><summary>Restructure Aggregation documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>docs</label></labels><created>2015-05-01T20:12:56Z</created><updated>2015-05-04T17:28:48Z</updated><resolved>2015-05-04T17:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-01T20:36:36Z" id="98229738">Hey Zach. When I wrote java-api doc about aggs, I tried to use the same naming as we have in reference doc. Do you think you can also update [java-api doc](https://github.com/elastic/elasticsearch/tree/master/docs/java-api/aggregations) within the same PR?
</comment><comment author="clintongormley" created="2015-05-04T12:15:11Z" id="98690035">Big improvement!  

Not in your PR, but the Controlling Diversity header under the Sample agg should be moved down one level.  

@dadoonet I don't understand your question - the naming hasn't changed, and the html links should still be the same?
</comment><comment author="dadoonet" created="2015-05-04T12:51:01Z" id="98698317">@clintongormley reading the change again, I would now say "ignore me!" :)
I thought we were changing dir names but I was wrong.

Sorry for the noise.
</comment><comment author="polyfractal" created="2015-05-04T13:19:58Z" id="98702950">@clintongormley Sampler agg docs update.
</comment><comment author="clintongormley" created="2015-05-04T15:09:47Z" id="98743511">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10929 from polyfractal/docs/aggs</comment></comments></commit></commits></item><item><title>elasticsearch is spanning lot of processes and then causing system crash.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10928</link><project id="" key="" /><description>1. I have installed elastiocsearch on linux
2. After sometime, I have experienced lot out of memory problems along with slow down of machine.
3. When I have done some digging, I have found following details.

[root@csa45-1461 ~]# ps -eLF | grep csauser | wc -l
1026
[root@csa45-1461 ~]# ps -eLF | grep elasticsearch | wc -l
818
[root@csa45-1461 ~]#
1. Apparently, elasticsearch is spanning lot of processes. I am not sure why elasticsearch is creating so many processes.

Please also not that my instance of elastic search is made to work on localhost only.

FYI: The elasticsearch product details are as follows,
{
  "status" : 200,
  "name" : "Human Torch",
  "cluster_name" : "pmalla-lt-csa-c28",
  "version" : {
    "number" : "1.4.3",
    "build_hash" : "36a29a7144cfde87a960ba039091d40856fcb9af",
    "build_timestamp" : "2015-02-11T14:23:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
}
</description><key id="72505670">10928</key><summary>elasticsearch is spanning lot of processes and then causing system crash.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjspka</reporter><labels><label>feedback_needed</label></labels><created>2015-05-01T19:38:33Z</created><updated>2015-11-04T13:27:57Z</updated><resolved>2015-11-04T13:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T12:02:01Z" id="98688445">Hi @mjspka 

Elasticsearch is multi-threaded. The `ps -eLF` command lists all the threads (not processes).  You can see how many threads are in use by running this command:

```
curl -XGET "http://localhost:9200/_nodes/stats/thread_pool?pretty"
```

I'm surprised that there are 800+ threads.  Do you have any custom config your `config/elasticsearch.yml` file?
</comment><comment author="mjspka" created="2015-05-06T03:45:32Z" id="99312695">Here is my elasticsearch config file

##################### Elasticsearch Configuration Example
#####################

# This file contains an overview of various configuration settings,

# targeted at operations staff. Application developers should

# consult the guide at http://elasticsearch.org/guide.

#

# The installation procedure is covered at

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html

&gt; .
&gt; #
&gt; 
&gt; # Elasticsearch comes with reasonable defaults for most settings,
&gt; 
&gt; # so you can try it out without bothering with configuration.
&gt; 
&gt; #
&gt; 
&gt; # Most of the time, these defaults are just fine for running a production
&gt; 
&gt; # cluster. If you're fine-tuning your cluster, or wondering about the
&gt; 
&gt; # effect of certain configuration option, please _do ask_ on the
&gt; 
&gt; # mailing list or IRC channel [http://elasticsearch.org/community].

# Any element in the configuration can be replaced with environment

variables

# by placing them in ${...} notation. For example:

#
#node.rack: ${RACK_ENV_VAR}

# For information on supported formats and syntax for the config file, see

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html


################################### Cluster
###################################

# Cluster name identifies your cluster for auto-discovery. If you're running

# multiple clusters on the same network, make sure you're using unique

names.
#
#cluster.name: elasticsearch

#################################### Node
#####################################

# Node names are generated dynamically on startup, so you're relieved

# from configuring them manually. You can tie this node to a specific name:

#
#node.name: "Franz Kafka"

# Every node can be configured to allow or deny being eligible as the

master,

# and to allow or deny to store the data.

#

# Allow this node to be eligible as a master node (enabled by default):

#
#node.master: true
#

# Allow this node to store data (enabled by default):

#
#node.data: true

# You can exploit these settings to design advanced cluster topologies.

#

# 1. You want this node to never become a master node, only to hold data.

# This will be the "workhorse" of your cluster.

#
#node.master: false
#node.data: true
#

# 2. You want this node to only serve as a master: to not store any data and

# to have free resources. This will be the "coordinator" of your cluster.

#
#node.master: true
#node.data: false
#

# 3. You want this node to be neither master nor data node, but

# to act as a "search load balancer" (fetching data from nodes,

# aggregating results, etc.)

#
#node.master: false
#node.data: false

# Use the Cluster Health API [http://localhost:9200/_cluster/health], the

# Node Info API [http://localhost:9200/_nodes] or GUI tools

# such as http://www.elasticsearch.org/overview/marvel/,

# http://github.com/karmi/elasticsearch-paramedic,

# http://github.com/lukas-vlcek/bigdesk and

# http://mobz.github.com/elasticsearch-head to inspect the cluster state.

# A node can have generic attributes associated with it, which can later be

used

# for customized shard allocation filtering, or allocation awareness. An

attribute

# is a simple key value pair, similar to node.key: value, here is an

example:
#
#node.rack: rack314

# By default, multiple nodes are allowed to start from the same

installation location

# to disable it, set the following:

#node.max_local_storage_nodes: 1

#################################### Index
####################################

# You can set a number of options (such as shard/replica options, mapping

# or analyzer definitions, translog settings, ...) for indices globally,

# in this file.

#

# Note, that it makes more sense to configure index settings specifically

for

# a certain index, either when creating it or by using the index templates

API.
#

# See &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html&gt;
and

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html

&gt; # for more information.

# Set the number of shards (splits) of an index (5 by default):

#
#index.number_of_shards: 5

# Set the number of replicas (additional copies) of an index (1 by default):

#
#index.number_of_replicas: 1

# Note, that for development on a local machine, with small indices, it

usually

# makes sense to "disable" the distributed features:

#
#index.number_of_shards: 1
#index.number_of_replicas: 0

# These settings directly affect the performance of index and search

operations

# in your cluster. Assuming you have enough machines to hold shards and

# replicas, the rule of thumb is:

#

# 1. Having more _shards_ enhances the _indexing_ performance and allows to

# _distribute_ a big index across machines.

# 2. Having more _replicas_ enhances the _search_ performance and improves

the

# cluster _availability_.

#

# The "number_of_shards" is a one-time setting for an index.

#

# The "number_of_replicas" can be increased or decreased anytime,

# by using the Index Update Settings API.

#

# Elasticsearch takes care about load balancing, relocating, gathering the

# results from nodes, etc. Experiment with different settings to fine-tune

# your setup.

# Use the Index Status API (http://localhost:9200/A/_status) to inspect

# the index status.

#################################### Paths
####################################

# Path to directory containing configuration (this file and logging.yml):

#
#path.conf: /path/to/conf

# Path to directory where to store index data allocated for this node.

#
#path.data: /path/to/data
#

# Can optionally include more than one location, causing data to be striped

across

# the locations (a la RAID 0) on a file level, favouring locations with

most free

# space on creation. For example:

#
#path.data: /path/to/data1,/path/to/data2

# Path to temporary files:

#
#path.work: /path/to/work

# Path to log files:

#
#path.logs: /path/to/logs

# Path to where plugins are installed:

#
#path.plugins: /path/to/plugins

#################################### Plugin
###################################

# If a plugin listed here is not installed for current node, the node will

not start.
#
#plugin.mandatory: mapper-attachments,lang-groovy

################################### Memory
####################################

# Elasticsearch performs poorly when JVM starts swapping: you should ensure

that

# it _never_ swaps.

#

# Set this property to true to lock the memory:

#
#bootstrap.mlockall: true

# Make sure that the ES_MIN_MEM and ES_MAX_MEM environment variables are set

# to the same value, and that the machine has enough memory to allocate

# for Elasticsearch, leaving enough memory for the operating system itself.

#

# You should also make sure that the Elasticsearch process is allowed to

lock

# the memory, eg. by using `ulimit -l unlimited`.

############################## Network And HTTP
###############################

# Elasticsearch, by default, binds itself to the 0.0.0.0 address, and

listens

# on port [9200-9300] for HTTP traffic and on port [9300-9400] for

node-to-node

# communication. (the range means that if the port is busy, it will

automatically

# try the next port).

# Set the bind address specifically (IPv4 or IPv6):

#
#network.bind_host: localhost

# Set the address other nodes will use to communicate with this node. If not

# set, it is automatically derived. It must point to an actual IP address.

#
#network.publish_host: 192.168.0.1

# Set both 'bind_host' and 'publish_host':

#
#network.host: 192.168.0.1

# Set a custom port for the node to node communication (9300 by default):

#
#transport.tcp.port: 9300

# Enable compression for all communication between nodes (disabled by

default):
#
#transport.tcp.compress: true

# Set a custom port to listen for HTTP traffic:

#
http.port: 9201

# Set a custom allowed content length:

#
#http.max_content_length: 100mb

# Disable HTTP completely:

#
#http.enabled: false

################################### Gateway
###################################

# The gateway allows for persisting the cluster state between full cluster

# restarts. Every change to the state (such as adding an index) will be

stored

# in the gateway, and when the cluster starts up for the first time,

# it will read its state from the gateway.

# There are several types of gateway implementations. For more information,

see

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html

&gt; .

# The default gateway type is the "local" gateway (recommended):

#
#gateway.type: local

# Settings below control how and when to start the initial recovery process

on

# a full cluster restart (to reuse as much local data as possible when

using shared

# gateway).

# Allow recovery process after N nodes in a cluster are up:

#
#gateway.recover_after_nodes: 1

# Set the timeout to initiate the recovery process, once the N nodes

# from previous setting are up (accepts time value):

#
#gateway.recover_after_time: 5m

# Set how many nodes are expected in this cluster. Once these N nodes

# are up (and recover_after_nodes is met), begin recovery process

immediately

# (without waiting for recover_after_time to expire):

#
#gateway.expected_nodes: 2

############################# Recovery Throttling
#############################

# These settings allow to control the process of shards allocation between

# nodes during initial recovery, replica allocation, rebalancing,

# or when adding and removing nodes.

# Set the number of concurrent recoveries happening on a node:

#

# 1. During the initial recovery

#
#cluster.routing.allocation.node_initial_primaries_recoveries: 4
#

# 2. During adding/removing nodes, rebalancing, etc

#
#cluster.routing.allocation.node_concurrent_recoveries: 2

# Set to throttle throughput when recovering (eg. 100mb, by default 20mb):

#
#indices.recovery.max_bytes_per_sec: 20mb

# Set to limit the number of open concurrent streams when

# recovering a shard from a peer:

#
#indices.recovery.concurrent_streams: 5

################################## Discovery
##################################

# Discovery infrastructure ensures nodes can be found within a cluster

# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered

# operational within the cluster. This should be set to a quorum/majority

of

# the master-eligible nodes in the cluster.

#
#discovery.zen.minimum_master_nodes: 1

# Set the time to wait for ping responses from other nodes when discovering.

# Set this option to a higher value on a slow or congested network

# to minimize discovery failures:

#
#discovery.zen.ping.timeout: 3s

# For more information, see

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html


# Unicast discovery allows to explicitly control which nodes will be used

# to discover the cluster. It can be used when multicast is not present,

# or to restrict the cluster communication-wise.

#

# 1. Disable multicast discovery (enabled by default):

#
#discovery.zen.ping.multicast.enabled: true

#

# 2. Configure an initial list of master nodes in the cluster

# to perform discovery when new nodes (master or data) are started:

#
#discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]

# EC2 discovery allows to use AWS EC2 API in order to perform discovery.

#

# You have to install the cloud-aws plugin for enabling the EC2 discovery.

#

# For more information, see

# &lt;

http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-ec2.html

&gt; #
&gt; 
&gt; # See http://elasticsearch.org/tutorials/elasticsearch-on-ec2/
&gt; 
&gt; # for a step-by-step tutorial.

# GCE discovery allows to use Google Compute Engine API in order to perform

discovery.
#

# You have to install the cloud-gce plugin for enabling the GCE discovery.

#

# For more information, see &lt;

https://github.com/elasticsearch/elasticsearch-cloud-gce&gt;.

# Azure discovery allows to use Azure API in order to perform discovery.

#

# You have to install the cloud-azure plugin for enabling the Azure

discovery.
#

# For more information, see &lt;

https://github.com/elasticsearch/elasticsearch-cloud-azure&gt;.

################################## Slow Log
##################################

# Shard level query and fetch threshold logging.

#index.search.slowlog.threshold.query.warn: 10s
#index.search.slowlog.threshold.query.info: 5s
#index.search.slowlog.threshold.query.debug: 2s
#index.search.slowlog.threshold.query.trace: 500ms

#index.search.slowlog.threshold.fetch.warn: 1s
#index.search.slowlog.threshold.fetch.info: 800ms
#index.search.slowlog.threshold.fetch.debug: 500ms
#index.search.slowlog.threshold.fetch.trace: 200ms

#index.indexing.slowlog.threshold.index.warn: 10s
#index.indexing.slowlog.threshold.index.info: 5s
#index.indexing.slowlog.threshold.index.debug: 2s
#index.indexing.slowlog.threshold.index.trace: 500ms

################################## GC Logging
################################

#monitor.jvm.gc.young.warn: 1000ms
#monitor.jvm.gc.young.info: 700ms
#monitor.jvm.gc.young.debug: 400ms

#monitor.jvm.gc.old.warn: 10s
#monitor.jvm.gc.old.info: 5s
#monitor.jvm.gc.old.debug: 2s

################################## Security ################################

# Uncomment if you want to enable JSONP as a valid return transport on the

# http server. With this enabled, it may pose a security risk, so disabling

# it unless you need it is recommended (it is disabled by default).

#
#http.jsonp.enable: true

On Mon, May 4, 2015 at 5:02 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @mjspka https://github.com/mjspka
&gt; 
&gt; Elasticsearch is multi-threaded. The ps -eLF command lists all the
&gt; threads (not processes). You can see how many threads are in use by running
&gt; this command:
&gt; 
&gt; curl -XGET "http://localhost:9200/_nodes/stats/thread_pool?pretty"
&gt; 
&gt; I'm surprised that there are 800+ threads. Do you have any custom config
&gt; your config/elasticsearch.yml file?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10928#issuecomment-98688445
&gt; .
</comment><comment author="clintongormley" created="2015-05-07T19:05:07Z" id="99982663">OK, you haven't changed the default config for any thread pools.  What is the output of this command?

```
curl -XGET "http://localhost:9200/_nodes/stats/thread_pool?pretty"
```
</comment><comment author="clintongormley" created="2015-11-04T13:27:57Z" id="153719104">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>must filter is not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10927</link><project id="" key="" /><description>1. I have two types of documents indexed in elasticsearch. One is of type, "SERVICE_OFFERING" and the second one is of type, "SUBSCRIPTION.". You can see the screenshot of this in the below image.

![image](https://cloud.githubusercontent.com/assets/12202653/7435879/b1af2718-effc-11e4-9543-431167c17c66.png)
1. Now if I query based on catalogId I am getting results back.

Elasticsearch TRACE: 2015-05-01T19:25:00Z
  -&gt; POST http://localhost:9201/inventory/_search
  {
    "query": {
      "query_string": {
        "fields": [
          "serviceType",
          "displayName",
          "description",
          "_.displayName",
          "_.description",
          "*.value"
        ],
        "query": "vm"
      }
    },
    "filter": {
      "bool": {
        "should": [
          {
            "term": {
              "catalogId": "90d9650a36988e5d0136988f03ab000f"
            }
          },
          {
            "term": {
              "catalogId": "fb2881884b787853014b7ada21670004"
            }
          }
        ]
      }
    },
    "size": "1",
    "from": "0"
  }
  &lt;- 200
  {
    "took": 4,
    "timed_out": false,
    "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
    },
    "hits": {
      "total": 8,
      "max_score": 0.7508366,
      "hits": [
        {
          "_index": "inventory",
          "_type": "subscription",
          "_id": "90e72e034ce89baf014ce8c0f8d20140",
    .....
    .....
1. But if I am doing query based on catalogId and docType then I not seeing any results.

Elasticsearch TRACE: 2015-05-01T19:27:08Z
  -&gt; POST http://localhost:9201/inventory/_search
  {
    "query": {
      "query_string": {
        "fields": [
          "serviceType",
          "displayName",
          "description",
          "_.displayName",
          "_.description",
          "*.value"
        ],
        "query": "vm"
      }
    },
    "filter": {
      "bool": {
        "should": [
          {
            "term": {
              "catalogId": "90d9650a36988e5d0136988f03ab000
            }
          },
          {
            "term": {
              "catalogId": "fb2881884b787853014b7ada2167000
            }
          }
        ],
        "must": [
          {
            "term": {
              "docType": "SUBSCRIPTION"
            }
          }
        ]
      }
    },
    "size": "1",
    "from": "0"
  }
  &lt;- 200
  {
    "took": 3,
    "timed_out": false,
    "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
    },
    "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
    }
  }

Elasticsearch DEBUG: 2015-05-01T19:27:08Z
  Request complete

As per elasticsearch documentation, The must filter should return the documents where it matches along with should filter.
1. Now I have used the query with must filter only as follows. I still do not see any results.

Elasticsearch TRACE: 2015-05-01T19:28:46Z
  -&gt; POST http://localhost:9201/inventory/_search
  {
    "query": {
      "query_string": {
        "fields": [
          "serviceType",
          "displayName",
          "description",
          "_.displayName",
          "_.description",
          "*.value"
        ],
        "query": "vm"
      }
    },
    "filter": {
      "bool": {
        "should": [],
        "must": [
          {
            "term": {
              "docType": "SUBSCRIPTION"
            }
          }
        ]
      }
    }
  }
  &lt;- 200
  {
    "took": 2,
    "timed_out": false,
    "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
    },
    "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
    }
  }

Elasticsearch DEBUG: 2015-05-01T19:28:46Z
  Request complete

FYI: The build information of elasticsearch that I am using is as follows,
{
  "status" : 200,
  "name" : "Human Torch",
  "cluster_name" : "pmalla-lt-csa-c28",
  "version" : {
    "number" : "1.4.3",
    "build_hash" : "36a29a7144cfde87a960ba039091d40856fcb9af",
    "build_timestamp" : "2015-02-11T14:23:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
}

I would appreciate if you can get back to me at the earliest.
</description><key id="72504334">10927</key><summary>must filter is not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjspka</reporter><labels /><created>2015-05-01T19:31:00Z</created><updated>2015-05-01T21:02:38Z</updated><resolved>2015-05-01T19:50:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-05-01T19:50:05Z" id="98218330">Typically a usage problem and a misunderstanding on how a search engine works.
`SUBSCRIPTION` has been indexed as `subscription`. You can't find `SUBSCRIPTION` term.

Please use the mailing list if you have any follow up question.
I recommend also reading this chapter: http://www.elastic.co/guide/en/elasticsearch/guide/current/mapping-analysis.html

Closing as not an issue.
</comment><comment author="mjspka" created="2015-05-01T21:02:38Z" id="98237641">Thank you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ScriptParameterParseException when upgrading from 1.4 -&gt; 1.5.2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10926</link><project id="" key="" /><description>After upgrading from 1.4 -&gt; 1.5.2 shards fail to load due to ScriptParameterParseException. 

Stacktrace:

```
org.elasticsearch.script.ScriptParameterParser$ScriptParameterParseException: Value must be of type String: [lang]
        at org.elasticsearch.script.ScriptParameterParser.parseConfig(ScriptParameterParser.java:111)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parseTransform(DocumentMapperParser.java:307)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:257)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:192)
        at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:434)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:307)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:430)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:376)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:467)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

The cause as I can best understand is the `lang` property is getting set to null for some transform scripts. We do not explicitly set the lang so this was unexpected. Our process is to use a template...

```
"mappings": {
            "&lt;type&gt;": {                
                "transform": [
                    {"script": "&lt;source&gt;"},
...
```

However, the mapping produced after an index creation (in 1.4) is...

```
{
    "&lt;index-name&gt;": {
        "mappings": {
            "&lt;type&gt;": {
                "transform": [
                    {"script": "&lt;source&gt;","lang": null}
...
```

The above mapping works in 1.4 but not in 1.5 it seems.
</description><key id="72500415">10926</key><summary>ScriptParameterParseException when upgrading from 1.4 -&gt; 1.5.2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">aewhite</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-05-01T19:15:05Z</created><updated>2015-05-07T08:52:12Z</updated><resolved>2015-05-07T08:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T11:55:10Z" id="98686389">@colings86 please could you take a look.  probably related to #7977 
</comment><comment author="aewhite" created="2015-05-04T12:50:29Z" id="98698251">Note to others; the only workaround I have found that works is to recreate the index and set lang property on transform explicitly. Perhaps there is a way to update an existing transform but it is not mentioned in the [Transform Documentation](http://www.elastic.co/guide/en/elasticsearch/reference/1.x/mapping-transform.html) at this time. 
</comment><comment author="colings86" created="2015-05-05T09:30:35Z" id="99008403">@aewhite thanks for raising this. When the transform is written in the mapping the script language is written even if it is set to null (null meaning use the default). I have opened PR #10976 which lets the parser reading the stored mapping read null for the script language instead of throwing an error
</comment><comment author="colings86" created="2015-05-07T08:52:12Z" id="99778438">@aewhite thanks again for raising this. The fix has now been merged in and should be available from 1.5.3
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/script/ScriptParameterParser.java</file><file>src/test/java/org/elasticsearch/bwcompat/ScriptTransformBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/TransformOnIndexMapperIntegrationTest.java</file></files><comments><comment>Scripting: allow script language to be null when parsing</comment></comments></commit></commits></item><item><title>securitymanager issue with windows temp dir: or at least for @Mpdreamz</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10925</link><project id="" key="" /><description>As seen on #10887, there is a problem where @Mpdreamz does not have access to java.io.tmpdir somehow. @rjernst has also been running tests in a VM and so far cannot reproduce it.

But I think there may be a sneaky bug. And we need to get to the bottom of it, otherwise the JDK not having access to the default temp dir will cause mayhem in this situation.
</description><key id="72482244">10925</key><summary>securitymanager issue with windows temp dir: or at least for @Mpdreamz</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-05-01T17:43:03Z</created><updated>2015-05-08T14:10:31Z</updated><resolved>2015-05-08T14:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-01T17:53:59Z" id="98187537">@Mpdreamz if you don't mind, could you run this:

```
%JAVA_HOME%\bin\java -XshowSettings -version
```

If this doesnt tell us what is happening, I will make a branch with some debugging logic to track it down.
</comment><comment author="rjernst" created="2015-05-01T17:59:19Z" id="98188413">For comparison, here is mine:
https://gist.github.com/rjernst/bd4886757068c03cefe6
</comment><comment author="gmarz" created="2015-05-01T17:59:28Z" id="98188440">For comparison, since I'm not able to reproduce this either, here's the output of mine: https://gist.github.com/gmarz/23ba9c3b7635bc3ccac7
</comment><comment author="Mpdreamz" created="2015-05-01T18:15:23Z" id="98193274">here's mine https://gist.github.com/anonymous/7d8f6ea699a0cd292562
</comment><comment author="rjernst" created="2015-05-01T18:43:33Z" id="98203109">I was able to recreate this issue by creating a temp dir exactly as @Mpdreamz has (`C:\Users\mpdreamz\AppData\Local\Temp`), setting `TMP` and `TEMP`, and running elasticsearch. I was then able to do it with an arbitrary temp dir (`C:\dev\temp`).
</comment><comment author="rjernst" created="2015-05-01T19:24:15Z" id="98212031">And now it stopped reproducing...
</comment><comment author="gmarz" created="2015-05-01T20:18:51Z" id="98223830">@rjernst I can consistently reproduce this as well by setting `TMP` and `TEMP` to anything other than `%USERPROFILE%\AppData\Local\Temp`
</comment><comment author="gmarz" created="2015-05-01T20:20:06Z" id="98224020">In @Mpdreamz's settings, the user is hard coded: 

`java.io.tmpdir = C:\Users\mpdreamz\AppData\Local\Temp\`

Is `mpdreamz` the actual user you're running as?
</comment><comment author="rmuir" created="2015-05-01T20:22:53Z" id="98224751">@gmarz when you set those environment variables, how does it reflect in java.io.tmpdir? Can you set them and run -XshowSettings -version again?

fyi I created a branch with additional debugging: https://github.com/rmuir/elasticsearch/tree/eight_point_three
</comment><comment author="gmarz" created="2015-05-01T20:27:43Z" id="98227508">Setting `TMP`/`TEMP` to `C:\temp` yields:

`java.io.tmpdir = C:\temp\`

as expected.  Here's the full output: https://gist.github.com/gmarz/2a76c7c2312a11a22347
</comment><comment author="rmuir" created="2015-05-01T20:44:04Z" id="98231483">@gmarz thank you.

my current suspicion is (as silly as this seems) some crazy bug involving the extra trailing slash there. See how java.home or other properties don't have that? 

Can you try from the latest commit here and see if my hack prevents the issue? I just want to rule that out:

https://github.com/rmuir/elasticsearch/tree/eight_point_three

If it fails on startup, I will try a different hack.
</comment><comment author="gmarz" created="2015-05-01T20:51:22Z" id="98233906">@rmuir still fails.  Here's the output with your added debugging: https://gist.github.com/gmarz/5b229eea3f179d2cc9c5
</comment><comment author="rmuir" created="2015-05-01T20:53:30Z" id="98234764">Thank you @gmarz ! Now the problem is clear:

```
 ("java.io.FilePermission" "C:\Users\GREGM_~1\AppData\Local\Temp\" "read,write")
 ("java.io.FilePermission" "C:\Users\GREGM_~1\AppData\Local\Temp\-" "read,write,delete")
 ("java.io.FilePermission" "C:\Users\GREGM_~1\AppData\Local\Temp\\-" "read,write,delete")
```

yet

```
access denied ("java.io.FilePermission" "C:\temp\7019676636582904207.tmp"
```

So somehow java.io.tmpdir is a chameleon.
</comment><comment author="gmarz" created="2015-05-01T20:55:44Z" id="98235538">Ah, that's interesting.  Nice catch!
</comment><comment author="gmarz" created="2015-05-01T20:59:35Z" id="98237228">I wonder though how it still knows about `C:\Users\GREGM_~1\AppData\Local\Temp\` though considering it's not in any environment variable.
</comment><comment author="rmuir" created="2015-05-01T21:10:42Z" id="98239728">JDK code uses GetTempPathW (https://msdn.microsoft.com/en-us/library/windows/desktop/aa364992%28v=vs.85%29.aspx) to initialize it, and that function can use USERPROFILE to derive it in some cases. But it makes no sense to me... Can you change this USERPROFILE to something else as a hack and see if it reflects in logs?
</comment><comment author="gmarz" created="2015-05-01T21:21:40Z" id="98241362">I changed `USERPROFILE` to `C:\foo`.  Doesn't look like it's being reflected anywhere: https://gist.github.com/gmarz/36b2869da353b5f4a734
</comment><comment author="rmuir" created="2015-05-01T21:24:01Z" id="98241636">Thanks for trying. I am going to stare at some of the code involved and maybe add some more debugging commits. I'll remove the extra-slash-hack as that is unrelated here. I really want to try to figure out where it gets C:\Users\GREGM_~1... from
</comment><comment author="rmuir" created="2015-05-08T14:10:31Z" id="100243274">Fixed by #10965 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exclude jackson-databind dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10924</link><project id="" key="" /><description>the jackson yaml data format pulls in the databind dependency, its important that we exclude it so we won't use any of its classes by mistake
</description><key id="72474767">10924</key><summary>Exclude jackson-databind dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T17:04:14Z</created><updated>2015-05-29T18:06:09Z</updated><resolved>2015-05-01T17:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-05-01T17:05:37Z" id="98178665">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10924 from kimchy/exclude_jackson_ann</comment></comments></commit></commits></item><item><title>Die cwd die</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10923</link><project id="" key="" /><description>See #10877 for reference.

Currently some code looks in CWD and places like $user.home for configuration files in a confusing way, in some cases even overriding other paths (so if you have a stray elasticsearch.yml there, good luck). 

Tests hide the problem because they add too many permissions. We fixed that here and then all tests were failing, and made hacky changes to get tests passing again.

So some of the changes here can be cleaned up / solved differently. For example we could look for the custom hunspell data directory in Environment, grant access to it, and deprecate it instead of throwing an error. But, with what is here all tests pass so I think we know enough to discuss so we can move forward.
</description><key id="72466735">10923</key><summary>Die cwd die</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T16:22:36Z</created><updated>2015-06-08T13:17:04Z</updated><resolved>2015-05-04T14:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-05-04T12:15:34Z" id="98690065">if no one has opinions then, I will push in 24 hours.
</comment><comment author="s1monw" created="2015-05-04T12:26:38Z" id="98692374">this looks great  - the only thing that I'd like to see is a note in the migration guide that this has been fixed and that for instance the dictionary files should be located in a different place. It's in `docs/reference/migration/migrate_2_0.asciidoc`
</comment><comment author="s1monw" created="2015-05-04T14:01:35Z" id="98715488">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkProcessorTests.java</file><file>src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java</file><file>src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java</file><file>src/test/java/org/elasticsearch/common/cli/CliToolTestCase.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/CJKFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/KeepTypesFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/LimitTokenCountFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/StopTokenFilterTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPlugin2Tests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>src/test/java/org/elasticsearch/indices/analyze/HunspellServiceTests.java</file><file>src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file><file>src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java</file><file>src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationTests.java</file><file>src/test/java/org/elasticsearch/tribe/TribeUnitTests.java</file></files><comments><comment>Merge pull request #10923 from elastic/die_cwd_die</comment></comments></commit></commits></item><item><title>Filter refactoring: Introduce toFilter() and fromXContent() in FilterBuilders and FilterParsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10922</link><project id="" key="" /><description>This PR is against the feature/query-refactoring branch.

Similar to #10580 but applied to all filters.

The planned refactoring of search queries requires to split the "parse()" method in FilterParsers into two methods, first a "fromXContent(...)" method that allows parsing to an intermediate filter representation (currently called FooFilterBuilder) and second a "Filter toFilter(...)" method on these intermediate representations that create the actual lucene filters.
This PR is a first step in that direction as it introduces the interface changes necessary for the further refactoring. It introduces the new interface methods while for now keeping the old Builder/Parsers still in place by delegating the new "toFilter()" implementations to the existing "parse()" methods, and by introducing a "catch-all" "fromXContent()" implementation in a BaseFilterParser that returns a temporary FilterBuilder wrapper implementation. This allows us to refactor the existing FilterBuilders step by step while already being able to start refactoring queries and filters with inner queries/filters.
</description><key id="72464357">10922</key><summary>Filter refactoring: Introduce toFilter() and fromXContent() in FilterBuilders and FilterParsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>non-issue</label></labels><created>2015-05-01T16:12:01Z</created><updated>2015-05-21T12:02:52Z</updated><resolved>2015-05-01T17:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-01T16:47:35Z" id="98175788">LGTM, left one minor comment
</comment><comment author="javanna" created="2015-05-07T13:32:07Z" id="99867073">I just reverted this in the feature/query-refactoring branch, given that filters are going away on master with #10985. No need to merge bunch of conflicts for classes that are going away.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Add Elasticsearch Indexer for WordPress to integrations.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10921</link><project id="" key="" /><description /><key id="72448503">10921</key><summary>Docs: Add Elasticsearch Indexer for WordPress to integrations.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikaelmattsson</reporter><labels><label>docs</label></labels><created>2015-05-01T14:58:11Z</created><updated>2015-05-04T11:17:28Z</updated><resolved>2015-05-04T11:17:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T11:17:16Z" id="98679688">thanks @mikaelmattsson - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add Elasticsearch Indexer for WordPress to integrations.asciidoc</comment></comments></commit></commits></item><item><title>java recovery api, stale/incorrect shard information, only correct after close/re-open</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10920</link><project id="" key="" /><description>I am using ES 1.5.2

For a given index I am trying to determine which ES node(s) hold the primary shards at any given time.

I am using the Java API

I make a RecoveryRequest for a specific index to a node, and get back a RecoveryResponse, I then do the following to create a simple list that only contains nodes which are primary for a given shard. I just switch on getPrimary then match on nodeId from a previously built map of nodes that I have.

```
response.shardResponses.values.foreach(srrList =&gt; {
                srrList.foreach(shardRecoveryResponse =&gt; {

                    val recoveryState = shardRecoveryResponse.recoveryState;
                    if (recoveryState.getPrimary) {
                        primaryNodes += nodeMap(recoveryState.getSourceNode.getId)
                    }
                })
        })
```

The issue is this. 

a) I startup my cluster with a test index, 5 shards and only a few documents. The cluster is 4 nodes. The shards are distributed as such (via the head plugin) (all green)

node1: s(1)-replica, s(2)-primary, s(3)-primary
node2: s(2)-replica, s(4)-replica
node3: s(0)-replica, s(3)-replica, s(4)-primary
node4: s(0)-primary, s(1)-primary

b) I run my little bit of code like the above and I get back what I would expect, (node1, node3 and node4) as the only nodes in my list because they are the only ones w/ primary shards

c) I then shut down node1 (currently holds 2 primary shards)

d) The cluster now balances and looks like this (via the head plugin) (all green)

node2: s(1)-replica, s(4)-replica, s(2)-primary
node3: s(0)-replica, s(3)-primary, s(4)-primary
node4: s(0)-primary, s(1)-primary, s(2)-replica, s(3)-replica

e) I run my little bit of code again and I DON'T get back what I expect the data within the RecoveryResponse states primary shard holding nodes are (node3 and node4). Node2 (according to data within RecoveryResponse) does not hold any primary shards..... Even after killing my client and completely re-connecting I get the same response. 

f) The only way I can get a correct view of the cluster after a rebalance is by closing the index, then re-opening it. Once this is done then the data in RecoveryResponse is correct (matches what head plugin says)

I am using this api wrong? Is this expected?
</description><key id="72444163">10920</key><summary>java recovery api, stale/incorrect shard information, only correct after close/re-open</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bitsofinfo</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2015-05-01T14:31:33Z</created><updated>2017-04-03T21:52:18Z</updated><resolved>2017-04-03T21:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2017-03-21T14:11:04Z" id="288089949">Are you still seeing this issue on the current version of Elasticsearch? If so, do you know the steps to reproduce the bug?</comment><comment author="jasontedor" created="2017-04-03T21:52:18Z" id="291285105">This is for an old version of Elasticsearch, and we have not received any additional feedback so closing.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove redundant BytesQueryBuilder in favour of using WrapperQueryBuilder internally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10919</link><project id="" key="" /><description>BytesQueryBuilder was introduced to be used internally by the phrase suggester and its collate feature. It ended up being exposed via Java api but the existing WrapperQUeryBuilder could be used instead. Added WrapperQueryBuilder constructor that accepts a BytesReference as argument.

One other reason why this query builder should be removed is that it gets on the way of the query parsers refactoring, given that it's the only query builder that allows to build a query through java api without having a corresponding query parser.
</description><key id="72424185">10919</key><summary>Remove redundant BytesQueryBuilder in favour of using WrapperQueryBuilder internally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T12:31:27Z</created><updated>2015-09-07T17:38:03Z</updated><resolved>2015-05-09T13:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-01T12:42:41Z" id="98125683">Heya @areek can you please double check that I am not missing anything here? :)
</comment><comment author="areek" created="2015-05-01T17:16:56Z" id="98180514">Hey @javanna, this looks good! Thanks for fixing this :)
</comment><comment author="areek" created="2015-05-01T17:56:28Z" id="98187950">Hey @javanna, I have added a test that breaks with this change. The test along with the fix can be found here https://github.com/elastic/elasticsearch/compare/elastic:master...areek:pr/10919?expand=1
</comment><comment author="javanna" created="2015-05-04T08:33:14Z" id="98634172">Hey @areek I did some more testing and compared the two constant score query outputs:

Before:

```
{
  "constant_score" : {
    "filter" : {
      "query" : {
        "match_phrase" : {
          "title" : "united states houses of representatives elections in washington 2006"
        }
      }
    }
  }
}
```

After:

```
{
  "constant_score" : {
    "filter" : {
      "wrapper" : {
        "filter" : "eyJxdWVyeSI6eyJtYXRjaF9waHJhc2UiOnsidGl0bGUiOiJ1bml0ZWQgc3RhdGVzIGhvdXNlcyBvZiByZXByZXNlbnRhdGl2ZXMgZWxlY3Rpb25zIGluIHdhc2hpbmd0b24gMjAwNiJ9fX0="
      }
    }
  }
}
```

where the binary portion contains the following:

```
{"query":{"match_phrase":{"title":"united states houses of representatives elections in washington 2006"}}}
```

There is an additional layer of wrapping, which is expected since we use the wrapper filter, but this looks good to me, I don't see any problem. I pulled in your additional test but didn't seem to fail for me. Let me know what you think.
</comment><comment author="javanna" created="2015-05-08T08:59:37Z" id="100161397">I just rebased this given that #10985 went in and BytesFilterBuilder became BytesQueryBuilder. Change looks similar though, we still have to wrap it into a constant score query. @jpountz can you please have a look too?
</comment><comment author="jpountz" created="2015-05-08T19:32:29Z" id="100335537">LGTM
</comment><comment author="areek" created="2015-05-08T20:01:58Z" id="100342412">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BytesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file></files><comments><comment>Java api: remove redundant BytesQueryBuilder in favour of using WrapperQueryBuilder internally</comment></comments></commit></commits></item><item><title>Add ability to prompt for selected settings on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10918</link><project id="" key="" /><description>Some settings may be considered sensitive, such as passwords, and storing them
in the configuration file on disk is not good from a security perspective. This change
allows settings to have a special value, `__prompt__`, that indicates elasticsearch
should prompt the user for the actual value on startup. This only works when
started in the foreground. In cases where elasticsearch is started as a service or
in the background, an exception will be thrown.

Closes #10838
</description><key id="72407265">10918</key><summary>Add ability to prompt for selected settings on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T11:03:19Z</created><updated>2015-06-02T14:40:34Z</updated><resolved>2015-06-02T14:40:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T08:26:12Z" id="98632605">I left one comment. I also wonder if we should add a `SecureString` that fully owns it's character array that we can potentially wipe after the fact? maybe as a different PR?
</comment><comment author="uboness" created="2015-05-04T08:31:36Z" id="98633842">How would you know when to wipe the array?
</comment><comment author="s1monw" created="2015-05-04T08:33:02Z" id="98634098">so there are multiple ways to do this. IMO there should only be ONE consumer. once the string is consumed we can wipe it. The other option is to have this thing MD5 or SHAxyz the input and never keep it around. 
</comment><comment author="uboness" created="2015-05-04T08:38:51Z" id="98636550">I like the one consumer approach... It's then the responsibility of the consumer to properly handle it and as far as the settings is concerned it's been handled 
</comment><comment author="s1monw" created="2015-05-04T09:40:17Z" id="98652389">@uboness wanna open a new issue or @jaymode maybe?
</comment><comment author="jaymode" created="2015-05-04T10:22:30Z" id="98670844">I will open the issue to add the `SecureString`
</comment><comment author="jaymode" created="2015-05-04T10:43:05Z" id="98673423">@s1monw updated the PR and opened #10950
</comment><comment author="s1monw" created="2015-05-05T09:52:37Z" id="99014865">I left one cosmetic comment - I will leave the final LGTM to @uboness here once you guys sorted out the different prompt options.
</comment><comment author="jaymode" created="2015-05-05T11:05:28Z" id="99033574">@uboness updated the PR to support prompting for text and secret values
</comment><comment author="s1monw" created="2015-05-13T20:29:58Z" id="101806194">@jaymode what's the status of this?
</comment><comment author="jaymode" created="2015-05-13T20:32:42Z" id="101806669">@uboness ping.

@s1monw still waiting on last LGTM
</comment><comment author="kimchy" created="2015-05-13T21:26:09Z" id="101820293">looks great!, left one small comment
</comment><comment author="jaymode" created="2015-05-14T11:42:01Z" id="102009980">added the constant and rebased to pickup other changes in bootstrap
</comment><comment author="s1monw" created="2015-05-28T07:45:44Z" id="106214611">@jaymode ping
</comment><comment author="s1monw" created="2015-05-29T09:17:05Z" id="106753314">@jaymode I think we are good to go here now?
</comment><comment author="jaymode" created="2015-05-29T12:58:58Z" id="106796582">@s1monw or @uboness can one you of do a once over on the commit I just pushed? I renamed the placeholder and that led to some special casing being needed so I want to make sure that part is reviewed. After that, I'll get this rebased and push it.
</comment><comment author="uboness" created="2015-05-29T13:21:23Z" id="106800957">I can live with pushing this for 1.6, just to have the functionality. But I would love to see this cleaned up and basically have all the placeholder replacement consolidated in the `replacePropertyPlaceholders' method.

would love to see the system placeholders follow the same convention and be in the form: `${env::foobar}`

We can also introduce a new setting: `config.ignore_placeholders`:

will ignore all placeholder settings:

``` yaml
config.ignore_placeholders: true
```

will only ignore `env` placeholders:

``` yaml
config.ignore_placeholders: env
```

will only ignore `prompt` placeholders:

``` yaml
config.ignore_placeholders: prompt
```

will ignore both the `env` and `prompt` settings:

``` yaml
config.ignore_placeholders: env, prompt
```

It's a breaking change, so something to consider for 2.0?
</comment><comment author="jaymode" created="2015-06-02T13:29:40Z" id="107953299">opened #11455 as discussion for these settings cleanups
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java</file><file>core/src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsTests.java</file></files><comments><comment>make prompt placeholders consistent with existing placeholders</comment></comments></commit></commits></item><item><title>Fixes Infinite values return from geo_bounds with non-zero bucket-ordinals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10917</link><project id="" key="" /><description>If the collect method was called with a bucketOrd of &gt; 0 the arrays holding the state for the aggregation would be grown but the initial values for the bucketOrds &gt; 0 were all set to Double.NEGATIVE_INFINITY meaning that for the bottom, posLeft and negLeft values no collected document would change the value since NEGATIVE_INFINITY is always less than every other value.

Closes #10804
</description><key id="72395579">10917</key><summary>Fixes Infinite values return from geo_bounds with non-zero bucket-ordinals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T09:53:32Z</created><updated>2015-05-05T09:02:31Z</updated><resolved>2015-05-05T09:00:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-01T17:53:42Z" id="98187504">LGTM
</comment><comment author="colings86" created="2015-05-05T09:00:48Z" id="99000467">Merged into master, 1.x, and 1.5.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting: dedup field names when using wildcard expression to define which fields to highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10916</link><project id="" key="" /><description>When specifying a wildcard expression to define which fields to highlight, we retrieve the matching field names from the mappers. The returned list of fields may contain duplicates, which is why we should make sure we don't go over the same field more than once. This is just to avoid performing the same task multiple times, but doesn't change the result of highlighting.
</description><key id="72385614">10916</key><summary>Highlighting: dedup field names when using wildcard expression to define which fields to highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>enhancement</label></labels><created>2015-05-01T08:38:15Z</created><updated>2015-05-29T15:32:47Z</updated><resolved>2015-05-27T14:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-01T09:49:48Z" id="98086649">Should the fix be in simpleMatchToFullName instead? Also maybe we should leave a comment explaining that we need to deduplicate fields today but this might not be necessary anymore if we store mappings at the index level?
</comment><comment author="javanna" created="2015-05-01T09:56:32Z" id="98087202">&gt; Should the fix be in simpleMatchToFullName instead?

good question, I was wondering myself, to be honest I have no clue :) I do have the feeling that I might be hiding some other issue with this fix, not sure.
</comment><comment author="javanna" created="2015-05-01T09:59:14Z" id="98087594">One other question could be if we should go ahead and try to highlight tons of metadata fields, like `_routing`, `_ttl` and such for instance when using `*`. I noticed this while looking at #10914 .
</comment><comment author="jpountz" created="2015-05-01T10:12:23Z" id="98092470">Let's ping @rjernst who might have ideas/opinions here as this is related to other refactorings he is performing.
</comment><comment author="clintongormley" created="2015-05-04T10:55:55Z" id="98676637">@javanna re

&gt; One other question could be if we should go ahead and try to highlight tons of metadata fields, like _routing, _ttl and such for instance when using *. 

We have an adoptme issue for that: https://github.com/elastic/elasticsearch/issues/9881
</comment><comment author="nik9000" created="2015-05-18T14:00:07Z" id="103069211">&gt; Should the fix be in simpleMatchToFullName instead? Also maybe we should leave a comment explaining that we need to deduplicate fields today but this might not be necessary anymore if we store mappings at the index level?

_probably_
</comment><comment author="javanna" created="2015-05-27T08:24:40Z" id="105820438">@rjernst any idea about whether `simpleMatchToFullName` should return duplicates or not? Should we fix the problem there (unless it was fixed already and I missed it) or in the highlighters code?
</comment><comment author="rjernst" created="2015-05-27T08:38:25Z" id="105824784">@javanna Yes, please make the fix in `simpleMatchToFullName`. It should be simpler in the future and not need a set (because only the full names will need to be checked).
</comment><comment author="javanna" created="2015-05-27T14:55:04Z" id="105943859">Superseded by #11377
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Internal: deduplicate field names returned by `simpleMatchToFullName` &amp; `simpleMatchToIndexNames` in FieldMappersLookup</comment></comments></commit></commits></item><item><title>Mappings: Remove ability to disable _source field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10915</link><project id="" key="" /><description>In order to reindex documents,  _source must be enabled. There are current features (eg. update API) and future features (eg. reindex API) that depend on _source. This change locks down the field so that
it can no longer be disabled. It also removes legacy settings compress/compress_threshold.

For users who "dont use" _source, we have a best_compression option that will store it with generally half the space of previous versions, still keeping the ability to reindex. For users concerned with indexing speed, a lot of work was put into stored fields merging/compression in Lucene 5.

closes #8142
</description><key id="72371265">10915</key><summary>Mappings: Remove ability to disable _source field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label></labels><created>2015-05-01T07:04:33Z</created><updated>2015-07-07T13:04:19Z</updated><resolved>2015-05-06T05:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-01T09:55:51Z" id="98087165">I am +1 on that change and the diff looks good to me. Could you add a note to the migration guide?

The issue was a bit controversial so @clintongormley could you confirm if this change is good to go?
</comment><comment author="javanna" created="2015-05-01T12:50:41Z" id="98126707">+1 on the change too. Does this mean we can also remove the option to store fields separately in the mapping given that we always have the source? Or are there still usecases for that?
</comment><comment author="rmuir" created="2015-05-01T14:21:43Z" id="98143335">+1 I think this is our only chance to do this.
</comment><comment author="kimchy" created="2015-05-01T15:02:32Z" id="98149725">the place where I saw it being used is for large documents, where most search requests require only a few fields, and parsing the whole request and extracting it was expensive (the user I helped with it ended up being significant). But, it is an outlier in terms of how often I actually saw it.

I like the idea of removing it, since it will simplify the whole crazy logic around handling stored fields and data extracted from source. I think we do need to have a pattern to users that have large documents and want fast access to only small amount of small fields quickly when loading hits, my ideas are doc values or parent/child now that we support fetching inner childs (or just parent)
</comment><comment author="jpountz" created="2015-05-01T18:07:25Z" id="98190238">&gt; Or are there still usecases for that?

It can still be useful for "generated" fields (eg. fields that are populated through copy_to)

&gt; I think we do need to have a pattern to users that have large documents and want fast access to only small amount of small fields quickly when loading hits

Maybe another option than doc values and parent/child could be to break up `_source` into several stored fields (https://github.com/elastic/elasticsearch/issues/9034)?
</comment><comment author="rjernst" created="2015-05-01T19:00:35Z" id="98205906">@jpountz Regarding migration docs, I pushed a commit with a simple migration note.
</comment><comment author="jpountz" created="2015-05-01T19:16:49Z" id="98210081">Thanks @rjernst it looks good!
</comment><comment author="kimchy" created="2015-05-01T19:43:51Z" id="98217094">+1, LGTM
</comment><comment author="clintongormley" created="2015-05-05T15:30:37Z" id="99116399">+1
</comment><comment author="faxm0dem" created="2015-07-06T10:15:34Z" id="118797851">Just FTR consider this a protest comment. Please do not remove this feature. There are use cases for ES as index-only and not storage
</comment><comment author="rjernst" created="2015-07-07T00:17:13Z" id="119033006">@faxm0dem This pull request was effectively reverted in #11171.
</comment><comment author="faxm0dem" created="2015-07-07T13:04:19Z" id="119198031">Awesome thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/compress/SearchSourceCompressTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Remove ability to disable _source field</comment></comments></commit></commits></item><item><title>Issue with highlighting on _all field with different analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10914</link><project id="" key="" /><description>Hey there,

Using 1.5.2, there seems to be some sort of issue, or at least confusion about how highlighting works on the `_all` field.

For example, when using dynamic mappings on fields, highlighting works as expected:

``` json
DELETE /testhighlightindexdynamic

POST testhighlightindexdynamic/type1/1
{
  "field1": "organization",
  "field2": "organization"
}

POST /testhighlightindexdynamic/_search
{
  "query": {
    "match": {
      "_all": "organization"
    }
  },
  "highlight": {
    "fields": {
      "*": {}
    }
  }
}
```

I get both fields highlighted as expected:

``` json
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.2712221,
      "hits": [
         {
            "_index": "testhighlightindexdynamic",
            "_type": "type1",
            "_id": "1",
            "_score": 0.2712221,
            "_source": {
               "field1": "organization",
               "field2": "organization"
            },
            "highlight": {
               "field1": [
                  "&lt;em&gt;organization&lt;/em&gt;"
               ],
               "field2": [
                  "&lt;em&gt;organization&lt;/em&gt;"
               ]
            }
         }
      ]
   }
}
```

However if I use a different analyzer on one of the fields and perform the same search, I no longer get both fields highlighted:

``` json
DELETE testhighlightindex

PUT /testhighlightindex
{
  "mappings": {
    "type1": {
      "dynamic": "strict",
      "properties": {
        "field1": {
          "type": "string",
          "analyzer": "snowball"
        },
        "field2": {
          "type": "string",
          "analyzer": "standard"
        }
      }
    }
  }
}

POST testhighlightindex/type1/1
{
  "field1": "organization",
  "field2": "organization"
}

POST /testhighlightindex/_search
{
  "query": {
    "match": {
      "_all": "organization"
      }
  },
  "highlight": {
    "fields": {
      "*": {}
    }
  }
}
```

This returns results with only `field2` highlighted (which is using the standard analyzer):

``` json
{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.2712221,
      "hits": [
         {
            "_index": "testhighlightindex",
            "_type": "type1",
            "_id": "1",
            "_score": 0.2712221,
            "_source": {
               "field1": "organization",
               "field2": "organization"
            },
            "highlight": {
               "field2": [
                  "&lt;em&gt;organization&lt;/em&gt;"
               ]
            }
         }
      ]
   }
}
```

It looks like the highlighter code only applies the analyzer of the search field, not the analyzer for the field defined in the mapping?  You can reverse the field that is highlighted in this second case by setting the analyzer on the `_all` field to "snowball".

Is this expected behaviour?
</description><key id="72356899">10914</key><summary>Issue with highlighting on _all field with different analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels /><created>2015-05-01T05:16:55Z</created><updated>2015-05-01T16:15:54Z</updated><resolved>2015-05-01T16:15:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-01T08:21:21Z" id="98073847">I see what happens: the query gets analyzed once with the `_all` analyzer, which gives `organization` as token to highlight. The content of each field to highlight gets retrieved and re-analyzed on-the-fly using the same analyzer that was used at index time (may differ per field). In this specific case `field` is analyzed using `snowball` which produces `organ` as output, thus there is nothing to highlight (`organization` != `organ`).

This is the expected behaviour. This is probably the bigger downside of querying one field and highlighting other fields. Highlighting the same fields that get queried probably makes more sense and solves the issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add span within/containing queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10913</link><project id="" key="" /><description>Expose new span queries from https://issues.apache.org/jira/browse/LUCENE-6083

Within returns matches from 'little' that are enclosed inside of a match from 'big'.
Containing returns matches from 'big' that enclose matches from 'little'.
</description><key id="72345674">10913</key><summary>Add span within/containing queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Query DSL</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-05-01T03:35:52Z</created><updated>2015-06-08T15:38:51Z</updated><resolved>2015-05-01T15:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-05-01T10:02:27Z" id="98088357">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #10913 from rmuir/spanspanspanspanspan</comment></comments></commit></commits></item><item><title>Docs: Cleanup meta field docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10912</link><project id="" key="" /><description>Meta fields were locked down to not allow exotic options to the
underlying field types in #8143. This change fixes the docs
to no longer refer to the old settings.

closes #10879
</description><key id="72293582">10912</key><summary>Docs: Cleanup meta field docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T22:05:20Z</created><updated>2015-05-07T18:27:34Z</updated><resolved>2015-05-07T18:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-05T06:00:11Z" id="98964303">As a side note, shouldn't we remove the ability to configure store/index on timestamp? http://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-timestamp-field.html#_store_index_2
</comment><comment author="rjernst" created="2015-05-06T00:50:56Z" id="99277366">@clintongormley I pushed a new commit addressing your comments. For `_timestamp` we never locked it down because it is essentially a user field (the only way to add a NOW to a date field). I think we should continue looking at changing that separately.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decaying part of the score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10911</link><project id="" key="" /><description>We use an exponential decay that scales only a portion of the score implemented in a [script score](https://github.com/wikimedia/mediawiki-extensions-CirrusSearch/blob/master/includes/Searcher.php#L1401). Specifically, we use it to express the notion that we care some about how old the content is. It works better for news use cases, but here is an example from Wikipedia which I give because its handy:
- [No score decay since edit time](https://en.wikipedia.org/w/index.php?title=Special%3ASearch&amp;profile=default&amp;search=nuclear+program&amp;fulltext=Search)
- [Small portion of score decays with edit time](https://en.wikipedia.org/w/index.php?title=Special%3ASearch&amp;profile=default&amp;search=prefer-recent%3A.1+nuclear+program&amp;fulltext=Search)
- [Most of the score decays with edit time](https://en.wikipedia.org/w/index.php?title=Special%3ASearch&amp;profile=default&amp;search=prefer-recent%3A.9+nuclear+program&amp;fulltext=Search)

It'd be nice if we could do that with a decay function. This:

``` js
                     {
                        "exp": {
                           "timestamp": {
                              "origin": "now",
                              "scale": "160d"
                           }
                        }
                     },
```

gets about half the job done, but it doesn't allow me to apply the portion of the scale.

I dug around in the code and didn't see a way to get what I wanted. Any ideas? I'm happy to implement something if it makes sense. Maybe a score function that wraps other score functions?

``` js
            {
               "portion": {
                  ".4": "no_change",
                  ".6":
                     {
                        "exp": {
                           "timestamp": {
                              "origin": "now",
                              "scale": "160d"
                           }
                        }
                     }
                  },
               }
            }
```
</description><key id="72289189">10911</key><summary>Decaying part of the score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-04-30T21:42:19Z</created><updated>2015-05-04T10:39:17Z</updated><resolved>2015-04-30T22:18:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-04-30T21:44:46Z" id="97978638">Or maybe its better to just use Lucene Expression Language for this.
</comment><comment author="nik9000" created="2015-04-30T22:18:00Z" id="97987521">Got it with expression language.
</comment><comment author="clintongormley" created="2015-05-04T10:39:17Z" id="98672876">++  for using scripting here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Access file-based stored search template through API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10910</link><project id="" key="" /><description>A file-based search template is currently opened on startup, but does not appear as a stored search template when attempting to list stored templates through the REST or Java API's.  These should be accessible.
</description><key id="72288994">10910</key><summary>Access file-based stored search template through API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Search Templates</label><label>discuss</label></labels><created>2015-04-30T21:41:23Z</created><updated>2016-09-27T14:38:57Z</updated><resolved>2016-09-27T14:38:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T10:38:08Z" id="98672779">Related to #10713
</comment><comment author="ppf2" created="2016-04-26T05:03:35Z" id="214617206">Also for index templates: https://github.com/elastic/elasticsearch/issues/17968
</comment><comment author="rjernst" created="2016-04-26T17:57:13Z" id="214828977">File based index templates were remove in 2.0: #11052
</comment><comment author="dakrone" created="2016-09-27T14:38:57Z" id="249884687">Since file-based templates were removed in 2.0 I am closing this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Read configuration file with .yaml suffix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10909</link><project id="" key="" /><description>Fixes a bug whereby we failed to read an elasticsearch config file with
the .yaml extension. This commit allows elasticsearch config files to
be suffixed with: .yml, .yaml, .json, .properties.

Closes #9706
</description><key id="72277597">10909</key><summary>Read configuration file with .yaml suffix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Settings</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T20:48:45Z</created><updated>2015-05-28T09:54:06Z</updated><resolved>2015-05-28T09:54:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-05-01T08:42:00Z" id="98077904">Change looks good but I think we should have a test for it too. Also, I remember making the same change on `LogConfigurator`, wonder if we could share some of that code, not sure though. What do you think?
</comment><comment author="javanna" created="2015-05-01T08:42:21Z" id="98077933">One more thing, this can go into 1.x too right?
</comment><comment author="aleph-zero" created="2015-05-04T22:22:15Z" id="98867896">I considered abstracting out the code from `LogConfigurator`, but I felt it's just too small to warrant it. 
</comment><comment author="aleph-zero" created="2015-05-04T22:22:26Z" id="98867932">Yes, it can go into 1.x.
</comment><comment author="javanna" created="2015-05-07T08:56:48Z" id="99781108">&gt; I considered abstracting out the code from LogConfigurator, but I felt it's just too small to warrant it.

fair enough, can we have a test for this change though?
</comment><comment author="aleph-zero" created="2015-05-07T20:27:49Z" id="100008678">@javanna Added a test
</comment><comment author="javanna" created="2015-05-08T06:56:47Z" id="100127700">LGTM thanks @aleph-zero 
</comment><comment author="javanna" created="2015-05-08T06:57:36Z" id="100127777">Added 1.6.0 and 1.5.3 labels, should be easy to backport ;)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file></files><comments><comment>Merge pull request #10909 from aleph-zero/issues/9706</comment></comments></commit></commits></item><item><title>Upgrade lucene snapshot to r1677039</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10908</link><project id="" key="" /><description>Two tests fail but these are about to be removed with #10897 so I put awaitsfix on them.
</description><key id="72276955">10908</key><summary>Upgrade lucene snapshot to r1677039</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2015-04-30T20:44:47Z</created><updated>2015-04-30T20:49:16Z</updated><resolved>2015-04-30T20:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-30T20:46:54Z" id="97962455">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add logging of slow cluster state tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10907</link><project id="" key="" /><description>Closes #10874

Logs how long each cluster state update task took on the debug level. If a task took longer than 30sec (dynamically configurable using `cluster.service.slow_task_logging_threshold`) this task is logged on the WARN level.
</description><key id="72235069">10907</key><summary>Add logging of slow cluster state tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T18:01:11Z</created><updated>2015-05-30T10:52:31Z</updated><resolved>2015-05-01T00:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-30T18:02:23Z" id="97899642">@imotov can we log the times not as ms but as `TimeValue`? Thats how we do it in other places
</comment><comment author="imotov" created="2015-04-30T19:11:41Z" id="97931676">@kimchy I pushed a new version that logs time as `TimeValue`, changes the wording in the warning and fixes a race condition in tests that I discovered while working on these changes.
</comment><comment author="kimchy" created="2015-04-30T19:20:25Z" id="97934875">lets another small comment, otherwise LGTM
</comment><comment author="clintongormley" created="2015-05-04T10:43:11Z" id="98673454">@imotov could you add a description of the changes to this PR please?
</comment><comment author="imotov" created="2015-05-04T13:36:37Z" id="98706138">@clintongormley updated the description in the first comment.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Memory leak in Elasticsearch v1.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10906</link><project id="" key="" /><description>Hello. I am running the ELK stack inside of docker. My elasticsearch container has v1.5.1 installed. I am seeing a consistent memory leak while running the elasticsearch container. While running the stack, I do the following:

1.) docker ps
2.) docker inspect {container id of es} (to get pid)
3.) top | grep {pid of es}

If you monitor the process with top while running logspout, performing Kibana searches, etc, the elasticsearch process never gives back any memory and the memory percentage increases gradually over time. Also, I had upgraded from 1.4.0 to 1.5.1 in an attempt to mitigate the problem, as the memory leak exists in 1.4.0 also.

This has become a serious concern as we would like to be able to stand up the ELK stack for QA and PROD, but cannot while the memory leak exists.

Thanks
</description><key id="72221360">10906</key><summary>Memory leak in Elasticsearch v1.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cold-gin</reporter><labels /><created>2015-04-30T17:01:02Z</created><updated>2015-05-05T20:02:24Z</updated><resolved>2015-05-04T10:35:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cold-gin" created="2015-04-30T19:55:25Z" id="97948996">Interesting, I don't see the memory leak when I upgrade to v1.5.2, so maybe it's fixed? Looks like the levels dropped after accessing the indices through Kibana, and doing searches against them.. I won't be able to verify this further until early next week, so I will update this issue after I test it and verify that the leak has been corrected. Thanks
</comment><comment author="clintongormley" created="2015-05-04T10:35:15Z" id="98672393">@cold-gin Elasticsearch is never going to give back memory that it uses for the heap.  You can control the size of the heap with ES_HEAP_SIZE (which you should definitely set before starting), plus mlockall etc.

See the docs here for more details: http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html

Elasticsearch also uses some off-heap memory, which may be released back to the OS. I don't believe that anything has changed in related code between 1.4 and 1.5.  I think the most likely thing is that you haven't configured ES correctly yet, and once you do, it will all behave as expected.

I suggest asking further questions on the mailing list instead.  If you do encounter a real memory leak, please reopen this issue.
</comment><comment author="cold-gin" created="2015-05-05T20:02:24Z" id="99204258">Thanks and sorry for any confusion. Whether it is coincidence or not, I would just like to say that when monitoring the elasticsearch container after upgrading to v1.5.2 I definitely do not see the increasing memory usage behavior that I was seeing earlier where the percentage continued to go up. The memory of the container process has definitely stabilized in 1.5.2 and it was definitely growing from 1.4.0 - 1.5.1. Again, just making this observation. Thank you once again for the information, I greatly appreciate it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>not_analyzed field empty vs null for docs before and after some value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10905</link><project id="" key="" /><description>I have a dynamic template so that any filed ending with "_r" is not_analyzed
now i use the following curl commands

curl -XPUT 'http://10.0.0.91:9200/twitter/tweet/1' -d '{"user":"user1", "message_r":""}'
curl -XPUT 'http://10.0.0.91:9200/twitter/tweet/2' -d '{"user":"user2", "message_r":"test message"}'
curl -XPUT 'http://10.0.0.91:9200/twitter/tweet/3' -d '{"user":"user1", "message_r":""}'

curl -XGET 'http://10.0.0.91:9200/twitter/tweet/_search?pretty' -d '{"query":{"filtered":{"filter":{"missing":{"field":"message_r"}}}}}'

here i get only one hit, twee/1

curl -XGET 'http://10.0.0.91:9200/twitter/tweet/_search?pretty' -d '{"query":{"filtered":{"filter":{"term":{"message_r":""}}}}}'
here i get one hit, tweet/3

why the difference in the results?
</description><key id="72201629">10905</key><summary>not_analyzed field empty vs null for docs before and after some value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vedil</reporter><labels /><created>2015-04-30T15:52:24Z</created><updated>2015-04-30T18:12:26Z</updated><resolved>2015-04-30T18:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-30T16:04:45Z" id="97854645">Super interesting. I don't have an answer yet but here is a full recreation:

```
DELETE twitter
PUT twitter
{
  "mappings": {
    "tweet": {
      "properties": {
        "message_r": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}
PUT /twitter/tweet/1
{"user":"user1", "message_r":""}
PUT /twitter/tweet/2
{"user":"user2", "message_r":"test message"}
PUT /twitter/tweet/3
{"user":"user1", "message_r":""}
GET /twitter/tweet/_search?pretty
{"query":{"filtered":{"filter":{"missing":{"field":"message_r"}}}}}
GET /twitter/tweet/_search?pretty
{"query":{"filtered":{"filter":{"term":{"message_r":""}}}}}
```

The funny thing is that document 1 does not come back though it has the same content.
</comment><comment author="vedil" created="2015-04-30T16:11:41Z" id="97860928">i think some thing happens after indexing the second doc, there is value for that field, so 1 and 3 are indexed differently
</comment><comment author="dadoonet" created="2015-04-30T16:16:45Z" id="97863597">So this is happening when you have more than one shard.
With a single shard, result is correct:

```
PUT twitter
{
  "settings": {
    "number_of_shards": 1
  }, 
  "mappings": {
    "tweet": {
      "properties": {
        "message_r": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}
```

It means that the "second" shard does not behave as the first one. 
Tested on 1.5.1. Will upgrade to see if I can reproduce on newer version.
Will also downgrade to see if it's a new bug.
</comment><comment author="vedil" created="2015-04-30T16:21:01Z" id="97865801">that is interesting, i did not try changing the shards, so i am seeing it with 5 shards. the version i am using is 
"version" : {
    "number" : "1.4.0",
    "build_hash" : "bc94bd81298f81c656893ab1ddddd30a99356066",
    "build_timestamp" : "2014-11-05T14:26:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"
  },
</comment><comment author="dadoonet" created="2015-04-30T16:21:47Z" id="97866212">On 1.4.2, it seems to work well. Going to run more tests
</comment><comment author="dadoonet" created="2015-04-30T16:27:50Z" id="97868970">Ok. It looks like a racing issue. I mean that I can not reproduce anymore on 1.5.1... o_O
</comment><comment author="vedil" created="2015-04-30T16:31:54Z" id="97870385">once you have the tweet/2 rest of the tweets are indexed differently, probably you need to clean up all so that previous mappings are not there.
after indexing twitter/1 when i see mapping i did not see message_r in the properties
after twitter/2 it will be there in mappings.
</comment><comment author="jpountz" created="2015-04-30T16:32:50Z" id="97870702">There is a known issue on Elasticsearch &lt; 1.5.0 which does not generate dynamic mappings for the empty string, which causes issues to the missing and exists filters: https://github.com/elastic/elasticsearch/pull/8329
</comment><comment author="dadoonet" created="2015-04-30T16:33:37Z" id="97870950">I restarted the full test by cleaning first "data" dir. So I don't get why I got also this issue on 1.5.1 cluster but I can't reproduce it anymore now.
</comment><comment author="dadoonet" created="2015-04-30T16:43:02Z" id="97874082">@vedil I can confirm what @jpountz said. If you are using dynamic mappings with 1.4.x, the requests don't give the same results. But with 1.5.x, both requests don't give any result.

When using `not_analyzed`, results are different.

Could you test with a newer version? And also share your dynamic template so I can try to reproduce exactly what you see?
</comment><comment author="vedil" created="2015-04-30T16:46:23Z" id="97875178">here is the mapping that i am using

{
          "string_template": {
            "match": "*_r",
            "match_mapping_type": "string",
            "mapping": {
              "type": "multi_field",
              "fields": {
                "{name}": {
                  "type": "string",
                  "analyzer": "string_lowercase"
                },
                "unanalyzed": {
                  "type": "string",
                  "index": "not_analyzed"
                }
              }
            }
          }
        }

will try on 1.5.x version
</comment><comment author="dadoonet" created="2015-04-30T16:51:10Z" id="97877027">This full script works well on 1.5:

```
DELETE twitter
PUT twitter
{
  "mappings": {
    "tweet": {
      "dynamic_templates": [
        {
          "es": {
            "match": "*_r",
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      ]
    }
  }
}

PUT /twitter/tweet/1
{"user":"user1", "message_r":""}
PUT /twitter/tweet/2
{"user":"user2", "message_r":"test message"}
PUT /twitter/tweet/3
{"user":"user1", "message_r":""}
GET /twitter/tweet/_search?pretty
{"query":{"filtered":{"filter":{"missing":{"field":"message_r"}}}}}

GET /twitter/tweet/_search?pretty
{"query":{"filtered":{"filter":{"term":{"message_r":""}}}}}
```

The fact is that for the first query `message_r` field is never missing so you don't get any result.
But the second query matches documents 1 and 3.
</comment><comment author="vedil" created="2015-04-30T17:56:53Z" id="97898103">yes with 1.5.2 version i am getting the above mentioned behaviour.
time to upgrade my server :)
</comment><comment author="vedil" created="2015-04-30T18:04:22Z" id="97900060">me wonders @dadoonet how were you able to reproduce the issue in the first place, one difference i see in mappings is i was using multifield and you were not using that :(
</comment><comment author="dadoonet" created="2015-04-30T18:08:42Z" id="97900891">If by "issue" you mean the fact that only one record was returned, I have no idea as I can't reproduce it anymore! 
I guess we can close it, right?
</comment><comment author="vedil" created="2015-04-30T18:12:24Z" id="97902275">yes i am closing it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change the default `min_doc_count` to 0 on histograms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10904</link><project id="" key="" /><description>The assumption is that gaps in histogram are generally undesirable, for instance
if you want to build a visualization from it. Additionally, we are building new
aggregations that require that there are no gaps to work correctly (eg.
derivatives).
</description><key id="72154243">10904</key><summary>Change the default `min_doc_count` to 0 on histograms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T13:13:56Z</created><updated>2015-06-06T15:33:20Z</updated><resolved>2015-04-30T13:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T13:17:20Z" id="97775880">@polyfractal @colings86 You might be interested in this one.
</comment><comment author="colings86" created="2015-04-30T13:21:45Z" id="97779080">Left a small comment but otherwise looks good
</comment><comment author="colings86" created="2015-04-30T13:43:07Z" id="97796039">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/DerivativeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/MaxBucketTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/MinBucketTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/reducers/moving/avg/MovAvgTests.java</file></files><comments><comment>Merge pull request #10904 from jpountz/enhancement/histogram_min_doc_count_0</comment></comments></commit></commits></item><item><title>Replica shard corrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10903</link><project id="" key="" /><description>We have an cluster with two data nodes and one non-data nodes. Our index 'location' has 5 shards and 1 replica. 

When we try to get a document

``` json
GET location/mapping/31307093
```

Once we get an answer with a document

``` json
{
   "_index": "location.b",
   "_type": "mapping",
   "_id": "31307093",
   "_version": 1,
   "found": true,
   "_source": {
      "MAPPINGTYP": "A",
      "LOKATIONSID_NEU": "31330483",
      "LETZTEAENDERUNG": "2015-04-28-02.05.25.547042"
   }
}
```

But next query gives us

``` json
{
   "_index": "location.b",
   "_type": "mapping",
   "_id": "31307093",
   "found": false
}
```

It seems that replica shard is corrupted?  Is there any way to check if any other document is corrupted? I guess shutting down one node, deleting the files and starting it again would help, but this doesn't solve the real problem. Would the upgrade to elasticsearch 1.5.x bring any improvement in that area?

We use elasticsearch 1.4.4 with jdk1.8.0_40 running on Red Hat Enterprise Linux Server release 6.6 running on VMware vSphere.

Kind regs
Vladimir Nisevic
</description><key id="72137117">10903</key><summary>Replica shard corrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niv0</reporter><labels><label>feedback_needed</label></labels><created>2015-04-30T11:49:31Z</created><updated>2015-05-22T09:43:12Z</updated><resolved>2015-05-22T09:43:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-04T10:00:39Z" id="98659876">Hi @niv0 

Yes it looks like the replica is out of sync with the primary.  You can confirm which copy of the shard holds the document by adding `?preference=_primary` to the GET request.

Assuming it is the replica, then you can get your replicas back in sync by setting the number of replicas to zero, then back to 1:

```
POST location.b/_settings 
{ "number_of_replicas": 0 }
```

I agree that this doesn't solve the original problem.  You may have experienced a bug that has already been fixed in 1.5, but could possibly be something else.  Do you use the delete-by-query or bulk APIs?  Are you aware of any issues you had with nodes falling over?  
</comment><comment author="javadevmtl" created="2015-05-14T19:10:20Z" id="102140381">I experienced this during a node failure with ES 1.5.2 but in a large cluster or with multiple indices it's almost impossible to tell is there's inconstancy.

Curious can their not be a way to check if primaries and shards match in size after a merge or after documents have been committed to the replica or something?
</comment><comment author="clintongormley" created="2015-05-15T18:37:27Z" id="102487783">@javadevmtl Presumably indexing is underway, and refreshes happen at different times on the primary and replica, so no, there isn't really a way to do this without pausing indexing.  
</comment><comment author="niv0" created="2015-05-22T09:43:12Z" id="104598139">Hi @clintongormley, thank you for your support.

We do not really use here either delete-by-query nor bulk APIs'. We had no fail over node issues in that time period.

We will try with upgrade to ES 1.5.x and see if issue still occurs.

Would close this issue at this point of time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugin Manager cannot resolve ${HOSTNAME} in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10902</link><project id="" key="" /><description>Originally seen by @agirbal. If elasticsearch.yml has the node name set as `${HOSTNAME}`, then any bin/plugin command fails with:

```
Exception in thread "main" java.lang.IllegalArgumentException: Could not resolve placeholder ‘HOSTNAME'
        at org.elasticsearch.common.property.PropertyPlaceholder.parseStringValue(PropertyPlaceholder.java:124)
        at org.elasticsearch.common.property.PropertyPlaceholder.replacePlaceholders(PropertyPlaceholder.java:81)
        at org.elasticsearch.common.settings.ImmutableSettings$Builder.replacePropertyPlaceholders(ImmutableSettings.java:1098)
        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:101)
        at org.elasticsearch.plugins.PluginManager.main(PluginManager.java:395)
```

Manually changing to a static value allows the commands to work. Relates to #9474 
</description><key id="72130637">10902</key><summary>Plugin Manager cannot resolve ${HOSTNAME} in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Packaging</label><label>v1.6.0</label></labels><created>2015-04-30T11:12:32Z</created><updated>2015-05-28T17:04:48Z</updated><resolved>2015-05-28T17:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Export hostname as environment variable for plugin manager</comment></comments></commit></commits></item><item><title>request: float interval in histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10901</link><project id="" key="" /><description>On my application we were trying to analyze floating data.
we tried to use histogram on a float field and we discovered that the "interval" field accepts only integer.
but sometimes we need to analyze floating number with more precision that the integer number cannot give us. For an example if you want to find the most frequently used radio frequency, just on FM range there is a more then 10 stations between 99 to 100 if you multiply it by 10.

I can resolve this problem by pre-indexing all my field with fixed factor, but its opposes a problem what if my client will want to change the factor dynamically?

And that why I came to the conclusion that we have to have a factor field (like in date histogram) or maybe to enable the  interval to be float number (both can be great)

thank you very much,
regards Raz Abadi 
</description><key id="72123235">10901</key><summary>request: float interval in histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">razaba</reporter><labels /><created>2015-04-30T10:32:07Z</created><updated>2015-04-30T10:36:33Z</updated><resolved>2015-04-30T10:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T10:36:33Z" id="97734146">@razaba I'm closing this issue as a duplicate of https://github.com/elastic/elasticsearch/issues/4847. I agree decimal intervals can be useful...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>min_bucket aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10900</link><project id="" key="" /><description>An aggregation to calculate the minimum value in a set of buckets.
An almost exact clone of the `max_bucket` aggregation

Closes #9999
</description><key id="72122853">10900</key><summary>min_bucket aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T10:30:08Z</created><updated>2015-04-30T12:36:35Z</updated><resolved>2015-04-30T12:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T12:15:30Z" id="97752017">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature Request: Aggregation on partial field value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10899</link><project id="" key="" /><description>I have a number of different load injector boxes and processes that generate load through our system under test. The tool in use produces masses of logs out but none of it is easily accessible or readable. I am therefore using ELK to process the loads with success, however, presenting some of the data is problematic.

The process names have a structure of:
&lt;process&gt;_&lt;name&gt;_Stress&lt;nn&gt;_Thread_&lt;nn&gt;
but there are around 180 of them. Each process thread generates 1 of 11 different message types. The message types are only distinguishable using a 10 character substring within a field containing strings and semi_colon delimited text and generally in the same location (between character 60 and character 70.

In elasticsearch none of these fields are analysed as this makes the queries and results even messier in Kibana and poses the same problem when choosing analysed elements of a field (only picking element 1&amp;2 or 10-12).

When aggregation is done on either of these fields, "message type" is presented as the long string in the visualisation key with only the first 10-15 characters showing, and "process name" resutls in all 180 processes rather than the 7 process types.

These processes are likely to change over time as we introduce new test scenarios and message types so I do not want to hard code them in just in case we miss something.

Is it possible to have elasticsearch return substrings, partials, lefts, rights etc of a field and group them as such rather than the entire field content so all &lt;process&gt;_&lt;name&gt;a are grouped together and _&lt;msg_type&gt;A_ are grouped together? Ideally without code edits to either elastic or kibana? Something in JSON Input such as { "field_length": 10 } or { "partial_start": 60, "partial_for": 15 } would suffice.
</description><key id="72118690">10899</key><summary>Feature Request: Aggregation on partial field value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stuartjk1606</reporter><labels /><created>2015-04-30T10:12:20Z</created><updated>2015-05-04T11:38:08Z</updated><resolved>2015-05-04T11:38:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-01T11:30:12Z" id="98108014">@stuartjk1606 you could do this with the script option on the terms aggregation. You can use the script to get the value of a particular field and extract a portion of it to be used as the key for the terms bucket as in this example: 

``` javascript
DELETE test

POST test/doc/1
{
  "my_field": "string_of_multiple_parts_1"
}
POST test/doc/2
{
  "my_field": "string_of_multiple_parts_2"
}
POST test/doc/3
{
  "my_field": "other1_of_multiple_parts_3"
}
POST test/doc/4
{
  "my_field": "other1_of_multiple_parts_4"
}

# Normal terms agg will return a bucket for each unique term
GET test/_search
{
  "size": 0, 
  "aggs": {
    "my_terms": {
      "terms": {
        "field": "my_field",
        "size": 10
      }
    }
  }
}

# Scripted terms aggregation will return buckets where the key is defined by the output of the script run on each document
GET test/_search
{
  "size": 0, 
  "aggs": {
    "my_terms": {
      "terms": {
        "script": "doc['my_field'].getValue().substring(0,6)", 
        "size": 10
      }
    }
  }
}
```

see this section of the docs for more information on useing scripts with the terms aggregation: http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-script

and this section for more information about scripting in general with Elasticsearch: http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html
</comment><comment author="stuartjk1606" created="2015-05-01T14:30:05Z" id="98144941">Just to clarify, I am trying to return these in the Kibana front end which does not allow for aggregation via direct scripting that I can see, which I am dissapointed with anyway.

I tried to add this directly to the JSON input in Kibana on the terms "Process" aggregation (no option for "scripted")

&gt; { "script": "doc['Process'].getValue().substring(0,6)" }

Which returns

"

&gt; &gt; Query Failed [Failed to execute main query]
&gt; &gt; ...
&gt; &gt; Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number
&gt; &gt; ...
&gt; &gt; "

I have also added a partial.groovy script in %elasticsearch%/config/scripts containing both the below lines

&gt; doc[field_name].getValue().substring(partial_start,partial_for)
&gt; doc[Process].getValue().substring(0,6)

Add then again added it to the aggregation json input box while setting Terms to Process

&gt; { "script_file": "partial",
&gt; "params": {
&gt;    "field_name": "'Process'",
&gt;    "partial_start": 0,
&gt;    "partial_for": 6 }}

Both instances of partial.groovy return GroovyScriptExecutionException[ArrayIndexOutOfBoundsException[null]]
</comment><comment author="colings86" created="2015-05-01T15:14:36Z" id="98154602">@stuartjk1606 I didn't realise you were doing this in Kibana. The first way you described won't work because Kibana only allows Lucene expressions by (for security reasons) which don't support strings, only numeric values. It looks like the script_file approach is working in that it is finding the script and attempting to execute it (hence the GroovyScriptExecutionException) but its erroring during execution, possibly due to getting an empty string back from the `getValue()` method and so throwing an ArrayIndexOutOfBoundsException. Maybe add a check for empty string to your script and also ensure the string length is greater than or equal to 6 before you substring it?
</comment><comment author="clintongormley" created="2015-05-04T11:38:07Z" id="98683963">Be aware that, while scripting allows you to do these kinds of aggregations with data you already have indexed, the query performance will be nowhere near as good as having your data indexed properly.  For instance you could use the `pattern` tokenizer to extract the relevant bits of the string into separate fields:

```
DELETE test
PUT test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "first_part": {
          "tokenizer": "first_part"
        },
        "second_part": {
          "tokenizer": "second_part"
        }
      },
      "tokenizer": {
        "first_part": {
          "type": "pattern",
          "pattern": "^([^_]+)",
          "group": 1
        },
        "second_part": {
          "type": "pattern",
          "pattern": "^[^_]+_([^_]+)",
          "group": 1
        }
      }
    }
  }
}

PUT test/_mapping/test
{
  "properties": {
    "code": {
      "type": "string",
      "index": "not_analyzed",
      "fields": {
        "first": {
          "type": "string",
          "analyzer": "first_part"
        },
        "second": {
          "type": "string",
          "analyzer": "second_part"
        }
      }
    }
  }
}

POST test/test/_bulk
{"index":{}}
{"code":"foo_abc"}
{"index":{}}
{"code":"Foo_abc"}
{"index":{}}
{"code":"foo_def"}
{"index":{}}
{"code":"Foo_def"}

GET /test/_search?size=0
{
  "aggs": {
    "first": {
      "terms": {
        "field": "code.first"
      },
      "aggs": {
        "second": {
          "terms": {
            "field": "code.second"
          }
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Derivative Aggregation x-axis units normalisation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10898</link><project id="" key="" /><description /><key id="72116335">10898</key><summary>Derivative Aggregation x-axis units normalisation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T09:59:59Z</created><updated>2015-05-06T09:48:00Z</updated><resolved>2015-05-06T09:47:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-30T10:02:47Z" id="97726742">@jpountz this is still in progress but if you have time to review it would be appreciated. I want to get an idea of what you think on the approach.
</comment><comment author="jpountz" created="2015-04-30T10:34:05Z" id="97733202">This looks like a good start! In particular it's great that this normalization is aware of the width of each interval instead of assuming they are all equal (which is not the case with daylight saving times).
</comment><comment author="colings86" created="2015-04-30T12:56:24Z" id="97764572">@jpountz have made some edits and left a reply on your DerivativeBuilder question
</comment><comment author="jpountz" created="2015-04-30T14:17:20Z" id="97813632">This looks good in general. I think we should add some documentation. Also I'm wondering what the right name for this parameter should be, `units` feels so general that I wouldn't really know what it refers to without reading the doc. I don't have better ideas unfortunately.
</comment><comment author="colings86" created="2015-05-05T12:38:09Z" id="99067221">@jpountz made some updates. I agreed that `unit` isn't a great name but like you I am yet to come up with a better choice.
</comment><comment author="jpountz" created="2015-05-05T15:33:42Z" id="99117696">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over to the Lucene filter cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10897</link><project id="" key="" /><description>This removes Elasticsearch's filter cache and uses Lucene's instead. It has some
implications:
- custom cache keys (`_cache_key`) are unsupported
- decisions are made internally and can't be overridden by users ('_cache`)
- not only filters can be cached but also all queries that do not need scores
- parent/child queries can now be cached, however cached entries are only
  valid for the current top-level reader so in practice it will likely only
  be used in practice on read-only indices
- the cache deduplicates filters, which plays nicer with large keys (eg. `terms`)
- better stats: we already had ram usage and evictions, but now also hit count,
  miss count, lookup count, number of cached doc id sets and current number of
  doc id sets in the cache
- dynamically changing the filter cache size is not supported anymore

Internally, an important change is that it removes the NoCacheFilter infrastructure
in favour of making Query.rewrite specializing the query for the current reader so
that it will only be cached on this reader (look for IndexCacheableQuery).

Note that consuming filters with the query API (createWeight/scorer) instead of
the filter API (getDocIdSet) is important for parent/child queries because
otherwise a QueryWrapperFilter(ParentQuery) would run the wrapped query per
segment while relations might be cross segments.
</description><key id="72115447">10897</key><summary>Cut over to the Lucene filter cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T09:54:55Z</created><updated>2015-06-06T16:07:46Z</updated><resolved>2015-05-04T07:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T09:58:22Z" id="97726015">The PR is large so feel free to ping me if you would like to discuss it.
</comment><comment author="rmuir" created="2015-04-30T14:17:32Z" id="97813734">look good.

I pushed a few cleanups.

I'm not happy with the complexity surrounding ShardCoreKeyMap etc: it is clearly not worth it. 
I am happy for this to be simplified in a follow-up, but I think in general we have to stop doing
stuff like this in the name of back compat / inertia / etc. The codebase is just too large and we should
apply software engineering and keep things simple when its the right thing to do.

Related to that, I hit a test fail while playing:
mvn test -Dtests.class="*ShardCoreKeyMapTests" -Dtests.method=testBasics -Dtests.seed=D176C7AD20084F10

I think the 10 places where we do 'new IndexSearcher' should be reviewed and always have an explicit cache set 
or not set in a followup as well (its rather unrelated to the change at hand).
</comment><comment author="jpountz" created="2015-04-30T16:03:22Z" id="97853876">@rmuir I pushed more commits to address your comments.

I tend to agree with the complexity of ShardCoreKeyMap, it needs to be balanced with the benefit of having per-shard stats. The reason why I tried to hard to keep having per-shard stats is that doing otherwise would require an important refactoring of our monitoring APIs.
</comment><comment author="rmuir" created="2015-04-30T16:07:10Z" id="97856541">Changes look great, +1

Yeah, as i said about the map, lets defer it to another issue. This one makes so much progress on its own.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/common/lucene/IndexCacheableQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/ShardCoreKeyMap.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/CachedFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/FilteredCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/IndexService.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheStats.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/ShardFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/index/IndexFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/none/NoneFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/ShardQueryCache.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineSearcherFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java</file><file>src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/lookup/LeafIndexLookup.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighterTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/IndexCacheableQueryTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/ShardCoreKeyMapTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java</file><file>src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTest.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/FloatNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java</file><file>src/test/java/org/elasticsearch/indices/cache/query/IndicesQueryCacheTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file><file>src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>src/test/java/org/elasticsearch/search/scriptfilter/ScriptFilterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file></files><comments><comment>Merge pull request #10897 from jpountz/fix/nocache</comment></comments></commit><commit><files><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/common/lucene/IndexCacheableQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/ShardCoreKeyMap.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/CachedFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/FilteredCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/IndexService.java</file><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheStats.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/ShardFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/index/IndexFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/none/NoneFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/ShardQueryCache.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineSearcherFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsLookupFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java</file><file>src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/lookup/LeafIndexLookup.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighterTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/IndexCacheableQueryTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/ShardCoreKeyMapTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java</file><file>src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTest.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/FloatNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java</file><file>src/test/java/org/elasticsearch/indices/cache/query/IndicesQueryCacheTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file><file>src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>src/test/java/org/elasticsearch/search/scriptfilter/ScriptFilterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file></files><comments><comment>Merge pull request #10897 from jpountz/fix/nocache</comment></comments></commit></commits></item><item><title>Prevent over allocation for multicast ping request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10896</link><project id="" key="" /><description>very tiny optimization but the BytesStreamOutput ctor by
default allocates 16KB which is way too much for a ping
request.

The actual size of the request depends on the clusterName so
150 isn't accurate either but should be enough.
</description><key id="72099256">10896</key><summary>Prevent over allocation for multicast ping request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T08:29:13Z</created><updated>2015-05-29T17:54:04Z</updated><resolved>2015-05-28T18:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-28T18:06:16Z" id="106538640">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postprocessing for aggregation results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10895</link><project id="" key="" /><description>Hi! Is there any way to subj? Often i need exclude some agg parts by the condition. Or, can i process (add/remove/modify keys and values) agg results with some groovy script before response to client?
</description><key id="72097931">10895</key><summary>Postprocessing for aggregation results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">a0s</reporter><labels /><created>2015-04-30T08:23:19Z</created><updated>2015-04-30T10:40:50Z</updated><resolved>2015-04-30T10:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T10:40:50Z" id="97735070">If you only need to remove content, we have this other issue where we are working on allowing to script the selection of returned elements from the response: https://github.com/elastic/elasticsearch/issues/7401

Regarding modifications, there are some ongoing efforts to add the ability to post-process aggregation results. Some of them will be in 2.0, see https://github.com/elastic/elasticsearch/issues/9876 for more information.

I'm closing this issue as I think the two other issues that I pointed to capture the same requirements.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>In which case ElasticSearch return 429 ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10894</link><project id="" key="" /><description>When doing pressure test on ES cluster，it sometimes may return 429(too many request).
</description><key id="72081524">10894</key><summary>In which case ElasticSearch return 429 ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shinyke</reporter><labels /><created>2015-04-30T07:02:59Z</created><updated>2015-04-30T07:07:18Z</updated><resolved>2015-04-30T07:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-30T07:04:29Z" id="97683903">Please join us in #elasticsearch on freenode or on the mailing list at https://groups.google.com/forum/#!forum/elasticsearch for troubleshooting help, we reserve github for confirmed bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix `_field_names` to not have doc values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10893</link><project id="" key="" /><description>When doc values were turned on a by default, most meta fields
had it explicitly disabled.  However, _field_names was missed.
This change forces doc values to be off always for _field_names
and removes the unnecessary support when creating index fields.

closes #10892
</description><key id="72072972">10893</key><summary>Fix `_field_names` to not have doc values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-30T06:15:09Z</created><updated>2015-06-08T09:00:14Z</updated><resolved>2015-04-30T06:21:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-30T06:15:43Z" id="97672777">looks good, thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_field_names should not get docvalues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10892</link><project id="" key="" /><description>When i try to reproduce @peterskim12 benchmark here (https://github.com/peterskim12/elk-index-size-tests), I see larger space usage for master than expected.

I first looked at fieldinfos and see _field_names with SORTED_SET, this should be pretty wasteful and probably is the smoking gun:

| field | indexed | docvalues |
| --- | --- | --- |
| _uid | DOCS | NONE |
| _source | NONE | NONE |
| _type | DOCS | NONE |
| _version | NONE | NUMERIC |
| @version | DOCS | SORTED_SET |
| @timestamp | DOCS | SORTED_NUMERIC |
| host | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| host.raw | DOCS | SORTED_SET |
| clientip | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| clientip.raw | DOCS | SORTED_SET |
| ident | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| ident.raw | DOCS | SORTED_SET |
| auth | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| auth.raw | DOCS | SORTED_SET |
| timestamp | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| timestamp.raw | DOCS | SORTED_SET |
| verb | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| verb.raw | DOCS | SORTED_SET |
| request | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| request.raw | DOCS | SORTED_SET |
| httpversion | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| httpversion.raw | DOCS | SORTED_SET |
| response | DOCS | SORTED_NUMERIC |
| bytes | DOCS | SORTED_NUMERIC |
| referrer | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| referrer.raw | DOCS | SORTED_SET |
| agent | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| agent.raw | DOCS | SORTED_SET |
| _all | DOCS_AND_FREQS_AND_POSITIONS | NONE |
| _field_names | DOCS | SORTED_SET |
</description><key id="72055485">10892</key><summary>_field_names should not get docvalues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-30T04:01:52Z</created><updated>2015-04-30T06:21:10Z</updated><resolved>2015-04-30T06:21:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-30T04:10:48Z" id="97655781">I peeked at the docvalues file and saw ~ 8MB ordinals alone for _field_names, so it accounts for the space increase that surprised me (75MB -&gt; 83MB total index size).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file></files><comments><comment>Mappings: Fix _field_names to not have doc values</comment></comments></commit></commits></item><item><title>Logging: Add logging of slow cluster state tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10891</link><project id="" key="" /><description>Closes #10874
</description><key id="72044175">10891</key><summary>Logging: Add logging of slow cluster state tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2015-04-30T02:47:23Z</created><updated>2015-04-30T04:26:09Z</updated><resolved>2015-04-30T04:26:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Field Methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10890</link><project id="" key="" /><description>Added infrastructure to allow basic member methods in the expressions
language to be called.  The methods must have a signature with no arguments.  Also
added the following member methods for date fields (and it should be easy to add more)
- getYear \* getMonth \* getDayOfMonth \* getHourOfDay \* getMinutes *
- getSeconds.

Removed the member variable ".value" for expressions as it is not
necessary since expressions do not support multi-valued fields.  It only
serves to complicate the way single-valued fields are currently
accessed.
</description><key id="72020391">10890</key><summary>Add Field Methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T23:55:28Z</created><updated>2015-06-06T17:22:53Z</updated><resolved>2015-04-30T22:50:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-30T01:09:04Z" id="97624630">I left some comments. I think also some documentation about what functions are available would be good (possibly separate out expressions into its own asciidoc now that there is stuff specific to it). Also please add the removal of `.value` to the migration guide for 2.0.
</comment><comment author="jdconrad" created="2015-04-30T19:47:56Z" id="97945724">@rjernst Submitted another commit.
</comment><comment author="rjernst" created="2015-04-30T20:16:40Z" id="97953886">@jdconrad This LGTM, there is just the one question about maybe we should keep backcompat for `.value` for now?
</comment><comment author="jdconrad" created="2015-04-30T22:10:40Z" id="97985170">@rjernst Would you please take a look at the last few changes?  I just want to make sure that I did the documentation correctly and added .value back in as you would expect.  Thanks!
</comment><comment author="rjernst" created="2015-04-30T22:28:03Z" id="97989575">@jdconrad A couple minor comments, LGTM.
</comment><comment author="jdconrad" created="2015-04-30T23:03:03Z" id="97995077">@rjernst Thank you for the reviews!
</comment><comment author="clintongormley" created="2015-05-04T10:41:54Z" id="98673100">@jdconrad I'm concerned about this:

&gt; Removed the member variable ".value" for expressions as it is not necessary since expressions do not support multi-valued fields. It only serves to complicate the way single-valued fields are currently accessed.

We have multi-value fields today, which can be manipulated by other scripting languages.  Would it not be possible to add support for multi-value fields to expressions in the future? In which case we'd need to change the syntax again?
</comment><comment author="jdconrad" created="2015-05-04T15:24:51Z" id="98752005">@clintongormley I added .value back in for back-compat.  On multi-valued fields, it should be possible to add in things like avg/min/max fairly easily, but the ability for the user to write his/her own loops and access each individual value is a ways down the road.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java</file><file>src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java</file><file>src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java</file><file>src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file></files><comments><comment>Scripting: Add Field Methods</comment></comments></commit></commits></item><item><title>Remove old 0.90 shard allocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10889</link><project id="" key="" /><description>the `even_shard` allocator has been replaced years ago in early 0.90.
We can remove it now in 2.0 since the new one is considered stable.
</description><key id="71966032">10889</key><summary>Remove old 0.90 shard allocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T19:28:56Z</created><updated>2015-06-06T16:08:12Z</updated><resolved>2015-05-04T09:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-29T19:43:32Z" id="97557499">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Require units for disk-aware allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10888</link><project id="" key="" /><description>Encountered a customer issue where the operator had set the high- and low- watermarks for disk aware allocation to '98' and '95', thinking they were setting to 98% and 95%.  Elasticsearch read these as 95 bytes and 98 bytes, which led to an out of disk space error when a node was restarted.  We should make units mandatory for this setting to avoid this kind of confusion, or at least enforce sensible settings if units are not specified.
</description><key id="71963089">10888</key><summary>Require units for disk-aware allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>:Settings</label><label>enhancement</label></labels><created>2015-04-29T19:17:31Z</created><updated>2015-06-04T18:03:39Z</updated><resolved>2015-06-04T18:03:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-29T19:24:06Z" id="97550701">++ thanks sean! @clintongormley can we schedule this for 2.0
</comment><comment author="clintongormley" created="2015-04-29T19:31:44Z" id="97553051">Related to #7633
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequest.java</file><file>src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeRequest.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/BaseNodesRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>src/main/java/org/elasticsearch/common/Booleans.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/common/unit/ByteSizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>src/main/java/org/elasticsearch/common/unit/MemorySizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/MetaStateService.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogConfig.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/query/TimeoutParseElement.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/SimpleClusterStateTests.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java</file><file>src/test/java/org/elasticsearch/common/unit/ByteSizeValueTests.java</file><file>src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTest.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>src/test/java/org/elasticsearch/network/DirectBufferNetworkTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TermsDocCountErrorTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/SearchScrollTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/stresstest/get/GetStressTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java</file><file>src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file><file>src/test/java/org/elasticsearch/versioning/SimpleVersioningTests.java</file></files><comments><comment>Merge pull request #11437 from mikemccand/require_units</comment></comments></commit></commits></item><item><title>`bootstrap.mlockall` for Windows (VirtualLock)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10887</link><project id="" key="" /><description>This PR supersedes #9186.  The original PR became a bit stale and was also working off my master branch.

I've incorporated all the feedback from #9186 as best as I could.  The biggest difference from the original PR are the changes that had to be made in order to work properly with the refactoring that was done in #9923.

I believe this is now good to go, but would love to get some final eyes/testing before merging it in.

/cc @clintongormley @pickypg @ppf2 @Mpdreamz @tlrx @kimchy 

Closes #8480
</description><key id="71952816">10887</key><summary>`bootstrap.mlockall` for Windows (VirtualLock)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Settings</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T18:32:19Z</created><updated>2015-05-29T15:54:32Z</updated><resolved>2015-05-08T13:52:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-29T18:38:39Z" id="97535454">left one comment, hard to tell if it works on my end :), I assume we tested it on windows, so I am good
</comment><comment author="pickypg" created="2015-04-29T22:12:06Z" id="97602781">Really minor stuff. Excited to see this get in! LGTM
</comment><comment author="Mpdreamz" created="2015-05-01T15:50:19Z" id="98163084">I'm being bit by the following:

```
[2015-05-01 17:42:00,360][INFO ][node                     ] [Troll] version[2.0.0-SNAPSHOT], pid[5676], build[e0560cf/2015-05-01T15:25:35Z]
[2015-05-01 17:42:00,361][INFO ][node                     ] [Troll] initializing ...
[2015-05-01 17:42:00,367][INFO ][plugins                  ] [Troll] loaded [], sites []
[2015-05-01 17:42:00,442][INFO ][env                      ] [Troll] using [1] data paths, mounts [[(c:)]], net usable_space [31.7gb], net total_space [80.2gb], spins? [unknown], types [NTFS]
[2015-05-01 17:42:03,192][INFO ][node                     ] [Troll] initialized
{2.0.0-SNAPSHOT}: Initialization Failed ...
- ExceptionInInitializerError
        AccessControlException[access denied ("java.io.FilePermission" "C:\Users\mpdreamz\AppData\Local\Temp\jna-280264762" "read")]
```

Both whether mlockall is enabled or not and whether I run as Admin or not.
</comment><comment author="dakrone" created="2015-05-01T15:52:19Z" id="98163547">@Mpdreamz that's the security manager, if you are running from an IDE you will need to add `-Des.security.manager.enabled=false` to the arguments
</comment><comment author="rmuir" created="2015-05-01T16:04:04Z" id="98166902">@Mpdreamz can you confirm whether this happens when you run bin/elasticsearch? Our policy file grants access to java.io.tmpdir (which it looks like is where you hit the issue), so if it happens then we need to open an issue, something is not right on windows maybe.
</comment><comment author="Mpdreamz" created="2015-05-01T16:05:47Z" id="98167150">It happens on the command line (bin/elasticsearch) not tested inside IntelliJ

`-Des.security.manager.enabled=false` does fix it now wondering wether its something in this PR that causing this or current master.
</comment><comment author="rmuir" created="2015-05-01T16:10:46Z" id="98167866">I suspect its caused by this issue, because @rjernst tested security manager changes on a windows VM before they went in. So we need to do some investigation.
</comment><comment author="Mpdreamz" created="2015-05-01T16:21:12Z" id="98169449">Yeah current `HEAD` in `master` runs without a problem, seems like something introduced in this branch.
</comment><comment author="gmarz" created="2015-05-01T16:25:33Z" id="98171366">My fork isn't up to date with master and is missing https://github.com/elastic/elasticsearch/commit/f599c237bdee7624bd5a67cf26828ccac32dbcc7.  I wonder if that could be the issue?

Strangely though, I'm not running into this issue running from the cmd line, and same snapshot build as @Mpdreamz.
</comment><comment author="rmuir" created="2015-05-01T16:40:35Z" id="98174561">It makes some sense: as described in that commit, mlockall tests don't run with security manager enabled, so that is why we mlockall first (so we know it works, since we dont test it).

What is maybe not explained is why you dont have access to what appears to be a temp dir. To satisfy curiosity, can you run ES with an additional VM argument, the windows equivalent of this?:

```
JAVA_OPTS=-Djava.security.debug=access:failure bin/elasticsearch
```

This guarantees full stacktrace on failure, which we don't have above.
</comment><comment author="Mpdreamz" created="2015-05-01T16:59:23Z" id="98177711">Output listed here:  https://gist.github.com/anonymous/be30448ed6b0e0fa58e8
</comment><comment author="rmuir" created="2015-05-01T17:08:14Z" id="98179103">Thank you, so its as i feared, something is wrong with temporary directory permissions in your case. Can you run the same way with current `master`, I want to see if we are still unable to delete our own temporary file there too (ES will not fail here, but something is not right)
</comment><comment author="Mpdreamz" created="2015-05-01T17:36:00Z" id="98184211">https://gist.github.com/Mpdreamz/9da3e21ab98bb413a54d

Does go into started state:

`[2015-05-01 19:09:13,288][INFO ][node                     ] [Taurus] started`

But still getting an access error:

`access: access denied ("java.io.FilePermission" "C:\Users\mpdreamz\AppData\Local\Temp\5508702577511658466.tmp" "delete")
java.lang.Exception: Stack trace`
</comment><comment author="rmuir" created="2015-05-01T17:45:18Z" id="98185769">Thanks @Mpdreamz I created #10925 to debug this, its important but unrelated to this issue, so we can figure it out over there.
</comment><comment author="Mpdreamz" created="2015-05-01T18:16:36Z" id="98193690">LGTM :+1: sInce a rebased build does not throw the exception and goes into started mode where it locks the memory as expected 

@rmuir moved my issue over to https://github.com/elastic/elasticsearch/issues/10925
</comment><comment author="clintongormley" created="2015-05-08T08:45:01Z" id="100157391">@gmarz is this ready to go in?
</comment><comment author="gmarz" created="2015-05-08T13:46:00Z" id="100237959">@clintongormley Yes :) Merging shortly.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/common/jna/Kernel32Library.java</file><file>src/main/java/org/elasticsearch/common/jna/Natives.java</file><file>src/main/java/org/elasticsearch/common/jna/SizeT.java</file></files><comments><comment>Merge pull request #10887 from gmarz/feature/virtuallock</comment></comments></commit></commits></item><item><title>Extract XContent API as separate library</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10886</link><project id="" key="" /><description>I'm currently using the XContent API from elasticsearch for performant JSON string building for complex object structures.  It would be nice if this API was extracted from the elasticsearch core.
</description><key id="71932353">10886</key><summary>Extract XContent API as separate library</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ruckc</reporter><labels /><created>2015-04-29T17:08:37Z</created><updated>2015-05-08T09:45:03Z</updated><resolved>2015-05-08T09:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-08T09:45:02Z" id="100175092">Hi @ruckc 

Thanks for the suggestion. We've talked about this internally and we'd prefer having this code where we can make changes quickly, without impacting others.  However, it is under an Apache2 license, so you're free to fork it and extract it yourself.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add template support to _msearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10885</link><project id="" key="" /><description>It would be great if the `_msearch` resource supported template queries or had a `template` subresource to allow multi-search using predefined templates. For example:

``` shell
$ cat requests
{"index" : "main"}
{ "template": { "id": "template1" },"params": { "q": "foo" } }
{"index" : "main"}
{ "template": { "id": "template2" },"params": { "q": "bar" } }

$ curl -XGET localhost:9200/_msearch/template --data-binary @requests; echo
```

Instead I either have to make multiple requests to `/_search/template` or take our relatively large search templates and send them as extremely verbose and repetitive ad-hoc requests to `/_msearch`. If `/_msearch` or `/_msearch/template` supported template queries it would allow much cleaner and more efficient multi-search queries, and more consistency between `/_search` and `/_msearch`.
</description><key id="71931170">10885</key><summary>Add template support to _msearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">jpotisch</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2015-04-29T17:00:45Z</created><updated>2015-09-02T08:21:37Z</updated><resolved>2015-09-02T08:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-04T08:14:22Z" id="98625423">@MaineC can you take a look at this? I will assign it to you
</comment><comment author="MaineC" created="2015-05-04T08:22:05Z" id="98629889">@s1monw Sure. The proposal makes a lot of sense to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file></files><comments><comment>Merge pull request #12414 from MaineC/feature/10885</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file></files><comments><comment>Adds template support to _msearch resource</comment></comments></commit><commit><files /><comments><comment>Add docs to template support for _msearch (#17382)</comment></comments></commit></commits></item><item><title>[TEST] Use a high shard delete timeout when clusterstates are delayed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10884</link><project id="" key="" /><description>`IndiceStore#indexCleanup` uses a disruption scheme to delay cluster state
processing. Yet, the delay is [1..2] seconds but tests are setting the shard
deletion timeout to 1 second to speed up tests. This can cause random not
reproducible failures in this test since the timeouts and delays are bascially
overlapping. This commit adds a longer timeout for this test to prevent these
problems.

here are test failures that had this problem: 
- http://build-us-00.elastic.co/job/es_feature_cluster_state_diffs/90/console
- http://build-us-00.elastic.co/job/es_core_1x_suse/470/consoleFull
</description><key id="71914854">10884</key><summary>[TEST] Use a high shard delete timeout when clusterstates are delayed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2015-04-29T15:56:06Z</created><updated>2015-04-29T16:02:23Z</updated><resolved>2015-04-29T16:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-29T15:56:33Z" id="97479206">@brwe  can you look at this
</comment><comment author="brwe" created="2015-04-29T15:58:36Z" id="97479685">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query string time zone not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10883</link><project id="" key="" /><description>If you define exactly the same date range query using either `DATE+0200` notation or `DATE` and set `timezone: +0200`, elasticsearch gives back different results:

```
DELETE foo
PUT /foo
{
  "mapping": {
    "tweets": {
      "properties": {
        "tweet_date": {
          "type": "date"
        }
      }
    }
  }
}

POST /foo/tweets/1/
{
  "tweet_date": "2015-04-05T23:00:00+0000"
}
POST /foo/tweets/2/
{
  "tweet_date": "2015-04-06T00:00:00+0000"
}

GET /foo/tweets/_search?pretty
{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00+0200 TO 2015-04-06T23:00:00+0200]"
        }
    }
}
GET /foo/tweets/_search?pretty
{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00 TO 2015-04-06T23:00:00]",
            "time_zone": "+0200"
        }
    }
}
```

This PR fixes it and will also allow us to add the same feature to simple_query_string as well in another PR.

Closes #10477.
</description><key id="71902424">10883</key><summary>Query string time zone not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Dates</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T15:14:34Z</created><updated>2015-05-30T10:57:06Z</updated><resolved>2015-04-29T16:10:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-29T15:20:38Z" id="97466312">LGTM
</comment><comment author="dadoonet" created="2015-04-29T16:34:59Z" id="97493262">&gt; This PR fixes it and will also allow us to add the same feature to simple_query_string as well in another PR.

@jpountz actually `simple_query_string` still not supports ranges in Lucene so we can't have that. :(
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dynamic mapping for nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10882</link><project id="" key="" /><description>Suppose we create an index with the following dynamic mapping in order to indicate that any element starting with 'nested_' should be typed as nested:

``` json
{
  "test": {
    "dynamic_templates":[
      {
        "nested_field": {
          "mapping":{
            "type":"nested"
          },
          "match":"nested_*"
        }
      }
    ]  
  }
}
```

If you want to perform a nested query when there is no data in the index yet, we'll get an exception. For example:

``` json
{
  "query": {
    "nested": {
      "path": "nested_field",
      "query": {
        "match": {"nested_field.a": "foo"} 
      } 
    }
  }
}
```

would give us a "failed to find nested object under path" exception.

We can avoid this error by declaring the mapping of the nested fields in a static way (instead of dynamic), but then you loose the power of these dynamic templates. 

So my question is: would it be possible to silently ignore these "failed to find nested object under path" exceptions so that it makes sense to declare nested types inside the dynamic templates?
</description><key id="71900674">10882</key><summary>Dynamic mapping for nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dvdeurse</reporter><labels /><created>2015-04-29T15:08:48Z</created><updated>2015-04-29T17:37:08Z</updated><resolved>2015-04-29T17:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T17:37:07Z" id="97514176">Hi @dvdeurse 

This idea is being discussed here: https://github.com/elastic/elasticsearch/issues/10415#issuecomment-89832189

I'll close this as a duplicate
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>detect_noop should work with bulk update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10881</link><project id="" key="" /><description>#6822 introduced the _detect_noop_ option for the Update API. It should be supported in the Bulk API, because bulk updates are much more efficient.

Error during update: Search::Elasticsearch::Error::Unknown params &lt;detect_noop&gt; in &lt;update&gt;.
</description><key id="71898635">10881</key><summary>detect_noop should work with bulk update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sewi-cpan</reporter><labels /><created>2015-04-29T14:58:44Z</created><updated>2015-04-29T22:14:45Z</updated><resolved>2015-04-29T17:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T17:32:10Z" id="97513223">@sewi-cpan this is a bug in the Perl client - please open here https://github.com/elastic/elasticsearch-perl/issues
</comment><comment author="nik9000" created="2015-04-29T22:14:45Z" id="97603345">I was going to say that I use it with bulk updates all the time!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>403 error on Debian repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10880</link><project id="" key="" /><description>Hello,
I followed the documentation to use the official repository for Debian Wheezy, but when installing the package `elasticsearch`, I get the error

```
Err http://packages.elasticsearch.org/elasticsearch/1.5/debian/ stable/main elasticsearch all 1.5.2
  403  Sorry, not allowed to fetch that type of file: elasticsearch-1.5.2.deb
Failed to fetch http://packages.elasticsearch.org/elasticsearch/1.5/debian/pool/main/e/elasticsearch/elasticsearch-1.5.2.deb  403  Sorry, not allowed to fetch that type of file: elasticsearch-1.5.2.deb
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
```

Thanks
</description><key id="71896623">10880</key><summary>403 error on Debian repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Anthony25</reporter><labels><label>feedback_needed</label></labels><created>2015-04-29T14:49:56Z</created><updated>2015-05-27T08:35:43Z</updated><resolved>2015-05-27T08:35:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T17:27:23Z" id="97511976">Hi @Anthony25 

I can retrieve http://packages.elasticsearch.org/elasticsearch/1.5/debian/pool/main/e/elasticsearch/elasticsearch-1.5.2.deb just fine.  I wonder if it was a temporary glitch?  Would you mind trying again?
</comment><comment author="Anthony25" created="2015-05-05T19:43:01Z" id="99200076">Sorry about my non response, I will test it tomorrow.

Thanks for your answer !
</comment><comment author="Anthony25" created="2015-05-27T08:35:42Z" id="105824354">Sorry for the delay, the repository seems to work now.

Thanks for your answer !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Update docs for mapping fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10879</link><project id="" key="" /><description>With the changes in #8870, the mapping fields (`_all`, `_id` etc) are now very limited in how they can be configured, but the docs still reflect the old config options: 

http://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-fields.html
</description><key id="71870013">10879</key><summary>Docs: Update docs for mapping fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T13:06:27Z</created><updated>2015-05-07T18:27:26Z</updated><resolved>2015-05-07T18:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Cleanup meta field docs</comment></comments></commit></commits></item><item><title>Transport: read/writeGenericValue to support BytesRef</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10878</link><project id="" key="" /><description>Add support for BytesRef to existing StreamInput#readGenericValue and StreamOutput#writeGenericValue
</description><key id="71863594">10878</key><summary>Transport: read/writeGenericValue to support BytesRef</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T12:32:45Z</created><updated>2015-04-30T08:59:45Z</updated><resolved>2015-04-30T08:59:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-29T14:57:24Z" id="97456596">@s1monw we talked about this change, can you please have a look?
</comment><comment author="jpountz" created="2015-04-29T19:47:14Z" id="97558189">LGTM
</comment><comment author="s1monw" created="2015-04-29T19:47:37Z" id="97558250">++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>Transport: read/writeGenericValue to support BytesRef</comment></comments></commit></commits></item><item><title>Don't try and look for "default-mapping.json" in CWD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10877</link><project id="" key="" /><description>to reproduce:

```
wget -O - http://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/origin/master/nightly/JDK7/elasticsearch-latest-SNAPSHOT.tar.gz | tar xz --directory=/tmp/elasticsearch --strip-components=1
/tmp/elasticsearch/bin/elasticsearch
```

This will break when your cwd is on a different filesystem you get:

```
 java.security.AccessControlException: access denied ("java.io.FilePermission" "default-mapping.json" "read")
```

Other way to reproduce is to remove this line:

```
permission java.io.FilePermission "${project.basedir}${/}target${/}-", "read"
```

and instead make it only grant access to target/classes and target/test-classes, then all tests fail trying to read those files from CWD.

I believe the correct solution is not to try and read anything from CWD since that can be anywhere and should not be relevant.
</description><key id="71862466">10877</key><summary>Don't try and look for "default-mapping.json" in CWD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2015-04-29T12:28:52Z</created><updated>2015-12-05T15:36:27Z</updated><resolved>2015-12-05T15:36:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-29T12:31:12Z" id="97411486">We also try to read elasticsearch.yml from the CWD (at least in tests).
</comment><comment author="tlrx" created="2015-04-29T12:33:39Z" id="97412844">I've done #10721 for this
</comment><comment author="rmuir" created="2015-04-29T12:40:46Z" id="97414678">@tlrx in addition we still have the issue of java code trying CWD in some circumstances, stuff like this logic in Environment.java:

```
    if (settings.get("path.home") != null) {
        homeFile = PathUtils.get(cleanPath(settings.get("path.home")));
    } else {
        homeFile = PathUtils.get(System.getProperty("user.dir"));
    }
```

I don't understand how the user's home directory makes sense.
</comment><comment author="rmuir" created="2015-04-29T12:44:22Z" id="97415310">Environment.resolveConfig is the other bad guy. It tries CWD _first_ then and only then, looks in configured config/ dir (imagine path="elasticsearch.yml" in this snippet below):

```
public URL resolveConfig(String path) throws FailedToResolveConfigException {
    String origPath = path;
    // first, try it as a path on the file system                   &lt;-- TRIES CWD BEFORE ANYTHING ELSE
    Path f1 = PathUtils.get(path);
    ...
    if (path.startsWith("/")) {
        path = path.substring(1);
    }
    // next, try it relative to the config location
    Path f2 = configFile.resolve(path);
    ...
    // try and load it from the classpath directly
    URL resource = settings.getClassLoader().getResource(path);
    if (resource != null) {
        return resource;
    }
    // try and load it from the classpath with config/ prefix
    if (!path.startsWith("config/")) {
```
</comment><comment author="rmuir" created="2015-04-29T16:08:16Z" id="97482608">created a branch with some commits to clean this up. But there is still more to fix and could use some help (the guice stuff etc a big annoyance here)

https://github.com/elastic/elasticsearch/tree/die_cwd_die
</comment><comment author="clintongormley" created="2015-12-05T15:36:27Z" id="162212658">#10923 has been merged, and we no longer allow file based mappings.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>adding missing single quote on PUT index request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10876</link><project id="" key="" /><description>adding missing single quote on PUT index request
</description><key id="71854286">10876</key><summary>adding missing single quote on PUT index request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels /><created>2015-04-29T11:50:46Z</created><updated>2015-04-29T12:51:24Z</updated><resolved>2015-04-29T12:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T12:45:45Z" id="97415929">thanks @nellicus - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: adding missing single quote on PUT index request</comment></comments></commit></commits></item><item><title>Flatten package structure for zen discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10875</link><project id="" key="" /><description>This package had lots of sub-packages with one or two classes each.
This commit flattens this structure for simiplification.
</description><key id="71845944">10875</key><summary>Flatten package structure for zen discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-04-29T11:07:51Z</created><updated>2015-07-13T09:30:35Z</updated><resolved>2015-07-13T09:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-29T11:35:47Z" id="97395888">At some point we said that our settings maps nicely with package names.
Should we change also settings name in that case like renaming `discovery.zen.fd.ping_interval` to `discovery.zen.ping_interval`?
</comment><comment author="s1monw" created="2015-04-29T16:05:07Z" id="97481702">@dadoonet I don't know what this scheme is but changing a settings is a big deal IMO I am happy to open a different issue for all of these but I will continue to fold packages together since 700 is just too much and not useful. I don't know what this mapping between packaging buys us to be honest. I am all for dropping this relationship on the floor.
</comment><comment author="jpountz" created="2015-05-05T15:28:43Z" id="99115582">&gt; I don't know what this mapping between packaging buys us to be honest.

I think it's useful so that if you change a setting and want to see effects in the logs, you can raise the log level of the logger which is associated with this setting.

However, I wouldn't care too much beyond the two first levels (ie. `discovery` and `discovery.zen`)  which this PR doesn't change so I'm +1 on the change.
</comment><comment author="s1monw" created="2015-07-13T09:30:30Z" id="120869642">outdated - closing for now
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log slow cluster state tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10874</link><project id="" key="" /><description>At the moment we have no visibility into which cluster state tasks are taking longer than they should.  We should add the following logging:
- TRACE: how long each task took
- WARN: any tasks which took longer than (eg) 30 seconds - this value should be dynamically configurable
</description><key id="71845355">10874</key><summary>Log slow cluster state tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Logging</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-29T11:04:39Z</created><updated>2015-05-01T00:28:05Z</updated><resolved>2015-05-01T00:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java</file><file>src/test/java/org/elasticsearch/test/MockLogAppender.java</file></files><comments><comment>Logging: Add logging of slow cluster state tasks</comment></comments></commit></commits></item><item><title>Remove Preconditions class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10873</link><project id="" key="" /><description>We don't need our own anymore, since we removed the Elasticsearch illegal argument exception
</description><key id="71828734">10873</key><summary>Remove Preconditions class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T09:39:28Z</created><updated>2015-06-08T13:17:13Z</updated><resolved>2015-04-29T10:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-29T10:21:10Z" id="97379205">LGTM
</comment><comment author="s1monw" created="2015-04-29T10:26:43Z" id="97380570">can we use `Objects.requireNonNull(T obj, String message)` instead?
</comment><comment author="kimchy" created="2015-04-29T10:28:33Z" id="97381107">@s1monw I first wanted to remove the Preconditions class first that we don't need anymore
</comment><comment author="s1monw" created="2015-04-29T10:29:23Z" id="97381282">ok not sure if it's worth 2 PRs ?
</comment><comment author="kimchy" created="2015-04-29T10:31:40Z" id="97381905">its a bigger change, since there are other places that use guava preconditions checkNonNull, also, we can't replace most of the ones here, which are for illegal argument 
</comment><comment author="s1monw" created="2015-04-29T10:32:34Z" id="97382169">ok fair enough!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/common/Preconditions.java</file><file>src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashMap.java</file><file>src/main/java/org/elasticsearch/common/io/Streams.java</file><file>src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java</file><file>src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>src/main/java/org/elasticsearch/common/unit/SizeValue.java</file><file>src/main/java/org/elasticsearch/common/util/CollectionUtils.java</file><file>src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/http/HttpServerModule.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/transport/TransportModule.java</file></files><comments><comment>Merge pull request #10873 from kimchy/remove_preconditions</comment></comments></commit></commits></item><item><title>Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10872</link><project id="" key="" /><description>Sorry, I was busy that's why I didn't answer on this issue:
  https://github.com/elastic/elasticsearch/issues/10259
So, I tested ZIP and TAR, and it doesn't works, it only works with the DEB archive.
When I run with zip or tar I get this:

```
root@debian:/home/user/elasticsearch-1.5.2# ./bin/elasticsearch
Exception in thread "main" java.lang.NoClassDefFoundError:     org/elasticsearch/bootstrap/Elasticsearch
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.Elasticsearch
at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
```

And I want to test the Marvel plugin but I also get a similar error:

```
root@debian:/home/user/elasticsearch-1.5.2# ./bin/plugin -i elasticsearch/marvel/latest
Exception in thread "main" java.lang.UnsupportedClassVersionError:      org/elasticsearch/plugins/PluginManager : Unsupported major.minor version 51.0
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.plugins.PluginManager. Program will exit.
```
</description><key id="71823356">10872</key><summary>Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mohamed70</reporter><labels /><created>2015-04-29T09:15:13Z</created><updated>2015-04-29T09:41:46Z</updated><resolved>2015-04-29T09:41:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-29T09:41:44Z" id="97371492">Let's follow up this discussion on #10259.

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Release script: Added option to check for environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10871</link><project id="" key="" /><description>In order to easily check for a correctly setup environment, you can
run

```
python3 dev-tools/build_release.py --check
```

It checks for env vars, needed commands and the correct java version.
Also supports colorful outputs.

A sample output looks like this

[![asciicast](https://asciinema.org/a/19326.png)](https://asciinema.org/a/19326?autoplay=1)
</description><key id="71819472">10871</key><summary>Release script: Added option to check for environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T09:00:42Z</created><updated>2015-05-30T10:53:35Z</updated><resolved>2015-05-26T12:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-29T09:03:13Z" id="97357711">w00t! Love your demo :)

Why we don't check MVN Java in check option?
</comment><comment author="spinscale" created="2015-04-30T07:36:56Z" id="97694760">@dadoonet added
</comment><comment author="dadoonet" created="2015-04-30T07:45:03Z" id="97695753">Just tested it locally and works really well.
LGTM.
</comment><comment author="spinscale" created="2015-05-26T08:11:21Z" id="105436675">@dadoonet fixed the python boto check, can you retry? worked on my linux system..
</comment><comment author="mikemccand" created="2015-05-26T08:12:57Z" id="105436899">Can we just always do `--check` by default so that when I try to release, it first checks and verifies I have the right env vars?
</comment><comment author="spinscale" created="2015-05-26T08:38:06Z" id="105445788">@mikemccand kept the `--check` parameter to exit early, but execute the checks all the time, also removed some redundant checks now
</comment><comment author="mikemccand" created="2015-05-26T08:40:02Z" id="105446169">Wonderful, thanks @spinscale!
</comment><comment author="rjernst" created="2015-05-26T08:44:56Z" id="105447582">Can the flag be called `--check-only`, to imply that is all it is doing? And this matches the var name used better anyways?
</comment><comment author="spinscale" created="2015-05-26T08:51:31Z" id="105450001">@rjernst good one, changed that as well
</comment><comment author="mikemccand" created="2015-05-26T11:59:51Z" id="105499790">LGTM, thanks @spinscale!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove file based default mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10870</link><project id="" key="" /><description>Using files that must be specified on each node is an anti-pattern
from the API based goal of ES. This change removes the ability
to specify the default mapping with a file on each node.

closes #10620
</description><key id="71801727">10870</key><summary>Remove file based default mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-29T07:34:26Z</created><updated>2015-12-07T11:11:31Z</updated><resolved>2015-04-30T20:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-29T07:44:03Z" id="97335833">LGTM. Let's just add a note to the 2.0 migration guide?
</comment><comment author="clintongormley" created="2015-04-29T13:19:03Z" id="97422914">Also need to delete this page: http://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-conf-mappings.html
</comment><comment author="nik9000" created="2015-04-29T13:25:38Z" id="97424660">&gt; Using files that must be specified on each node is an anti-pattern
&gt; from the API based goal of ES.

I appreciate the irony of having to use local files for scripting.
</comment><comment author="clintongormley" created="2015-04-29T13:30:15Z" id="97426831">&gt; I appreciate the irony of having to use local files for scripting.

&amp;lt;sigh&gt; yes &amp;lt;/sigh&gt;
</comment><comment author="jpountz" created="2015-04-30T08:33:15Z" id="97706896">LGTM
</comment><comment author="KlavsKlavsen" created="2015-05-30T18:48:39Z" id="107074972">so for those of us who actually want changelog based operations - that is no longer an option? We must "trust" whomever gets access to the API - to remember to also write to a changelog - so we know the change has been made.. ? (or write "yet another script" myself)

I -  and many others - prefer to make centralized configuration changes, which we then commit and roll out into production.. using puppet f.ex. This change means its no longer possible to let puppet manage how ones indexes is setup.. a fairly major part of ES configuration :(

I can hack around it.. write some script, which is executed on the change of a template - which then loads it into ES using http.. but I would call it an anti-pattern to make sure users can't use configuration management to manage their ES installations (without scripting it themselves). I do realize the current ES handling of templates is subpar.. it would be better to set it up to f.ex. use inotify - so it is informed by the kernel when an index template file is updated - and then simply try to verify and load it.. and if this fails it could log it - and even mail it if need be..
</comment><comment author="VascoVisser" created="2015-08-27T22:52:28Z" id="135577145">What exactly is the scope of this change? Is the ability to specify a default index template via files also removed? The docs are not clear:

&gt; The ability to specify mappings in configuration files has been removed. To specify default mappings that apply to multiple indexes, use index templates instead.

I am assuming for now template files are also subject to this change, because mine are being ignored.

From an operations perspective one would like to completely configure an ES node before bringing it up. Im thinking for example how to otherwise robustly prevent situations like where an index operation hits a node that hasn't got its mapping provided via the API yet. Is the idea nodes have to be "shielded" from the network until the mappings have been uploaded? Is there an easy way to prevent an ES instance from accepting operations until a template/mapping has been provided?
</comment><comment author="rjernst" created="2015-08-28T02:37:24Z" id="135615853">@VascoVisser A node joining an existing cluster should not start accepting index/search operations until it has the necessary cluster state to perform those operation (eg knowing metadata of existing indexes). Do you have an example setup where that is not true? If so, please open a new issue and we will be happy to investigate.
</comment><comment author="VascoVisser" created="2015-08-28T08:29:43Z" id="135675012">I have a single node setup, running ES from within a docker container. With 1.6 I ship the index templates with the container, allowing me to provision by just starting the container. With 2.0 I know of no other way but that I would have to make sure nothing is able to write to the a newly deployed instance, then "manually" send over the templates via the REST API, and then make the new ES instance available to clients.
</comment><comment author="rjernst" created="2015-08-30T19:13:45Z" id="136175807">@VascoVisser file based templates and mappings have a key inherent problem: the resolution priority is undefined. What if you set a default file based mapping, and then set _default_ in your mappings? Which one wins? What if you then restart the node? Mappings need to be exclusively part of the cluster state, so that the cluster controls how they are synchronized across the cluster as nodes go up and down.

As for your problem, I don't know docker, but could you send the index template to elasticsearch during the instance startup, after elasticsearch has started? Let's move any further discussion to https://discuss.elastic.co, as this PR is closed.
</comment><comment author="brusic" created="2015-11-24T16:14:23Z" id="159318301">I am also disappointed that this feature was removed. Just like KlavsKlavsen, I use a file based approach so that my mappings can be place in source control. Is using source control an anti-pattern nowadays? Removing features because someone _might_ break something is catering to newbies at the expenses of power users. We do not need kid gloves.
</comment><comment author="s1monw" created="2015-11-24T16:23:28Z" id="159321238">@brusic this is not about kid gloves. At the stage of the project we really need to streamline approaches to maintain high quality. We are struggeling a lot with legacy code because of tons of different code-paths and ways to do the same thing. For us and I am convinced as well for the user going though the cluster and the clusterstate for things like this is superior to file based approaches. It's also less error-prone. If you use version control you can do the same thing, everytime you update your mapping just update it in the cluster and it will be consistent on all nodes. 
</comment><comment author="shayts7" created="2015-12-07T09:48:03Z" id="162466180">@s1monw, I do understand the need to reduce code paths, and I appreciate that way-of-thinking, really.
But, this change complicates the deployment. I believe there are more ES admins that would like to have a different defaults ("not analyzed", "date field", etc..), and the config file was the best way to achieve that, especially if you deploy your ES using docker (then you can prepare a preconfigured docker and use it whenever you need a new ES node).
Now I must have a post-deployment process that creates my template. Also, I need to make sure ES is out of the network until my template is created successfully (to avoid situations where a new index will be created with the wrong defaults before I applied my template).
</comment><comment author="s1monw" created="2015-12-07T09:51:02Z" id="162466709">&gt; Now I must have a post-deployment process that creates my template. Also, I need to make sure ES is out of the network until my template is created successfully (to avoid situations where a new index will be created with the wrong defaults before I applied my template).  

wait, you templates are stored in the clusterstate so if a new node joins it gets the templates first and then can handle indices creation etc. I can only see a problem when you are initially starting up your cluster which is a one off? That is actually the way to go, you have all your stuff in the clusterstate and all nodes are consistent.
</comment><comment author="shayts7" created="2015-12-07T09:58:58Z" id="162468356">I'm referring to the initial starting up of the cluster.
</comment><comment author="s1monw" created="2015-12-07T10:09:13Z" id="162470692">The way elasticsearch should be used is non-filebased configuration. If your cluster is ready to serve index creations before it's ready you have to prevent this. I am sorry that we are complicating your env. but we have to simplify for the majority here. The common case is to have a long running cluster and startup initially is not necessarily a huge concern.
</comment><comment author="shayts7" created="2015-12-07T11:11:31Z" id="162486651">I think that in this particular use case, a file based configuration will simplify things. Anyway, thanks for the attention and the quick response. really appreciate that!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/index/mapper/FileBasedMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Settings: Remove file based index templates</comment></comments></commit></commits></item><item><title>Auto open/close index functionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10869</link><project id="" key="" /><description>For time series users, most of the queries happen within the relatively close past, 24-48 hours, up to a week or a month. However retention requirements mean that this data may need to be kept around for months to years and currently we recommend that people use a hot/cold setup with [shard allocation filtering](http://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-allocation.html#shard-allocation-filtering).

Other (additional) options include closing indices but persisting them to disk to reduce the impact to heap.

The last choice is definitely viable, but Elasticsearch is relatively ignorant around this, from a user end point; An admin will need to open an index, allow the end-user to query, and then close when, to ensure best resource use for their infrastructure. None of that is automatic.

It'd be handy for these use cases if we had configurable functionality that allows an admin to set (eg);
- Allow closed indices to be opened automatically
- Set a limit on number of indices that were previously closed to be opened. eg indexes only allow Y closed indices to be opened at any one time, to ensure resources are kept under any limit.
- Time frame from when closed index was opened to when it will be re-closed as a;
  - Hard time. eg 1 hour from open.
  - Time from last use. eg 1 hour from last query seen.
- Automatically close the index after period above.

There are a few things to be careful of here, particularly around stability;
- Setting re-opened limit too high
- Opening a large number of indices at a single time

There's going to be more to it than this but I've seen this sort of thing mentioned a few times in the community and I think it'd be a good feature to have for larger installs using time-series data.
</description><key id="71800364">10869</key><summary>Auto open/close index functionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>enhancement</label></labels><created>2015-04-29T07:25:41Z</created><updated>2016-06-21T11:40:26Z</updated><resolved>2015-05-08T10:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ashpynov" created="2015-05-05T14:31:42Z" id="99095794">I'm one of person requested it.
One more suggestion - open indices only by direct query only for query time (time need only to search and fetch data from indices). May be have some kind of LRU cache of "semi-opened" indices.

for information: I've 3.4TB stored data on each node for 1 week, it request 8.6 GB Lucene memory. But i have to store and provide on demand query up to 1 year long ago data. And it will requery 447.2GB heap, that looks unreasonable even with g1gc garbage collector (we used now).
</comment><comment author="clintongormley" created="2015-05-08T10:09:00Z" id="100182584">Hi @markwalkom and @ashpynov 

We talked about this on our FixItFriday call today.  While the feature sounds appealing, I think there are lots of gotchas that are not immediately apparent, and would result in adding a huge amount of complexity in order to try to support widely differing policies, eg:
- opening an index can be a heavy task, especially if the translog needs replaying
- the query might result in lots of fielddata being loaded into memory, which could also take a significant amount of time and heap
- what happens if you query 10 closed indices, but your auto-open limit is 5?
- what happens if you query the first 5, then query the next 5?  Does the user have to wait for (eg) 1 hour before they can run the second query?
- what happens if the 5 indices you open use up more than the available heap?
- what happens when multiple people query different closed indices at the same time?

Today we provide a simple open and close API, which allows the administrator to decide on policies, and to build an interface to implement those policies.  I think this is the correct approach - complexity is handled on a case by case basis, rather than Elasticsearch trying to cater for every need.

@ashpynov you mention that each of your indices requires about 9GB of heap.  Presumably you're using fielddata rather than doc values. You can switch to using docvalues today and in v2 we'll be switching them on by default.
</comment><comment author="ashpynov" created="2015-05-08T10:13:48Z" id="100183363">@clintongormley no, we use docvalues already, otherwise it will take up to 2% of index size (x10 more).
</comment><comment author="ashpynov" created="2015-10-07T08:52:32Z" id="146121600">Hi, @clintongormley @markwalkom 
We started some prototyping of such feature based on version 1.7. Having such decission on policies:
- freezed index are READ ONLY, so no translog. Opening index time is about 0,5% index size readtime from HDD about 12 sec on 500GB index (16 HDD 7200 rpm at RAID6) its very acceptable.
- of course field data loading is problem even on "hot" indices and it more or less protected by breaker.
- in case of indices count on limit - breaker can be solution. The amout of loaded cold indices limited by seach thread pool size and amount of indexes per query

The open/close functionality is very hard to implement due to cluster architecture:
1. we have to proxy query and decide - need to open index or not
2. we have to proxy query trough single node/
3. we need to know where "involved" index shards is located, and how many indices at that node is already open.

But most complexity is
1. Index opening make cluster RED at open time. So in our case cluster will be red half time or even always.
2. In case on searching in several day indices we need to place result reducing logic on client side
</comment><comment author="makeyang" created="2016-06-21T10:19:21Z" id="227399360">why is it closed? u guys already made up the decision?
</comment><comment author="clintongormley" created="2016-06-21T10:20:39Z" id="227399654">Yes
</comment><comment author="ashpynov" created="2016-06-21T11:40:25Z" id="227414985">We did it over version 1.7.3. In our use case (store data per daily basis for 2 years, but common search interval about 1 month) it reduced memory usage from 256GB to 32GB per server. SSD cache on RAID storage also reduce penalty of  loading/unloading data during query.
But we did not use it in production due to changing search engine to our custom and migration to ES 2.3 in specific search cases.
Also the code was developed by C++ coders so we are ashamed to publish it even here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Stop - start sequence drives to IllegalThreadStateException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10868</link><project id="" key="" /><description>After a node is created, and stop()-start() method is called sequentially, an IllegalThreadStateException is thrown.

Se below:

&lt;pre&gt;
Exception in thread "main" java.lang.IllegalThreadStateException
    at java.lang.Thread.start(Thread.java:705)
    at org.elasticsearch.indices.ttl.IndicesTTLService.doStart(IndicesTTLService.java:95)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
    at org.elasticsearch.node.internal.InternalNode.start(InternalNode.java:245)
    at no.capraconsulting.esbug.App.main(App.java:31)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
&lt;/pre&gt;


The problem seems to be in the `IndicesTTLService` class. Its `purgerThread` attribute is in `Thread.State.TERMINATED` as `purgerThread.start()` is invoked from `IndicesTTLService::start`.
</description><key id="71796831">10868</key><summary>Stop - start sequence drives to IllegalThreadStateException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">atomcat1978</reporter><labels /><created>2015-04-29T07:03:12Z</created><updated>2015-05-22T09:25:35Z</updated><resolved>2015-05-22T09:25:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-29T07:51:08Z" id="97337713">@atomcat1978 actually, we removed the ability to stop a node, and moved to just being able to create one and close it. The benefit of stopping and starting compared to recreating the node is negligible, since already almost all the "work" is done in the start method
</comment><comment author="clintongormley" created="2015-04-29T11:42:09Z" id="97398082">Is there anything we need to fix here?
</comment><comment author="atomcat1978" created="2015-04-29T16:49:57Z" id="97501750">@kimchy Ok. Maybe would be wise to clean up the code so that a noop is done? Getting an exception is not too nice:)

Actually, in IOC frameworks (like Spring) there might be a need to start and stop a wrapped node... I would have needed this functionality in integration tests, but ended up with node recreation at the end.
</comment><comment author="kimchy" created="2015-05-08T09:46:25Z" id="100175311">++, thanks @atomcat1978, agreed on cleaning up the Node code here and remove its start/stop external semantics
</comment><comment author="s1monw" created="2015-05-22T09:25:27Z" id="104590873">Node has only `start` and `close` I don't see what else needs to be fixed here? I will close this issue here since it's already cleaned up.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Eliminating results with hate words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10867</link><project id="" key="" /><description>It would be great to have feature to efficiently filter out docs with hate words in my elasticsearch result. Currently we are having bool filter in every search query for the list of all words.

Preprocessing and NOT indexing the docs with bad words in one solution and another is using a percolator before every query. Both the solutions can not enforce the rule if the index is made by some client out of our control.

It would be great to have a ES level solution for the same.
</description><key id="71767418">10867</key><summary>Eliminating results with hate words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lazywizar</reporter><labels /><created>2015-04-29T03:25:24Z</created><updated>2015-04-29T11:35:58Z</updated><resolved>2015-04-29T11:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T11:35:57Z" id="97395954">@lazywizar this request is very context sensitive.  I suggest that you add an alias to the index with a filter on the hate word, and allow users to search via that alias instead.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mapping type long question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10866</link><project id="" key="" /><description>I meet a problem . When I index an value , for example `{"id": -8848340816900692111}`,
then i search it ,it shows that `"id": -8848340816900692000`. 
Anyone can help? I want know why does elasticsearch do this and how can i deal with.
Thanks.
</description><key id="71762672">10866</key><summary>mapping type long question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Stabaoc</reporter><labels /><created>2015-04-29T02:45:45Z</created><updated>2015-04-29T03:45:52Z</updated><resolved>2015-04-29T02:57:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-29T02:51:26Z" id="97289513">Please join us in #elasticsearch on freenode or on the mailing lists at https://groups.google.com/forum/#!forum/elasticsearch for troubleshooting help, we reserve github for confirmed bugs and feature requests.
</comment><comment author="Stabaoc" created="2015-04-29T02:57:15Z" id="97290194">OK!
</comment><comment author="imotov" created="2015-04-29T03:45:52Z" id="97296503">Most likely a duplicate of #5518. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Fix] _cat APIs add space at the end of a line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10865</link><project id="" key="" /><description>Fixed #9464
</description><key id="71747850">10865</key><summary>[Fix] _cat APIs add space at the end of a line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">stephenfournier</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label></labels><created>2015-04-29T00:52:18Z</created><updated>2016-03-08T19:27:58Z</updated><resolved>2016-03-08T19:27:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-05-29T16:33:01Z" id="106865259">when running the rest tests against this branch after rebasing they are failing and one test is hanging somewhat...

```
HEARTBEAT J1 PID(14574@kryptic-2.local): 2015-05-29T18:32:11, stalled for  162s at: Rest6Tests.test {yaml=cat.nodes/10_basic/Test cat nodes output}
```

You can reproduce if you want via `mvn test -Dtests.slow=false -Dtests.filter=@Rest`

I will try to take a closer look at this soonish, but would be happy for any help.
</comment><comment author="spinscale" created="2015-06-26T08:45:16Z" id="115578599">so I fixed the outstanding tests here, and refactored your patch a tiny bit, my commit is at https://github.com/spinscale/elasticsearch/commit/e599ae6b5b46678c3171cc4bed381212ae9bf5b4

If you want you can include that in your PR, rebase it against the current master and get it in. I can also just do it myself, whatever your prefer, @Myll 
</comment><comment author="clintongormley" created="2015-12-04T14:28:07Z" id="161980109">@spinscale want to get this rebased and merged?
</comment><comment author="clintongormley" created="2016-03-08T19:27:58Z" id="193932291">Closed by #15250
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove (dfs_)query_and_fetch from the REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10864</link><project id="" key="" /><description>Remove the ability to specify search type ‘query_and_fetch’ and ‘dfs_query_and_fetch’ from the REST API.
- Adds REST tests
- Updates REST API spec to remove ‘query_and_fetch’ and ‘dfs_query_and_fetch’ as options
- Removes documentation for these options

Closes #9606
</description><key id="71725260">10864</key><summary>Remove (dfs_)query_and_fetch from the REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T22:30:20Z</created><updated>2015-06-08T13:17:28Z</updated><resolved>2015-05-07T19:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-28T22:37:51Z" id="97246335">+1 on making them purely internal

Should we do something about the Java client API too?
</comment><comment author="aleph-zero" created="2015-04-28T22:39:47Z" id="97246620">@jpountz I was wondering about the Java API, but the ticket is pretty explicit on it being a REST issue. I suppose we can leave it in Java for 'power' users? 
</comment><comment author="jpountz" created="2015-04-28T22:43:54Z" id="97247322">I asked the question because I think these options are equally trappy for consumers of the REST API and of the Java API. It might be harder to fix since the SearchType class is something we both need internally (so can't really remove these QUERY_AND_FETCH types ) but I think we should at least fix the documentation to make them sound less cool and label as internal options explicitely?
</comment><comment author="aleph-zero" created="2015-04-28T23:29:26Z" id="97262459">I removed the remaining references to these two options from the reference part of the docs. I searched the docs for the Java API, but did not find any reference to SearchType.(DFS_)QUERY_AND_FETCH, so I added a 'NOTE' about them being purely for internal purposes.
</comment><comment author="s1monw" created="2015-05-04T09:30:35Z" id="98649699">+1
</comment><comment author="jpountz" created="2015-05-05T15:34:45Z" id="99117975">LGTM
</comment><comment author="clintongormley" created="2015-05-07T18:47:15Z" id="99974979">@aleph-zero you want to get this in?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file></files><comments><comment>Merge pull request #10864 from aleph-zero/issues/9606</comment></comments></commit></commits></item><item><title>Remove global `source` parameter from individual APIs in REST spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10863</link><project id="" key="" /><description>Same way we don't define pretty anywhere, we shouldn't have source
</description><key id="71722344">10863</key><summary>Remove global `source` parameter from individual APIs in REST spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T22:14:36Z</created><updated>2015-04-29T12:46:49Z</updated><resolved>2015-04-29T12:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-29T11:33:07Z" id="97394173">LGTM
</comment><comment author="HonzaKral" created="2015-04-29T12:03:54Z" id="97401742">Pushed to master (28e5a64) and 1.x (53bf072)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove ElasticsearchIAE and ElasticsearchISE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10862</link><project id="" key="" /><description>This commit removes ElasticsearchIAE and ElasticsearchISE in favor of
the JDKs IAE and ISE.

Closes #10794
</description><key id="71699012">10862</key><summary>Remove ElasticsearchIAE and ElasticsearchISE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T20:50:43Z</created><updated>2015-08-13T13:44:19Z</updated><resolved>2015-04-29T08:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-28T20:59:09Z" id="97205166">+1

would be nice in the future to nuke lines of 'import java.lang.IllegalArgumentException;' and 'import java.lang.IllegalStateException;' too. But there are probably unnecessary imports in other places as well.

In some cases (e.g. IndexMetaData), there are throws clauses for these exceptions, which is a bit wierd too and something to revisit. I think it confuses lots of IDEs into generating separate catch blocks (we saw this forever from users list in lucene), and `@throws` is a better solution to declare what runtimeexceptions might happen. 
</comment><comment author="kimchy" created="2015-04-28T21:01:43Z" id="97205696">LGTM, +1 to @rmuir suggestions
</comment><comment author="jpountz" created="2015-04-28T21:02:37Z" id="97205863">Huge +1 too. Nuking the java.lang imports should be easy to automate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove index/indices replication infra code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10861</link><project id="" key="" /><description>now that delete by query is out, we don't need this infrastructure code. The delete by query will be implenented as a plugin, with scan scroll + bulk delete, so it will not need this infra anyhow
</description><key id="71698979">10861</key><summary>Remove index/indices replication infra code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T20:50:27Z</created><updated>2015-06-08T13:31:33Z</updated><resolved>2015-04-28T20:54:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T20:52:41Z" id="97203857">WOW +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/IndexReplicationOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/IndicesReplicationOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/IndicesReplicationOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndexReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file></files><comments><comment>Merge pull request #10861 from kimchy/remove_indices_replication</comment></comments></commit></commits></item><item><title>Does not honor cache sizes changed on the fly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10860</link><project id="" key="" /><description>You can use the API to change settings on the fly. 

```
PUT /_cluster/settings -d '{ "persistent" : { "indices.breaker.fielddata.limit" : "75%" } }'
PUT /_cluster/settings -d '{ "persistent" : { "indices.fielddata.cache.size" : "65%" } }'
```

If you then look at the settings, you will find that the `indices.breaker.fielddata.limit` field was applied, but the cache.size is not. 

``` bash
curl -XGET 'elasticsearchpool.example.com:9200/_cluster/settings?pretty'
{
  "persistent" : {
    "indices" : {
      "breaker" : {
        "fielddata" : {
          "limit" : "75%"
        }
      }
    }
  },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "all"
        }
      }
    }
  }
}
```

Also tried the following requests

```
GET /_all/_settings?pretty
GET /_cluster/settings?pretty
GET /_stats/fielddata?pretty&amp;fields=*
```

jpsandiego42 and afx114 in the IRC channel also confirmed that the cache settings don't work when changed on the fly. 
</description><key id="71670267">10860</key><summary>Does not honor cache sizes changed on the fly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spuder</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-04-28T18:44:45Z</created><updated>2015-08-26T19:42:53Z</updated><resolved>2015-08-26T19:42:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spuder" created="2015-04-28T18:45:39Z" id="97167323">Related to https://github.com/elastic/cookbook-elasticsearch/pull/314
</comment><comment author="clintongormley" created="2015-04-29T11:27:59Z" id="97393093">The fielddata cache size is not dynamically updatable, which is why it is ignored.  I'm not sure whether it is possible to change this value on the fly without invalidating everything already in the cache, which presumably is why it is not dynamic.
</comment><comment author="jpountz" created="2015-05-01T18:47:58Z" id="98203879">I am currently removing the ability to change the cache size on the filter cache because it added too much complexity (https://github.com/elastic/elasticsearch/pull/10897) so I don't think we should add it on the fielddata cache.
</comment><comment author="jpountz" created="2015-08-26T19:42:53Z" id="135149746">Closing: supporting this would be tricky and very prone to memory leaks due to existing segments referencing old caches through their close listeners.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove core delete-by-query implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10859</link><project id="" key="" /><description>This removes the core implementation of DBQ, which is trappy because it does an unexpected refresh every time and it can create inconsistent replicas since they can delete different documents from one another and the primary.  See #10067 for details.

The plan is to create a separate "scan/scroll/delete" plugin for 2.0, which will do a scan/scroll query and then bulk-delete the IDs, for users who really need DBQ but can't implement it client side on upgrade.

I left DBQ in Translog / IndexShard / InternalEngine so that on upgrade if a DBQ is in the translog it can still be applied.

On backport to 1.x I'll just add back the deprecations in the docs and public APIs.
</description><key id="71650802">10859</key><summary>Remove core delete-by-query implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:CRUD</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T17:24:47Z</created><updated>2015-06-06T15:38:55Z</updated><resolved>2015-04-28T20:06:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T19:01:05Z" id="97170948">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/IndexDeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/IndexDeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportIndexDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/package-info.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java</file><file>src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParserUtils.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/ParentChildDeleteByQueryBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/document/ShardInfoTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/routing/AliasRoutingTests.java</file><file>src/test/java/org/elasticsearch/routing/SimpleRoutingTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/stresstest/search1/Search1StressTest.java</file></files><comments><comment>Remove core delete-by-query implementation, to be replaced with a plugin</comment></comments></commit></commits></item><item><title>Install error on Mac OS due to the Java version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10858</link><project id="" key="" /><description>An error show up if you don't have update your Java version on your Mac 
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

run on terminal 
java -version

if you get 
java version "1.6.0_65"
Java(TM) SE Runtime Environment (build 1.6.0_65-b14-466.1-11M4716)
Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-466.1, mixed mode)

this is the standart version install ! So there is no chance you could run elasticsearch on your computer.
if you install the latest version this would patch the problem
</description><key id="71647432">10858</key><summary>Install error on Mac OS due to the Java version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ritoon</reporter><labels /><created>2015-04-28T17:09:28Z</created><updated>2015-04-29T12:33:39Z</updated><resolved>2015-04-29T12:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-28T18:23:16Z" id="97161877">we already point that the minimum requirement to install and use Elasticsearch is Java 7 and above, was there anything missing? I agree that the failure is pretty ugly, but nothing much we can do there sadly (at least not easily, since its before ES even starts)
</comment><comment author="ritoon" created="2015-04-29T08:14:46Z" id="97346424">When I first install Elasticsearch, I went to the official website whent to the download page :
https://www.elastic.co/downloads/elasticsearch

The installation steps are clear but nothing show or detect the requirements of JAVA 7
so my first install failed and it was a frustrating user experience to see such a simple notice.

Then I dig the web for solutions of people that would have had the same failure
I get into many advices in many web pages... an hour lost for a simple advice ! 
I saw the README with no mention of it and figure out that there where something missing.

Therefore my recommandation is to detect into the Elastic website the current client JAVA version and to add a simple download button, and add a simple line into the README.
In this way lots of hours for lots of people would be saved :+1: 
</comment><comment author="clintongormley" created="2015-04-29T12:33:28Z" id="97412757">&gt; Therefore my recommandation is to detect into the Elastic website the current client JAVA version and to add a simple download button,

We can't do this because of Java's licensing.  However, the first page in the Setup chapter of the docs tells you that you need to install Java 7 or 8: 

http://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html

I've added a Requirements section to the README, which links to the JVM section of the setup page.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add a Requirements section to the README mentioning recent Java</comment></comments></commit></commits></item><item><title>Expose Lucene 5's heap explain API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10857</link><project id="" key="" /><description>The new explain API for detailing heap usage can be a huge help in debugging large Elasticsearch systems - for performance or crashes

Obviously if you can add the ES's structures to this functionality , all the better

https://issues.apache.org/jira/browse/LUCENE-5949
</description><key id="71644928">10857</key><summary>Expose Lucene 5's heap explain API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2015-04-28T16:57:09Z</created><updated>2015-04-28T17:00:29Z</updated><resolved>2015-04-28T16:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-28T16:58:31Z" id="97136846"> Duplicate of #9111
</comment><comment author="synhershko" created="2015-04-28T17:00:29Z" id="97137272">Cool, thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the query parser cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10856</link><project id="" key="" /><description>The original goal of this cache was to avoid parsing the same query several
times in case several shards are held on the same node. While this might
sound like a good idea, this would only help when parsing the query takes
non-negligible time compared to actually running the query, which should not
be the case.
</description><key id="71633955">10856</key><summary>Remove the query parser cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T16:14:50Z</created><updated>2015-06-07T11:43:09Z</updated><resolved>2015-04-29T07:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-04-28T16:52:41Z" id="97135554">+1
</comment><comment author="rjernst" created="2015-04-29T05:26:57Z" id="97310852">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/QueryParserCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/QueryParserCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/none/NoneQueryParserCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/resident/ResidentQueryParserCache.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/test/java/org/elasticsearch/index/cache/query/parser/resident/ResidentQueryParserCacheTest.java</file></files><comments><comment>Merge pull request #10856 from jpountz/fix/remove_qparser_cache</comment></comments></commit></commits></item><item><title>Archives on github seems to have a "v" before version number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10855</link><project id="" key="" /><description>Normally, it's a little fix when you try to have an archive from github
</description><key id="71627514">10855</key><summary>Archives on github seems to have a "v" before version number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ThreadElric</reporter><labels><label>:Packaging</label><label>review</label></labels><created>2015-04-28T15:51:24Z</created><updated>2016-03-08T19:27:28Z</updated><resolved>2016-03-08T19:27:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T19:27:28Z" id="193932102">Fetching plugins from github is no longer supported.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only check existence for absolute paths in env.resolveConfig()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10854</link><project id="" key="" /><description>When resolving a path using `env.resolveConfig(String)`, it first checks for the path existence in the current working directory. This can fails when the security manager is enabled and elasticsearch not started from the ES_HOME directory like with init.d/systemd scripts or also with ./path/to/elasticsearch/bin/elasticsearch: Files.exist() resolves against the working directory and fails with a SecurityException.

I'm not sure we always need to set "user.dir" so I add a check for absolute path. This couldn't hurt much since the following `f1.toUri().toURL()` expects an absolute path.

What do you think?
</description><key id="71602115">10854</key><summary>Only check existence for absolute paths in env.resolveConfig()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels /><created>2015-04-28T14:18:36Z</created><updated>2015-06-08T13:31:47Z</updated><resolved>2015-05-07T12:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-28T14:30:32Z" id="97085021">More background: with ES installed with an RPM, creating a new index failed because the MapperService resolves `env.resolveConfig("default-mapping.json")` which calls `Files.exists("default-mapping.json")` and fails with a `SecurityException`.
</comment><comment author="rjernst" created="2015-04-29T05:29:06Z" id="97310989">Why wouldn't we make the paths relative to ES_HOME if they are not absolute?
</comment><comment author="tlrx" created="2015-04-29T07:04:05Z" id="97328745">When ES is installed with an RPM or Deb package, the configuration dir is not necessarily relative to ES_HOME, so we can't resolve path relative to ES_HOME.
</comment><comment author="s1monw" created="2015-04-29T09:01:53Z" id="97357544">hmm I think this look good to me - this is actually more intuitive than what we did before? @rjernst
</comment><comment author="rjernst" created="2015-04-29T15:57:17Z" id="97479401">Ok, I was confused about what we were resolving here, and after looking at the rest of the method I am now more confused. :)

I'm not sure this entire try/catch that is being modified should exist at all.  It does not appear to be useful.  What I see is the following logic:
1. Try to resolve the path on its own (only as an absolute path with your change)
2. Remove "/" from the beginning of path, making it relative (but this should no longer be possible with your change)
3. Take the path (which may have been absolute and made relative, at least on a nix system), and try to resolve it against the config dir.

It seems to me 2 is no longer valid (and was pretty bogus in the first place) and that 1 can actually be rolled into 3 because `configFile.resolve(path)` will just return `path` if it is absolute.
</comment><comment author="tlrx" created="2015-05-07T12:32:07Z" id="99847102">Better fix has been merged in #10923, closing this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Cleanup Direcotry and Searcher mock code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10853</link><project id="" key="" /><description>We deployed our own code to check if directories are closed etc an d
if serachers are still open. Yet, since we don't have a global cluster
anymore we can just use lucene's internal mechanism to do that. This commit
removes all special handling and usese LuceneTestCase.closeAfterSuite to
fail if certain resources are not closed
</description><key id="71601147">10853</key><summary>[TEST] Cleanup Direcotry and Searcher mock code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2015-04-28T14:15:12Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-28T15:12:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-28T14:33:41Z" id="97085852">+1 for the cleanup. I think the suiteFailureMarker stuff should be handled internally by LuceneTestCase.closeAfterSuite, for any closeable (separate from any isOpen check and unrelated to whether its a directory or not). 
</comment><comment author="s1monw" created="2015-04-28T14:37:55Z" id="97086859">&gt; +1 for the cleanup. I think the suiteFailureMarker stuff should be handled internally by LuceneTestCase.closeAfterSuite, for any closeable (separate from any isOpen check and unrelated to whether its a directory or not).

we did get this before anyway since we delete the index after the test anyway so I think there is no regression?
</comment><comment author="rmuir" created="2015-04-28T14:39:41Z" id="97087283">yeah, it does not make anything worse
</comment><comment author="s1monw" created="2015-04-28T14:51:01Z" id="97091453">@rmuir I added some reflection hack - wanna take anotehr look?
</comment><comment author="rmuir" created="2015-04-28T14:53:52Z" id="97092173">hack looks fine to me. we will straighten this out in LTC so its cleaner and upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/indices/leaks/IndicesLeaksTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/test/engine/AssertingSearcher.java</file><file>src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file><file>src/test/java/org/elasticsearch/test/engine/MockShadowEngine.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java</file><file>src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java</file></files><comments><comment>[TEST] Cleanup Direcotry and Searcher mock code</comment></comments></commit></commits></item><item><title>Logging: add the ability to specify an alternate logging configuration location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10852</link><project id="" key="" /><description>Replaces #7395 , which introduced support for a system property that allows to override the logging config location. This PR moves the logic to `Environment` from `LogConfigurator` and replaces `es.logging` with `path.logging`, which can be set from either sysprop (with `es.` prefix) or from the `elasticsearch.yml` config file. If `path.logging` is not set, behaviour is the same as before, it takes the same value as `path.config`.

Closes #2044
</description><key id="71597833">10852</key><summary>Logging: add the ability to specify an alternate logging configuration location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T14:00:33Z</created><updated>2015-06-26T09:15:30Z</updated><resolved>2015-06-26T09:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-28T14:07:29Z" id="97075533">Suggestions are welcome about the new config parameter, especially naming. The original PR made it a sysprop only (ala `es.config`) but I thought it would be useful to expose it as a `path.logging` parameter.

Also not sure about security policies, I think we can't do much right now given that logging gets initialized before extending security policy, but can you double check @rmuir ? let me know if you have any suggestions!
</comment><comment author="rmuir" created="2015-04-28T15:08:39Z" id="97099868">&gt; Also not sure about security policies, I think we can't do much right now given that logging gets initialized before extending security policy, but can you double check @rmuir ? let me know if you have any suggestions!

I think based on your PR its fine, since logger config is currently "read once". We could be defensive too and just add a addPath line for it in Security.java and a test in SecurityTests, in case reconfiguration of logging is ever supported. 

My only reason for suggesting this paranoia is just that if we ever supported reconfiguration of logging in the future, then it would not always have correct permissions, and might be hard to track down since the default value inside config/ would probably be fine due its location.
</comment><comment author="javanna" created="2015-05-08T12:06:23Z" id="100212053">@bleskes can you please review this?
</comment><comment author="bleskes" created="2015-05-21T10:57:08Z" id="104226327">@javanna the change looks good to me (though this is no my area of expertise). My only concern is the name of the property. We already have `path.logs` which dictates where to put the logs. `path.logging` is confusing in that context. How about `path.config.logging` (if we want a future config path) or `path.logging_config_file`. Maybe even better is to have a `config.logging` property which is equivalent the general `config` properties (which maps to the es.yml file).

Also I think the suggestion made by @rmuir still need to be added?
</comment><comment author="javanna" created="2015-05-28T13:54:00Z" id="106319915">Thanks for looking @bleskes . We already have `path.conf`, hence I went for `path.conf.logging`. Can you have another look? I also added the security permission, maybe @rmuir wants to have a look too.
</comment><comment author="rmuir" created="2015-05-28T16:28:29Z" id="106466026">The logic all looks good to me, but i echo @bleskes concern about logs vs logging. `path.conf.logging` looks different than `path.logs` but that does not help the code itself (e.g. Environment.loggingFile vs Environment.logsFile). 

I feel this is certain to cause us bugs in the future. If someone asked me the difference between the two, i would not have a good answer for them. So I think we should try to adjust the naming.
</comment><comment author="clintongormley" created="2015-05-29T09:11:11Z" id="106752417">Just been talking about this on FixItFriday - the original request #2044 wants to be able to choose between a test logging config and a production logging config.  This can already be done today by having two config directories: one for production and one for testing.

Of course, you still need to keep eg the elasticsearch.yml files in sync, but this proposed include functionality will help with that: https://github.com/elastic/elasticsearch/issues/11362
</comment><comment author="rmuir" created="2015-05-29T09:19:11Z" id="106753740">We really need to be careful of overengineering around this stuff. I'm worried about where this is going. 

Elasticsearch already has _SO MUCH OVERENGINEERED CODE_, we are literally drowning under the pressure of it. 

I'm seriously worried about where this is going. I think now its the wrong direction. We need to keep it as simple as possible instead.
</comment><comment author="s1monw" created="2015-05-29T09:20:13Z" id="106753874">I agree with robert here I think this is going in the wrong direction for little benefit.
</comment><comment author="javanna" created="2015-05-29T09:33:23Z" id="106758175">&gt; Of course, you still need to keep eg the elasticsearch.yml files in sync

That is what this PR (or better, the original one #7395) tries to avoid, allowing to specify only the `logging.yml` location by still using the same `elasticsearch.yml`. If people don't like it because we are adding yet another config option I can understand, then we can just close it.
</comment><comment author="rmuir" created="2015-05-29T09:45:52Z" id="106760139">Mainly my issue is now we have alternative proposal #11362

Without a doubt, having both is silliness. So we should be looking at what is really needed and what is the simplest solution.

_Not_ what is the most flexible solution
_Not_ what is the most user friendly solution

But what keeps the codebase smallest and easiest to maintain.

Definitely having both this and #11362 is a huge no-go, we don't need so many ways to do the same thing.

This is the wrong place in our codebase to have unnecessary complexity. Configuration today is already crazy? no.... ABSOLUTELY BONKERS with its reading of environment variables, sysprops, you name it.

Again my problem is not this PR in particular, and sorry @javanna for the comments ending up here, but they gotta go somewhere, we need to sort out what we want to do and not add lots of config options that then nobody will ever remove for back compat reasons.
</comment><comment author="javanna" created="2015-05-29T10:20:18Z" id="106766371">Agreed, let's take a step back and decide how we want to move forward in general. Marking for discussion then.
</comment><comment author="rjernst" created="2015-05-29T11:40:35Z" id="106780371">Regardless of whether #11362 is finished, I don't think this PR should happen. Right now, if you need different logging, but the same elasticsearch.yml, then use symlinks to a shared elasticsearch.yml within 2 different config dirs.
</comment><comment author="clintongormley" created="2015-06-26T09:15:29Z" id="115596394">We've had a long discussion about this in our FixItFriday session.  Our feeling is that we're trying to remove complication from the code base.  Having one way to do things just makes it simpler to understand and debug.  So for now, we're going to close this PR.  We may revisit this in the future if there is popular demand and the change makes life easier for the majority of users.

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use http for lucene snapshot downloads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10851</link><project id="" key="" /><description>This is much faster. The https gives me unpredictable read perf/timeouts
</description><key id="71578781">10851</key><summary>Use http for lucene snapshot downloads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-28T12:39:46Z</created><updated>2015-04-28T12:44:16Z</updated><resolved>2015-04-28T12:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T12:40:14Z" id="97048259">++ we should fix the root cause as well if possible.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10851 from rmuir/http</comment></comments></commit></commits></item><item><title>Render non-elasticsearch exception as root cause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10850</link><project id="" key="" /><description>if we don't have an ElasticsearchException as the wrapper of the
actual cause we don't render a root cause today. This commit adds
support for 3rd party exceptions as root causes.

Closes #10836
</description><key id="71569113">10850</key><summary>Render non-elasticsearch exception as root cause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T12:01:23Z</created><updated>2015-06-08T00:20:40Z</updated><resolved>2015-04-28T15:50:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-28T15:43:11Z" id="97110742">LGTM
</comment><comment author="spalger" created="2015-04-28T16:10:47Z" id="97122611">LGTM too
</comment><comment author="s1monw" created="2015-04-28T16:14:48Z" id="97123910">w00t thanks @spalger 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unicast Ping should close temporary connections after returning ping results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10849</link><project id="" key="" /><description>During pinging we open light , temporary connections to the the unicast hosts. After the pinging is done we close those. At the moment we do so before returning the results of the pings to the caller. On the other hand, in our transport logic we acquire a lock specific to the node id while opening a connection. When disconnecting from node, we have to acquire the same lock in order to guarantee the the connection opening has finished. This can cause big delays in environments where opening a connection is very slow, as the connection closing has to wait _after_ the pinging was done.. This can be problematic as  it causes master election to use stale data.
</description><key id="71546830">10849</key><summary>Unicast Ping should close temporary connections after returning ping results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T10:21:29Z</created><updated>2015-04-28T19:41:55Z</updated><resolved>2015-04-28T19:35:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-28T10:21:51Z" id="97003822">@martijnvg / @kimchy can you take a quick look?
</comment><comment author="martijnvg" created="2015-04-28T11:19:49Z" id="97023405">LGTM
</comment><comment author="kimchy" created="2015-04-28T11:20:53Z" id="97023979">I am concerned that we exit the pinging phase without properly disconnected from the node(s), it might get tricky? The most important part is internally to remove the node from the connected nodes map, less around waiting for the disconnection to actually end. Maybe we can add a flag if we should wait for the disconnect, and in this case, simple disconnect without waiting on the Future of the disconnect itself (as we do in NettyTransport)?
</comment><comment author="kimchy" created="2015-04-28T18:52:48Z" id="97169191">now I get it, its the lock while a connection is in progress. LGTM
</comment><comment author="bleskes" created="2015-04-28T19:37:21Z" id="97180181">@s1monw I think we  can put this in 1.5 as well. It's  a very small fix.. what's your take on this?
</comment><comment author="bleskes" created="2015-04-28T19:40:37Z" id="97181650">discussed this offline agreed to push to 1.5 as well.
</comment><comment author="s1monw" created="2015-04-28T19:41:55Z" id="97182122">&gt; discussed this offline agreed to push to 1.5 as well.

++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: Unicast Ping should close temporary connections after returning ping results</comment></comments></commit></commits></item><item><title>Remove reflection permission for sun.management.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10848</link><project id="" key="" /><description>This is no longer needed after #10553
</description><key id="71534431">10848</key><summary>Remove reflection permission for sun.management.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T09:27:07Z</created><updated>2015-06-08T13:31:57Z</updated><resolved>2015-04-28T16:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-28T15:01:13Z" id="97095848">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10848 from rmuir/sm_cleanup2</comment></comments></commit></commits></item><item><title>Add multi-valued text support to the analyzer API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10847</link><project id="" key="" /><description>Add support array text as a multi-valued for AnalyzeRequestBuilder and REST API

Closes #3023
</description><key id="71488441">10847</key><summary>Add multi-valued text support to the analyzer API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T05:55:16Z</created><updated>2015-08-13T17:31:11Z</updated><resolved>2015-05-15T11:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-29T15:03:24Z" id="97458050">Left some comments but generally looks good, thanks @johtani!
</comment><comment author="johtani" created="2015-04-30T02:18:27Z" id="97635381">Thanks for your comments. Fixed. @dakrone 
</comment><comment author="dakrone" created="2015-05-01T15:44:57Z" id="98161551">LGTM
</comment><comment author="johtani" created="2015-05-02T05:11:41Z" id="98306452">I push changing names of methods with varargs.
</comment><comment author="dakrone" created="2015-05-14T17:02:23Z" id="102103435">LGTM, thanks @johtani!
</comment><comment author="javanna" created="2015-05-15T08:08:49Z" id="102310124">left a couple more comments, mainly around bw comp but this is targeted for 2.0 so it shouldn't be a problem. LGTM!
</comment><comment author="johtani" created="2015-05-15T11:21:26Z" id="102373015">Pushed
</comment><comment author="johtani" created="2015-05-15T11:52:34Z" id="102377247">This PR breaks the java API.
This PR changes the return type of the text getter String[] instead of String in `AnalyzeRequest`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>common grams doesn't work with stacked tokens (ie. synonyms)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10846</link><project id="" key="" /><description>When using the common grams token filter after a token filter that can stack tokens (synonyms in my case), it does not correctly bigram each of the stacked tokens and uses an incorrect token position thus breaking phrase searches.

For example, in the test below we get the following output:

``` json
{
  "tokens" : [ {
    "token" : "chinese",
    "start_offset" : 0,
    "end_offset" : 7,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  }, {
    "token" : "academy",
    "start_offset" : 8,
    "end_offset" : 15,
    "type" : "SYNONYM",
    "position" : 2
  }, {
    "token" : "akademy_of",
    "start_offset" : 8,
    "end_offset" : 18,
    "type" : "gram",
    "position" : 3
  }, {
    "token" : "of_sciences",
    "start_offset" : 16,
    "end_offset" : 27,
    "type" : "gram",
    "position" : 4
  } ]
}
```

However, I would expect it to output this (note the token positions):

``` json
{
  "tokens" : [ {
    "token" : "chinese",
    "start_offset" : 0,
    "end_offset" : 7,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  }, {
    "token" : "academy_of",
    "start_offset" : 8,
    "end_offset" : 18,
    "type" : "gram",
    "position" : 2
  }, {
    "token" : "akademy_of",
    "start_offset" : 8,
    "end_offset" : 18,
    "type" : "gram",
    "position" : 2
  }, {
    "token" : "of_sciences",
    "start_offset" : 16,
    "end_offset" : 27,
    "type" : "gram",
    "position" : 3
  } ]
}
```

Here is a full example:

``` bash
#!/bin/bash

curl -XPUT 'http://localhost:9200/test' -d '{
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": {
            "analyzer": {
                "commonstop_index": {
                    "tokenizer": "standard",
                    "position_offset_gap": 512,
                    "filter": ["standard", "lowercase", "commonstop_index"]
                },
                "commonstop_syn_search": {
                    "tokenizer": "standard",
                    "position_offset_gap": 512,
                    "filter": ["standard", "lowercase", "en_synonym", "commonstop_search"]
                }
            },
            "filter": {
                "commonstop_index": {
                    "type": "common_grams",
                    "common_words": ["of"],
                    "query_mode": false
                },
                "commonstop_search": {
                    "type": "common_grams",
                    "common_words": ["of"],
                    "query_mode": true
                },
                "en_synonym": {
                    "type": "synonym",
                    "synonyms": ["academy, akademy"]
                }
            }
        }
    }
}'

sleep 5

curl -XGET 'http://localhost:9200/test/_analyze?analyzer=commonstop_index&amp;pretty' -d 'Chinese Academy of Sciences'
curl -XGET 'http://localhost:9200/test/_analyze?analyzer=commonstop_syn_search&amp;pretty' -d 'Chinese Academy of Sciences'

exit 0
```

/cc @s1monw
</description><key id="71444007">10846</key><summary>common grams doesn't work with stacked tokens (ie. synonyms)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label></labels><created>2015-04-28T01:09:42Z</created><updated>2016-01-18T14:28:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-28T09:17:44Z" id="96985116">I agree - this does appear to be doing the wrong thing.  Also, if you have two stacked terms, it is only creating a bigram with the last one, eg: if I add the synonyms to the `commonstop_index` analyzer, I get:

```
chinese, (academy,akademy,akademy_of), (of,of_sciences), sciences
```

@rmuir what's your take on this?
</comment><comment author="rmuir" created="2015-04-28T09:50:02Z" id="96992909">I think there are a couple bugs. The biggest problem here is in CommonGramsQueryFilter. It has logic that looks at the output from CommonGramsFilter: if it is a "n-gram" token, then it increases position always:

```
if (isGramType()) {
  posIncAttribute.setPositionIncrement(1); // thats bogus
}
```

But i also moved your synonym filter to index-time to see how things looked without this bug in the way. Its closer to what you want, in that it does not totally screw up positions, but it still doesn't make academy_of (only akademy_of).

That's a separate bug... and tricky to support without blowing performance away, or without making this thing really complicated (having all the logic of ShingleFilter and more).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Function score: Add `default` to `field_value_factor`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10845</link><project id="" key="" /><description>field_value_factor now takes a default that is used if the document doesn't
have a value for that field. It looks like:

``` js
"field_value_factor": {
  "field": "popularity",
  "default": 1
}
```

Closes #10841
</description><key id="71442583">10845</key><summary>Function score: Add `default` to `field_value_factor`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T01:01:05Z</created><updated>2015-06-07T16:53:22Z</updated><resolved>2015-04-28T15:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-04-28T01:02:17Z" id="96863187">@dakrone I can't get ElasticsearchF running locally and more. Its been a few months since I tried. I'm getting:

```
[2015-04-27 20:58:29,955][INFO ][node                     ] [Kick-Ass] version[2.0.0-SNAPSHOT], pid[18345], build[${build/NA]
[2015-04-27 20:58:29,955][INFO ][node                     ] [Kick-Ass] initializing ...
[2015-04-27 20:58:29,960][INFO ][plugins                  ] [Kick-Ass] loaded [], sites []
[2015-04-27 20:58:30,022][INFO ][env                      ] [Kick-Ass] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [58.5gb], net total_space [227.1gb], spins? [no], types [ext4]
{2.0.0-SNAPSHOT}: Initialization Failed ...
1) ExceptionInInitializerError
    NullPointerException2) NoClassDefFoundError[Could not initialize class org.apache.lucene.analysis.ar.ArabicAnalyzer$DefaultSetHolder]
```

Is that something I'm doing?
</comment><comment author="dakrone" created="2015-04-28T02:46:21Z" id="96890339">@nik9000 no, this is a byproduct of #10717 that we're trying to figure out the best way around

For now, you can append `-Des.security.manager.enabled=false` when running from an IDE, or use `mvn clean compile exec:exec` to run ES (which also passes this parameter)
</comment><comment author="nik9000" created="2015-04-28T10:46:04Z" id="97012684">Thanks! That did it.
</comment><comment author="nik9000" created="2015-04-28T11:27:42Z" id="97026278">I ran some local tests comparing this implementation which performs one auto-unboxing per document to a change that uses a primative instead. Ran across 1 million documents missing the field and both implementations were the same speed.

I just wanted to test my instinct that auto-unboxing has the overhead of a method call and that looks true. 
</comment><comment author="nik9000" created="2015-04-28T14:05:55Z" id="97074860">@dakrone, is there a chance you can review today? I want to backport to https://github.com/wikimedia/search-extra soon and I would prefer to backport from a merged pull request.
</comment><comment author="jpountz" created="2015-04-28T14:15:36Z" id="97077489">We have a similar parameter on `_sort` which is called `missing`, maybe this would be more consistent naming-wise? http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_missing_values
</comment><comment author="nik9000" created="2015-04-28T14:35:20Z" id="97086282">&gt; We have a similar parameter on _sort which is called missing, maybe this would be more consistent naming-wise? http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_missing_values

Sure.
</comment><comment author="dakrone" created="2015-04-28T14:37:52Z" id="97086843">Yeah, +1 on `missing` to be explicit that this takes the place of missing values. Other than the documentation thing Adrien suggested, I think this looks good to me!
</comment><comment author="nik9000" created="2015-04-28T14:43:23Z" id="97088303">Woops forgot one place.
</comment><comment author="nik9000" created="2015-04-28T14:47:27Z" id="97090077">OK - I think I got it but I'm rerunning all the tests just in case. And because I like to hear my laptop's fan.
</comment><comment author="jpountz" created="2015-04-28T14:56:31Z" id="97093394">This looks good to me too!
</comment><comment author="nik9000" created="2015-04-28T14:58:34Z" id="97094418">```
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 9:46.420s
[INFO] Finished at: Tue Apr 28 10:54:08 EDT 2015
[INFO] Final Memory: 38M/723M
[INFO] ------------------------------------------------------------------------
```

OK. Going to go and backport now!
</comment><comment author="nik9000" created="2015-04-28T14:58:38Z" id="97094453">Thanks!
</comment><comment author="nik9000" created="2015-04-28T14:58:44Z" id="97094521">Do you want me to squash?
</comment><comment author="dakrone" created="2015-04-28T14:59:20Z" id="97094837">@nik9000 sure, if you squash I'll merge :)
</comment><comment author="nik9000" created="2015-04-28T15:08:22Z" id="97099804">Squashed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Security manager cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10844</link><project id="" key="" /><description>1. initialize SM after things like mlockall. Their tests currently
   don't run with securitymanager enabled, and its simpler to just
   run mlockall etc first.
2. remove redundant test permissions (junit4.childvm.cwd/temp). This
   is already added as java.io.tmpdir.
3. improve tests to load the generated policy with some various
   settings and assert things about the permissions on configured
   directories.
4. refactor logic to make it easier to fine-grain the permissions later.
   for example we currently allow write access to conf/. In the future
   I think we can improve testing so we are able to make improvements here.
</description><key id="71438727">10844</key><summary>Security manager cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-28T00:36:31Z</created><updated>2015-06-08T13:32:05Z</updated><resolved>2015-04-28T09:12:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-28T08:00:53Z" id="96966933">LGTM, left one comment
</comment><comment author="s1monw" created="2015-04-28T08:47:30Z" id="96979974">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file></files><comments><comment>Merge pull request #10844 from rmuir/sm_cleanup</comment></comments></commit></commits></item><item><title>Unable to install rpm on CentOS 5.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10843</link><project id="" key="" /><description>Hello,

On CentOS 5.10 rpm bombs with

[root@eshost ~]# rpm -i elasticsearch-1.5.2.noarch.rpm
error: elasticsearch-1.5.2.noarch.rpm: Header V4 RSA/SHA1 signature: BAD, key ID d88e42b4
error: elasticsearch-1.5.2.noarch.rpm cannot be installed

Is it possible to sign the rpm with V3? 

Thanks,

David
</description><key id="71431592">10843</key><summary>Unable to install rpm on CentOS 5.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dvdklnr</reporter><labels /><created>2015-04-27T23:41:14Z</created><updated>2015-05-28T18:18:50Z</updated><resolved>2015-05-28T18:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-04-28T04:50:57Z" id="96907580">The issue appears to be that it cannot verify the public key for Elasticsearch running on CentOS/RHEL 5.

Trying to import the public key on a fresh install of CentOS 5.11:

``` bash
$ sudo rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
error: https://packages.elasticsearch.org/GPG-KEY-elasticsearch: import read failed(-1).
```

This issue does not exist on CentOS/RHEL 6 or later.
</comment><comment author="s1monw" created="2015-04-28T08:50:38Z" id="96980454">`5.11` seems pretty old and the wiki recommends to move away from it. I wonder if we should support it or just recommend to move on as centos does? http://wiki.centos.org/Download
</comment><comment author="markwalkom" created="2015-05-28T00:28:41Z" id="106122546">Technically we don't support RHEL/CentOS &lt;6.X - https://www.elastic.co/subscriptions/matrix
</comment><comment author="clintongormley" created="2015-05-28T18:18:49Z" id="106546604">Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Backcompat: Add test for missing filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10842</link><project id="" key="" /><description>The _field_names field was fixed in 1.5.1 (#10268) to correctly be
disabled for indexes before 1.3.0.  However, only the exists filter
was updated to check this enabled flag on 1.x/1.5. The missing
filter on those branches still checks the field type to see if it
is indexed, which causes the filter to always try and use
the _field_names field for those old indexes.

This change adds a test to the old index tests for missing filter.

Note: Something alarming about this test is it does not always fail.
If I narrow down to run just on say 1.1.0, it fails most of the time,
but not always. @jpountz Maybe you have an idea on why this might be?
</description><key id="71412317">10842</key><summary>Backcompat: Add test for missing filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T21:49:20Z</created><updated>2015-05-29T18:06:16Z</updated><resolved>2015-04-28T08:08:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-28T06:30:50Z" id="96944868">LGTM

I didn't manage to reproduce the failure locally. Can you log the generated filters and see if they are different when the test fails?
</comment><comment author="jpountz" created="2015-04-28T06:34:50Z" id="96945807">OK sorry I was against master. Against 1.x it fails easily as you describe. Looking more...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file></files><comments><comment>Backcompat: Add test for missing filter</comment></comments></commit></commits></item><item><title>field_value_factor should support a default value for documents that don't have the field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10841</link><project id="" key="" /><description>I want to use field_value_factor to remove the last places we use dynamic scripting but I need to support default values if the field isn't set. I have unset fields because my load process is two stepped - I load all the documents first and then I run queries against the loaded set of documents to set the value of this field.

I _could_ make sure to always set the value of the field but that seems brittle - I'd like to just default the field being missing to a 0 input to the field_value_factor function.

This looks pretty simple to implement and I'd be happy to do it if its something Elasticsearch wants.
</description><key id="71399351">10841</key><summary>field_value_factor should support a default value for documents that don't have the field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label><label>enhancement</label></labels><created>2015-04-27T20:55:43Z</created><updated>2015-04-28T17:22:15Z</updated><resolved>2015-04-28T15:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-27T20:56:45Z" id="96817300">+1 to this, would happily take a PR!
</comment><comment author="nik9000" created="2015-04-28T16:53:31Z" id="97135778">Thanks!
</comment><comment author="dakrone" created="2015-04-28T17:22:15Z" id="97142685">Thank _you_!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueTests.java</file></files><comments><comment>Add default to field_value_factor</comment></comments></commit></commits></item><item><title>No way to use cross_field over nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10840</link><project id="" key="" /><description>Hi, here is my example 

Mapping:

```
{
    "contact": {
        "_source": {
            "enabled": true
        },
        "dynamic": false,
        "properties": {
            "address": {
                "dynamic": false,
                "type": "nested",
                "properties": {
                    "city": {
                        "type": "string",
                        "analyzer": "standard"
                    },
                    "country": {
                        "type": "string",
                        "analyzer": "standard"
                    }
                }
            },
            "name": {
                "type": "string",
                "analyzer": "standard"
            }
        }
    }
}
```

Doc:

```
 {
    "name": "Ivan Bobkovits",
    "address": [
        {
            "country": "Ukraine",
            "city": "Kherson"
        }
    ]
}
```

Query:

```
{
  "multi_match": {
    "operator": "and",
    "query": "Ivan Kherson Ukraine",
    "type": "cross_fields",
    "fields": [
      "name^9999",
      "address.*",
    ]
  }
}
```

Which finds nothing. Is there a way to avoid indexing of the plain nested object into the main doc to be able to search crosswise?
</description><key id="71398504">10840</key><summary>No way to use cross_field over nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2015-04-27T20:50:57Z</created><updated>2016-06-03T12:31:57Z</updated><resolved>2015-04-28T08:59:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-28T08:59:15Z" id="96982058">Hi @serj-p 

Nested docs are entirely separate documents (although externally they are hidden).  You need to use a `nested` query to access the fields in those docs, and this may match multiple docs, each with their own `_score`, so these scores need to be reduced to a single number (eg min/max/av/sum).  

The `multi_match` query that you specify is only accessing the root doc, so you can only access the fields in that doc.  You can copy the values of the nested doc into the root doc with `include_in_parent` or `include_in_root`.
</comment><comment author="eratio08" created="2016-06-03T09:14:29Z" id="223530385">Is the `include_in_root` still a valid parameter? - I can't find anything about in the doc
Or is there another way to enable nested field to be used in a multi_match?
</comment><comment author="clintongormley" created="2016-06-03T11:43:57Z" id="223558066">@eratio08 yes it is, or you could use `copy_to` to just copy the nested fields of interest.  See https://github.com/elastic/elasticsearch/issues/12461
</comment><comment author="eratio08" created="2016-06-03T11:58:24Z" id="223560521">How would the `copy_to` equivalent look like for a nested type like the following?

``` JSON
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "nested",
          "include_in_root": true,
          "properties": {
            "bar": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```

something like this?

``` JSON
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "bar": {
              "type": "string",
              "copy_to": "foo.bar",
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-06-03T12:19:19Z" id="223564182">You can't copy to the same path, but you could do:

```
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "bar": {
              "type": "string",
              "copy_to": "foo_bar"
            },
            "baz": {
              "type": "string",
              "copy_to": "foo_baz"
            }
          }
        },
        "foo_bar": {
          "type": "string"
        },
        "foo_baz": {
          "type": "string"
        }
      }
    }
  }
}
```

Or even:

```
{
  "mappings": {
    "my_type": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "bar": {
              "type": "string",
              "copy_to": "all_foo"
            },
            "baz": {
              "type": "string",
              "copy_to": "all_foo"
            }
          }
        },
        "all_foo": {
          "type": "string"
        }
      }
    }
  }
}
```

in which case you don't need cross-fields
</comment><comment author="eratio08" created="2016-06-03T12:31:57Z" id="223566498">So the `"include_in_root": "true"` is a little more convenient because I don't have to address new field names.

Thanks so far!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bug in bulk java api?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10839</link><project id="" key="" /><description>Hi, 

I have noticed what looks like a small bug in the java bulk api for ES.
I'm not sure if this is the right place to post it but here goes anyways:

Assume that we set the bulk size to execute every 10 requests.
The expected response is that the afterBulk Callback should be called, once the response is executed.
Unfortunately if I add just 10 items to the bulk queue, there is no response, i.e the afterBulk Callback is not triggered.
I can see that the items are added through sense, or even in the ES console.

In case I add more than 10 items, I get responses for everything after the first 10.

Code to reproduce the issue:
N.B:
*ElasticClientHolder in the code below, is a singleton holding a connection to ES

Elasticsearch v - 1.4.4
Ubuntu 12.04 LTS

```
  public class BulkProcessorTestClasses {
 private BulkProcessor bulkprocessor;
 private BulkRequestBuilder builder;

public BulkProcessor getBulkprocessor() {
    if (bulkprocessor == null) {
        this.bulkprocessor = BulkProcessor
                .builder(ElasticClientHolder.getInstance(),
                        new BulkProcessor.Listener() {

                            @Override
                            public void afterBulk(long execution_id,
                                    BulkRequest req, BulkResponse resp) {
                                // TODO Auto-generated method stub
                                System.out.println("execution id:" + execution_id);
                                System.out.println(req.toString());
                                BulkItemResponse[] resp_items_arr = resp.getItems();
                                for(int i=0; i &lt; resp_items_arr.length; i++){
                                    System.out.println(resp_items_arr[i].getItemId());
                                }
                                System.out.println(resp.hasFailures());
                            }

                            @Override
                            public void afterBulk(long arg0,
                                    BulkRequest arg1, Throwable arg2) {
                                // TODO Auto-generated method stub
                                System.out.println("the whole bulk failed.---&gt;"
                                        + arg2.getLocalizedMessage());
                            }

                            @Override
                            public void beforeBulk(long arg0,
                                    BulkRequest arg1) {
                                // TODO Auto-generated method stub
                                System.out
                                        .println("these were the number of requests sent to es. bulk");
                                System.out.println(arg1.numberOfActions());

                            }

                        }).setBulkActions(10)
                .setBulkSize(new ByteSizeValue(1, ByteSizeUnit.GB))
                .setFlushInterval(TimeValue.timeValueSeconds(30))
                .setConcurrentRequests(1).build();
    }
    return bulkprocessor;
}
    //**

public BulkRequestBuilder getBuilder() {
    if (this.builder == null) {
                     //elasticsearchclientholder is a singleton holding a connection to ES.
        this.builder = ElasticClientHolder.getInstance().prepareBulk();
    }
    return builder;
}


public void build_and_test_bulk_request(){
    for(int i=0; i &lt; 10; i++){
        JSONObject elastic_object = new JSONObject();
        elastic_object.put("test_name", "name_" + String.valueOf(i));
        getBulkprocessor().add(
                new IndexRequest("test_index",
                        "test_mapping")
                        .source(elastic_object.toJSONString()));
    }
}

public BulkProcessorTestClasses(){
    build_and_test_bulk_request();
}

    /***
    * the maping contains a single string property called test_name.
    * in an index called test_index, and a type test_mapping.
    *
    */

  }
```

Calling the build_and_test_bulk_requests returns nothing.

Regards,
RR.
</description><key id="71397496">10839</key><summary>bug in bulk java api?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rrphotosoft</reporter><labels /><created>2015-04-27T20:46:44Z</created><updated>2015-04-29T15:13:42Z</updated><resolved>2015-04-29T14:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-27T21:07:08Z" id="96820585">The problem here is that you don't close the bulkprocessor when you exit your test method.
So bulk is not flushed.
</comment><comment author="rrphotosoft" created="2015-04-27T21:10:32Z" id="96821800">Hi, 
Thanks for the clarification, I realize my mistake, it wasn't in the example on the website, sorry for the trouble, another problem I'm having is how to access the request json string, for those items which failed? I cannot find any methods on the ActionRequest Class that give me access to the json string of the failed request. I need this for logging purposes. Can you help me on that?
Regards,
RR
</comment><comment author="dadoonet" created="2015-04-27T21:14:32Z" id="96823855">So we close this issue.
Please use the mailing list for your questions. We can definitely help you there.
</comment><comment author="rrphotosoft" created="2015-04-27T21:15:19Z" id="96824188">Im sorry,it still does not work  , I tried getbulkprocessor.close() , and getbulkprocessor.flush()
I get not callback.
</comment><comment author="rrphotosoft" created="2015-04-27T21:16:08Z" id="96824498">I would request you to please reopen the issue.
</comment><comment author="rrphotosoft" created="2015-04-28T01:19:20Z" id="96869915">Hi,
I am trying to debug the issue.
The problem does not reside with flush or close.
As I mentioned in my initial post, the records are &lt;i&gt;being indexed&lt;/i&gt;. So flush or close is not necessary, execute() is being correctly called.
It is the callback that is not triggered.
I hope that helps to solve the issue.
Regards,
RR.
</comment><comment author="imotov" created="2015-04-28T19:05:18Z" id="97171859">@rrphotosoft I slightly modified your test case to remove the reference to ElasticClientHolder  https://gist.github.com/imotov/d3fe3043949bab3e3e71 and executed it against locally running v1.4.4. I have got the following output:

```
[2015-04-28 15:00:47,428][INFO ][org.elasticsearch.plugins] [Shadowcat] loaded [], sites []
these were the number of requests sent to es. bulk
10
```

Do you expect to see something else?
</comment><comment author="rrphotosoft" created="2015-04-29T03:44:55Z" id="97296444">@imotov Yes, you expect to see "execution_id : 0" 

In case you want to test in more detail, try inserting a defective jsonstring as the index request, in that case the after bulk callback should show the errors in the bulk response items array.
But you don't see those either.

I was looking through the source, and suspect that the BulkProcessor is exiting before the "after bulk" callback can be triggered, because the queue is empty.
However I do not know why that happens only if you send in a request count that is either lower than or exactly equal to the bulk actions size.

Hope you guys can find the root cause,
Regards,
RR. 
</comment><comment author="imotov" created="2015-04-29T04:55:59Z" id="97305117">@rrphotosoft oh, sorry. The line `bulkprocessor.close();` is what messes it up. It closes the processor immediately. I have updated the gist https://gist.github.com/imotov/d3fe3043949bab3e3e71 Could you give it a try?
</comment><comment author="rrphotosoft" created="2015-04-29T14:04:25Z" id="97437533">@imotov , Hi, it still doesn't work. The first 'n' requests do not trigger the callback, irrespective of the bulk size or whether you call .close/.flush or not.
</comment><comment author="imotov" created="2015-04-29T14:06:32Z" id="97438353">@rrphotosoft when I execute the latest code that I posted I get the following:

```
[2015-04-29 00:49:28,162][INFO ][org.elasticsearch.plugins] [Rafferty] loaded [], sites []
these were the number of requests sent to es. bulk
10
execution id:1
org.elasticsearch.action.bulk.BulkRequest@21152a7a
0
1
2
3
4
5
6
7
8
9
false
```

Seems to be working for me.
</comment><comment author="rrphotosoft" created="2015-04-29T14:10:38Z" id="97439883">@imotov , ok, I was testing it with my original code.
 The difference is that , you have added the "awaitclose" call at the end. 

1.This is not in the docs - I would vote for a docs update which mentions that "awaitclose" needs to be called explicitly.
2.How can you decide for how long to set the awaitclose call?

PS:

Why not include the awaitclose call in the "execute" method , 

&lt;a&gt;https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L302&lt;/a&gt;

Regards,
RR.
</comment><comment author="imotov" created="2015-04-29T15:11:25Z" id="97460362">@rrphotosoft I have added description of close and awaitClose to documentation. The timeout in awaitClose depends on how long it takes for your documents to get to elasticsearch, so you will have to find it experimentally. The awaitclose call is not part of the execute method because some clients are using different strategies to determine when the bulk processing is done. For example, you can have a countdown latch that gets decremented in each afterBulk call. In this case the close method would be the way to go. If you have any other questions regarding usage of bulk processor, please feel free to ask them on the mailing list. 
</comment><comment author="rrphotosoft" created="2015-04-29T15:13:42Z" id="97461330">@imotov , thanks for your help, and for updating the docs. Cheers!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: add notes about using close and awaitClose with bulk processor</comment></comments></commit></commits></item><item><title>allow prompts for passwords in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10838</link><project id="" key="" /><description>The listing of passwords in elasticsearch.yml for accessing key stores is
dangerous. If someone steals the yml and key store file, they can open
the keystore and access to mimic the system. I wonder if there is a way to 
create a tag for the passwords in the yml file that will have the process 
prompt for the password at startup.

For example, in the yml:

```
marvel.agent.exporter.es.ssl.truststore.password: 123password
```

will become:

```
marvel.agent.exporter.es.ssl.truststore.password: -
```

and the system will then prompt the user for the password at start up:

```
Enter password for marvel.agent.exporter.es.ssl.truststore: 
```

Hopefully, the password is a strong one. But it won't be recorded and 
be visible.

If the yml file is stolen, it won't have sufficient information to crack the key
store. For multiple password entries, they will be prompted in their order
of appearance.

Thanks
</description><key id="71369345">10838</key><summary>allow prompts for passwords in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">pwli</reporter><labels><label>:Settings</label></labels><created>2015-04-27T18:48:52Z</created><updated>2015-06-02T14:40:33Z</updated><resolved>2015-06-02T14:40:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-28T08:51:47Z" id="96980631">@jaymode any thoughts on this?
</comment><comment author="jaymode" created="2015-04-28T15:18:22Z" id="97102893">This is something we've discussed and looked into, but ran into issues with multiple passwords prompts I believe (/cc @spinscale). I'm also not sure how or if this will work with the services used for package installations.

If we were to implement it as described, I think it would need to be a startup time scan through all settings with a special value (maybe `__prompt__` or something) and prompt for each one with the name of the setting in the prompt, then replace the values in the settings internally. This could wind up being a lot of prompts with multiple password settings and could be a lot of work for someone to restart an elasticsearch process.
</comment><comment author="pwli" created="2015-04-28T19:29:32Z" id="97177350">I realize the complication associated with multiple passwords, hence different prompts.  One solution is a key-value file with root only access, or a here file (&lt;&lt;eof) in the auto-startup script with root only access. Another is to encrypt the whole elasticsearch.yml file and on start up, a single password is used to decrypt in memory. The password can be passed by stdin. 

In my situation, bare passwords are not allowed to be in the nodes for security reasons. So we always start the services remotely from command and control hosts and with control programs, not manually. Also, if we have to restart a node, we rather it didn't automatically. We want to know why it failed first. 

Thanks
Peter

&gt; On Apr 28, 2015, at 10:19 AM, Jay Modi notifications@github.com wrote:
&gt; 
&gt; This is something we've discussed and looked into, but ran into issues with multiple passwords prompts I believe (/cc @spinscale). I'm also not sure how or if this will work with the services used for package installations.
&gt; 
&gt; If we were to implement it as described, I think it would need to be a startup time scan through all settings with a special value (maybe **prompt** or something) and prompt for each one with the name of the setting in the prompt, then replace the values in the settings internally. This could wind up being a lot of prompts with multiple password settings and could be a lot of work for someone to restart an elasticsearch process.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jaymode" created="2015-04-28T19:44:18Z" id="97183041">Peter, thanks for the follow up. I'm going to look into adding the functionality to prompt on startup for settings with a special placeholder.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/common/cli/CliTool.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file><file>src/test/java/org/elasticsearch/common/cli/CliToolTests.java</file><file>src/test/java/org/elasticsearch/common/settings/SettingsTests.java</file><file>src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java</file><file>src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>src/test/java/org/elasticsearch/test/ExternalNode.java</file><file>src/test/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/tribe/TribeUnitTests.java</file></files><comments><comment>add ability to prompt for selected settings on startup</comment></comments></commit></commits></item><item><title>Return positions of parse errors found in JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10837</link><project id="" key="" /><description>Extend SearchParseException and QueryParsingException to report position information in query JSON where errors were found. All query DSL parser classes that throw these exception types now pass the underlying position information (line and column number) at the point the error was found.

Closes #3303 
</description><key id="71353932">10837</key><summary>Return positions of parse errors found in JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T17:44:56Z</created><updated>2015-06-07T16:53:50Z</updated><resolved>2015-04-29T14:06:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-27T17:47:20Z" id="96755017">@s1monw New PR to replace https://github.com/elastic/elasticsearch/pull/7891 - easier to start afresh than rebase old PR
</comment><comment author="s1monw" created="2015-04-28T08:43:17Z" id="96979248">left some minor comments otherwise LGTM
</comment><comment author="s1monw" created="2015-04-28T11:13:53Z" id="97022608">LGTM
</comment><comment author="kimchy" created="2015-04-28T11:36:25Z" id="97028352">if we are changing all the places that call `QueryParserException`, maybe we can change it to accept `QueryParserContext`, and then we can get the index name and the XContentParser from it (for the location). It will reduce the calling site for all those places, and will allow us to add more info if we want to the exception down the road.
</comment><comment author="s1monw" created="2015-04-28T11:44:32Z" id="97032324">&gt; if we are changing all the places that call QueryParserException, maybe we can change it to accept QueryParserContext, and then we can get the index name and the XContentParser from it (for the location). It will reduce the calling site for all those places, and will allow us to add more info if we want to the exception down the road.

lets do that in a sep PR?
</comment><comment author="kimchy" created="2015-04-28T11:59:18Z" id="97035247">&gt; lets do that in a sep PR?

sure (my thought was if we already change all this files now)
</comment><comment author="markharwood" created="2015-04-28T12:19:19Z" id="97041418">If there's no objections I'll revise this PR with @kimchy 's suggestion
</comment><comment author="s1monw" created="2015-04-28T12:41:27Z" id="97048454">ok sure go ahead @markharwood 
</comment><comment author="markharwood" created="2015-04-29T08:15:55Z" id="97346964">The change to pass QueryParseContext to QueryParsingException made all parser code cleaner. 
One side effect was need for a protected constructor as a back door for unit tests that can't pass non-mockable QueryParseContext.
Good to go here?
</comment><comment author="javanna" created="2015-04-29T08:53:36Z" id="97356193">I had a quick look and I really like it, having the `QueryParseContext` as argument also addresses my concern about forgetting the `location` argument, that's great, left a couple of minor comments.
</comment><comment author="markharwood" created="2015-04-29T09:36:21Z" id="97370041">Thanks, @javanna - I moved the test exception out into same package in test framework and changed visibilities as per your comments
</comment><comment author="s1monw" created="2015-04-29T09:48:25Z" id="97373757">LGTM
</comment><comment author="javanna" created="2015-04-29T10:04:31Z" id="97376532">LGTM too
</comment><comment author="markharwood" created="2015-04-29T14:06:12Z" id="97438244">Pushed to master  https://github.com/elastic/elasticsearch/commit/528f6481eaadd2a0585dc6731a94d7a024b8ce29
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Response error.root_cause shouldn't empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10836</link><project id="" key="" /><description>While playing around with master I received the following error response:

``` json
{
  "error": {
    "root_cause": [],
    "type": "access_control_exception",
    "reason": "access denied (\"java.io.FilePermission\" \"default-mapping.json\" \"read\")"
  },
  "status": 500
}
```

The `root_cause` should probably contain something, even if it is just a copy of the outer level error.
</description><key id="71349374">10836</key><summary>Response error.root_cause shouldn't empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spalger</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T17:23:43Z</created><updated>2015-04-28T15:50:50Z</updated><resolved>2015-04-28T15:50:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T09:13:45Z" id="96984531">yeah that´s a bug if there is no `ElasticsearchException` wrapping this thing I guess it omits it... I will fix today or tomorrow thanks for opening this
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file></files><comments><comment>[REST] Render non-elasticsearch exception as root cause</comment></comments></commit></commits></item><item><title>[TEST] Run tests with [1..3] nodes by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10835</link><project id="" key="" /><description>This commit adds support for running with only one node and sets the
maximum number of nodes to 3 by default. if run with test.nighly=true
at most 6 nodes are used. This gave a 20% speed improvement compared to
the previoulys minimum number of nodes of 3.
</description><key id="71343818">10835</key><summary>[TEST] Run tests with [1..3] nodes by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T17:00:31Z</created><updated>2015-04-28T12:48:24Z</updated><resolved>2015-04-28T12:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-27T18:24:20Z" id="96770238">LGTM
</comment><comment author="javanna" created="2015-04-28T08:22:52Z" id="96971218">+1 left a small comment
</comment><comment author="s1monw" created="2015-04-28T11:45:55Z" id="97032524">@javanna pushed a new commit to simplify things
</comment><comment author="javanna" created="2015-04-28T12:07:59Z" id="97037960">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Doc: fix typo in cluster.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10834</link><project id="" key="" /><description>added a missing comma in one of examples
</description><key id="71328258">10834</key><summary>Doc: fix typo in cluster.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">minde-eagleeye</reporter><labels><label>docs</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T15:55:28Z</created><updated>2015-04-28T16:06:06Z</updated><resolved>2015-04-28T09:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T09:14:41Z" id="96984686">@dadoonet wanna pull this in?
</comment><comment author="clintongormley" created="2015-04-28T09:48:22Z" id="96992187">thanks @minde-eagleeye - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update cluster.asciidoc</comment></comments></commit></commits></item><item><title>Wait forever (or one day) for indices to close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10833</link><project id="" key="" /><description>Today we wait 30 sec for shards to flush and close and then simply exit the process.
This is often not desired and we should by default wait long enough for shards to
close etc. This commit adds a default timeout of one day which simplifies the code
and gives us _enough_ time to shut down.

Closes #10680
</description><key id="71323107">10833</key><summary>Wait forever (or one day) for indices to close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T15:39:01Z</created><updated>2015-05-29T15:34:32Z</updated><resolved>2015-05-04T14:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-27T17:51:52Z" id="96757220">LGTM
</comment><comment author="clintongormley" created="2015-04-28T08:39:06Z" id="96977732">Should we also (separately) change the default timeout in the init script? It's already configurable by the user but wondering if we should up the default?
</comment><comment author="s1monw" created="2015-04-28T09:20:33Z" id="96985495">@bleskes any comments
</comment><comment author="ivanilves" created="2015-05-04T14:29:23Z" id="98726803">:+1: Great! Finally.
</comment><comment author="bleskes" created="2015-05-18T08:32:06Z" id="102974170">For what it's worth now, LGTM as well. 
</comment><comment author="s1monw" created="2015-05-18T08:55:07Z" id="102980455">;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>syntax bug in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10832</link><project id="" key="" /><description>missing comma in one of examples can be found in 
http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#allocation-filtering

![image](https://cloud.githubusercontent.com/assets/7629410/7351106/16ce57d6-ecfb-11e4-9517-5520805958fe.png)
</description><key id="71321577">10832</key><summary>syntax bug in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">minde-eagleeye</reporter><labels /><created>2015-04-27T15:34:21Z</created><updated>2015-04-27T15:54:41Z</updated><resolved>2015-04-27T15:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-27T15:38:54Z" id="96714693">@minde-eagleeye Thanks for reporting. Did you know that you can send a PR with the change by clicking on the small "edit" button just on this page? 
Wanna do contribute it?
</comment><comment author="minde-eagleeye" created="2015-04-27T15:54:23Z" id="96721610">I did not see that, I'll do it now thank you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `_shutdown` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10831</link><project id="" key="" /><description>Thsi commit removes the `_shutdown` API entirely without any replacement.
Nodes should be managed from the operating system not via REST APIs.
</description><key id="71318146">10831</key><summary>Remove `_shutdown` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cluster</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T15:24:29Z</created><updated>2015-10-24T22:45:41Z</updated><resolved>2015-04-27T15:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-27T15:35:33Z" id="96712928">LGTM
</comment><comment author="rmuir" created="2015-04-27T15:35:43Z" id="96713034">+1, I would like to remove exitVM permissions too. I can look into it as a followup.
</comment><comment author="s1monw" created="2015-04-27T15:37:14Z" id="96713682">&gt; +1, I would like to remove exitVM permissions too. I can look into it as a followup.

++
</comment><comment author="TwP" created="2015-04-28T17:50:05Z" id="97151550">![visa](https://cloud.githubusercontent.com/assets/6323/7376435/fc811236-ed9c-11e4-999d-c2d697e7dbc4.png)

Seriously, though, thank you for doing this. My future self awoken at 3:15AM by our pager system will appreciate not making things worse by accidentally shutting down a node.
</comment><comment author="beiske" created="2015-05-04T10:40:00Z" id="98672946">The shutdown api was actually really convenient for clusters running on Found as we have mechanisms in place that restarts any Elasticsearch process that stops. It would be nice if there would still be a way to restart an instance through the api.
</comment><comment author="dhohle" created="2015-10-24T18:25:39Z" id="150839544">It must be me, but how should I stop the ES node? kill process? There is, at least not in version 1.7.x a 'stop' option in the bin/elasticsearch... "Let the OS handle" seems a bit too (argh, lack of English skill, there should be a good word for it) ¿harsh?
</comment><comment author="dadoonet" created="2015-10-24T18:30:49Z" id="150839782">There is one if you install elasticsearch as a service.

```
 service elasticsearch stop 
```

Will do it.

If you start elasticsearch with

```
  bin/elasticsearch 
```

Just CTRL C then.
</comment><comment author="nik9000" created="2015-10-24T22:45:41Z" id="150863739">Letting the OS handle it is what well behaved services should do.  It's
good that elasticsearch does the right thing when killed. Kill isn't
supposed to be harsh. Kill -9 is. Kill gives elasticsearch plenty of leeway
to shutdown properly.
On Oct 24, 2015 2:30 PM, "David Pilato" notifications@github.com wrote:

&gt; There is one if you install elasticsearch as a service.
&gt; 
&gt;  service elasticsearch stop
&gt; 
&gt; Will do it.
&gt; 
&gt; If you start elasticsearch with
&gt; 
&gt;   bin/elasticsearch
&gt; 
&gt; Just CTRL C then.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/10831#issuecomment-150839782
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Release: Fix build repositories script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10830</link><project id="" key="" /><description>Minor issue with specifying the correct version when starting the package release script.
Another issue fixed to make sure that the S3 `from` and `to` bucket parameters act the same.
</description><key id="71304405">10830</key><summary>Release: Fix build repositories script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2015-04-27T14:43:52Z</created><updated>2015-04-28T08:06:17Z</updated><resolved>2015-04-28T08:06:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-27T18:18:08Z" id="96766792">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlight more_like_this query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10829</link><project id="" key="" /><description>This is a feature request:

would it be possible to enable highlighting for more_like_this queries?

That would be extremely helpful, as it's not transparent which terms have been used to match a document!
</description><key id="71277860">10829</key><summary>Highlight more_like_this query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">svola</reporter><labels /><created>2015-04-27T13:04:03Z</created><updated>2015-10-15T09:09:39Z</updated><resolved>2015-10-15T09:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-04-27T16:24:37Z" id="96730855">I seem to highlight it just [fine](https://en.wikipedia.org/w/index.php?title=Special%3ASearch&amp;profile=default&amp;search=morelike%3Acats&amp;fulltext=Search). [This](https://en.wikipedia.org/w/index.php?title=Special%3ASearch&amp;profile=default&amp;search=morelike%3Acats&amp;fulltext=Search&amp;cirrusDumpQuery=yes) is how I do it. I do use the [experimental highlighter](https://github.com/wikimedia/search-highlighter) and haven't tested the others but I don't see why they _can't_ do it.
</comment><comment author="svola" created="2015-04-27T19:55:22Z" id="96796550">@nik9000 Thanks for the link. I'll try out that plugin. But with core ElasticSearch highlighting doesn't work for all queries. With match-queries it's working, but not with more_like_this queries.

I'm not the only desperate person...
http://stackoverflow.com/questions/28723308/elasticsearch-more-like-this-with-highlights
</comment><comment author="VorticonCmdr" created="2015-05-11T09:17:03Z" id="100829203">I would like this feature as well and according to this https://discuss.elastic.co/t/keyword-extraction/224/2 it already should work, but for me it does not.
</comment><comment author="maxcom" created="2015-10-14T12:56:36Z" id="148041529">Highlight on mlt query works with fast vector highlighter (you need to index term positions and offsets to use it).
</comment><comment author="svola" created="2015-10-14T13:22:32Z" id="148047710">@maxcom Ok, so I need to change the mapping of the fields, right? Thanks! I will try that... 
</comment><comment author="maxcom" created="2015-10-14T13:46:15Z" id="148054333">Yes, you need to change mapping and reindex your data. Note that storing term positions significantly increases index size and may have an impact on search performance.
</comment><comment author="svola" created="2015-10-14T13:52:58Z" id="148056038">Ok, but it's good to know that it's possible! Thanks!
</comment><comment author="svola" created="2015-10-15T09:09:25Z" id="148325497">@maxcom you made my day! Super happy about it...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add environment path for snapshot repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10828</link><project id="" key="" /><description>Now the security manager is enabled by default (see #10717 and comments), the repositories of type `fs` must use an environment path (something like `path.snapshots`?) as a base directory for storing snapshots. This path will be added to the policy file by the `org.elasticsearch.bootstrap.Security`. 
</description><key id="71272962">10828</key><summary>Add environment path for snapshot repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T12:41:51Z</created><updated>2015-05-21T04:14:25Z</updated><resolved>2015-05-21T04:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-27T14:49:24Z" id="96682464">+1
</comment><comment author="kimchy" created="2015-04-27T15:18:46Z" id="96700064">Today, I believe we allow for users to add fs repository through an API with a path to the endpoint. Maybe the change would be that on the node config, one configures the name+path tuples for folders that within them a custom FS repository can be added to? That way its well contained within paths that are pre configured on node level? If we do this, it will be a backward comp. breaking change, so we need to explain how to upgrade from previous versions to it. Thoughts? /cc @imotov 
</comment><comment author="imotov" created="2015-04-27T15:25:14Z" id="96706845">@kimchy I couldn't say I really like it, but I think this is the best proposal so far. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update index.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10827</link><project id="" key="" /><description>The references in the documentation do not seem to point to the currently maintained distribution of the groovy client, but they do point to valid records on mavencentral (one of which has a 1.5.0 distribution).  I have just spent about an hour trying to work out why the API didn't seem to work...
</description><key id="71268990">10827</key><summary>Update index.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">npvincent</reporter><labels><label>docs</label></labels><created>2015-04-27T12:25:40Z</created><updated>2015-06-19T15:56:21Z</updated><resolved>2015-06-19T15:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-27T12:32:06Z" id="96633252">Hi @npvincent 

Thanks for the PR.  Please could I ask you to sign the CLA? https://www.elastic.co/contributor-agreement

@pickypg could you have a look at this please?
</comment><comment author="pickypg" created="2015-06-19T15:48:14Z" id="113554046">LGTM. Think I got assigned this when I was on my honeymoon. :)
</comment><comment author="clintongormley" created="2015-06-19T15:56:20Z" id="113555720">CLA not signed - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DEB] Unable to install older versions via packages.elasticsearch.org</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10826</link><project id="" key="" /><description>I cannot install e.g. 1.5.1 via the repository. When I add this line:

```
deb http://packages.elasticsearch.org/elasticsearch/1.5/debian stable main
```

and after `apt-get update` perform

```
$ sudo /usr/bin/apt-get install elasticsearch=1.5.1
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Version '1.5.1' for 'elasticsearch' was not found
```

I can, however, download it manually from the download page but changing the URL myself: https://www.elastic.co/thank-you?url=https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.5.1.deb

Is this intentional?

I'm using 1.5.1 for a specific case in a Vagrant machine which doesn't install now.
</description><key id="71265413">10826</key><summary>[DEB] Unable to install older versions via packages.elasticsearch.org</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mfn</reporter><labels /><created>2015-04-27T12:15:04Z</created><updated>2015-04-27T13:03:57Z</updated><resolved>2015-04-27T13:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-04-27T12:23:10Z" id="96630062">Hey,

this is not intentional. I'll take a look. Thx for reporting!
</comment><comment author="spinscale" created="2015-04-27T12:43:23Z" id="96636220">can you retry and report back? I just tested with a fresh ubuntu VM and it worked on that one.
</comment><comment author="mfn" created="2015-04-27T12:53:20Z" id="96639104">Works now for me too ... ?!?!
</comment><comment author="spinscale" created="2015-04-27T13:03:56Z" id="96642870">The `Packages` file wasnt created correct, recreated it the right way.

nice, thx a lot for confirming! Closing then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Typed parameters in Java index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10825</link><project id="" key="" /><description>I believe the method signature for `IndexRequestBuilder.setSource(Map&lt;String, Object&gt; source)` should be changed to `IndexRequestBuilder.setSource(Map&lt;String, ?&gt; source)`.
Since the method is overloaded, a call to `setSource` with a `Map&lt;String, String&gt;` as parameters will invoke the method `IndexRequestBuilder.setSource(Object... source)`, to great confusion. 

The same goes for `CreateIndexRequestBuilder.setSettings(Map&lt;String, Object&gt; source)` in the indices admin client, and a few others.
</description><key id="71261515">10825</key><summary>Typed parameters in Java index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">magnhaug</reporter><labels><label>:Java API</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T11:55:53Z</created><updated>2015-05-29T12:56:55Z</updated><resolved>2015-05-29T12:56:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-27T11:59:57Z" id="96621807">@javanna could you take a look please?
</comment><comment author="javanna" created="2015-05-08T10:07:53Z" id="100182172">I think this makes sense, will mark as adoptme.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java</file><file>src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java</file><file>src/test/java/org/elasticsearch/rest/HeadersAndContextCopyClientTests.java</file><file>src/test/java/org/elasticsearch/rest/NoOpClient.java</file></files><comments><comment>Fix typed parameters in IndexRequestBuilder and CreateIndexRequestBuilder</comment></comments></commit></commits></item><item><title>[CI failure] org.elasticsearch.transport.netty.NettyTransportMultiPortTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10824</link><project id="" key="" /><description>There are leaked threads, however I found this exception before that in the output. Might be related?

2015-04-27 10:14:36,536][INFO ][transport.netty          ] Exception [BindTransportException] occurred, rerunning the test after [1] failures
  1&gt; org.elasticsearch.transport.BindTransportException: Failed to bind to [56378]
  1&gt;    at org.elasticsearch.transport.netty.NettyTransport.bindServerBootstrap(NettyTransport.java:426)
  1&gt;    at org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:295)
  1&gt;    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
  1&gt;    at org.elasticsearch.transport.netty.NettyTransportMultiPortTests.startNettyTransport(NettyTransportMultiPortTests.java:186)

See http://build-us-00.elastic.co/job/es_core_14_centos/3919/
</description><key id="71252881">10824</key><summary>[CI failure] org.elasticsearch.transport.netty.NettyTransportMultiPortTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>test</label></labels><created>2015-04-27T11:10:50Z</created><updated>2016-01-18T14:27:45Z</updated><resolved>2016-01-18T14:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-04-28T09:04:36Z" id="96982979">Maybe this one is related?
http://build-us-00.elastic.co/job/es_core_15_regression/179

Expected to get exception when connecting to port 49730

Stacktrace

java.lang.AssertionError: Expected to get exception when connecting to port 49730
    at __randomizedtesting.SeedInfo.seed([D687F4FC67362C05:9D6F87E86388EC96]:0)
    at org.junit.Assert.fail(Assert.java:93)
    at org.elasticsearch.transport.netty.NettyTransportMultiPortTests.assertConnectionRefused(NettyTransportMultiPortTests.java:185)
</comment><comment author="clintongormley" created="2016-01-18T14:27:45Z" id="172540300">Much has changed since this was opened. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Vectorize API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10823</link><project id="" key="" /><description>The vectorize API operates on top of the Term Vectors API and on `_source` in order to return a document-term matrix according to some user given specification. In a document-term (or term-document matrix), the rows correspond to documents in an index and the columns correspond to terms, or more precisely to some numerical value associated with each term such as tf or tf-idf.
## Defining the Doc-Term Matrix

First a vectorizer has to be registered. A vectorizer defines the shape and entries of the desired outputted matrix.

```
PUT /index/.vectorizer/my_vectorizer
{
    "vectorizer": [
        {
            "field": "text",
            "span": [ ... list of terms ...],
            "value": "term_freq"
        },
        {
            "field": "field_numeric_1",
            "span": 1
        },
        {
            "field" : "field_numeric_2",
            "span": 1,
            "script": "if _value &gt; 0.5 then 1 else 0"
        },
        {
            "field": "field_numeric_3",
            "span": 5
        },
        {
            "field": "label",
            "span": 1,
            "script": "if _value == "yes" then 1 else 0"
        }
    ]
}
```

In this example, the first entry defines a first list of columns whose values are the term frequencies of the terms given in `span` in this order. The next column is simply defined as the value of the numerical field named `field_numeric_1`. For the next column, we binarize the numerical value at the field using a script. Next we assign 5 columns for the first 5 values in the multi-valued numerical field `field_numeric_3`. Last but not least, the last column is binarized as well and could be used as the target label for learning.

More precisely:
- `field`: Name of the field in the index to which a mapping from field values to real values should be applied.
- `span`: For numerical fields, specifies a number of columns to be assigned in the resulting vector. For string fields, specifies a list of terms, each term occupying a column in the vector if present in the document.
- `value`: For numerical fields, defaults to the value at the field. For string fields, defaults to term frequency. For the later case other values are possible such as document frequency, or payload.
- `script`: Allows for any field value transformation such as thresholding, categorizing, etc ...
## Retrieving Vectors

We can now use the vectorize API:

```
GET /index/type/id/_vectorize&amp;vectorizer=my_vectorizer

Response:

{
    "shape": [1, 9],
    "vector": [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0}
    ]
}
```

The matrix returned is in a sparse format, together with its dimension. This vector can then be loaded in memory in your favorite statistical environments, or fed to a machine learning package. The parameters are exactly the same as the term vectors API with the additional `vectorizer` parameter. A vectorizer could also be provided inline, meaning as part of the request.

```
GET /index/type/id/_vectorize
{
    "vectorizer": [
        {
            "field": "text",
            "span": [ ... list of terms ...],
            "value": "term_freq"
        },
        {
            "field": "label",
            "span": 1,
            "script": "if _value == "yes" then 1 else 0"
        }
    ]
}
```

In order to retrieve more than one vector, use the `_mvectorize` API:

```
GET /index/type/_mvectorize
{
    "docs": [
    {
        "_index": "index",
        "_type": "type",
        "_id": "id",
        "vectorizer": my_vectorizer
    },
    {
        "_index": "index",
        "_type": "type",
        "_id": "id2",
        "vectorizer": my_vectorizer
    }
   ]
}

Response:

{
    "shape": [2, 9],
    "vector": [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0},
        {"0": 2, "3": 1, "6": 0.21, 7": 1, "8": 1}
    ]
}
```

Here different vectorizers could have been chosen. In this case, the vectors are stacked up, columns over columns, with the largest vector defining the dimension of the matrix.
## Generating Datasets

Finally, we can obtain a dataset by using a new search type called `vectorize` together with a scroll:

```
GET /index/type/_search?search_type=vectorize&amp;scroll=1m
{
    "query": { "match_all": {}},
    "size":  1000,

    "from": 0,
    "step": 1,
    "sample": "10%",

    "vectorizer": "my_vectorizer",
    "slice": start:stop:step
}

After submitting the scroll request, we get the response:

{
    "shape": [1000, 9],
    "vector: [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0},
        {"0": 2, "3": 1, "6": 0.21, 7": 1, "8": 1},
        {"3": 5, "3": 5, "6": 0.45, 7": 0, "8": 0},
        {"3": 5, "5": 3, "6": 0.56, 7": 0, "8": 1},
        ...
    ]
}
```

Here `slice` allows us to only select some columns in this matrix. The `from`, `step` or `sample` retain documents starting after having processed a given number of documents and at every chosen step, or with x% chances. These options are useful in order to generate datasets such as a training and test set.
</description><key id="71244185">10823</key><summary>Vectorize API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>feature</label><label>Meta</label><label>stalled</label></labels><created>2015-04-27T10:26:37Z</created><updated>2016-11-16T10:39:44Z</updated><resolved>2016-10-26T08:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-05-04T15:57:41Z" id="98765043">I like it!  I can definitely see this being used with plug-n-play tools like WEKA or scikit-learn.  Some questions / thoughts:
- Does everything just throw an exception if term vectors are not enabled?  Is this a hard requirement or just a way to improve speed?
- Why is `_source` required?
- How are missing values handled?  Is `null` or `NaN` encoded in the matrix if the document is missing a particular field?  I know some ML packages are particular about how they handle missing fields, perhaps there could be an optional `missing_value` parameter that inserts a default if the field is missing?  Same for spans + numerics.  E.g. if you request `span: 5`, but a field only has 3 values ... the extra two will likely need to be filled with `NaN` or `null` or something?
- With `_mvectorize`, is there an option to "unstack" the matrices?  E.g. you may just want to request multiple matrices in one request, but not combine them.
- The slice syntax seems clunky, and only sorta flexible.  E.g what if you want uneven combinations such as `0-5, 7, 9, 10-12` ?  Perhaps some kind of array syntax:  `[0..5, 7, 9, 10..12]` ?
- Perhaps show the resulting "shape" after `slice`ing a matrix to make it clear it is different from the underlying vectorizer?
- Would it be useful to also add the vectorizer name to the output?  Not sure if that is something people would want.
- Not necessarily a problem, just something to think about, but `step` and `sample` could get strange if a routing scheme is used.  E.g. `sample: 10%` may exhaust some shards faster than others, because certain shards are "hot" due to routing.  This could unbalance the sampling and skew it towards the particular routed values.  Might just be a documentation issue so that users are aware sampling doesn't necessarily mean truly random sample depending on circumstances.
- I wonder if there should be a "csv" or "tab" format option, so you can get a flat matrix rather than JSON?  Might make it easier to interface with other packages?  Not sure...it's very un-ES to emit non-JSON :)
</comment><comment author="alexksikes" created="2015-05-04T16:51:30Z" id="98777028">Thank you for reading through this. 

&gt; Does everything just throw an exception if term vectors are not enabled? Is this a hard requirement or just a way to improve speed?

Term vectors are generated on the fly if they are not stored in the index, so this should not be an issue, only that vectorizing term vectors would be slower in this case.

&gt; Why is _source required?

This is in order to retrieve numerical values which the TVs API does not return. But we don't necessarily need _source to be enabled, as long as the values are stored.

&gt; How are missing values handled? Is null or NaN encoded in the matrix if the document is missing a particular field? I know some ML packages are particular about how they handle missing fields, perhaps there could be an optional missing_value parameter that inserts a default if the field is missing? Same for spans + numerics. E.g. if you request span: 5, but a field only has 3 values ... the extra two will likely need to be filled with NaN or null or something?

If the document is missing a particular field then it means that it has none of these terms, so this part of the vector would zero, meaning not shown in the sparse vector format representation. The same behavior would apply if the some value could not be fetched. We could also fill up with a default missing values, but in this case the vector would no longer be sparse, leading to potentially huge outputs.

&gt; With _mvectorize, is there an option to "unstack" the matrices? E.g. you may just want to request multiple matrices in one request, but not combine them.

If the vectorizer is the same for every request then this would not be an issue. A vectorizer defines the desired outputted matrix. It is true that stacking these vectors is arbitrary, but all these (say m) vectors retrieved would have the same shape (say of length n), naturally leading to a matrix of shape m by n. If the vectorizers are not the same, then we should only stack the matrices according to the vectorizer that they share. Thanks for noting that.

&gt; The slice syntax seems clunky, and only sorta flexible. E.g what if you want uneven combinations such as 0-5, 7, 9, 10-12 ? Perhaps some kind of array syntax: [0..5, 7, 9, 10..12] ?

Yes that could be an option, definitely more flexible. Something which could be worth exploring is returning multiple matrices under different array syntax combinations. This in combination with `from`, `step` and `sample` would then return multiple matrices. So we would not have to go through different runs just in order to generate say a training and test set.

&gt; Perhaps show the resulting "shape" after sliceing a matrix to make it clear it is different from the underlying vectorizer?

Thanks for catching that, indeed if we are slicing the matrix then the resulting shape should be different as well.

&gt; Would it be useful to also add the vectorizer name to the output? Not sure if that is something people would want.

Yes that makes sense, but if the vectorizer is "inlined", meaning as part of the request, then that could mean returning potentially a lot of terms.

&gt; Not necessarily a problem, just something to think about, but step and sample could get strange if a routing scheme is used. E.g. sample: 10% may exhaust some shards faster than others, because certain shards are "hot" due to routing. This could unbalance the sampling and skew it towards the particular routed values. Might just be a documentation issue so that users are aware sampling doesn't necessarily mean truly random sample depending on circumstances.

Yes it should definitely be documented, sampling would be true to the shard, not to the index.

&gt; I wonder if there should be a "csv" or "tab" format option, so you can get a flat matrix rather than JSON? Might make it easier to interface with other packages? Not sure...it's very un-ES to emit non-JSON :)

We have plans to make this into a plugin, and yes in this case it would make a lot of sense to support different output formats.
</comment><comment author="polyfractal" created="2015-05-04T17:07:52Z" id="98781594">&gt; If the document is missing a particular field then it means that it has none of these terms, so this part of the vector would zero, meaning not shown in the sparse vector format representation. 

Oh, I see... the key in the vector is numbered to represent the column...missed that on the first pass.  Gotcha :)

&gt; A vectorizer defines the desired outputted matrix. It is true that stacking these vectors is arbitrary, but all these (say m) vectors retrieved would have the same shape (say of length n), naturally leading to a matrix of shape m by n.

Well, my question was more if you wanted to `_mvectorize` two different vectorizers:  "my_vectorizer" with length `n`, and "my_other_vectorizer" with length `j`.  Basically like all the other multi-APIs, like _mget or _mpercolate, where you are just batching stuff together.

Something like this:

```
GET /index/type/_mvectorize
{
    "docs": [
    {
        "vectorizer": "my_vectorizer"
    },
    {
        "vectorizer": "my_other_vectorizer"
    }
   ]
}
```
</comment><comment author="alexksikes" created="2015-05-04T17:10:18Z" id="98782665">&gt; Well, my question was more if you wanted to _mvectorize two different vectorizers: "my_vectorizer" with length n, and "my_other_vectorizer" with length j. Basically like all the other multi-APIs, like _mget or _mpercolate, where you are just batching stuff together.

I had just updated the comment as you were writing this :) Yes basically it would make sense to only stack matrices according to the vectorizer that they have in common.
</comment><comment author="clintongormley" created="2015-05-05T07:11:23Z" id="98978376">&gt; Well, my question was more if you wanted to _mvectorize two different vectorizers: "my_vectorizer" with length n, and "my_other_vectorizer" with length j. Basically like all the other multi-APIs, like _mget or _mpercolate, where you are just batching stuff together.

+1 to named vectors, just like we do with aggs, suggesters, etc

&gt; `from`, `step` and `sample`

The `from` parameter really has no meaning when scanning is used.  `step` and `sample` are probably best implemented using the `function_score` query, with a random function or a modulus on the ID or something, instead of adding new top level parameters.
</comment><comment author="alexksikes" created="2015-07-01T22:41:12Z" id="117843304">I rebased the vectorizer branch on master and added some tests. I'll create a new repo for the actual plugin but let's have this at least for reference.

https://github.com/alexksikes/elasticsearch/commits/feature/tvs-vectorize
</comment><comment author="alexksikes" created="2015-08-06T12:23:11Z" id="128347015">For reference, this is now a plugin. Please keep in mind this is still very alpha.

https://github.com/alexksikes/elasticsearch-vectorize/
</comment><comment author="diugalde" created="2016-10-26T08:01:18Z" id="256276738">Hello, Is there any current work on this feature?
</comment><comment author="s1monw" created="2016-10-26T08:20:11Z" id="256280576">no this is not being worked on and won't happen any time soon... closing for now
</comment><comment author="pkphlam" created="2016-11-16T10:39:44Z" id="260912701">Is there a reason why there is no work planned on this any time soon? Given the focus and rage about Machine Learning, this seems like an excellent feature that could potentially make ES the de facto datastore for a lot of machine learning/statistical work, especially with respect to text analytics. Currently, trying to replicate something like this requires a ton of work on both client and ingest sides with very little efficiency. There is really very little other software stacks out there that can combine ES's great query capabilities with a machine learning pipeline in an efficient way and this could be a gamechanger. Definitely a +1 for this feature.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix to let eclipse ignore the antigun plugin in it's maven integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10822</link><project id="" key="" /><description>This plugin should be ignored as it will make the internal eclipse build fail when there are NOCOMMIT comments in files, which are expected during feature development. This change only affects eclipse users and only when they are using the m2e eclipse integration
</description><key id="71242028">10822</key><summary>Fix to let eclipse ignore the antigun plugin in it's maven integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T10:17:19Z</created><updated>2015-06-07T11:46:01Z</updated><resolved>2015-04-27T11:51:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-27T11:42:44Z" id="96618171">LGTM
</comment><comment author="colings86" created="2015-04-27T11:51:21Z" id="96619219">Pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Request more emphasis on JVM compatibility and recommandation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10821</link><project id="" key="" /><description>In the process of testing upgrade from 1.3.6 to 1.5.1 I got the following message 

```
[2015-04-24 14:47:55,960][ERROR][bootstrap                ] Exception
java.lang.RuntimeException: Java version: 1.7.0_51 suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JVM_OPTS environment variable.
Upgrading is preferred, this workaround will result in degraded performance.
```

I read again release notes between 1.3.6 and 1.5.1 but don't found any mention on this restriction.

It's explained on this page [Java (JVM) version](http://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html#jvm-version) but I think it should be clearly mentioned on released notes.
</description><key id="71226499">10821</key><summary>Request more emphasis on JVM compatibility and recommandation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benoit-intrw</reporter><labels /><created>2015-04-27T09:10:16Z</created><updated>2015-04-27T09:23:38Z</updated><resolved>2015-04-27T09:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-27T09:23:37Z" id="96581330">Hi @benoit-intrw 

You're correct - it was missing from the release notes.  That's now fixed: https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-0

thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix JSON encoding for Mustache templates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10820</link><project id="" key="" /><description>This pull request replaces the current self-made implementation of JSON encoding special chars with re-using the Jackson JsonStringEncoder. Turns out the previous implementation also missed a few special chars so had to adjust the tests accordingly (looked at RFC 4627 for reference).

Note: There's another JSON String encoder on our classpath (org.apache.commons.lang3.StringEscapeUtils) that essentially does the same thing but adds quoting to more characters than the Jackson Encoder above.

Relates to #5473
</description><key id="71225661">10820</key><summary>Fix JSON encoding for Mustache templates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Templates</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T09:06:12Z</created><updated>2015-05-29T18:19:41Z</updated><resolved>2015-04-28T08:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-04-27T09:07:03Z" id="96576354">@s1monw You originally added the JSON encoding stuff. Would be great if you could have a look to validate the changes. Feel free to re-assign as you see fit.
</comment><comment author="s1monw" created="2015-04-27T14:50:44Z" id="96683165">LGTM
</comment><comment author="MaineC" created="2015-04-28T08:12:31Z" id="96968569">Pushed to 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reduce code duplication in TransportIndexAction/TransportShardBulkAction.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10819</link><project id="" key="" /><description>We have some duplication in TransportIndexAction/TransportShardBulkAction due
to the fact that we have totally different branches for INDEX and CREATE
operations. This commit tries to share the logic better between these two cases.
</description><key id="71215640">10819</key><summary>Reduce code duplication in TransportIndexAction/TransportShardBulkAction.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T08:27:04Z</created><updated>2015-06-08T13:32:16Z</updated><resolved>2015-04-27T08:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-27T08:29:51Z" id="96559063">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file></files><comments><comment>Merge pull request #10819 from jpountz/fix/transport_index_duplication</comment></comments></commit></commits></item><item><title>Java API documentation: document admin().cluster().prepareHealth().*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10818</link><project id="" key="" /><description>We should document the document admin().cluster().prepareHealth().\* and other admin().cluster() functions, which are useful for end users to interrogate the health of their clusters.  For example, a customer asked how to know when an index is actually reading for writing (i.e., status yellow/green) as opposed to just created (i.e., status red).
</description><key id="71196546">10818</key><summary>Java API documentation: document admin().cluster().prepareHealth().*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2015-04-27T06:57:30Z</created><updated>2015-12-31T14:23:33Z</updated><resolved>2015-12-31T14:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-30T17:40:44Z" id="168042828">Will be added once #15713 is merged.

Some doc in the meantime:

``` java
// Wait for yellow status
client.admin().cluster().prepareHealth()
        .setWaitForYellowStatus()
        .get();

// Wait for green status
client.admin().cluster().prepareHealth("company")
        .setWaitForGreenStatus()
        .get();

// Wait for green status but only wait for 2 seconds
ClusterHealthResponse yellow = client.admin().cluster().prepareHealth("yellow")
        .setWaitForGreenStatus()
        .setTimeout(TimeValue.timeValueSeconds(2))
        .get();
```

And 

``` java
// Get information for all indices
ClusterHealthResponse healths = client.admin().cluster().prepareHealth().get();
String clusterName = healths.getClusterName();
int numberOfDataNodes = healths.getNumberOfDataNodes();
int numberOfNodes = healths.getNumberOfNodes();

System.out.println("clusterName = " + clusterName);
System.out.println("numberOfDataNodes = " + numberOfDataNodes);
System.out.println("numberOfNodes = " + numberOfNodes);

for (ClusterIndexHealth health : healths) {
    String index = health.getIndex();
    int numberOfShards = health.getNumberOfShards();
    int numberOfReplicas = health.getNumberOfReplicas();
    ClusterHealthStatus status = health.getStatus();

    System.out.println("\tindex = " + index);
    System.out.println("\t\tnumberOfShards = " + numberOfShards);
    System.out.println("\t\tnumberOfReplicas = " + numberOfReplicas);
    System.out.println("\t\tstatus = " + status);
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add documentation for Java API health API</comment></comments></commit></commits></item><item><title>[DOCS] Link in README.md is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10817</link><project id="" key="" /><description>As of now, https://github.com/elastic/elasticsearch/blob/master/docs/README.md points to https://github.com/elastic/docs which is 404.

Side-question: where are these docs sources? :p
</description><key id="71194407">10817</key><summary>[DOCS] Link in README.md is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfn</reporter><labels /><created>2015-04-27T06:45:45Z</created><updated>2015-04-27T06:55:45Z</updated><resolved>2015-04-27T06:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-27T06:55:45Z" id="96523699">I've updated the README to point to the new repo.  The doc sources live in the `docs/` directory, where you found the README :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated docs/README.asciidoc to point to the new docs repo</comment></comments></commit></commits></item><item><title>Java API documentation - mapping API usage and examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10816</link><project id="" key="" /><description>The Java API docs currently lack any information about applying/updating mappings with the mapping API.  It would be nice to provide some examples how to create and update mappings in Java, as I've seen both customers and the community confused over what is the right way.
</description><key id="71193167">10816</key><summary>Java API documentation - mapping API usage and examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2015-04-27T06:36:25Z</created><updated>2015-12-30T17:41:55Z</updated><resolved>2015-12-30T17:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add documentation for Java API create index and put mapping</comment></comments></commit></commits></item><item><title>HTTP: Ensure url path expansion only works inside of plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10815</link><project id="" key="" /><description>This prevents reading of files that are not part of the plugin
directory by specifically crafted paths.
</description><key id="71189176">10815</key><summary>HTTP: Ensure url path expansion only works inside of plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.4.5</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-27T06:15:16Z</created><updated>2015-05-29T18:06:35Z</updated><resolved>2015-04-27T06:16:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-27T06:23:22Z" id="96518747">LGTM thx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove includes and excludes from _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10814</link><project id="" key="" /><description>Regardless of the outcome of #8142, we should at least enforce that
when _source is enabled, it is sufficient to reindex. This change
removes the excludes and includes settings, since these modify
the source, causing us to lose the ability to reindex some fields.

Note: I also renamed one of the UpdateMappingTests so the class names are unique.
</description><key id="71157985">10814</key><summary>Remove includes and excludes from _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>non-issue</label></labels><created>2015-04-27T02:33:16Z</created><updated>2015-08-13T14:05:17Z</updated><resolved>2015-04-28T22:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-27T06:32:42Z" id="96519986">LGTM
</comment><comment author="rjernst" created="2015-04-28T22:07:20Z" id="97240804">I also added migration docs and removed docs in master.  Will add deprecation in 1.x as well and link here.
</comment><comment author="uschindler" created="2015-05-08T06:42:30Z" id="100120939">This is a big no-go for me and would cause big headaches to me. I have excluded a lot of data from the original source field (which is still duplicate information, but cannot be easily modelled with copy_to). Reindexing is not an issue to me, I will always be able to regenerate the data from the external source or even _source.

So big -1, sorry. This is a desaster. This now slows down searching, because it transfers a large amount of useless data.
</comment><comment author="clintongormley" created="2015-05-08T18:20:08Z" id="100319361">@uschindler we have plans to support disabling `_source` again, but in a way that makes it less attractive to the uninitiated.  But I'm surprised by the include/exclude thing.  Could you elaborate more on your use case?  Wondering if https://github.com/elastic/elasticsearch/issues/9034 could help.
</comment><comment author="uschindler" created="2015-05-08T21:12:37Z" id="100369934">Hi @clintongormley !
#9034 looks like a good improvement, but there is no activity at the moment.

In general I use the _source field and I am happy with it, so I would never disable it completely, so I am just complaining about removing the functionality to filter parts during indexing.

In general there are 2 user types:
- Users that want Elasticsearch as their primary data storage. Of course those users need to be a ble to reindex and change mappings. Those users may also need to update some of the fields (like you can do "update table set column=..." in RDBMS)
- Users that just use Elasticsearch as a search engine for semi-structured fulltext data. Reindexing is not a problem for them, because they can generate the data from database, filesystem,...

In our case its the second type of user. So theoretically I could disable the _source field completely, but it is still good for easily fetching the information to show search results. Because of this, it is perfectly fine to only store the JSON information in the index thats needed for displaying search results and some statistics,...

Basically the Idea behind Lucene is to have an index where you index documents and only store the information in the index to show the document results. With this change (unable to disable _source or unable to filter _source) this is no longer possible.

The performance improvements are fine for stored fields, but if you have a lot of textual data solely there for searching (which never get retrieved) - partly up to several megabytes of text from PDF files, xml documents,... the additional overhead to extract the required fields is immense.

By changing to CBOR encoding of the _source field, this got better, but loading the whole stuff is still an overhead, if you just show like 2 kilobytes of thumbnail information. Instead you have to load 3 megabytes of CBOR data from the source field and let Elasticsearch filter it during returning search results.

One additional thing that happens on our side (as described before): We sometimes have to redundantly add the same data to the _source document several times, because a simple copy_to is not always easily possible. If copy_to would allow groovy scripts to dynamically copy stuff to other fields, I could live with that. E.g. one thing is to build suggestion fields or combining several fields with a special formatting, e.g. for Facets. In fact the data is redundant and could completely be regenerated also from the "filtered" source.

I agree with the problems that happen with disabled _source or applied filtering on _source when people want to do some things like updating or reindexing, but there should be done 2 things:
- add warnings to the user
- if a mapping contains filters on _source or has _source completely disabled, just report "unsupported operation" to the user if one tries to reindex or update existing fields. A clear error message is better than silently missing fields. By that a developer knows ASAP that disabling source or filtering source is a bad idea for primary data storage where you want reindexing and updates.
</comment><comment author="clintongormley" created="2015-05-12T10:11:22Z" id="101221963">@uschindler thanks for the reply.  I'm going to open a separate issue to discuss alternative solutions. Just one comment in the meantime:

&gt; One additional thing that happens on our side (as described before): We sometimes have to redundantly add the same data to the _source document several times, because a simple copy_to is not always easily possible. If copy_to would allow groovy scripts to dynamically copy stuff to other fields, I could live with that. E.g. one thing is to build suggestion fields or combining several fields with a special formatting, e.g. for Facets. In fact the data is redundant and could completely be regenerated also from the "filtered" source.

Have you seen the `transform` script? While the original `_source` field is stored, the transform script allows you to change the `_source` field that is sent for indexing, which sounds like what you need.

See http://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-transform.html
</comment><comment author="clintongormley" created="2015-05-12T12:59:24Z" id="101266801">@uschindler ticket opened here: #11116
</comment><comment author="uschindler" created="2015-05-12T13:34:08Z" id="101279794">Hi Clinton,

thanks for opening this issue! 

&gt; Have you seen the transform script? While the original _source field is stored, the transform script allows you to change the _source field that is sent for indexing, which sounds like what you need.

I know this functionality and I already thought about using it. The common use-case is to prepare the "suggest" field, which needs a different processing than all the other fields. This is for example here currently done in the preprocessing of the XML data using XSL... I could rewrite that using groovy!

My general problem is, as said on the new issue: I just want to keep _source, but decide on myself what I really want to store in the index, because I index tons of data, but only want to display very small snippets.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>Mappings: Remove includes and excludes from _source</comment></comments></commit></commits></item><item><title>Nicer exception when no scroll ID is provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10813</link><project id="" key="" /><description>#9469
</description><key id="71103068">10813</key><summary>Nicer exception when no scroll ID is provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephenfournier</reporter><labels><label>:Scroll</label><label>enhancement</label><label>feedback_needed</label></labels><created>2015-04-26T18:27:34Z</created><updated>2016-03-08T19:26:47Z</updated><resolved>2016-03-08T19:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-29T05:31:03Z" id="97311095">@Myll I left some comments.
</comment><comment author="stephenfournier" created="2015-04-29T05:39:30Z" id="97313249">@rjernst okay, fixed
</comment><comment author="rjernst" created="2015-04-29T05:57:18Z" id="97315675">@Myll Can you remove the leftover import and rebase?
</comment><comment author="clintongormley" created="2015-12-04T14:36:27Z" id="161981752">Hi @stephenfournier 

Any chance you want to pick this up again, or should we take it over?
</comment><comment author="clintongormley" created="2016-03-08T19:26:47Z" id="193931901">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Fix] Immediately clear the filter cache when requested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10812</link><project id="" key="" /><description>Fixes #8285

@dakrone splitting up my previous pull request #10178 as requested.
</description><key id="71098709">10812</key><summary>[Fix] Immediately clear the filter cache when requested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephenfournier</reporter><labels /><created>2015-04-26T17:51:19Z</created><updated>2015-04-27T09:02:26Z</updated><resolved>2015-04-27T09:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T18:15:35Z" id="96417580">@jpountz is this change still relevant given the reworking of queries/filters in master?
</comment><comment author="kimchy" created="2015-04-26T18:18:46Z" id="96417778">this fix is problematic, since it clears all the filters on the node level, not only the ones relating to a this specific index. Also, the reference to #8285 is incorrect, as its a different case of problem, its not the fact that `cleanup` is not called on the guava cache.

We can make `IndicesFilterCache#ReaderCleaner` a instance variable and explicitly call something like "forceCleanup" (synchronized) that calls the the parts of the run method (without scheduling another run) that ends up cleaning the respective parts relating to that index.
</comment><comment author="jpountz" created="2015-04-27T06:29:34Z" id="96519442">@clintongormley If we move to the Lucene filter cache, this should not be necessary since it has synchronous removals.
</comment><comment author="clintongormley" created="2015-04-27T09:02:24Z" id="96574414">thanks @jpountz.

thanks for the PR @Myll but I think we won't fix it as the whole mechanism is changing with Lucene 5
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add wildcard support for header names in the _cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10811</link><project id="" key="" /><description>It would be nice to request multiple columns in the _cat API using wildcards like:

```
GET _cat/thread_pool?v&amp;h=bulk.*
```
</description><key id="71084275">10811</key><summary>Add wildcard support for header names in the _cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:CAT API</label><label>feature</label><label>low hanging fruit</label></labels><created>2015-04-26T15:43:15Z</created><updated>2015-05-27T14:10:42Z</updated><resolved>2015-05-27T14:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/support/RestTable.java</file><file>src/test/java/org/elasticsearch/rest/action/support/RestTableTest.java</file></files><comments><comment>Cat API: Add wildcard support for header names</comment></comments></commit></commits></item><item><title>Make script parameters in significant terms same as elsewhere</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10810</link><project id="" key="" /><description>The `script_type` parameter in sigterms is non-standard.  

http://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-significantterms-aggregation.html#_scripted

We should use the same script construct that we use elsewhere.
</description><key id="71073963">10810</key><summary>Make script parameters in significant terms same as elsewhere</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Scripting</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-26T14:15:28Z</created><updated>2015-06-15T13:04:34Z</updated><resolved>2015-06-15T13:04:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T14:18:38Z" id="96390570">@markharwood See https://github.com/elastic/elasticsearch/pull/10649 and https://github.com/elastic/elasticsearch/pull/7977
</comment><comment author="colings86" created="2015-06-15T08:22:41Z" id="111975209">@markharwood this should have been fixed in https://github.com/elastic/elasticsearch/pull/11164. You might need to test to confirm though
</comment><comment author="markharwood" created="2015-06-15T13:04:33Z" id="112063004">Just tried it and it works. Thanks @colings86 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail #snapshot if translog is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10809</link><project id="" key="" /><description>If the translog is closed while a snapshot opertion is in progress
we must fail the snapshot operation otherwise we end up in an endless
loop.

Closes #10807
</description><key id="71057620">10809</key><summary>Fail #snapshot if translog is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-26T12:05:21Z</created><updated>2015-05-29T17:49:14Z</updated><resolved>2015-04-26T12:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-26T12:06:09Z" id="96370359">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/test/java/org/elasticsearch/index/translog/AbstractSimpleTranslogTests.java</file></files><comments><comment>Merge pull request #10809 from s1monw/issues/10807</comment></comments></commit></commits></item><item><title>bulk operation,docs has a redundant "}",the search api has some problem vesion 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10808</link><project id="" key="" /><description>![image](https://cloud.githubusercontent.com/assets/3814081/7336251/b5981cc0-ec25-11e4-8b04-34cf5d6e7bd2.png)
，then when I search it, somethings happen,
![image](https://cloud.githubusercontent.com/assets/3814081/7336256/10434438-ec26-11e4-929a-3b2cc3aaa7a0.png)
The result is not a JSON structure!
</description><key id="71023638">10808</key><summary>bulk operation,docs has a redundant "}",the search api has some problem vesion 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhengfengshaw</reporter><labels><label>feedback_needed</label></labels><created>2015-04-26T07:09:09Z</created><updated>2015-04-26T18:00:52Z</updated><resolved>2015-04-26T18:00:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-26T08:11:16Z" id="96341573">It'd be easier if you pasted the text, rather than screenshots :)

Also which docs page are you referring to?
</comment><comment author="clintongormley" created="2015-04-26T18:00:52Z" id="96414164">@zhengfengshaw not quite following what you're saying, but if you're saying that currently Elasticsearch accepts invalid JSON, and is then quite happy to return it, then yes.  This is a known issue.  

Closing as a duplicate of https://github.com/elastic/elasticsearch/pull/2315

Please reopen if I have misunderstood
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FSTranslog#snapshot() can enter infinite loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10807</link><project id="" key="" /><description>If a translog is closed while there is still a recovery running we can end up in an infinite loop keeping the shard and the store etc. open. I bet there are more bad thing that can happen here...

it manifested in exceptions like this:

```
 2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=89A1F19C6ECBCF0C -Dtests.class=org.elasticsearch.test.rest.Rest2Tests -Dtests.slow=true -Dtests.method="test {yaml=indices.get_field_mapping/10_basic/Get field mapping with include_defaults}" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=tr -Dtests.timezone=Europe/Isle_of_Man -Dtests.rest.blacklist=cat.recovery/10_basic/*
FAILURE 30.4s J4 | Rest2Tests.test {yaml=indices.get_field_mapping/10_basic/Get field mapping with include_defaults} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: Delete Index failed - not acked
```

and exceptions like:

```
  1&gt; [2015-04-25 16:10:50,790][DEBUG][indices                  ] [node_s1] [test_index] failed to delete index store - at least one shards is still locked
  1&gt; org.apache.lucene.store.LockObtainFailedException: Can't lock shard [test_index][2], timed out after 0ms
  1&gt;    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:520)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:448)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:392)
  1&gt;    at org.elasticsearch.env.NodeEnvironment.deleteIndexDirectorySafe(NodeEnvironment.java:342)
  1&gt;    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:494)
  1&gt;    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:401)
  1&gt;    at org.elasticsearch.indices.IndicesService.deleteIndex(IndicesService.java:443)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndex(IndicesClusterStateService.java:845)
```

all subsequent tests also fail with the delete not acked and print the same thread always sitting on a yield call in `FSTranslog`

```
  1&gt;         at java.lang.Thread.run(Thread.java:745)
  1&gt;    8) Thread[id=178, name=elasticsearch[node_s0][generic][T#1], state=RUNNABLE, group=TGRP-Rest2Tests]
  1&gt;         at java.lang.Thread.yield(Native Method)
  1&gt;         at org.elasticsearch.index.translog.fs.FsTranslog.snapshot(FsTranslog.java:362)
  1&gt;         at org.elasticsearch.index.translog.fs.FsTranslog.snapshot(FsTranslog.java:61)
  1&gt;         at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:845)
  1&gt;         at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:730)
  1&gt;         at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
  1&gt;         at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
  1&gt;         at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:135)
```

if the translog is actually closed `current.snapshot()` always returns `null` and we will spin forever....

``` Java
 @Override
    public FsChannelSnapshot snapshot() throws TranslogException {
        while (true) {
            FsChannelSnapshot snapshot = current.snapshot();
            if (snapshot != null) {
                return snapshot;
            }
            Thread.yield();
        }
    }
```

phew I am happy I finally tracked it down, it's super rare but annoying :)
</description><key id="70933662">10807</key><summary>FSTranslog#snapshot() can enter infinite loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-25T16:26:45Z</created><updated>2015-04-26T13:20:02Z</updated><resolved>2015-04-26T12:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-25T16:27:47Z" id="96233965">@bleskes I wonder if this can also cause funky other things like large translogs etc?
</comment><comment author="s1monw" created="2015-04-25T16:28:29Z" id="96234113">here is the failure that shows the bug http://build-us-00.elastic.co/job/es_g1gc_master_metal/6078/consoleFull
</comment><comment author="bleskes" created="2015-04-25T21:49:55Z" id="96284727">good catch! I don't think this can explain big translogs as the translog is only closed after the engine is closed, so there are no writes possible while this is ongoing. For what it's worth - this is also fixed in #10624 as the snapshot is retrieved from a view held by the recovery code. That view has it's own reference to the relevant translog files. Which means they can not be closed.
</comment><comment author="s1monw" created="2015-04-26T12:09:43Z" id="96371508">@bleskes I kept the fix minimal since we are refactoring this anyways
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/translog/fs/BufferingFsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/SimpleFsTranslogFile.java</file><file>src/test/java/org/elasticsearch/index/translog/AbstractSimpleTranslogTests.java</file></files><comments><comment>Allow double-closing of FSTranslog</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/test/java/org/elasticsearch/index/translog/AbstractSimpleTranslogTests.java</file></files><comments><comment>[TRANSLOG] Fail #snapshot if translog is closed</comment></comments></commit></commits></item><item><title>correct three mis-match of brackets in json doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10806</link><project id="" key="" /><description>three little mistakes to be correct.
</description><key id="70932394">10806</key><summary>correct three mis-match of brackets in json doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ishare</reporter><labels /><created>2015-04-25T16:16:58Z</created><updated>2015-04-26T17:44:19Z</updated><resolved>2015-04-26T17:44:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T17:44:06Z" id="96412946">thanks @ishare - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: correct three mis-match of brackets</comment></comments></commit></commits></item><item><title>Add recovery progress (% recovered) to the cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10805</link><project id="" key="" /><description>It'd be useful to have a cat API which reports the percentage of shards recovered, to make it easy for users to assess how much longer recovery will take.
</description><key id="70906950">10805</key><summary>Add recovery progress (% recovered) to the cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:CAT API</label><label>feature</label></labels><created>2015-04-25T13:03:48Z</created><updated>2015-06-22T13:05:38Z</updated><resolved>2015-06-22T13:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T13:09:13Z" id="96200754">Other useful data to display in the result include:
- number of pending tasks in the queue
- how long the first task in the queue has been waiting
- cluster health
</comment><comment author="spinscale" created="2015-05-27T12:32:20Z" id="105890260">A couple of APIs are already there to do similar tasks
- `/_cat/recovery/{index}` has `files_percent` and `bytes_percent` per shard
- `/_cat/pending_tasks` lists all pending tasks and thus also the first task in the queue
- `/_cat/health` lists the health status in the `status` field

So, we need a new endpoint here. This basically is an endpoint to ask, when the full cluster will be ready or give you a very general overview. Something like `/_cat/status` (very generic though)

Proposed output format

```
# curl '127.0.0.1:9200/_cat/status?v'
health     pending_tasks   pending_task_time_in_queue     recovery_percent
green     1234                   423234                                       100.0%
```
</comment><comment author="bleskes" created="2015-05-27T13:40:49Z" id="105913970">This what we expose with `GET _cluster/health`  for inspiration:

```
{
   "cluster_name": "boaz",
   "status": "yellow",
   "timed_out": false,
   "number_of_nodes": 1,
   "number_of_data_nodes": 1,
   "active_primary_shards": 2,
   "active_shards": 2,
   "relocating_shards": 0,
   "initializing_shards": 0,
   "unassigned_shards": 2,
   "number_of_pending_tasks": 0,
   "number_of_in_flight_fetch": 0
}
```

 I also think every number that we feel the need to add that the proposed _cat/status api and isn't part of that list should be added to it. Concretely I hear `pending_task_time_in_queue` and `recovery_percent` . Also, if we add `_cat/status` I wonder if we should have a dedicated json `_cluster/status`. Maybe we should extend `_cat/heath` to give more.
</comment><comment author="spinscale" created="2015-05-27T14:14:24Z" id="105927098">@bleskes I tend to agree to extend the `/_cat/health` and `/_cluster/health` with the two required metrics instead of adding a new endpoint - I was not aware that the pending tasks are included in the cluster health
</comment><comment author="bleskes" created="2015-05-27T14:27:38Z" id="105931163">&gt; I was not aware that the pending tasks are included in the cluster health

Yeah, I got annoyed with having to grep pending tasks dumps a long time ago :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedRunnable.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/test/cluster/NoopClusterService.java</file><file>core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java</file></files><comments><comment>Cluster Health: Add max wait time for pending task and active shard percentage</comment></comments></commit></commits></item><item><title>Infinity values at bounding-box when calculating the geo-bounds aggregate with many buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10804</link><project id="" key="" /><description>Hi,

I am following the example at: 
http://www.elastic.co/guide/en/elasticsearch/guide/current/geo-bounds-agg.html
to create an aggregation of some sample data according to their geohash cells, 
and return the geo-bounds of the result set in the bucket.

```
    "aggs": {
        "bucket" : {
            "geohash_grid" : {
                "field": "centroid",
                "precision": 8
            },
            "aggs": {
                "cell": {
                    "geo_bounds": {
                        "field": "centroid"
                    }
                }
            }
        }
    }
```

I have noticed that when my precision becomes higher the returned geo-bounds contain
infinity values for longitude and latitude. The bbox in every bounds object is of the following form: 

```
                  "bounds": {
                     "top_left": {
                        "lat": 49.611142831107415,
                        "lon": "-Infinity"
                     },
                     "bottom_right": {
                        "lat": "-Infinity",
                        "lon": "-Infinity"
                     }
                  }
```

(only top-left.lat changes)

Reducing the number of buckets by geo-filtering seems to help but is very limiting.
For example, I applied a geo-bounding-box filter with the bounds of a 
single geo-hash cell of precision 6, then geo-aggregated using precision 7,
with 32 buckets that returned fine. However geo-aggregating with precision 8
(returning 1024 buckets) have produced the incorrect geo-bounds.

The mapping that I've used:

```
            "properties": {
               "centroid": {
                  "type": "geo_point",
                  "lat_lon": true,
                  "geohash": true,
                  "geohash_prefix": true,
                  "geohash_precision": 11
               },
               "geom": {
                  "type": "geo_shape",
                  "tree_levels": 10
               },
               "name": {
                  "type": "string"
               },
               "osmId": {
                  "type": "long"
               },
               "type": {
                  "type": "string",
                  "index": "not_analyzed"
               }
            }
```

Thanks
</description><key id="70894781">10804</key><summary>Infinity values at bounding-box when calculating the geo-bounds aggregate with many buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">plroit</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-25T11:24:50Z</created><updated>2015-10-22T13:07:16Z</updated><resolved>2015-05-05T08:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T17:41:21Z" id="96412823">Hi @plroit 

Would be very useful if you could provide some example docs, in fact a full simple recreation would be awesome.

thanks

/cc @colings86 
</comment><comment author="plroit" created="2015-04-29T05:13:36Z" id="97307885">Sorry for the delay,

The query that I have used:

```
POST /osm/building/_search
{
    "size": 0, 
    "aggs": {
        "bucket" : {
            "geohash_grid" : {
                "field": "centroid",
                "precision": 8
            },
            "aggs": {
                "cell": {
                    "geo_bounds": {
                        "field": "centroid"}}}}}}
```

The dataset comes from OpenStreetMap vector data of Luxemburg,
[You can find it here](http://download.geofabrik.de/europe/luxembourg.html)
I am trying to put into geohash buckets  all the buildings, which are polygon shapes.
The field 'centroid' is the point center of the polygon.

Reviewing the indexed data reveals nothing special or erroneous.  
</comment><comment author="plroit" created="2015-04-29T05:17:40Z" id="97308811">I can provide you a generated csv file of the 77K buildings, and the C# code that I have used for indexing them. Would that help?
</comment><comment author="colings86" created="2015-04-29T08:38:50Z" id="97352695">@plroit the csv file would be very useful if you could provide that. Don't worry about the C# code for now.
</comment><comment author="plroit" created="2015-05-01T05:19:36Z" id="98047343">Try this file, each line is the object serialized to json
https://www.dropbox.com/s/49i73lfe2ebdvml/building.csv?dl=0
</comment><comment author="colings86" created="2015-05-01T09:32:26Z" id="98085238">@plroit thanks for the file and for raising this bug. I have reproduced the problem with the docs you provided so I'll look into what's causing this.
</comment><comment author="colings86" created="2015-05-05T09:01:55Z" id="99000673">@plroit Thanks again for raising this, I have just merged the fix via #10917 and it should be available from version 1.5.3 onwards
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java</file></files><comments><comment>Aggregations: Fixes Infinite values return from geo_bounds with non-zero bucket-ordinals</comment></comments></commit></commits></item><item><title>analysis: add sentence tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10803</link><project id="" key="" /><description>A tokenizer of type `sentence` providing breaks a text into sentences.

For example, if you have:

```
Hello world! How are you? I am fine.
This is a difficult sentence because I use I.D.

Newlines should also be accepted. Numbers should not cause
sentence breaks, like 1.23.
```

the output will be six tokens:
- `Hello world!`
- `How are you?`
- `I am fine.\n`
- `This is a difficult sentence because I use I.D.\n\n`
- `Newlines should also be accepted.`
- `Numbers should not cause \nsentence breaks, like 1.23.`
</description><key id="70886529">10803</key><summary>analysis: add sentence tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Analysis</label></labels><created>2015-04-25T10:02:18Z</created><updated>2015-06-10T09:18:38Z</updated><resolved>2015-04-29T11:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-25T10:05:04Z" id="96172043">I'd speak to @rmuir about this I wonder if there is a break iterator that can help u here?
</comment><comment author="rmuir" created="2015-04-25T11:50:43Z" id="96184632">Lucene has a sentence tokenizer actually. So maybe we should expose that. Its abstract, What you do with tokens is configurable. 
</comment><comment author="rmuir" created="2015-04-25T12:07:51Z" id="96188670">Here is a testcase with some examples. http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.java?revision=1664404&amp;view=markup
</comment><comment author="dadoonet" created="2015-04-25T14:12:45Z" id="96210645">@rmuir I pushed a new commit. Is that what you meant?

I wonder if we should first create this in Lucene itself then expose in elasticsearch?
</comment><comment author="rmuir" created="2015-04-26T12:08:30Z" id="96371113">Well i think the question should be what to do with the sentences. Personally i am a bit concerned about outputting them as tokens directly. Thats why the infra for this in lucene is abstract and is designed to make it easier to further subdivide into words. Its used internally by thai and chinese segmenters for this purpose.
</comment><comment author="dadoonet" created="2015-04-26T12:23:51Z" id="96372312">I do agree. I mean that I would prefer that users split on client side sentences in nested array strings So they can search using nested docs for terms within sentences.

I just came to this PR as someone asked me about it recently. 

I'm not sure it's something really useful or not.
I mean that we can totally close this PR if it does not really make sense to pull it in. We actually can do the same using a pattern tokenizer.
</comment><comment author="dadoonet" created="2015-04-29T11:46:42Z" id="97399136">Closing this for now as it sounds we have no real need for that.
There is already a workaround using pattern tokenizer `(\\S.+?[.!?])(?=\\s+|$)` with `DOTALL` flag.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consolidate document parsing logic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10802</link><project id="" key="" /><description>The code to parse a document was spread across 3 different classes,
and depended on traversing the ObjectMapper hiearchy. This change
consolidates all the doc parsing code into a new DocumentParser.
This should allow adding unit tests (future issue) for document
parsing so the logic can be simplified.  All code was copied
directly for this change with only minor modifications to make
it work within the new location.
</description><key id="70869160">10802</key><summary>Consolidate document parsing logic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-25T07:21:45Z</created><updated>2015-06-08T09:01:04Z</updated><resolved>2015-04-28T21:53:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-25T19:09:04Z" id="96264116">This is great! LGTM

Also +1 on handling multi-fields like copy_to
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java</file><file>plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java</file><file>test-framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java</file></files><comments><comment>Merge pull request #15213 from brwe/copy-to-in-multi-fields-exception</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/DynamicMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file></files><comments><comment>Mappings: Consolidate document parsing logic</comment></comments></commit></commits></item><item><title>Can not update mapping of inner object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10801</link><project id="" key="" /><description>```
curl -XPUT "http://elasticsearch/users/_mapping/user-type" -d'
{
  "user-type": {
    "properties": {
      "address.state.name": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```

this ends up creating a whole new mapping

```
"properties": {
               "address": {
                  "properties": {
                     "state": {
                        "properties": {
                           "abbreviation": {
                              "type": "string"
                           },
                           "id": {
                              "type": "long"
                           },
                           "name": {
                              "type": "string"
                           }
                        }
                     },
                     "zip_code": {
                        "type": "string"
                     }
                  }
               },
               "address.state.name": {
                  "type": "string",
                  "index": "not_analyzed"
               },
```

I am on 1.5 release
</description><key id="70859989">10801</key><summary>Can not update mapping of inner object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2015-04-25T05:57:38Z</created><updated>2015-04-25T08:10:33Z</updated><resolved>2015-04-25T08:08:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-25T08:08:46Z" id="96155272">@hurshprasad indeed. There are misunderstandings here.
1. you can not update an existing field. You need to create another mapping (or index) and reindex your documents. So even if your syntax would have been correct, it would have failed at the end.
2. your syntax is incorrect. You should use nested objects to define your mapping:

``` json
DELETE issue
PUT issue
PUT issue/10801/_mapping
{
  "10801": {
    "properties": {
      "address": {
        "properties": {
          "state": {
            "properties": {
              "abbreviation": {
                "type": "string"
              },
              "id": {
                "type": "long"
              }
            }
          },
          "zip_code": {
            "type": "string"
          }
        }
      }
    }
  }
}
PUT issue/10801/_mapping
{
  "10801": {
    "properties": {
      "address": {
        "properties": {
          "state": {
            "properties": {
              "abbreviation": {
                "type": "string"
              },
              "id": {
                "type": "long"
              },
              "name": {
                "type": "string",
                "index": "not_analyzed"
              }
            }
          },
          "zip_code": {
            "type": "string"
          }
        }
      }
    }
  }
}
GET issue/10801/_mapping
```

If you have any question like this, please prefer the mailing list.

Closing as I don't think it's an issue here. Feel free to reopen if you think it is.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't create a new BigArrays instance for every call of `withCircuitBreaking`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10800</link><project id="" key="" /><description>Since the circuit breaking service doesn't actually change for
BigArrays, we can lazily create a new instance only once and use that for all
further invocations of `withCircuitBreaking`.

This is a follow-up to #10798
</description><key id="70806953">10800</key><summary>Don't create a new BigArrays instance for every call of `withCircuitBreaking`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T22:12:29Z</created><updated>2015-04-28T08:52:07Z</updated><resolved>2015-04-27T18:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-24T22:15:00Z" id="96080990">LGTM
</comment><comment author="mfussenegger" created="2015-04-25T09:10:17Z" id="96165350">I'm not sure if this has changed with java7/8 but shouldn't the circuitBreakingInstance be volatile in order to make the [double-checked locking](https://en.wikipedia.org/wiki/Double-checked_locking#Usage_in_Java) work?
</comment><comment author="s1monw" created="2015-04-25T09:55:34Z" id="96169820">&gt; I'm not sure if this has changed with java7/8 but shouldn't the circuitBreakingInstance be volatile in order to make the double-checked locking work?

yes it has to be volatile, I wonder why we don't create it in the ctor and have it around? it the lazyness needed?
</comment><comment author="jpountz" created="2015-04-25T18:57:13Z" id="96261778">+1 to create eagerly to avoid the laziness
</comment><comment author="dakrone" created="2015-04-25T19:06:15Z" id="96264014">Yeah, +1 on creating it in the constructor, I'll change.
</comment><comment author="dakrone" created="2015-04-25T19:17:47Z" id="96264669">Pushed another commit
</comment><comment author="jpountz" created="2015-04-25T19:30:59Z" id="96267890">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move to Java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10799</link><project id="" key="" /><description>move master to Java 8 min version.
</description><key id="70792375">10799</key><summary>Move to Java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels><label>upgrade</label></labels><created>2015-04-24T20:58:16Z</created><updated>2015-09-03T17:11:29Z</updated><resolved>2015-09-03T17:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-05-22T09:34:40Z" id="104594334">We agreed to go with the Lucene requirements for Java. Since 2.0 is on Lucene 5.0 we will stick with Java 7 until we branched off 2.x
</comment><comment author="kimchy" created="2015-05-22T09:40:44Z" id="104597031">@s1monw ++
</comment><comment author="jasontedor" created="2015-09-03T17:11:29Z" id="137514562">This was closed by #13314.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change BigArrays to not extend AbstractComponent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10798</link><project id="" key="" /><description>In order to avoid the `getLogger(getClass())` calls in the
AbstractComponent constructor.

Seems like BigArrays used to be a Singleton but it actually
no longer is one. Every time a SearchContext is created a
new BigArrays instance is created via the
`withCircuitBreaking` call.
</description><key id="70789106">10798</key><summary>Change BigArrays to not extend AbstractComponent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T20:40:16Z</created><updated>2015-05-29T15:34:59Z</updated><resolved>2015-04-24T22:07:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-24T22:04:15Z" id="96079740">Thanks @mfussenegger, this LGTM.
</comment><comment author="dakrone" created="2015-04-24T22:05:10Z" id="96079851">I think we can be smarter about not creating a new BigArrays instance for every `withCircuitBreaking` call also, working on a fix for this.
</comment><comment author="dakrone" created="2015-04-24T22:12:46Z" id="96080731">Opened #10800 as a followup
</comment><comment author="kimchy" created="2015-04-26T12:20:55Z" id="96372224">@dakrone / @rjernst its not tagged in which version we merged this, can we do it? I suggest we push it to 1.x as well.
</comment><comment author="dakrone" created="2015-04-26T23:20:48Z" id="96447753">This was pushed to 1.x but not labeled, labeled it with 1.6.0
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java</file><file>src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java</file><file>src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file></files><comments><comment>Merge pull request #10798 from mfussenegger/bigarrays</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java</file><file>src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java</file><file>src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file></files><comments><comment>Merge pull request #10798 from mfussenegger/bigarrays</comment></comments></commit></commits></item><item><title>Replication: how to apply back pressure from stalled replication options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10797</link><project id="" key="" /><description>With https://github.com/elastic/elasticsearch/pull/10786 shard operations on a replica might need to wait for mappings to be published on the replica so that the indexing request is processed. Is it ok to wait indefinitely / what options do we have to apply back pressure?
</description><key id="70780904">10797</key><summary>Replication: how to apply back pressure from stalled replication options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:CRUD</label><label>enhancement</label></labels><created>2015-04-24T20:06:28Z</created><updated>2017-06-16T09:25:37Z</updated><resolved>2017-06-16T09:25:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-24T20:09:31Z" id="96049795">IMO this should be done on the primary, once we have the op in flight counter @brwe  is working on in #10610 
</comment><comment author="clintongormley" created="2016-01-17T17:56:17Z" id="172359011">@bleskes anything to do here?
</comment><comment author="bleskes" created="2016-01-18T09:57:15Z" id="172482753">yeah, this is still an issue. If something goes wrong on this level, indexing operations will be accumulating in memory and nodes can go OOM. We had a couple of things we talked about that could help here:

1) A request circuit breaker with a limit to the total memory of in-flight requests.
2) A hard but configurable limit to the total number of concurrent operations on a shard. The idea here is that indexing threads will block if there are two many ongoing operations, applying back pressure to the client.

I like #1 better and it's more generic, but #2 is trivial to implement when we get #15956 in. The implementation in that PR has a such limit built in (which we disabled by making it MAX_INT). 
</comment><comment author="jpountz" created="2017-06-15T08:58:27Z" id="308672018">@bleskes Do you know if recent changes made this issue invalid or fixed somehow?</comment><comment author="bleskes" created="2017-06-15T10:53:20Z" id="308697824">@jpountz it's still the same, although we do have a request circuit breaker on the node with the primary now, so if this really accumulates it will start rejecting requests. I don't think it addresses this issue but I'm also not sure if it's worth keeping around. </comment><comment author="jpountz" created="2017-06-16T09:25:37Z" id="308978512">OK, I'll close then.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to lucene-5.2-snapshot-1675927</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10796</link><project id="" key="" /><description>This is to pull in https://issues.apache.org/jira/browse/LUCENE-6451.
</description><key id="70775354">10796</key><summary>Upgrade to lucene-5.2-snapshot-1675927</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2015-04-24T19:32:24Z</created><updated>2015-04-24T20:40:45Z</updated><resolved>2015-04-24T19:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T19:37:02Z" id="96043373">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log template management requests at INFO level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10795</link><project id="" key="" /><description>It will be helpful to move template management task entries (create, delete, update, etc..) from DEBUG to INFO level:

```
[2015-04-24 12:21:19,747][DEBUG][cluster.service          ] [Stiletto] processing [create-index-template [ls_template2], cause [api]]: execute
[2015-04-24 12:21:19,748][DEBUG][cluster.service          ] [Stiletto] cluster state updated, version [12], source [create-index-template [ls_template2], cause [api]]
```
</description><key id="70773291">10795</key><summary>Log template management requests at INFO level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-04-24T19:23:50Z</created><updated>2015-05-08T10:07:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T17:18:13Z" id="96410231">Or possibly provide a different context for logging, eg `cluster.service.template`?
</comment><comment author="ppf2" created="2015-04-26T20:27:20Z" id="96431337">Sounds like a good idea to create a new context and write some useful INFO level messages to this new logger. thx
</comment><comment author="javanna" created="2015-05-08T10:07:13Z" id="100181821">We discussed this during fixit-friday, we shouldn't go and change the log level for those existing lines, because they get logged for every single cluster state updates, which would make logs way too big at INFO level. But we can add additional log lines specifically to the create/remove index template code though if needed e.g. `MetaDataIndexTemplateService` similar to what `MetaDataCreateIndexService` does when an index gets created. We should do that for both `removeTemplates` &amp; `putTemplate` methods. I think this is reasonable and wouldn't cause problems at INFO level, assuming that index templates don't get changed all the time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `ElasticsearchIllegalArgumentException` and `ElasticsearchIllegalStateException` in favor of the JDK one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10794</link><project id="" key="" /><description>These exceptions are obsolete and our REST layer handles the response code just fine now.
</description><key id="70768215">10794</key><summary>Remove `ElasticsearchIllegalArgumentException` and `ElasticsearchIllegalStateException` in favor of the JDK one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T18:56:12Z</created><updated>2015-06-08T08:49:02Z</updated><resolved>2015-04-29T08:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-24T19:24:24Z" id="96041425">@s1monw can we remove `Preconditions.checkNotNull` in favor of the JDK `Objects.requireNonNull` also?
</comment><comment author="s1monw" created="2015-04-24T19:24:50Z" id="96041511">IMO we can
</comment><comment author="dadoonet" created="2015-04-25T08:33:25Z" id="96157611">Could we have any issue with the transport layer if we change the JDK version and if JDK does not serialize the same way those exceptions?
</comment><comment author="s1monw" created="2015-04-28T09:19:27Z" id="96985335">&gt; Could we have any issue with the transport layer if we change the JDK version and if JDK does not serialize the same way those exceptions?

@dadoonet that makes no difference we also use the same serialization no matter if the exception is in our code or not.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/PrefixAnalyzer.java</file><file>src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalArgumentException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalStateException.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/action/ActionFuture.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestValidationException.java</file><file>src/main/java/org/elasticsearch/action/ThreadingModel.java</file><file>src/main/java/org/elasticsearch/action/WriteConsistencyLevel.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStage.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotShardsStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/Alias.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchType.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>src/main/java/org/elasticsearch/action/support/IndicesOptions.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlockLevel.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RestoreMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>src/main/java/org/elasticsearch/cluster/routing/ShardRoutingState.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocationCommands.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/Booleans.java</file><file>src/main/java/org/elasticsearch/common/ParseField.java</file><file>src/main/java/org/elasticsearch/common/PidFile.java</file><file>src/main/java/org/elasticsearch/common/Preconditions.java</file><file>src/main/java/org/elasticsearch/common/Priority.java</file><file>src/main/java/org/elasticsearch/common/RandomBasedUUIDGenerator.java</file><file>src/main/java/org/elasticsearch/common/Table.java</file><file>src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java</file><file>src/main/java/org/elasticsearch/common/breaker/CircuitBreaker.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java</file><file>src/main/java/org/elasticsearch/common/cli/CliTool.java</file><file>src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>src/main/java/org/elasticsearch/common/component/Lifecycle.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllEntries.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java</file><file>src/main/java/org/elasticsearch/common/recycler/DequeRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/NoneRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recyclers.java</file><file>src/main/java/org/elasticsearch/common/regex/Regex.java</file><file>src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>src/main/java/org/elasticsearch/common/unit/ByteSizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>src/main/java/org/elasticsearch/common/util/BloomFilter.java</file><file>src/main/java/org/elasticsearch/common/util/LocaleUtils.java</file><file>src/main/java/org/elasticsearch/common/util/MultiDataPathUpgrader.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/CountDown.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsAbortPolicy.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/SizeBlockingQueue.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/UncategorizedExecutionException.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/NotMasterException.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/index/IndexException.java</file><file>src/main/java/org/elasticsearch/index/IndexService.java</file><file>src/main/java/org/elasticsearch/index/VersionType.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>src/main/java/org/elasticsearch/index/analysis/ChineseAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/CommonGramsTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeywordMarkerTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LengthTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/MappingCharFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PathHierarchyTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternCaptureGroupTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternReplaceCharFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternReplaceTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StemmerOverrideTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/TrimTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/TruncateTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/EngineSearcher.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexGeoPointFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/DynamicTemplate.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/AbstractMergePolicyProvider.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFlag.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java</file><file>src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ScoreType.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShardState.java</file><file>src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>src/main/java/org/elasticsearch/index/similarity/AbstractSimilarityProvider.java</file><file>src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java</file><file>src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslogFile.java</file><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/HotThreads.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>src/main/java/org/elasticsearch/rest/RestController.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/put/RestIndexPutAliasAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/script/RestPutIndexedScriptAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/script/NativeScriptEngineService.java</file><file>src/main/java/org/elasticsearch/script/Script.java</file><file>src/main/java/org/elasticsearch/script/ScriptContext.java</file><file>src/main/java/org/elasticsearch/script/ScriptContextRegistry.java</file><file>src/main/java/org/elasticsearch/script/ScriptMode.java</file><file>src/main/java/org/elasticsearch/script/ScriptModes.java</file><file>src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/script/expression/ExpressionScript.java</file><file>src/main/java/org/elasticsearch/search/MultiValueMode.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java</file><file>src/main/java/org/elasticsearch/search/aggregations/InternalMultiBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BestBucketsDeferringCollector.java</file></files><comments><comment>Remove ElasticsearchIAE and ElasticsearchISE</comment></comments></commit></commits></item><item><title>Make `node.master` a dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10793</link><project id="" key="" /><description>It would be nice to add dedicated master nodes to an existing cluster without requiring a full restart of each node in the cluster. To accomplish this, the `node.master` would need to be dynamically configurable.

Changing the `node.master` setting at runtime would **not** force a new master election. It would only apply to the cluster moving forward. To force a master election, the current master node would need to be restarted as is currently the case today.
</description><key id="70746185">10793</key><summary>Make `node.master` a dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TwP</reporter><labels><label>:Settings</label><label>adoptme</label><label>discuss</label></labels><created>2015-04-24T17:26:20Z</created><updated>2016-07-22T12:52:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T18:29:02Z" id="96026089">@TwP we will discuss this internally but it might take a while to get here, just a headsup
</comment><comment author="clintongormley" created="2015-04-26T17:14:25Z" id="96410062">I like the idea.  That said:

&gt; It would be nice to add dedicated master nodes to an existing cluster without requiring a full restart of each node in the cluster.

You currently only need to restart the node you're promoting to master, unless I misunderstand you?
</comment><comment author="dakrone" created="2015-04-26T22:11:29Z" id="96439783">&gt; You currently only need to restart the node you're promoting to master, unless I misunderstand you?

Let's say you start with a 9-node cluster with `minimum_master_nodes` set to 5, then you decide at some point you want to move to dedicated master nodes. If the `node.master` setting were dynamic, you could add 3 nodes with `node.master: true` and update `node.master: false` on the other 9 nodes (also updating `minimum_master_nodes` to 2). Then you could (optionally) bounce the current master node and have one of the dedicated master nodes take over.

Right now you would have to restart each of the 9 data nodes (which stinks if you have a lot of data) in order to mark them all as non-master-eligible, because the `node.master` setting can't be dynamically changed.
</comment><comment author="clintongormley" created="2015-04-27T08:53:26Z" id="96572876">OK.  I can see one issue here.  If you change the setting dynamically and you don't update the config file,  and the node reboots (expectedly or unexpectedly), it'll pick up the `node.master` setting from the config file.  

Currently, all node-specific (as opposed to cluster-wide) settings are set at node startup only. We have no API to set node settings, and nowhere to persist them.
</comment><comment author="clintongormley" created="2015-04-27T08:53:52Z" id="96572936">Isn't the better solution for this problem fixing the slow restarts?
</comment><comment author="dakrone" created="2015-04-27T19:35:37Z" id="96791094">&gt; Isn't the better solution for this problem fixing the slow restarts?

I don't think so, (don't get me wrong, fixing slow restarts would be fantastic), but I still think it is nice to be able to make changes to `node.master` without requiring a restart.
</comment><comment author="rjernst" created="2015-04-27T20:13:48Z" id="96802333">A real world scenario where you may want this is when transitioning to a new set of nodes in a cloud environment.  You would want to nicely transition to those new nodes, but you need to force the system to transition off of the old nodes.  While killing the old nodes should work, this makes the exceptional case (ie master dies) the path taken, which shouldn't be necessary.  There should be a clean way to transition from one set of master nodes to another set.
</comment><comment author="s1monw" created="2015-04-28T09:09:44Z" id="96983874">This kind of scenario is something that wonders me for a while now. You have a setup where you realize it`s not optimal ie. want to move toe dedicated master nodes etc. But you have to change this node level setting and bounce processes, do recoveries etc. set minimum master nodes what have you. (I bet folks miss at least on important step here regularly). I wonder if we should have a dedicated API that bakes the cluster. Ie you wanna move to 3 master nodes from a 9 node cluster you would have ideally a simple API call like this:

``` JSON
PUT /_cluster/bake
{
  "master_nodes" : [ "master_1", "master_2", "master_3"],
  "set_minimum_master_nodes" : "true|false #optional true by default", 
  "force" : "true|false #false by default to barf if something is not safe (ie. only one master)"
}
```

with this call we basically move away form everything is dynamic and ignore other master nodes even if they are master eligible. It will force a new election if the current  master is not in the list. This might also help use with some safety mechanisms or allows us to harden master election since we know the nodes now and the list is not dynamic anymore.  I am not an expert on this but I wanted to throw out the idea here...
</comment><comment author="clintongormley" created="2015-04-28T09:45:55Z" id="96991753">You still have to deal with what happens if any nodes reboot unexpectedly.  Do they take their local `elasticsearch.yml` into account, or just use whatever setting is current in the cluster?  If the latter, then the settings applied via the API should always override the local settings (which is confusing if you see settings from the yaml not being applied).  What happens if three ex-masters reboot, and can't see the rest of the cluster initially?  They'd use their local settings and form their own cluster.
</comment><comment author="TwP" created="2015-04-29T15:04:54Z" id="97458417">@clintongormley you bring up a very valid point. However, that problem is not isolated to this proposed change to the `node.master` eligibility setting. All dynamically configurable settings have this same problem. Although with the `node.master` eligibility the consequences could be much more severe as you pointed out.

The first step is making the setting dynamically configurable. Changing the master eligibility via the API is one route for setting this value. The other route is to update the `elasticsearch.yaml` configuration file and then signal the running process to reload settings from the configuration file. Static settings are ignored. Dynamic settings are updated accordingly.

How dynamic settings are handled across restarts is an orthogonal problem. It should not prevent this feature from being implemented. However, taking the broader view will definitely highlight problems that can be introduced by making master eligibility configurable at runtime.
</comment><comment author="ml4spark" created="2015-05-20T08:01:39Z" id="103801173">I just ran into the situation of having to restart a full production cluster to enable this setting and hence would've loved it we could change that dynamically.
Please keep in mind that if we change the master node (e.g. going from a cluster where each node can be a master to a cluster with a smaller number of dedicated masters) we also need to change/adapt other settings (such as minimum_master_nodes) dynamically.

I dont see an issue with the fact that the config file may have outdated information. We need to keep the config file updated with other settings as well as we evolve our cluster to accomodate for node failures/restarts. Being able to change this dynamically would however avoid the "yellow" cluster state and reduced redundancy that happens when a node (all nodes) needs to be restarted while indexing is going on as its indexes need to be brought up to speed. 
</comment><comment author="rjsm" created="2015-11-02T21:16:36Z" id="153158867">I would assume any cluster that this would be an issue on is under config management.  You can update the elasticsearch.yml under the node, and then tell to not be master eligible.  If it unexpectedly restarts, it'll pick up the intended configuration from the file. I'm transitioning my cluster to dedicated masters here shortly. 
</comment><comment author="yehosef" created="2015-12-14T08:42:31Z" id="164379009">+1  - this would be great.  I can change the elasticsearch.yml file behind the scenes to handle a reboot - But I shouldn't have to restart the node to do it.

So I assume if I set node.master = false on a machine that's currently the master the cluster would start a new master selection process?   I think it's important to be able to manually force a master - or migrate a master when I know a machine is going down.  Even though the cluster will rebound, there is no reason to enter a failover state and any risks that could involve, when I don't need to.
</comment><comment author="charlesmims" created="2016-01-26T17:40:56Z" id="175136676">Would a possible solution to this problem be not necessarily making node.master a dynamic setting, but among the nodes which are master eligible, providing a way to force election of a particular node as the master?  Perhaps a transient cluster API setting?
</comment><comment author="pkusnail" created="2016-07-22T08:08:52Z" id="234481016">I really love it if a non-master node in a cluster can be dynamically upgrade to a  master eligible node ,  in order to prevent potential   problems such as split brain  , we set only one master eligible node in a cluster  which is the master of the cluster for sure , and we add supervisor process which can restart the master node with 20 seconds, but if the node is physically damaged or power off ,  it is better that we can dynamically upgrade  a node (such as client node ) to be  master of the cluster.
</comment><comment author="bleskes" created="2016-07-22T12:52:42Z" id="234535547">&gt; we set only one master eligible node in a cluster which is the master of the cluster for sure
&gt; if the node is physically damaged or power off , it is better that we can dynamically upgrade a node

The right solution for this problem is having at least 3 master nodes and properly configure minimum master nodes to 2. 

The issue is requesting for a  master transition which doesn't require a 3 second period with no master, which is different.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disable security manager when running with `mvn exec:exec`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10792</link><project id="" key="" /><description /><key id="70739876">10792</key><summary>Disable security manager when running with `mvn exec:exec`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T17:01:25Z</created><updated>2015-06-08T13:32:25Z</updated><resolved>2015-04-24T17:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-24T17:02:38Z" id="95993060">+1, we only need it for master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TotalHitCountCollector message for top hits aggregation can be improved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10791</link><project id="" key="" /><description>If you attempt to specify a size of 0 or -1 for a top hits aggregation, you will get this error:

```
IllegalArgumentException[numHits must be &gt; 0; please use TotalHitCountCollector if you just need the total hit count
```

Probably not a good idea to return all hits to begin with, but it will be nice to return a more intuitive message if one does attempt to set this to 0 or -1, etc..
</description><key id="70739000">10791</key><summary>TotalHitCountCollector message for top hits aggregation can be improved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-24T16:56:35Z</created><updated>2016-05-25T20:13:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T17:09:18Z" id="96409833">@ppf2 can you suggest one?
</comment><comment author="martijnvg" created="2015-05-01T09:42:19Z" id="98086049">I think if `size` is set to `0` we should just return a `hits` that only contains a total count and if a negative value is specified we should fail with a parse exception or something like that.
</comment><comment author="ppf2" created="2015-05-01T18:06:00Z" id="98189830">I agree, it makes sense to allow size=0, and a parse error for size &lt; 0 is helpful.  Maybe indicate in the error message what is the range of size that is allowed (or even warn users about setting size to something very large like integer max, etc..)
</comment><comment author="jpountz" created="2015-09-27T22:32:06Z" id="143598976">When counting when the size is 0 is what we do for regular search operations. However for aggregations, you already know the match count anyway thanks to the parent aggregation, to I think it's right to raise an error is `size` is less than or equal to 0 as it's almost certainly a user error.
</comment><comment author="linkwoman" created="2016-05-25T20:05:03Z" id="221690721">This would be useful to have size:0 allowed.  Why? Because I want the max_score for each bucket which I can only get when I return at least 1 top_hit.  I do not want to see the top_hit - I just want a report of all buckets, doc counts in each bucket, and max score.  Is there another way to get max_score of each bucket?  Also, I need min_score.  Is there a way to get that @clintongormley ?  
thanks.
</comment><comment author="jpountz" created="2016-05-25T20:13:12Z" id="221692941">@linkwoman you could use a max aggregation on a script that just consists of `_score` for that. This would also be much more efficient.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable Lucene ranking behaviour for numeric term queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10790</link><project id="" key="" /><description>This changes the default ranking behaviour of single-term queries on numeric fields to use the usual Lucene TermQuery scoring logic rather than a constant-scoring wrapper.

Closes #10628
</description><key id="70734502">10790</key><summary>Enable Lucene ranking behaviour for numeric term queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T16:34:24Z</created><updated>2015-06-07T16:54:06Z</updated><resolved>2015-04-27T09:06:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-24T16:37:23Z" id="95987238">@clintongormley Please can you review this?
The Java change is trivial but there's a note in the migration docs that feels like the important bit that could do with your approval
</comment><comment author="clintongormley" created="2015-04-24T17:54:58Z" id="96012585">LGTM
</comment><comment author="jpountz" created="2015-04-25T19:29:28Z" id="96267671">LGTM
</comment><comment author="markharwood" created="2015-04-27T09:06:54Z" id="96576260">Pushed to master https://github.com/elastic/elasticsearch/commit/1b8b9939124a460e0844e57066b78466539d57c0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove workdir, thats unused</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10789</link><project id="" key="" /><description>We don't need to keep around code for something we are not using.

If we ever need it, we can just revert this commit instead of maintaining dead code.
</description><key id="70725120">10789</key><summary>Remove workdir, thats unused</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-24T15:52:20Z</created><updated>2015-06-08T13:32:56Z</updated><resolved>2015-04-24T16:09:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-24T16:09:53Z" id="95981064">duplicate of #10672
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Integration testing framework overrides System.out which breaks logging output to console</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10788</link><project id="" key="" /><description>First of all in a separate Thread `org.apache.lucene.util.TestRuleLimitSysouts` gets loaded:

```
at org.apache.lucene.util.TestRuleLimitSysouts.&lt;clinit&gt;(TestRuleLimitSysouts.java:79)
at org.apache.lucene.util.LuceneTestCase.&lt;clinit&gt;(LuceneTestCase.java:552)
at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:35)
at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
at java.lang.Thread.run(Thread.java:745)
```

It contains following static init block:

```
  /**
   * We capture system output and error streams as early as possible because
   * certain components (like the Java logging system) steal these references and
   * never refresh them.
   * 
   * Also, for this exact reason, we cannot change delegate streams for every suite.
   * This isn't as elegant as it should be, but there's no workaround for this.
   */
  static {
    System.out.flush();
    System.err.flush();

    final String csn = Charset.defaultCharset().name();
    capturedSystemOut = new DelegateStream(System.out, csn, bytesWritten);
    capturedSystemErr = new DelegateStream(System.err, csn, bytesWritten);

    System.setOut(capturedSystemOut.printStream);
    System.setErr(capturedSystemErr.printStream);
  }
```

As you can see `System.out` is overridden here. 
The problem with `DelegateStream capturedSystemOut` which replaces original stream is that it also limits number of bytes that could be output:

```
private void validateClassAnnotations() {
    Class&lt;?&gt; target = RandomizedTest.getContext().getTargetClass();
    if (target.isAnnotationPresent(Limit.class)) {
      int bytes = target.getAnnotation(Limit.class).bytes();
      if (bytes &lt; 0 || bytes &gt; 1 * 1024 * 1024) {
        throw new AssertionError("The sysout limit is insane. Did you want to use "
            + "@" + LuceneTestCase.SuppressSysoutChecks.class.getName() + " annotation to "
            + "avoid sysout checks entirely?");
      }
    }
  }
```

and somehow removes line-ending chars (\n) so that all logging output which goes to console consists of many logging event clued together into one big line.

Since original output stream is never returned back it also affects all output which happens in the same JVM - e.g. log output formatting of other integration tests is broken.
</description><key id="70710439">10788</key><summary>Integration testing framework overrides System.out which breaks logging output to console</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Unisay</reporter><labels><label>discuss</label><label>test</label></labels><created>2015-04-24T14:55:45Z</created><updated>2016-01-18T13:02:37Z</updated><resolved>2016-01-18T13:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T17:05:55Z" id="96409688">Why not just use the annotation to suppress that check?  eg  https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/index/codec/postingformat/ElasticsearchPostingsFormatTest.java#L36
</comment><comment author="Unisay" created="2015-04-27T13:23:34Z" id="96651300">@clintongormley 
Because code in `TestRuleLimitSysouts` which checks if annotation present is never executed when my test extends `ElasticsearchIntegrationTest`:

```
  // This method is not run 
  protected boolean isEnforced() {
    Class&lt;?&gt; target = RandomizedTest.getContext().getTargetClass();

    if (LuceneTestCase.VERBOSE || 
        LuceneTestCase.INFOSTREAM ||
        target.isAnnotationPresent(SuppressSysoutChecks.class)) {
      return false;
    }

    if (!target.isAnnotationPresent(Limit.class)) {
      return false;
    }

    return true;
  }
```

because this method in `TestRuleLimitSysouts`:

```
  @Override
  protected void before() throws Throwable {
    if (isEnforced()) {
      checkCaptureStreams();
    }
    ...
  }
```

is also never run, 
because `ElasticsearchIntegrationTest` doesn't extend `LuceneTestCase`
</comment><comment author="clintongormley" created="2015-04-27T13:54:11Z" id="96660753">@javanna do you have any suggestions here?
</comment><comment author="bbednarek" created="2015-06-22T15:49:20Z" id="114160179">@clintongormley @javanna any updates on this?
</comment><comment author="clintongormley" created="2016-01-17T17:52:53Z" id="172358498">@rjernst what's your take on this?
</comment><comment author="rjernst" created="2016-01-17T18:59:45Z" id="172366145">I'm not sure this is an issue anymore. ESIntegTestCase now (indirectly, through ESTestCase) extends LuceneTestCase.
</comment><comment author="clintongormley" created="2016-01-18T13:02:37Z" id="172521935">thanks @Unisay - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unbox Tuple at call sites.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10787</link><project id="" key="" /><description>See #10784 for an example doing this for Bootstrap class. IMO this is an easy win for readability of the code and it would be good to do it everywhere.

Unboxing these immediately gives the developer back control of the variable and parameter names. Even when the name is simple like 'settings' it is better than 'v1' or 'v2'. It tells something, as opposed to 'newSettings' or 'oldSettings'. 'v1' and 'v2' are never good names.
</description><key id="70704507">10787</key><summary>Unbox Tuple at call sites.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2015-04-24T14:27:40Z</created><updated>2015-08-27T08:57:26Z</updated><resolved>2015-08-27T08:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T18:49:10Z" id="96032561">++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/ParsedScrollId.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/ScrollIdForNode.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollScanAction.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperForType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsSubSearchContext.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Remove usage of tuple as a method parameter</comment></comments></commit></commits></item><item><title>Wait for required mappings to be available on the replica before indexing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10786</link><project id="" key="" /><description>Due to timing issues, mappings that are required to index a document might not
be available on the replica at indexing time. In that case the replica starts
listening to cluster state changes and re-parses the document until no dynamic
mappings updates are generated.
</description><key id="70704457">10786</key><summary>Wait for required mappings to be available on the replica before indexing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T14:27:28Z</created><updated>2015-06-08T09:54:54Z</updated><resolved>2015-04-24T20:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T14:28:55Z" id="95948950">@s1monw I assigned it to you.
</comment><comment author="s1monw" created="2015-04-24T18:43:50Z" id="96030009">i left a bunch of comments, the only thing that I am concerned is not having a timeout... somehow we need to figure this out what to do if we pile up lots of requests? Maybe we can open a blocker issue to follow up on this?
</comment><comment author="jpountz" created="2015-04-24T19:38:25Z" id="96043595">I pushed a new commit. Regarding the timeout, you know this code better than me, I'm not as much aware of the consequences as you are. However @bleskes seemed to consider that having no timeout was better (please correct me if I misunderstood). For the record, having no timeout also allows to not call `System.currentTimeMillis` in ClusterStateObserver.
</comment><comment author="s1monw" created="2015-04-24T19:42:33Z" id="96044199">Looks good on my end, I think we should push as it is and open a followup to re-evaluate the approach. Makes sense?
</comment><comment author="jpountz" created="2015-04-24T20:06:49Z" id="96048693">I opened https://github.com/elastic/elasticsearch/issues/10797
</comment><comment author="jpountz" created="2015-04-28T09:49:03Z" id="96992456">For the record I tagged as `non-issue` since it's fixing a non released issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionWriteResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/Strings.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java</file><file>src/test/java/org/elasticsearch/indices/state/RareClusterStateTests.java</file></files><comments><comment>Merge pull request #10786 from jpountz/fix/dynamic_mappings_on_replicas</comment></comments></commit></commits></item><item><title>Sampler agg could not be used with Terms agg’s order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10785</link><project id="" key="" /><description>The Sampler agg was not capable of collecting samples for more than one parent bucket.
Added a Junit test case and changed BestDocsDeferringCollector to internally maintain collections per parent bucket.

Closes #10719
</description><key id="70703768">10785</key><summary>Sampler agg could not be used with Terms agg’s order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T14:25:58Z</created><updated>2015-06-07T17:45:28Z</updated><resolved>2015-05-22T14:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-24T14:26:50Z" id="95948467">@jpountz + @colings86 suggested reviewers if you have the time please :)
</comment><comment author="colings86" created="2015-04-29T09:09:12Z" id="97361764">@markharwood left a couple of small comments
</comment><comment author="markharwood" created="2015-05-12T10:56:36Z" id="101235640">Rebased on latest master
</comment><comment author="jpountz" created="2015-05-19T20:50:58Z" id="103664315">Left some minor comments, otherwise it looks good to me
</comment><comment author="markharwood" created="2015-05-22T12:41:31Z" id="104650028">@jpountz  changed to always return 0 for docCount if no prior collections rather than erroring.
I believe that is what you meant here https://github.com/elastic/elasticsearch/pull/10785#discussion_r30645186. Good to go?
</comment><comment author="jpountz" created="2015-05-22T12:42:35Z" id="104650163">This is what I meant indeed! LGTM
</comment><comment author="markharwood" created="2015-05-22T14:47:07Z" id="104679237">Pushed to master in https://github.com/elastic/elasticsearch/commit/8c3500a6760847578245f5c34ea224b036d70d81
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use Tuple only as return value in Bootstrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10784</link><project id="" key="" /><description>I wanted to raise this separately (it impacts more of the code than just here). I think in general this Tuple should just be used only as a return-value mechanism and then unboxed by calling code. Otherwise we are stuck working with "variable" names v1 and v2, which can make code hard to follow: you only have types and no context about the instances.
</description><key id="70700574">10784</key><summary>Use Tuple only as return value in Bootstrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T14:13:43Z</created><updated>2015-06-08T13:33:19Z</updated><resolved>2015-04-24T14:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-24T14:15:56Z" id="95945720">LGTM, ++
</comment><comment author="mikemccand" created="2015-04-24T14:16:23Z" id="95945808">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file></files><comments><comment>Merge pull request #10784 from rmuir/reformat_bootstrap</comment></comments></commit></commits></item><item><title>Support * wildcard to retrieve stored fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10783</link><project id="" key="" /><description>Currently you can retrieve stored fields of a search hit using the [fields](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-fields.html) option.
However you cannot use wildcards.

Suppose I have document with several title fields such as "book_title", "chapter_title", "paragraph_title", "section_title" and I want to have all those title fields returned from a query.

It would be nice to have something like this:

```
{
    "fields" : [ "*_title" ],
    "query" : {
        "term" : { "category" : "quote" }
    }
}
```

No need to specify all the fields and when new title fields are added they are automatically returned.
Wildcard \* is already supported when specifying the fields in a multi_match query or when specifying the fields to highlight for example.

A current workaround is using [source-filtering](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html), which does allow the \* wildcard. 

```
{
    "_source": "*_title" ,
    "query" : {
        "term" : { "category" : "quote" }
    }
}
```
</description><key id="70692664">10783</key><summary>Support * wildcard to retrieve stored fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lvernaillen</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-24T13:36:31Z</created><updated>2015-11-30T10:01:21Z</updated><resolved>2015-11-30T10:01:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T16:52:16Z" id="96407721">Agreed.  Btw, using `_source` is the preferred way of doing this, rather than using stored fields to store all of your fields a second time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/AllFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/CustomFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractGeoTestCase.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java</file><file>plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file></files><comments><comment>Fixes #14489</comment><comment>  Do not to load fields from _source when using the `fields` option.</comment><comment>  Non stored (non existing) fields are ignored by the fields visitor when using the `fields` option.</comment></comments></commit></commits></item><item><title>Update the experimental annotations in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10782</link><project id="" key="" /><description>Changes in this PR:
- Removed the docs for `index.compound_format` and `index.compound_on_flush` - these are expert settings which should probably be removed (see https://github.com/elastic/elasticsearch/issues/10778)
- Removed the docs for `index.index_concurrency` - another expert setting
- Labelled the segments verbose output as experimental
- Marked the `compression`, `precision_threshold` and `rehash` options as experimental in the cardinality and percentile aggs
- Improved the experimental text on `significant_terms`, `execution_hint` in the terms agg, and `terminate_after` param on count and search
- Removed the experimental flag on the `geobounds` agg
- Marked the settings in the `merge` and `store` modules as experimental, rather than the modules themselves
</description><key id="70687692">10782</key><summary>Update the experimental annotations in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T13:15:09Z</created><updated>2015-06-30T12:45:26Z</updated><resolved>2015-04-26T16:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-24T13:15:29Z" id="95930137">@jpountz and @markharwood please review
</comment><comment author="jpountz" created="2015-04-24T13:21:13Z" id="95930992">LGTM
</comment><comment author="clintongormley" created="2015-04-24T13:29:19Z" id="95932239">@jpountz you're good ;)  pushed fixes
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated the experimental annotations in the docs as follows:</comment></comments></commit></commits></item><item><title>Remove fielddata.cache.expire</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10781</link><project id="" key="" /><description>Timed expiry of the fielddata cache is never useful and should be removed.
</description><key id="70683345">10781</key><summary>Remove fielddata.cache.expire</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Cache</label><label>adoptme</label><label>deprecation</label><label>v2.1.0</label></labels><created>2015-04-24T12:57:11Z</created><updated>2015-09-01T07:43:21Z</updated><resolved>2015-09-01T07:43:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-24T17:33:13Z" id="96008102">+1
</comment><comment author="jpountz" created="2015-08-26T19:32:32Z" id="135147352">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Remove the experimental indices.fielddata.cache.expire</comment></comments></commit></commits></item><item><title>Improve request creation for request handler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10780</link><project id="" key="" /><description>Follow up from #10730, we should have another step on the refactoring where we don't use reflection and use another method to generate the request instance, aiming at making requests immutable
</description><key id="70681414">10780</key><summary>Improve request creation for request handler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>blocker</label><label>enhancement</label></labels><created>2015-04-24T12:46:10Z</created><updated>2016-01-10T19:12:33Z</updated><resolved>2015-12-23T08:33:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-23T08:33:42Z" id="166833430">I will close this since in 3.0 we don't use the reflection hacks anymore but use method references
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove deprecated METADATA cluster block level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10779</link><project id="" key="" /><description>This commit removes the deprecated ClusterBlockLevel.METADATA, replaced in #9203 with METADATA_READ and METADATA_WRITE.
</description><key id="70677662">10779</key><summary>Remove deprecated METADATA cluster block level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T12:28:47Z</created><updated>2015-06-08T08:52:09Z</updated><resolved>2015-04-28T08:26:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-24T12:30:27Z" id="95918090">@javanna it refers to [your comment](https://github.com/elastic/elasticsearch/pull/9203#commitcomment-10875679) in #9203. Can you please have a look? thanks a lot!
</comment><comment author="javanna" created="2015-04-28T07:53:36Z" id="96966032">left a small comment
</comment><comment author="tlrx" created="2015-04-28T08:18:52Z" id="96969772">@javanna I just updated the code following your comment
</comment><comment author="javanna" created="2015-04-28T08:19:52Z" id="96970270">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove compound format index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10778</link><project id="" key="" /><description>The `index.compound_format` and `index.compound_on_flush` settings shouldn't be changeable any more.  We should remove these settings.
</description><key id="70673975">10778</key><summary>Remove compound format index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Settings</label><label>deprecation</label><label>v5.0.0-alpha1</label></labels><created>2015-04-24T12:15:50Z</created><updated>2016-03-04T18:11:24Z</updated><resolved>2016-01-17T17:49:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:49:26Z" id="172358190">Closed by #15594
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add validation method to QueryBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10777</link><project id="" key="" /><description>As part of #10669 we discussed adding the ability to validation capability to the refactored QueryBuilders. This could be a new method in the QueryBuilder interface that all queries have to implement and that can be called either after parsing (on coordinating node in the future), before or after the query gets serialized to other nodes or by hooking into the existing request validation mechanism (ActionRequest#validate).
</description><key id="70660900">10777</key><summary>Add validation method to QueryBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-24T11:11:05Z</created><updated>2015-09-14T18:04:56Z</updated><resolved>2015-09-14T18:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-04-24T11:14:15Z" id="95898804">Thoughts by @javanna: https://github.com/elastic/elasticsearch/pull/10669#discussion_r29000261
</comment><comment author="cbuescher" created="2015-09-14T18:04:56Z" id="140162105">We have a validate() in most query builders, although unused. Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Investigate more generic handling of `queryName` field in QueryBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10776</link><project id="" key="" /><description>Working on #10669 the issue came up that most of the queries implementing "QueryBuilder" can be named queries supporting the `_name` parameter and also have internal `queryName` field. When constructing the actual lucene query this is used to store the resulting named query in the QueryParseContext. We should try to think about a more generic way to handle this for all queries.

From the 37 queries currently implementing QueryBuilder only 7 don't support having a "queryName":
- BoostingQB
- ConstantScoreQB
- FunctionScoreQB
- MatchAllQB
- SpanMultiTermQB
- TemplateQB
- WrapperQB
</description><key id="70659883">10776</key><summary>Investigate more generic handling of `queryName` field in QueryBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-24T11:03:52Z</created><updated>2015-07-01T16:01:30Z</updated><resolved>2015-07-01T16:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-24T17:54:01Z" id="96012391">I think all queries should support `_name` for the sake of completeness
</comment><comment author="clintongormley" created="2015-04-26T17:11:01Z" id="96409909">agree with @dakrone - if a query doesn't support it, it's a bug
</comment><comment author="javanna" created="2015-04-28T14:31:58Z" id="97085420">+1 on supportong `_name` across the board. As part of the query refactoring we should indeed make this more generic, to make sure we don't forget about it for some queries.
</comment><comment author="javanna" created="2015-05-13T09:28:48Z" id="101590485">I think we should do the same with `boost` rather than repeating it for every query. We should support boost across the board too no?
</comment><comment author="javanna" created="2015-07-01T16:01:27Z" id="117727796">Closed via #11974 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query refactoring: unify boost and query name</comment></comments></commit></commits></item><item><title>use sync id when recovering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10775</link><project id="" key="" /><description>Skip phase 1 of recovery in case an identical sync id was found on primary and replica. Relates to #10032
</description><key id="70657637">10775</key><summary>use sync id when recovering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-04-24T10:54:10Z</created><updated>2015-05-04T15:09:01Z</updated><resolved>2015-05-04T15:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-24T14:31:24Z" id="95949413">looks good. left some comments as discussed.
</comment><comment author="brwe" created="2015-04-29T09:14:56Z" id="97365421">Thanks for the review! I addressed all comments now. I found that GatewayAllocator has some heuristic built in that checks how much of a shard on a node coincides with the primary before assigning replicas and added a check for sync ids there as well. Let me know if that makes sense.
</comment><comment author="s1monw" created="2015-04-29T09:26:36Z" id="97367853">this looks very good I left some minor comments
</comment><comment author="s1monw" created="2015-04-29T09:27:25Z" id="97367964">&gt; I found that GatewayAllocator has some heuristic built in that checks how much of a shard on a node coincides with the primary before assigning replicas and added a check for sync ids there as well. Let me know if that makes sense.

it does make a lot of sense to me
</comment><comment author="brwe" created="2015-04-30T14:29:15Z" id="97818536">Thanks for the review! Addressed all comments.
</comment><comment author="s1monw" created="2015-04-30T19:51:22Z" id="97947724">left one comment other than that LGTM
</comment><comment author="clintongormley" created="2015-05-04T10:25:25Z" id="98671135">@brwe can you add a description to the PR plus a link to the main issue?
</comment><comment author="brwe" created="2015-05-04T14:22:28Z" id="98725573">Pushed another commit.
</comment><comment author="s1monw" created="2015-05-04T14:46:50Z" id="98737012">it looks good to me to go on the feature branch... I left two minor comments.
</comment><comment author="brwe" created="2015-05-04T15:03:15Z" id="98741547">added another commit with docs
</comment><comment author="s1monw" created="2015-05-04T15:03:55Z" id="98741751">++ go push
</comment><comment author="brwe" created="2015-05-04T15:09:00Z" id="98743202">pushed to https://github.com/elastic/elasticsearch/tree/feature/synced_flush
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file><file>src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTest.java</file></files><comments><comment>use sync id when recovering</comment></comments></commit></commits></item><item><title>CommitStats doesn't need to allow for null values in commit user data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10774</link><project id="" key="" /><description>Lucene forbids writing those and MapBuilder.immutableMap doesn't like them either, as discovered by @brwe 

see #10687 
</description><key id="70656965">10774</key><summary>CommitStats doesn't need to allow for null values in commit user data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T10:51:00Z</created><updated>2015-05-29T15:54:56Z</updated><resolved>2015-04-24T18:59:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-24T10:51:09Z" id="95891425">@brwe can you take a look?
</comment><comment author="brwe" created="2015-04-24T10:54:46Z" id="95892335">+1
</comment><comment author="dakrone" created="2015-04-24T17:54:37Z" id="96012512">@bleskes ping
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/CommitStats.java</file></files><comments><comment>CommitStats doesn't need to allow for null values in commit user data</comment></comments></commit></commits></item><item><title>Simplify IndexStore and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10773</link><project id="" key="" /><description>Today we have a lot of bloat in the IndexStore and related classes. THe IndexStore interface
is unneeded as we always subclass AbstractIndexStore and it hides circular dependencies
that are problematic when added. Guice proxies them if you have an interface which is bad in
general. This commit removes most of the bloat classes and unifies all the classes we have
into a single one since they all just structural and don't encode any functionality.
</description><key id="70647925">10773</key><summary>Simplify IndexStore and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T10:10:04Z</created><updated>2015-06-06T17:32:43Z</updated><resolved>2015-04-24T11:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T10:56:15Z" id="95892898">@kimchy pushed new commits
</comment><comment author="kimchy" created="2015-04-24T11:09:15Z" id="95897612">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested and Multi-field should not re-use field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10772</link><project id="" key="" /><description>I would like to use spring-data-elastic search as part of a larger data-project. 

While I was pleasantly surprised to see the fine-grained control the @Field annotation is providing I am a bit confused by the repeating field.name repetition as part of the multi-field and nested Types: 

in MappingBuilder.java

```
    private static void addNestedFieldMapping(...) throws IOException {
    builder.startObject(field.getName() + "." + annotation.dotSuffix());
        ...
    }
```

should it not read:

```
   private static void addNestedFieldMapping(...) throws IOException {
      builder.startObject(annotation.suffix());
   }
```
</description><key id="70635221">10772</key><summary>Nested and Multi-field should not re-use field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gamars</reporter><labels /><created>2015-04-24T09:15:34Z</created><updated>2015-04-24T09:41:18Z</updated><resolved>2015-04-24T09:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-24T09:33:28Z" id="95870775">I think you are not opening this issue in the right project.
MapperBuilder is there: https://github.com/spring-projects/spring-data-elasticsearch

Closing.
</comment><comment author="gamars" created="2015-04-24T09:35:17Z" id="95871027">indeed, that is what you get when having too many tabs open. Sorry for the inconvenience :/
</comment><comment author="dadoonet" created="2015-04-24T09:41:18Z" id="95871890">lol! No problem :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term positions in analyze API should start at 1, not 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10771</link><project id="" key="" /><description>Now, Analyze API return 1 as 1st token.
Analyze API should return 0 as 1st token, looks like TermVector.
</description><key id="70633400">10771</key><summary>Term positions in analyze API should start at 1, not 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>breaking</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T09:07:09Z</created><updated>2015-06-06T15:36:34Z</updated><resolved>2015-04-28T08:46:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-24T18:18:30Z" id="96022043">LGTM
</comment><comment author="clintongormley" created="2015-04-26T10:38:10Z" id="96361962">@johtani Let's make this change only master - it's a breaking change.
</comment><comment author="johtani" created="2015-04-27T02:25:14Z" id="96473240">@clintongormley I see. 
I add comment to breaking changes 2.0 doc.
</comment><comment author="johtani" created="2015-04-28T07:32:38Z" id="96960147">@jpountz Fix your comment and add the test .
</comment><comment author="jpountz" created="2015-04-28T07:33:21Z" id="96960649">LGTM. Thanks @johtani !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add multi data.path to migration guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10770</link><project id="" key="" /><description>this commit removes the obsolete settings for distributors and updates
the documentation on multiple data.path. It also adds an explain to the
migration guide.
</description><key id="70630963">10770</key><summary>Add multi data.path to migration guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T08:55:11Z</created><updated>2015-04-29T09:55:26Z</updated><resolved>2015-04-29T09:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-28T20:00:20Z" id="97188937">@clintongormley pushed a new commit
</comment><comment author="clintongormley" created="2015-04-29T07:48:03Z" id="97336323">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/IndexService.java</file><file>src/main/java/org/elasticsearch/index/store/StoreModule.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Add multi data.path to migration guide</comment></comments></commit></commits></item><item><title>Deprecation of facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10769</link><project id="" key="" /><description>According to your documentation facets are deprecated and users are encouraged to switch to aggregations.

But aggregations only deliver approx. counts (in contrast to facets, which's counts are exact)

I'd suggest to keep "facets" but still encourage users to switch to aggreagations and only suggest use of facets where exact count are relevant
</description><key id="70621510">10769</key><summary>Deprecation of facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SaSa1983</reporter><labels><label>feedback_needed</label></labels><created>2015-04-24T08:16:33Z</created><updated>2015-04-26T16:54:04Z</updated><resolved>2015-04-26T16:54:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T13:38:45Z" id="95934817">&gt; But aggregations only deliver approx. counts (in contrast to facets, which's counts are exact)

Are you talking about the `terms` facet/aggregation? In that case they work essentially the same way. The aggregation is actually even better since it has better heuristics to request more terms at the shard level which tends to improve accuracy. Also aggregations report on an error bound since 1.4.

I'm wondering that the confusion might come from the fact that this limitation is documented for aggs for not faceets?
</comment><comment author="clintongormley" created="2015-04-26T16:54:04Z" id="96408443">Yep, this looks like a misunderstanding to me.  Feel free to reopen if this is not the case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mustache tags syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10768</link><project id="" key="" /><description>Hi there. I've been experimenting with the search templates recently and I'm a bit confused. Shouldn't the Mustache tags be written like `{{tagname}}` instead of `{tagname}`? Your using `{{...}}` [here](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html) BTW.

Using the first example in that page  seems to indicate that something's wrong, or am I missing something?

```
$ curl 'localhost:9200/test/_search' -d '{"query":{"template":{"query":{"match":{"text":"{keywords}"}},"params":{"keywords":"value1_foo"}}}}'
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

$ curl 'localhost:9200/test/_search' -d '{"query":{"template":{"query":{"match":{"text":"{{keywords}}"}},"params":{"keywords":"value1_foo"}}}}'
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test","_type":"testtype","_id":"1","_score":1.0,"_source":{"text":"value1_foo"}}]}}
```
</description><key id="70615330">10768</key><summary>Mustache tags syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olivere</reporter><labels><label>docs</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T07:41:51Z</created><updated>2015-04-24T19:23:05Z</updated><resolved>2015-04-24T19:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-04-24T07:51:02Z" id="95837793">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sharing calculations between filter, sort and field scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10767</link><project id="" key="" /><description>For a current project, we have an index containing products which have a complex pricing model. Some price components are stored in the documents, where others are influenced by external parameters. Prices are calculated on the fly by a groovy script which is passed these external parameters from our application. The calculation performed by this script is pretty cpu-intensive and may take several ms per document.

The use case here is "_I have a budget between X and Y. Show me the 10 products that have a price within this range, sorted by price descending, and show me the price buildup (including all price components)_".

Since it doesn't seem to be possible to share information between scripts executed in different query phases, we currently need three groovy scripts in order to achieve this:
- A filter script, performing the entire calculation and returning a boolean indicating whether the calculated price is between X and Y
- A sorting script, performing the entire calculation and returning the total price as a number
- A field script, performing the entire calculation and returning an object containing the total price along with a summary of how the price was calculated.

These scripts are almost exactly alike, only differing in the type of value they return. This creates a lot of overhead. Let's say we have 10000 documents in our index, 5000 of which are eligible for a given price range and parameter set. That means the scripts will be evaluated:
- 10000x for filtering purposes (`script_filter`)
- 5000x for sorting documents that passed through the filter
- 10x for displaying the price components for the top 10 matching products that will actually be returned (`script_field`)

For a grand total of 15010 full calculations. About a third of these (those in sort and display) should not have been necessary because the bulk of what they're calculating has already been found by the filter script.

Ideally, the scripted field would be evaluated first and remain available during the rest of the query scope, so that the filtering and sorting scripts could access its calculated results (or one of its properties) and not have to perform the entire calculation again. This would reduce our total script execution time greatly.

Is there, or will there be, any way to achieve this in elasticsearch?
</description><key id="70611127">10767</key><summary>Sharing calculations between filter, sort and field scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsander</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-04-24T07:23:21Z</created><updated>2017-04-11T15:44:10Z</updated><resolved>2015-08-26T19:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T16:01:42Z" id="96403157">Hi @bsander 

Interesting question.  It would be nice to be able to cache the output of a script, although we'd need to be able to mark it as cacheable (eg can't contain `rand()` etc).

However, the problem as you describe it above can be solved today using the `function_score` query.  The script would run once for every document, and use the calculated price for the `_score`.  Any prices outside the desired range (calculated as part of the script) could be excluded with `min_score`, and the resulting price would be returned in the `score` field, eg:

```
DELETE t

POST t/t/_bulk
{"index":{}}
{"num":1}
{"index":{}}
{"num":2}
{"index":{}}
{"num":3}
{"index":{}}
{"num":4}
{"index":{}}
{"num":5}
{"index":{}}
{"num":6}
{"index":{}}
{"num":7}
{"index":{}}
{"num":8}
{"index":{}}
{"num":9}
{"index":{}}
{"num":10}


GET t/_search
{
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "min_score": 0,
      "boost_mode": "replace", 
      "functions": [
        {
          "script_score": {
            "script": "num = doc['num'].value; if (num &gt;= min_val &amp;&amp; num &lt;= max_val) { return num}; return -1",
            "params": {
              "min_val": 3,
              "max_val": 5
            }
          }
        }
      ]
    }
  }
}
```

returns:

```
"hits": {
  "total": 3,
  "max_score": 5,
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "AUz2cefd6wt16ozAgRtq",
        "_score": 5,
        "_source": {
           "num": 5
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "AUz2cefd6wt16ozAgRtp",
        "_score": 4,
        "_source": {
           "num": 4
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "AUz2cefd6wt16ozAgRto",
        "_score": 3,
        "_source": {
           "num": 3
        }
     }
  ]
}
```
</comment><comment author="bsander" created="2015-05-12T07:28:48Z" id="101166274">Hi @clintongormley, 

Thanks for your thorough response. We've been experimenting with your approach during the last week and indeed it does reduce the time spent calculating prices by quite a margin. It does however complicate other interactions with the content, for instance when sorting by price _ascending_ or sorting by an entirely different field. Where we would normally just change the `sort` directive, we need to change our query in various different places to support sorting by price. Also, queries where price isn't necessarily relevant become much slower than they used to with this method, so we need to watch out for that too.

So, for the sake of maintainability, it would still be much better for us to be able to use cached values from previously executed scripts instead.
</comment><comment author="clintongormley" created="2015-05-15T16:58:11Z" id="102458543">&gt; sorting by price ascending

just add:

```
"sort": { "_score": {"order": "asc" }}
```

&gt; or sorting by an entirely different field

Just do:

```
"sort": [
    { "some_field": { "order": "asc" }},
    { "_score": { "order": "asc" }}
]
```

&gt; Also, queries where price isn't necessarily relevant become much slower than they used to with this method, so we need to watch out for that too.

Yes, you need to send the right query for the right purpose.

&gt; So, for the sake of maintainability, it would still be much better for us to be able to use cached values from previously executed scripts instead.

Elasticsearch has no idea what is happening inside your script.  Even if we were to add a `_cache` flag, then we'd need to know how long to cache it for, how many values we should cache etc, all of which makes the execution more complex, and produces more garbage to use up the heap and require collection.

I'll leave this as discuss, but I think: better to send the right query for the right job.
</comment><comment author="jpountz" created="2015-08-26T19:35:56Z" id="135148169">Closing. As @clintongormley pointed out, this can be improved a lot by just formulating the query differently.
</comment><comment author="gm42" created="2016-09-08T06:18:09Z" id="245504207">Is this advice still valid? I have opened a discussion topic: https://discuss.elastic.co/t/script-result-caching-reusing/60042
</comment><comment author="gm42" created="2016-09-20T13:54:29Z" id="248307889">The discussion I opened would need to be updated now, as I dug deeper into it.

However I just noticed that document score is not accessible in the post_filter, as per #20131
</comment><comment author="dmitry" created="2016-09-24T09:26:21Z" id="249355180">I have the same case, and I don't know how to optimize it with `_score`. Also, as @gm42 noticed #20131 not allows to get `_score` while `post_filter`-ing.

Another question: is there are a way to apply a filter first in a `post_filter` with boolean condition, and then only in case if it will be not filtered out to apply second filter with script filter?
</comment><comment author="dmitry" created="2016-09-24T09:41:17Z" id="249355829">As I understand here is already 3 ES users, who have very similar use case and since one year here is no solution for this case.

My understanding:
- `_cache` (across multiple ES requests) not allowed for parametrized or non-parametrized script function as they could be undeterministic;
- script functions cannot be cached to be used across one ES request (first in function_score, then in `aggs`, then in `post_filter`, and afterwards in `sort`);
- `_score` not accessible in a `post_filter` (#20131), and nobody know when it could be solved;
- `min_score` is just a tiny optimization, which will not  work in our case (what about `max_score`, why it's missing?);

After listing possible optimization variants I find out there are no solution at the moment to write a good ES query to optimizely get filtered, aggregated and sorted results in a resasonable time frame.

@clintongormley  @jpountz 
Can you please at least suggest some additional optimization practices, which are invisible for us?
</comment><comment author="gm42" created="2016-09-26T07:18:02Z" id="249496454">@dmitry the sad truth might be that we need a different technology when it's necessary to perform custom map/reduce at this level. It's a kind-of compute-search-and-throw-away scenario, rather than everything-indexed scenario which seems better covered by ES.

I need to stick with ES for the time being, so I will keep trying to make it fit for my use case - but some tell-tale signs are evident.

My workaround is to simply get all results (without post_filter on score) and then filter them out on the application side. I know, very inefficient and not scalable, but couldn't find any other approach so far
</comment><comment author="mausch" created="2017-04-11T15:44:10Z" id="293305415">Just found this issue, seems to be very similar to one I create a while ago: https://github.com/elastic/elasticsearch/issues/13469</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Hanging threads with TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10766</link><project id="" key="" /><description>We have had some instability issues with our ES cluster lately, where our application using the Java TransportClient stops responding to requests towards the cluster. Our setup consists of 3 master nodes and 4 data nodes, and our application consists of 5 nodes. The symptoms we are experiencing is that after some time, a situation occurs that freezes most or all ES requests. We have been profiling and digging through thread dumps, and when the freeze occurs we see a lot of these (typically 20+):

```
"AkkaSystem-akka.actor.default-dispatcher-445" - Thread t@540
   java.lang.Thread.State: WAITING
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for &lt;2f9d8510&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:279)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:118)
```

The number of these seems to grow steadily but slowly, which might indicate that not all the requests that are coming in are blocked, but maybe certain requests that reach one particular node in the cluster? What is also interesting is that in the freeze situation, there is alway at least one of these threads hanging:

```
"elasticsearch[Quentin Beck][transport_client_worker][T#1]{New I/O worker #1}" - Thread t@34
   java.lang.Thread.State: WAITING
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for &lt;44a58cd8&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:279)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:118)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
```

So it appears that all our application threads that are blocked are waiting for this single `transport_client` thread to complete. I suspect there are two issues here really. One is that one of the `transport_client` thread is never timed out. And the second is that there seems to be some synchronization going on between all (or many) of the other requests that are blocking on `BaseFuture$Sync`

We are running ES 1.5.0 with OpenJDK 1.8.0_31 on both server and client.
</description><key id="70598012">10766</key><summary>Hanging threads with TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">nilsga</reporter><labels><label>:Java API</label></labels><created>2015-04-24T06:13:01Z</created><updated>2016-08-02T12:28:17Z</updated><resolved>2015-06-04T12:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nilsga" created="2015-04-24T07:28:08Z" id="95834064">Here's a gist with output from the stats/health endpoints: https://gist.github.com/nilsga/cd1f0f037b43340f7ad0
</comment><comment author="kjbekkelund" created="2015-04-24T10:08:25Z" id="95881054">This just occured again, but now with:

```
{"version":"1.8.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"25.31-b07","vm_vendor":"Oracle Corporation","count":1}
```

This time it ran on localhost with one server connecting to one Elasticsearch node (compared to above with our cluster of 5 nodes connecting to 7 elasticsearch nodes).

I've included a thread dump at https://gist.github.com/kjbekkelund/405580860216d1d69b8f
</comment><comment author="clintongormley" created="2015-04-26T16:41:18Z" id="96406220">Wondering if this is related to https://github.com/elastic/elasticsearch/issues/10704
</comment><comment author="clintongormley" created="2015-04-26T16:41:29Z" id="96406228">@spinscale any thoughts here?
</comment><comment author="nilsga" created="2015-04-26T18:29:06Z" id="96418350">I think we have located the trigger of the problem. The thread dump for the `transport_client` thread was missing some interesting information. This is a more complete stack:

```
 elasticsearch[Magneto][transport_client_worker][T#4]{New I/O worker #4} [WAITING] [DAEMON]
*** java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(int) AbstractQueuedSynchronizer.java:1304
*** org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get() BaseFuture.java:279
*** org.elasticsearch.common.util.concurrent.BaseFuture.get() BaseFuture.java:118
*** org.elasticsearch.action.support.AdapterActionFuture.actionGet() AdapterActionFuture.java:45

&lt;some company specific classes&gt;

*** org.elasticsearch.action.support.AbstractListenableActionFuture.executeListener(ActionListener) AbstractListenableActionFuture.java:120
*** org.elasticsearch.action.support.AbstractListenableActionFuture.done() AbstractListenableActionFuture.java:97
*** org.elasticsearch.common.util.concurrent.BaseFuture.set(Object) BaseFuture.java:167
org.elasticsearch.action.support.AdapterActionFuture.onResponse(Object) AdapterActionFuture.java:96
org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onResponse(Object) TransportClientNodesService.java:227
org.elasticsearch.action.TransportActionNodeProxy$1.handleResponse(ActionResponse) TransportActionNodeProxy.java:73
org.elasticsearch.action.TransportActionNodeProxy$1.handleResponse(TransportResponse) TransportActionNodeProxy.java:57
org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(Channel, StreamInput, TransportResponseHandler) MessageChannelHandler.java:163
org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(ChannelHandlerContext, MessageEvent) MessageChannelHandler.java:132
org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(ChannelHandlerContext, ChannelEvent) SimpleChannelUpstreamHandler.java:70
org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline$DefaultChannelHandlerContext, ChannelEvent) DefaultChannelPipeline.java:564
org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(ChannelEvent) DefaultChannelPipeline.java:791
org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(ChannelHandlerContext, Object, SocketAddress) Channels.java:296
org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(ChannelHandlerContext, SocketAddress, Object) FrameDecoder.java:462
org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(ChannelHandlerContext, Channel, ChannelBuffer, SocketAddress) FrameDecoder.java:443
org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(ChannelHandlerContext, MessageEvent) FrameDecoder.java:303
org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(ChannelHandlerContext, ChannelEvent) SimpleChannelUpstreamHandler.java:70
org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline$DefaultChannelHandlerContext, ChannelEvent) DefaultChannelPipeline.java:564
org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(ChannelEvent) DefaultChannelPipeline.java:559
org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channel, Object, SocketAddress) Channels.java:268
org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channel, Object) Channels.java:255
org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(SelectionKey) NioWorker.java:88
org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(Selector) AbstractNioWorker.java:108
org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run() AbstractNioSelector.java:337
org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run() AbstractNioWorker.java:89
org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() NioWorker.java:178
org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() ThreadRenamingRunnable.java:108
org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() DeadLockProofWorker.java:42
java.lang.Thread.run() Thread.java:745
```

So what has happened is that our application code executed a new call to ES within the completion handler of the previous ES call, which in turn is executed on the `transport_client` thread. This is obviously not a good idea, so we have to change this. But still, it would be interesting to know why this would cause (what looks like) a deadlock, and why it impacts other ES requests as well.
</comment><comment author="s1monw" created="2015-04-26T18:35:48Z" id="96418697">I think we this is the same issue https://github.com/elastic/elasticsearch/pull/10644 ?
</comment><comment author="kjbekkelund" created="2015-04-26T18:52:51Z" id="96420088">@s1monw Not sure I've understood that issue correctly, but is this only when there is a connection error? Will we see these connection error in the Elasticsearch logs? I can't seem to find anything in the logs that indicate any issues. 
</comment><comment author="s1monw" created="2015-04-26T18:53:41Z" id="96420283">@kjbekkelund I think this is can happen on all retries of some sort?
</comment><comment author="kjbekkelund" created="2015-04-26T18:58:27Z" id="96420811">@s1monw Ah, of course, `RetryListener`. Btw, this also happenend on `localhost`, so there shouldn't be any connection errors at least. I'm not sure when Elasticsearch retries — we have some update calls, but I can't seem to be able to correlate this error with those calls.
</comment><comment author="nilsga" created="2015-04-26T19:08:13Z" id="96421393">It looks related, but I'm not sure its the same issue. In our case, it is the success handler that has been invoked on the `transport_client` thread.

Edit: Didn't see the responses in between. Yeah, I guess if there have been some retries in between, it could be the same issue.
</comment><comment author="s1monw" created="2015-04-27T15:05:20Z" id="96694080">&gt; It looks related, but I'm not sure its the same issue. In our case, it is the success handler that has been invoked on the transport_client thread.

so IMO what happens here and as well as in #10644 is that we are executing stuff on the transport clients thread which is actually the default behavior. if you want this you can set `#listenerThreaded(true)` on your request then this shouldn't happen anymore. We need to find a better solution for this in the future...
</comment><comment author="nilsga" created="2015-04-27T20:35:04Z" id="96811772">But why the apparent deadlock? 
</comment><comment author="s1monw" created="2015-04-28T08:54:13Z" id="96981246">&gt; But why the apparent deadlock?

I don't know your code but if you execute a blocking network operation on a thread that is supposed to execute this operation as well as handling it's response your are running into a deadlock since you are blocking for the call to come back but there is no thread to handle the response. Does this make sense?
</comment><comment author="nilsga" created="2015-05-06T07:38:10Z" id="99356039">I can see that it is a "typical" deadlock scenario, but I still don't quite get why it only happens some times, and why it seems to block many other (seemingly) unrelated requests while there are many free transport threads available. Anyway, we refactored the problematic code, so it is no longer a problem for us, so if this is "by design", then you can go ahead and close the issue.
</comment><comment author="clintongormley" created="2015-06-04T12:24:36Z" id="108871374">Closed by https://github.com/elastic/elasticsearch/pull/10940
</comment><comment author="makeyang" created="2016-08-01T07:30:03Z" id="236508693">@clintongormley 
I have checked the code and it turns out the pull: https://github.com/elastic/elasticsearch/pull/10644/commits/91e2bb193ce0655ed4e80fede321abcba281caac
 hasn't merged into any code base below:
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
https://github.com/elastic/elasticsearch/blob/v2.0.2/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
https://github.com/elastic/elasticsearch/blob/v2.1.2/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
do I miss something or what?
</comment><comment author="colings86" created="2016-08-02T12:26:56Z" id="236888715">Not sure how you determined that this has not been merged. I can see this commit in some of the lines of the master branch and other branches links you listed above by doing looking at the blame view: https://github.com/elastic/elasticsearch/blame/master/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L272
https://github.com/elastic/elasticsearch/blame/v2.0.2/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L238
https://github.com/elastic/elasticsearch/blame/v2.1.2/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L248

Maybe it would solve your problem quicker if you explained clearly on the discuss forums what issue you are seeing which is leading you to believe this?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Join MergeResults with MergeContext since they are almost the same</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10765</link><project id="" key="" /><description>MergeContext currently exists to store conflicts, and providing
a mechanism to add dynamic fields. MergeResults store the same
conflicts. This change merges the two classes together, as well
as removes the MergeFlags construct.

This is in preparation for simplifying the callback structures
to dynamically add fields, which will require storing the mapping
updates in the results, instead of having a sneaky callback to
the DocumentMapper instance. It also just makes more sense that
the "results" of a merge are conflicts that occurred, along with
updates that may have occurred. For MergeFlags, any future needs
for parameterizing the merge (which seems unlikely) can just be
added directly to the MergeResults as simlulate is with this change.
</description><key id="70584385">10765</key><summary>Join MergeResults with MergeContext since they are almost the same</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T04:33:17Z</created><updated>2015-06-08T09:01:15Z</updated><resolved>2015-04-24T15:20:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T07:32:53Z" id="95834742">LGTM I like it better now!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard stuck Initializing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10764</link><project id="" key="" /><description>On Elasticsearch 1.3.5, one shard went to initializing and wasn't able to be recovered. From the logs we see that there is an Invalid Alias Name exception on a delete by query action for the shard with the problem: 

```
[21:04:05,703][TRACE][action.deletebyquery     ] [Boobytrap] failure on replica [9424d5fc870d4909adb3b96c5fb21bdc][1]
org.elasticsearch.indices.InvalidAliasNameException: [9424d5fc870d4909adb3b96c5fb21bdc] Invalid alias name [7bde63b5d9268c12368a5bc8f52435a3df26490605d68d0ababf29eedc547262], Unknown alias name was passed to alias Filter
        at org.elasticsearch.index.aliases.IndexAliasesService.aliasFilter(IndexAliasesService.java:93)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:452)
        at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:143)
```

```
[21:04:05,704][WARN ][index.engine.internal    ] [Boobytrap] [9424d5fc870d4909adb3b96c5fb21bdc][1] failed engine [deleteByQuery/shard failed on replica]
[21:04:05,720][TRACE][action.index             ] [Boobytrap] failure on replica [9424d5fc870d4909adb3b96c5fb21bdc][1]
org.elasticsearch.index.IndexShardMissingException: [9424d5fc870d4909adb3b96c5fb21bdc][1] missing
        at org.elasticsearch.index.service.InternalIndexService.shardSafe(InternalIndexService.java:184)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:230)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:250)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:229)
```

Later on the it seems that the Elasticsearch tries to recover the shard but this also fails:

```
[21:05:30,481][WARN ][indices.recovery         ] [Boobytrap] [9424d5fc870d4909adb3b96c5fb21bdc][1] recovery from [[Emil Blonsky][yAfL58lmRz-4m4yBLqPotA][polaris-prod-135-w-esnode-brck7y9opyj5l][inet[/10.0.0.97:9300]]{master=false}] failed
org.elasticsearch.transport.RemoteTransportException: [Emil Blonsky][inet[/10.0.0.97:9300]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [9424d5fc870d4909adb3b96c5fb21bdc][1] Phase[2] Execution failed
        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1109)
        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:637)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)
        at org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:465)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:451)

Caused by: org.elasticsearch.transport.RemoteTransportException: [Boobytrap][inet[/10.0.0.94:9300]][index/shard/recovery/translogOps]
Caused by: org.elasticsearch.indices.InvalidAliasNameException: [9424d5fc870d4909adb3b96c5fb21bdc] Invalid alias name [7bde63b5d9268c12368a5bc8f52435a3df26490605d68d0ababf29eedc547262], Unknown alias name was passed to alias Filter
        at org.elasticsearch.index.aliases.IndexAliasesService.aliasFilter(IndexAliasesService.java:93)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:452)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:781)
        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:431)
        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:410)

```
</description><key id="70552671">10764</key><summary>Shard stuck Initializing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msimos</reporter><labels><label>:Translog</label><label>bug</label></labels><created>2015-04-24T00:52:09Z</created><updated>2015-10-08T11:49:51Z</updated><resolved>2015-10-08T11:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T15:40:56Z" id="96401465">Hi @msimos 

It looks like an alias was deleted just before the delete-by-query action arrived in the translog.  I think your only option now is to delete that translog, I'm afraid.

Delete-by-query has a number of issues, and we're planning on removing it in its current form in 2.0 (see https://github.com/elastic/elasticsearch/issues/10067), hopefully to be replaced by something safer.

/cc @mikemccand 
</comment><comment author="markwalkom" created="2015-04-28T00:58:10Z" id="96861677">This appears to be happening a bit on one of our customers clusters.
What can we do to pinpoint and resolve this, or is it a case of not using delete-by?
</comment><comment author="clintongormley" created="2015-10-08T11:49:51Z" id="146514580">This is a case of "don't use delete-by-query", which has been reimplemented in a safer way as a plugin in  2.0.  Only solution here is to delete the transaction log.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent injection of unannotated dynamic settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10763</link><project id="" key="" /><description>Dynamic settings has to be injected into constructor with either @ClusterDynamicSettings or @IndexDynamicSettings. If annotations are not specified an empty instance of Dynamic Settings is injected that can lead to difficult to discover errors such as #10614. This commit will make any attempt to inject unannotated dynamic settings to generate a giuce error.

Alternatively, we could bind unannotated DynamicSettings to cluster dynamic settings by default, this will prevent the issue as well but in more quiet manner. Any other ideas?
</description><key id="70552609">10763</key><summary>Prevent injection of unannotated dynamic settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-24T00:50:59Z</created><updated>2015-06-07T10:15:22Z</updated><resolved>2015-05-08T22:52:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-04T20:30:31Z" id="98842425">LGTM, I am +1 on this change, I think it's good to avoid this, I would love to see us do something similar to this with `@IndexSettings` also (but that's a separate story entirely)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`_default_` mapping should be picked up from index template during auto create index from bulk API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10762</link><project id="" key="" /><description>Fields defined in the `_default_` mapping of an index template should be picked up when an index alias filter is parsed if a new index is introduced when a document is indexed into an index that doesn't exist yet via the bulk api.

PR for #10609
</description><key id="70531182">10762</key><summary>`_default_` mapping should be picked up from index template during auto create index from bulk API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.4.5</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T22:42:36Z</created><updated>2015-06-07T17:54:22Z</updated><resolved>2015-04-24T07:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-23T22:49:07Z" id="95738791">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix default field type for geo_point mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10761</link><project id="" key="" /><description>`geo_point` type defaults as a StringField making the mapping useless without the user specifying a desired index type.  At present the indexed types are:

field as a GeoHash string type
lat/lon as two distinct DoubleFields

FieldData and DocValues are not used. This task is linked with LUCENE-6450 (https://issues.apache.org/jira/browse/LUCENE-6450) to use a NumericLong as the base type for a morton encoded GeoPointField so bounding box, polygon, and distance queries can be improved using an initial NumericRangeQuery.
</description><key id="70522313">10761</key><summary>Fix default field type for geo_point mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-04-23T21:51:19Z</created><updated>2015-11-13T05:08:36Z</updated><resolved>2015-11-13T05:05:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to use a stored script with a terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10760</link><project id="" key="" /><description>Currently, the only way to use a script with a terms aggregation is to use dynamic scripting (which is now disabled by default). 

There should instead be a way to use an indexed script or a script stored on disk, similar to what is possible for a query filter script.

Note that this is only valid for unsandboxed languages.
</description><key id="70521561">10760</key><summary>Add ability to use a stored script with a terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tagg7</reporter><labels><label>docs</label></labels><created>2015-04-23T21:46:27Z</created><updated>2015-04-26T15:31:39Z</updated><resolved>2015-04-26T15:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T15:31:30Z" id="96399710">Hi @tagg7 

You can use the `script_file` or `script_id` parameters instead of `script`. I've updated the docs to reflect this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Mentioned script_id and script_file parameters across all aggs</comment></comments></commit></commits></item><item><title>Added maven target to sign jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10759</link><project id="" key="" /><description>Closes #10758
</description><key id="70483750">10759</key><summary>Added maven target to sign jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">alexanderkjall</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label></labels><created>2015-04-23T18:45:21Z</created><updated>2015-08-11T17:49:20Z</updated><resolved>2015-08-11T17:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexanderkjall" created="2015-04-23T18:51:36Z" id="95683850">I have now signed the CLA.
</comment><comment author="s1monw" created="2015-05-06T12:08:47Z" id="99426499">huge +1 to do this - I think we should always sign these artifacts
</comment><comment author="dadoonet" created="2015-08-11T17:49:19Z" id="129989711">Sounds like we can close this PR as this commit added it: https://github.com/elastic/elasticsearch/commit/295f1971d2e4bf5d65cfe1aa10ac0e15557dba9b
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Publish signed jar files to maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10758</link><project id="" key="" /><description>The jar files provided on maven doesn't have a pgp signature provided with them.

This makes it hard to verify the authenticity of the packages that we download.
</description><key id="70482689">10758</key><summary>Publish signed jar files to maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexanderkjall</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-04-23T18:39:57Z</created><updated>2015-08-28T11:47:23Z</updated><resolved>2015-08-26T19:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T15:04:01Z" id="96396568">@dadoonet what do you think?
</comment><comment author="dadoonet" created="2015-04-28T08:54:02Z" id="96981183">I need to think a bit more about it and run some tests. A first thought is that now we have  f733fda55e0a49ffc3e96e3e691ddaf4bb12b615 merged in, we can support it now without breaking our release script.

I mean that IIRC, maven gpg plugin ask you explicitly for a passphrase.
We can probably set gpg plugin to use the new `GPG_KEY_ID`, `GPG_PASSPHRASE`, optionally `GPG_KEYRING` @spinscale created.

Will try it.
</comment><comment author="jpountz" created="2015-08-26T19:40:06Z" id="135148959">This seems to be fixed in master.
</comment><comment author="alexanderkjall" created="2015-08-28T11:45:53Z" id="135747055">Verified that the latest version have .asc files in maven central. Big thanks to everyone involved.
</comment><comment author="s1monw" created="2015-08-28T11:47:23Z" id="135747518">w00t!! @alexanderkjall thanks for raising this in the first place
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Range Aggregation: no way to set "keyed": true with the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10757</link><project id="" key="" /><description>According to [the v1.4 documentation](http://www.elastic.co/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-range-aggregation.html#_keyed_response), the range aggregation has a `keyed` parameter that can be set to `true` to indicate the aggregation should use unique keys like "0-100" and return the results in a map instead of an array.

However, I cannot find any way to set the `keyed` boolean flag with the Java API (1.4). There is a method to manually specify the key name when you add a range `RangeBuilder.addRange(String key, double from, double to)`, but no apparent way to have ES auto-create the keys as the documentation suggests.

Also, the docs first mention a `key` flag, but in the JSON sample it's labeled as `keyed`
</description><key id="70479212">10757</key><summary>Range Aggregation: no way to set "keyed": true with the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvido</reporter><labels><label>:Aggregations</label></labels><created>2015-04-23T18:23:02Z</created><updated>2015-04-29T12:35:14Z</updated><resolved>2015-04-29T12:35:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T15:02:46Z" id="96396199">@colings86 any ideas?
</comment><comment author="colings86" created="2015-04-29T08:51:34Z" id="97355751">@bradvido In the Java API you are returned a Range object which holds the buckets. in version 1.4 there is a method `getBuckets()` which will return you a `List` of buckets (equivalent of giving you an array) and a method `getBucketByKey(String)` (equivalent to map lookups) which will return you a specific bucket given a string key. The `keyed` parameter is not available in the Java API as it would have no affect. Regardless of whether `keyed` is set to `true` or `false` the same object would be returned and those methods would still be available. The purpose of the `keyed` method is to allow clients which deal directly with the JSON response to specify the structure of the JSON. The Java API does not deal with the JSON response but directly with the response objects created in the aggregation.
</comment><comment author="clintongormley" created="2015-04-29T12:35:13Z" id="97413273">thanks @colings86 - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Vectorizers: export consistent datasets for statistical / ML tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10756</link><project id="" key="" /><description>## Introduction

Today if we want to use Elasticsearch as a data source for external statistical or machine learning packages, we can either export the source, use fields, fielddata_fields, scripted fields or export term vectors. However, machine learning algorithms expect the data to be returned in a document-term or term-document matrix, where the rows correspond to documents in a collection and columns correspond to terms, or more precisely to some numerical value associated with each term such as tf or tf-idf.

The idea behind vectorizers consists of specifying the format of this matrix in advance, so that each vector could be generated on the fly according to that specification. Enforcing how data should be turned into a vector in advance has a couple of benefits such as:
- This approach generates consistent datasets. This is important because a model can be trained, fed more data to it and evaluated at any time. The datasets generated in between all have the same number of features and the feature values are at the same expected column index. This also means that data could be streamed to a machine learning model.
- Most machine learned models work on a fix set of features, therefore adding new features means that the model should be retrained on these new features anyway. This approach makes this step explicit by clearly stating that the dataset is generated according to a given choice of vectorizer, and if new features are to be taken into account, then a new vectorizer should be defined.
- Although there are potentially millions of features, only a few might be discriminating for the task. This approach leaves the feature selection step to user by asking for these features before hand. This is not necessarily a bad thing because feature selection could be a complex matter and the sole goal of vectorizers should be about facilitating interfacing between ES data and external statistical tools, not about feature selection itself. For string fields, depending on the task, a good set of features could already be obtained using significant terms.
- The term-column dictionary required for text features would be stored in memory, with the user having direct control over its usage. Keeping track of more features mean more memory is required. This is to be contrasted with approaches that would consider every term to be a potential feature, bloating the index with noisy features, consuming memory and disk space in an uncontrollable manner.
- This approach provides an iterative approach to machine learning, in which the user can try out different vectorizers on a small sample of the document collection. This would fit well with statistical environments such as R, where for example a data frame could be created according to the specification given by a vectorizer.
## Defining a vectorizer

What we need is a map from a document to a real valued vector. The map could be stored for a given index in a `.vectorizer` hidden type.

```
PUT /index/.vectorizer/my_vectorizer
{
    "vectorizer": [
        {
            "field": "text",
            "span": [ ... list of terms ...],
            "value": "term_freq"
        },
        {
            "field": "field_numeric_1",
            "span": 1
        },
        {
            "field" : "field_numeric_2",
            "span": 1,
            "script": "if _value &gt; 0.5 then 1 else 0"
        },
        {
            "field": "field_numeric_3",
            "span": 5
        },
        {
            "field": "label",
            "span": 1,
            "script": "if _value == "yes" then 1 else 0"
        }
    ]
}
```

A vectorizer specifies how the values of a given field in the index should be mapped to a real valued (fixed size) vector. For example, the first entry defines a first list of columns whose values are the term frequencies of the terms given in `span` in this order. The next column is simply defined as the value of the numerical field named `field_numeric_1`. For the next column, we binarize the numerical value at the field using a script. Next we assign 5 columns for the first 5 values in the multi-valued numerical field `field_numeric_3`. Last but not least, the last column is binarized as well and could be used as the target label for learning.

`field`: Name of the field in the index to which a mapping from field values to real values should be applied.
`span`: For numerical fields, specifies a number of columns to be assigned in the resulting vector. For string fields, specifies a list of terms, each term occupying a column in the vector if present in the document.
`value`: For numerical fields, defaults to the value at the field. For string fields, defaults to term frequency. For the later case other values are possible such as document frequency, or payload.
`script`: Allows for any field value transformation such as thresholding, categorizing, etc ...
## Pulling vectorized data out of ES

A vector or set of vectors could be pulled out of the index, given a choice of vectorizer. A vectorizer operates on top of the term vectors API in the following manner:

```
GET /index/type/id/_termvectors&amp;vectorizer=my_vectorizer

Response:

{
    "shape": [1, 9],
    "vector": [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0}
    ]
}
```

The vector returned is in a sparse format, together with its dimension. This vector can then be loaded in memory in your favorite statistical environments, or fed to a machine learning package.

When using the multi-term vectors API we obtain a n dimensional matrix:

```
GET /index/type/_mtermvectors
{
    "docs": [
    {
        "_index": "index",
        "_type": "type",
        "_id": "id",
        "vectorizer": my_vectorizer
    },
    {
        "_index": "index",
        "_type": "type",
        "_id": "id2",
        "vectorizer": my_vectorizer
    }
   ]
}

Response:

{
    "shape": [2, 9],
    "vector": [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0},
        {"0": 2, "3": 1, "6": 0.21, 7": 1, "8": 1}
    ]
}
```

Here different vectorizers could have been chosen. In this case, the vectors are stacked up, columns over columns, with the largest vector defining the dimension of the matrix.

Finally, we can obtain a dataset by using the scan and scroll together with a new search type called `matrix`:

```
GET /index/_search?search_type=matrix&amp;scroll=1m
{
    "query": { "match_all": {}},
    "size":  1000,

    "sample": "10%",
    "step": step,

    "vectorizer": "my_vectorizer",
    "slice": start:stop:step
}

After scroll request, we get the response:

{
    "shape": [1000, 9],
    "vector: [
        {"3": 5, "5": 2, "6": 0.55, 7": 1, "8": 0},
        {"0": 2, "3": 1, "6": 0.21, 7": 1, "8": 1},
        {"3": 5, "3": 5, "6": 0.45, 7": 0, "8": 0},
        {"3": 5, "5": 3, "6": 0.56, 7": 0, "8": 1},
        ...
    ]
}
```

Here `slice` allows us to only select some columns in this matrix. For example, this could be useful in order to create a training set with labels, and a final test set without labels. The `step` or `sample` options could be added to scan and scroll in order to only retain documents at every x steps, or with x% chances. Again this could be useful in order to generate datasets.
## Implementation Notes

Vectorizers would operate on top of term vectors, one field at a time. In fact, a vectorizer could be seen as specifying a different output format for term vectors. The Term Vectors API would have to be extended in order to return numerical values in the case a vectorizer is specified. By operating on top of term vectors, the implementation should be pretty straight forward, and we would also gain the other features of the API such as per field analyzers, on the fly term vectors creation, dfs or terms filtering. For scripting, a new term level scope would have to be introduced. The main difficulty seems to be in how to handle the in memory term-column dictionary.
## Roadmap
1. Make search also returns term vectors if specified.
2. Make vectorizers as part of a term vectors request. This would be useful for debugging.
3. Allow to store vectorizers that could then be referenced in term vectors request.
4. Allow for term level scripting.
5. Make vectorizers also operate on top of numerical fields.
6. Add other options such as step, sample or slice.
</description><key id="70462699">10756</key><summary>Vectorizers: export consistent datasets for statistical / ML tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>feature</label></labels><created>2015-04-23T17:08:20Z</created><updated>2015-04-27T10:27:05Z</updated><resolved>2015-04-27T10:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-04-27T10:27:03Z" id="96599267">closing in favor to #10823 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor SSD/FileStore logic out of NodeEnvironment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10755</link><project id="" key="" /><description>This logic is rather hairy and nicer to be in its own place. It currently has to do things like "match" the result of Files.getFileStore with the "better" data found in FileSystem.getFileStores(), detect SSDs to set "spins", etc.

We can encapsulate a lot of the logic by just providing an "enhanced" filestore that takes care of the pain points. For example, exposes "spins" as an optional attribute so calling code doesn't have to worry about all the hairiness here.

Finally the current logic being in NodeEnvironment causes security challenges/complexity on #10717:
- Iterating FileSystem.getFileStores() requires permissions to every mount point on the system.
- SSD detection requires scary permissions for system and device files.

On the other hand, with this patch, asking for Files.getFileStore("/usr/foo") only requires read permission to "/usr/foo" itself, and "getFileStoreAttributes".
</description><key id="70443986">10755</key><summary>Refactor SSD/FileStore logic out of NodeEnvironment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T15:39:31Z</created><updated>2015-06-06T19:10:21Z</updated><resolved>2015-04-23T16:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T15:58:26Z" id="95632957">LGTM, I like tying the spins attribute into the filestore itself!
</comment><comment author="mikemccand" created="2015-04-23T16:42:41Z" id="95649902">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/env/ESFileStore.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file></files><comments><comment>Merge pull request #10755 from rmuir/filestores</comment></comments></commit></commits></item><item><title>Tests: Add backward compatibility test for cluster state with blocks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10754</link><project id="" key="" /><description>This commit adds a bwc test when the cluster state has blocks. It refers to https://github.com/elastic/elasticsearch/pull/9203#issuecomment-94755847.
</description><key id="70432711">10754</key><summary>Tests: Add backward compatibility test for cluster state with blocks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T14:55:53Z</created><updated>2015-05-29T17:56:53Z</updated><resolved>2015-04-28T08:52:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-23T14:57:17Z" id="95613504">@javanna when you have time (no hurry), could you please have a look a this? Thanks :)
</comment><comment author="javanna" created="2015-04-23T19:46:08Z" id="95696645">left a small comment, looks good though, thanks a lot for taking care of this
</comment><comment author="tlrx" created="2015-04-27T07:52:22Z" id="96543963">@javanna updated, let me know what you think. Thanks.
</comment><comment author="javanna" created="2015-04-28T08:25:29Z" id="96971598">LGTM
</comment><comment author="tlrx" created="2015-04-28T08:53:31Z" id="96980961">Thanks @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested query returns no result when nested object has the same field name as parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10753</link><project id="" key="" /><description>1. create index with nested mappings:

{
   "mappings": {
      "movie": {
         "properties": {
            "cast": {
               "type": "nested"
            }
         }
      }
   }
}
1. post a document:

{
   "title": "The Matrix",
   "cast": [
      {
         "title": "test"
      }
   ]
}
1. search nested:

{
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "nested": {
               "path": "cast",
               "filter": {
                  "term": {
                     "title": "test"
                  }
               }
            }
         }
      }
   }
}

no result found -&gt; bug

It works if nested object uses different field name rather then 'title'
</description><key id="70431825">10753</key><summary>Nested query returns no result when nested object has the same field name as parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">agliznetsov</reporter><labels /><created>2015-04-23T14:51:37Z</created><updated>2015-04-26T15:05:44Z</updated><resolved>2015-04-26T15:05:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T15:01:32Z" id="95616388">I don't think this has anything to do with the field names being the same, since the type of both are `string`. I believe this is related to #8872. Can you change your term query to use `cast.title` as the field and see if it works?
</comment><comment author="agliznetsov" created="2015-04-23T19:10:30Z" id="95688551">Indeed it works with "cast.title". Does it mean that a full path must be used in the term despite the fact that there's also a path specified one level higher?
</comment><comment author="clintongormley" created="2015-04-26T15:05:41Z" id="96396974">@agliznetsov yes that is the case with nested clauses.  In fact, in 2.0 it will be the case for ALL fields.  See #8070
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing documentation for http.max_header_size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10752</link><project id="" key="" /><description>I'd expect to see documentation for `http.max_header_size` [here](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html), but I don't....
</description><key id="70399621">10752</key><summary>Missing documentation for http.max_header_size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>feedback_needed</label></labels><created>2015-04-23T12:26:52Z</created><updated>2015-04-27T14:00:04Z</updated><resolved>2015-04-27T14:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T14:31:55Z" id="96392257">@robin13 why not send a PR ;)
</comment><comment author="robin13" created="2015-04-27T09:02:58Z" id="96574497">Discovered this parameter being used by a user who had huge http headers (110kb), and had already set `htttp.max_header_size: 100kb`
I was not able to identify why/how the customer was creating such huge headers.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Document `http.max_header_size`</comment></comments></commit></commits></item><item><title>Fix minor spelling mistakes in Match Query documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10751</link><project id="" key="" /><description>I came across a couple of minor spelling mistakes while going through this doc, fixed it up a bit :-)
</description><key id="70395953">10751</key><summary>Fix minor spelling mistakes in Match Query documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittersweet</reporter><labels /><created>2015-04-23T12:13:16Z</created><updated>2015-04-26T14:30:31Z</updated><resolved>2015-04-26T14:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T14:30:29Z" id="96392215">Thanks @bittersweet - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix minor spelling mistakes in Match Query doc</comment></comments></commit></commits></item><item><title>Cant assign http to localhost and transport to 0.0.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10750</link><project id="" key="" /><description>Elasticsearch 1.4.4
Java 1.7.0_25-b15

Trying to make http listen on localhost:9201 and cluster/transport on 192.168.30.30:9300

elasticsearch.yml:

```
network.bind_host: 127.0.0.1
network.publish_host: 192.168.30.30    
http.port: 9201
transport.tcp.port: 9300
```

Log:

```
[2015-04-23 07:35:20,066][INFO ][transport                ] [dev-es1] bound_address {inet[/127.0.0.1:9300]}, publish_address {inet[/192.168.30.30:9300]
[2015-04-23 07:35:24,029][INFO ][http                     ] [dev-es1] bound_address {inet[/127.0.0.1:9201]}, publish_address {inet[/192.168.30.30:9201]}
```

netstat:

```
tcp6       0      0 127.0.0.1:9201          :::*                    LISTEN      29266/java      
tcp6       0      0 127.0.0.1:9300          :::*                    LISTEN      29266/java  
```

It listens on 127.0.0.1 on transport as well as http according to netstat (and telnet host 9300 gives connection refused)
</description><key id="70337106">10750</key><summary>Cant assign http to localhost and transport to 0.0.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roflmao</reporter><labels /><created>2015-04-23T07:37:34Z</created><updated>2015-04-23T07:41:40Z</updated><resolved>2015-04-23T07:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roflmao" created="2015-04-23T07:41:39Z" id="95478612">Solved this by using transport.bind_host. A bit confusing with publish_host and the transport_bind_host not being in defaut config
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor TransportShardReplicationOperationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10749</link><project id="" key="" /><description>Refactors TransportShardReplicationOperationAction state management into clear separate Primary phase and Replication phase. The primary phase is responsible for routing the request to the node  holding the primary, validating it and performing the operation on the primary. The Replication phase is responsible for sending the request to the replicas and managing their responses.

This refactoring is aimed at simplifying adding operation start and end hooks that are needed for the counter mentioned in #10032 . 

This also adds unit test infrastructure for this class, and some basic tests. We can extend later as we continue developing.

For now, this is planned to go to 2.0.0 but we make backport it to 1.6.0, depending how work goes with #10032
</description><key id="70330985">10749</key><summary>Refactor TransportShardReplicationOperationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T07:07:46Z</created><updated>2015-04-24T19:55:37Z</updated><resolved>2015-04-24T10:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-23T07:08:39Z" id="95469572">@martijnvg @brwe can you take a look?
</comment><comment author="bleskes" created="2015-04-23T20:17:51Z" id="95707409">@martijnvg @brwe pushed an update with all the feedback + extra testing.
</comment><comment author="s1monw" created="2015-04-23T20:56:56Z" id="95716694">I have to review the logic agian but the test alone makes me +1 this 
</comment><comment author="martijnvg" created="2015-04-24T10:38:15Z" id="95886459">LGTM
</comment><comment author="bleskes" created="2015-04-24T19:55:31Z" id="96046082">pushed to 1.x as well.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>src/test/java/org/elasticsearch/action/support/replication/ShardReplicationOperationTests.java</file><file>src/test/java/org/elasticsearch/test/cluster/TestClusterService.java</file><file>src/test/java/org/elasticsearch/test/transport/CapturingTransport.java</file></files><comments><comment>Refactor TransportShardReplicationOperationAction</comment></comments></commit></commits></item><item><title>Get one to one match in aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10748</link><project id="" key="" /><description>I am storing sms communication in elastic search for sender and receiver

Can we implement searching for sender to only one reciever

Case: A send sms  to B
Case: A send sms  to B, C, D

I only want one to one so if I search only included A to B than only give me first case a result. How to exclude second case when I need to one to one exact.
</description><key id="70329934">10748</key><summary>Get one to one match in aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels /><created>2015-04-23T07:03:14Z</created><updated>2015-04-26T13:43:24Z</updated><resolved>2015-04-26T13:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T13:43:23Z" id="96379429">Hi @dobariya 

Please ask questions like these on the mailing list.  The GitHub issues list is for bug reports and feature requests.  I'd suggest indexing the number of receivers, and adding that condition to the query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get term aggregation based on doc count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10747</link><project id="" key="" /><description>Is it possible get term aggregation where doc count is 1 or according to the specified thresh hold 
</description><key id="70323336">10747</key><summary>Get term aggregation based on doc count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels /><created>2015-04-23T06:28:24Z</created><updated>2015-04-26T13:40:07Z</updated><resolved>2015-04-26T13:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T13:40:07Z" id="96379019">You can sort by doc count ascending:

```
{
  "aggs": {
    "fewest_docs": {
      "terms": {
        "field": "foo",
        "order": {
          "_count": "asc"
        }
      }
    }
  }
}
```

And when https://github.com/elastic/elasticsearch/issues/9876 is in, you'll be able to remove anything with higher doc counts.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggester V2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10746</link><project id="" key="" /><description># Completion Suggester V2

The completion suggester provides auto-complete/search-as-you-type functionality. 
This is a navigational feature to guide users to relevant results as they are typing, improving search precision. It is not meant for spell correction or did-you-mean functionality like the term or phrase suggesters.

The completions are indexed as a weighted FST (finite state transducer) to provide fast Top N prefix-based
searches suitable for serving relevant results as a user types.

**Notable Features**: 
- Document oriented suggestions:
  - Near-real time.
  - Deleted document filtering.
  - Multiple Context support.
  - Return document field values via `payload`. 
- Query Interface:
  - Regular expression support via `regex`.
  - Typo tolerance via `fuzzy`.
  - Context boosting at query time.

Completion Suggester V2 is based on [LUCENE-6339](https://issues.apache.org/jira/browse/LUCENE-6339) and [LUCENE-6459](https://issues.apache.org/jira/browse/LUCENE-6459), the first iteration of Lucene's new _suggest_ API.
## Mapping

The completion fields are indexed in a special way, hence a field mapping has to be defined.
Following shows a field mapping for a completion field named _title_suggest_:

``` bash
PUT {INDEX_NAME}
{
 "mappings": {
  {TYPE_NAME}: {
   "properties": {
    "title_suggest": {
     "type": "completion"
   }
  }
 }
}
```

You can choose index and search time analyzer for the completion field by adding `analyzer` 
and `search_analyzer` options. 
### Context Mappings

Adding a `contexts` option in the field mapping defines a context-enabled completion field. You may want 
a context-enabled completion field, if you require filtering or boosting suggestions by a criteria other than 
just its prefix. Note that adding high-cardinality context values will increase the size of the in-memory 
index significantly.

There are two types of supported context types: `category` and `geo`.
#### Category Context Mapping

Category contexts are indexed as prefixes to the completion field value.

The following adds a category context named _genre_:

``` bash
...
"contexts": [
 {
   "name": "genre",
   "type": "category"
 }
]
```

You can also pull context values from another field in a document by using a `path` option specifying the field name.
#### Geo Context Mapping

Geo points are encoded as geohash strings and prefixed to the completion field value.
The following adds a geo context named _location_:

``` bash
...
"contexts": [
 {
   "name": "location",
   "type": "geo"
 }
]
```

You can also set `precision` option to choose the geohash length and `path` to pull context values from another
field in the document.
## Indexing

Just like any other field, you can add multiple completion fields to a document. You can also index multiple completions 
for a completion field per document. Each completion value is tied to its document and can be assigned an index-time
weight, which determines its relative rank among other completion values which share a common prefix.

The following indexes a completion value and its weight for the _title_suggest_ completion field:

``` bash
POST {INDEX_NAME}/{TYPE_NAME}
{
 "title_suggest": {
  "input": "title1",
  "weight": 7
 }
}
```

You can use the short-form, if you prefer not to add `weight` to the completions:

``` bash
POST {INDEX_NAME}/{TYPE_NAME}
{
 "title_suggest": "title1",
}
```

Arrays are also supported to index multiple values,

The following indexes multiple completion entries (`input` and `weight`) for a single document:

``` bash
POST {INDEX_NAME}/{TYPE_NAME}
{
 "title_suggest": [
  {
   "input": "title1",
   "weight": 14
  },
  {
   "input": "alternate_title",
   "weight": 7
  }
 ]
}
```
#### Indexing context-enabled fields

You can use the `path` option previously mentioned to pull context values from another field
in the document or add `contexts` option to the completion entry while indexing.

The following explicitly indexes context values along with completions:

``` bash
POST {INDEX_NAME}/{TYPE_NAME}
{
 "genre_title_suggest": {
  "input": "title1",
  "contexts": {
   "genre": ["genre1", "genre2"]
  },
  "weight": 7
 }
}
```

You can also configure the `path` option in the context mapping to pull values from another 
field as follows (assuming `path` for the _genre_ context has been set to _genre_ field):

``` bash
POST {INDEX_NAME}/{TYPE_NAME}
{
 "genre_title_suggest": "title1",
 "genre": ["genre1", "genre2"]
}
```
## Query Interface

The point of indexing values as completions is to be able to run fast prefix-based searches on them. 
You can run **Prefix**, **Fuzzy** and **Regex** queries on all completion fields. In case of a context-
enabled completion field, providing no context indicates  _all_ contexts will be considered. But you
can not run a **Context** query on a completion field with no contexts. When a query is run on a context-
enabled field, the contexts for a completion is returned with the suggestion.
#### Prefix Query

The following suggests completions from the field _title_suggest_ that start with _titl_:

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "prefix" : "titl",
  "completion" : {
   "field" : "title_suggest"
  }
 }
}
```

The suggestions are sorted by their index-time `weight`.
#### Fuzzy Prefix Query

A fuzzy prefix query can serve typo-tolerant suggestions. It scores suggestions _closer_ (based on its edit distance) 
to the provided _prefix_ higher, regardless of their _weight_.

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "prefix" : "sug",
  "completion" : {
   "field" : "suggest",
   "fuzzy" : {        (1)
    "fuzziness" : 2
   }
  }
 }
}
```

Specify _fuzzy_ as shown in (1) to use typo-tolerant suggester. [Full options for fuzzy](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html#fuzzy)
#### Regex Prefix Query

A regex prefix query matches all the term prefixes that match a regular expression. Regex is anchored at the begining but not at the end. 
The suggestions are sorted by their index-time `weight`.

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "regex" : "s[u|a]g",    (1)
  "completion" : {
   "field" : "suggest"
  }
 }
}
```

Specify _regex_ as shown in (1), instead of _prefix_ to use regular expressions. [Supported regular expression syntax](http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#regexp-syntax)
### Context Query

Adding `contexts` (1) option to the query enables filtering and/or boosting suggestions based on their context values.
This query scores suggestions by multiplying the query-time _boost_ withe the suggestion _weight_. 

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "prefix" : "sug",
  "completion" : {
   "field" : "genre_title_suggest",
   "contexts": {           (1)
    "genre": [
     {
      "value" : "rock", 
      "boost" : 3
     },
     {
      "value" : "indie",
      "boost" : 2
     }
    ]
   }
  }
 }
}
```

The contexts can also be specified without any boost:

``` bash
  ...
  "contexts": {
    "genre" : ["rock", "indie"]
  }
```
#### Geo Context Query:

The result will be scored such that the suggestions are first sorted by the distance between the corresponding geo context and the provided 
geo location and then by the weight of the suggestions.

``` bash
  ...
  "contexts" : {
    "location" : {
      "context" : {
        "lat" : ..,
        "lon" : ..
      },
      "precision" : ..
    }
  }
```
#### Example

The following performs a **Fuzzy Prefix Query** combined with a **Context Query** on a context-enabled completion field named _genre_song_suggest_.

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "prefix" : "like a roling st",
  "completion" : {
   "field" : "genre_song_suggest",
   "fuzzy" : {
    "fuzziness" : 2
   },
   "contexts" : {
    "genre": [
     {
      "context" : "rock", 
      "boost" : 3
     },
     {
      "context" : "indie",
      "boost" : 2
     }
    ]
   }
  }
 }
}
```

This query will return **all** song names for the genre _rock_ and _indie_ that are within an edit distance of _2_ from the prefix _like a roling st_.
The song names with genre of _rock_ will be boosted higher then that of _indie_.
The completion field values that share the longest prefix with _like a roling st_ will be additionally boosted higher.
### Payload

You can retrieve any document field values along with its completions using the `payload` option.
The following returns the _url_ field with each suggestion entry:

``` bash
POST {INDEX}/_suggest
{
 "suggest-namespace" : {
  "prefix" : "titl",
  "completion" : {
   "field" : "title_suggest",
   "payload" : ["url"]
  }
 }
}
```

The response format is as follows:

``` bash
{
 ...
 "suggest-namespace" : [ 
  {
   "prefix" : "sugg",
   "offset" : 0,
   "length" : 4,
   "options" : [ 
    {
     "text" : "suggestion",
     "score" : 34.0, 
     "payload": {
       "url" : [ "url_1" ]
     }
    }
   ]
  } 
 ]
}
```
</description><key id="70299996">10746</key><summary>Completion Suggester V2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label><label>v5.0.0-alpha1</label></labels><created>2015-04-23T04:00:50Z</created><updated>2017-03-09T20:47:39Z</updated><resolved>2015-11-07T23:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T13:35:11Z" id="96378859">Hi @areek 

I like the look of this a lot.  Btw, at the top you say we can use an arbitrary filter, but then later you say only a prefix/regex/etc filter.  Which is it?

Also, under "Context Query" you say "Prefix, Regex and Fuzzy queries can be used with this query."  Can you provide an example of this?
</comment><comment author="clintongormley" created="2015-04-26T13:35:31Z" id="96378874">@s1monw please could you review this as well.
</comment><comment author="s1monw" created="2015-04-28T15:17:30Z" id="97102694">I like this a lot the only thing that I think is confusing is that we use `text` as the property name if it's prefix  / fuzzy prefix, should we just name it `prefix` since we also use `regex`? I wonder if we finally plan to return the entire document here?
</comment><comment author="areek" created="2015-04-29T23:10:34Z" id="97611309">@clintongormley &amp; @s1monw thanks for taking a look!

I have updated the **Query Interface** section to clarify how multiple queries can be used together (includes example). 

`text` has been renamed to `prefix` for prefix queries and `regex` for regex prefix queries. At the moment, `_source` and `fields` can be used to retrieve field values for all resulting documents (like the `_search` API).

Also updated the issue with link to [LUCENE-6459](https://issues.apache.org/jira/browse/LUCENE-6459), query interface for _suggest_ API.

Let me know if there is still any confusion with the API.
</comment><comment author="clintongormley" created="2015-05-04T09:23:46Z" id="98648941">Looking awesome.  A few thoughts:
-  The `search_analyzer` should default to the `analyzer` setting - just checking that this is the case.
- We should probably remove the `fuzzy` wrapper and just allow specifying `fuzziness` directly?
- For geo-context searching, you sort on distance and THEN on weight?  Feels like we should be able to combine these two factors in a user configurable way.
</comment><comment author="areek" created="2015-05-07T15:56:40Z" id="99921185">Thanks @clintongormley for the feedback!

&gt; The search_analyzer should default to the analyzer setting - just checking that this is the case.

That is the case :)

&gt; We should probably remove the fuzzy wrapper and just allow specifying fuzziness directly?

If we remove the fuzzy wrapper, there would be no way for the user to specify `transpositions`, `min_length`, `prefix_length` and `unicode_aware`. I think it is useful to expose at least the `min_length` and `prefix_length` to the user?

&gt; For geo-context searching, you sort on distance and THEN on weight? Feels like we should be able to combine these two factors in a user configurable way.

We can allow this to be configurable (distance then weight or weight then distance) in the future. I think sorting the suggestions by distance and then weight for the first iteration is good?
</comment><comment author="clintongormley" created="2015-05-08T07:26:34Z" id="100132007">&gt; If we remove the fuzzy wrapper, there would be no way for the user to specify transpositions, min_length, prefix_length and unicode_aware. I think it is useful to expose at least the min_length and prefix_length to the user?

OK, that's fine.  Btw, do we really need `unicode_aware`?  We use unicode everywhere, not sure why we would ever disable that?

&gt; We can allow this to be configurable (distance then weight or weight then distance) in the future. I think sorting the suggestions by distance and then weight for the first iteration is good?

OK
</comment><comment author="abhijitiitr" created="2015-09-23T16:07:53Z" id="142649858">@areek Scoring for fuzzy queries is still doesn't not appear to be correct. Results having exact match with the input string for suggest queries are not having better scores than fuzzy matches. I guess you have noted it in the code itself.
 [Code link](https://github.com/elastic/elasticsearch/blob/completion_suggester_v2/core/src/main/java/org/apache/lucene/search/suggest/xdocument/FuzzyCompletionQuery.java#L223)
Do you have any suggestions for handling fuzziness correctly in the proposed Completion Suggester?
</comment><comment author="ivpusic" created="2015-09-28T22:18:50Z" id="143889686">what about `duplicate_output` option? It was mentioned here https://github.com/elastic/elasticsearch/issues/8909
</comment><comment author="areek" created="2015-09-29T12:59:00Z" id="144050702">@abhijitiitr fuzziness scoring can not distinguish between a perfect match and a match with only the last character being a typo. You could sort the results you get back in your application to correctly score the fuzziness. Unfortunately, this is what I would suggest atm.
BTW, see https://github.com/elastic/elasticsearch/pull/13659 for https://github.com/elastic/elasticsearch/pull/11740#issuecomment-123989933, now you can return values of document fields, along with each suggestion.

@ivpusic the `duplicate_output` has been removed. now each suggestion is associated with a unique document instead. If you have two identical `input`s for two documents, you will get suggestions back both the documents with the same input. 
</comment><comment author="abhijitiitr" created="2015-09-30T11:37:58Z" id="144370545">@areek :thumbsup: for the payload feature.

For fuzzy queries, we have to fetch (&gt; N) results for getting N correct results. In some cases I tested I had to set the `size` option as 5N. That's a bummer I guess but its a Lucene issue and I think it would be solved in future.
</comment><comment author="djschny" created="2015-10-20T17:36:28Z" id="149642409">I have some concerns over performance with the change to remove the `payload` functionality. Let me try to elaborate and please let me know if I stated anything incorrectly.

With the current 1.X completion suggester, the entire FST (including payloads) is held on heap (but persisted to disk). By leveraging payloads we were able to strive extreme performance as we never had to do a FETCH of the associated docs for field values, parse the json, etc. This made for very fast response times.

My concern is not allowing clients who continue to want to make indices with completion suggester mappings that leverage payloads due to performance reasons. Instead I believe it would be really nice to default to the new approach, but one could still opt-in to using payloads. While payloads can be problematic if abused, many clients are smart about it and/or have small enough suggester FSTs that the extra memory associated with payloads is a non-issue compared to the performance expectation they have.

Hopefully there has been testing or other work that I missed reading this issue and the associated PRs that proves this a non-issue and apologies if I missed it. Just want to make sure we don't have surprises and regression for customers.
</comment><comment author="kabcampbell" created="2015-11-01T20:25:57Z" id="152860851">Hi all,
I am running up against the hard-coded limit to the number of context categories that a suggestion can have, as referenced in [#9466](https://github.com/elastic/elasticsearch/issues/9466). Is that limit still on track to be removed? Is this version of the completion suggester to be included in elastic 2.1?
Thanks!
</comment><comment author="mikemccand" created="2015-11-04T10:52:49Z" id="153682533">&gt; By leveraging payloads we were able to strive extreme performance as we never had to do a FETCH of the associated docs for field values, parse the json, etc. This made for very fast response times.

Hi @djschny, you are correct, that payloads are moving out of the FST to disk.

However, with this PR, we 1) pull the payloads from doc values, which is much more efficient than loading from re-parsing `_source` or loading stored fields, and 2) we do so eagerly in a single pass (no second fetch phase), though this may change in the future if we merge suggest into search implementation.

Also, the number of values we are pulling is typically tiny (the `size` passed in the suggest request, i.e. number of suggestions, default is 5).  We pull them only once we have the top suggestions for that shard.

So while there will be some perf hit, I expect it won't matter in most use cases.
</comment><comment author="djschny" created="2015-11-04T13:52:32Z" id="153724649">&gt; So while there will be some perf hit, I expect it won't matter in most use cases.

I agree as well. Where my concern lies is in the 20%:
- We don't have concrete numbers (that I'm aware of) to validate the expectation
- For the 20% of folks (made up number following the 80/20 rule with our userbase) that need that blistering performance, that it will hurt them and they won't have an option to choose the old approach.

Generally speaking I tend to always follow the "golden rule" that says to "always put the user in control". So therefore allowing our users to choose which approach works best for them based upon a tradeoff of speed for increased memory, vs. large scale and off heap memory usage at the cost of some slight performance.

Hope I'm not being too noisy, but would hate to see it be an unpleasant surprise or an upgrade blocker for that 20% of our user base. One of the things that has made Elasticsearch so wonderfully accepted by everyone was that users had options to configure and use it for what made sense for their particular requirements/needs (since everyone is different) and just don't want to loose that.
</comment><comment author="djschny" created="2015-11-09T19:29:30Z" id="155166076">Was this prematurely closed? I'm still pretty concerned about removing the payloads ability the users have now without concrete numbers that show the speed is the same.
</comment><comment author="areek" created="2015-11-09T20:02:41Z" id="155175249">@djschny This was closed as the feature is merged to master and will be available for v2.2, but is still open for discussion. I will benchmark the feature and update this issue with the benchmark results.
</comment><comment author="areek" created="2015-11-17T05:41:20Z" id="157276138">@djschny I have [benchmarked](https://gist.github.com/areek/4fdd319b855be919e1ac) the completion suggester on a single shard environment using the [geonames](http://download.geonames.org/export/dump/allCountries.zip) dataset of ~9.3 million city names. The FST size for the dataset was ~201.5 mb. 
The following shows the comparison of search performance (in KQPS) of the completion suggester (with and without using the newly added query-time doc-values based `payload`) and an equivalent `prefix` query with increasing number of prefix length:
![benchmark_completion_master](https://cloud.githubusercontent.com/assets/753679/11201257/78eedf72-8caa-11e5-8ad0-34c944394d00.png)

The completion suggester was at most `+19KQPS` faster and at least `+9KQPS` faster compared with its prefix query counterpart for the prefix lengths of 1 to 6. Retrieving a non-analyzed doc-values based string `payload` at query-time was at most `-6KQPS` slower and at least `-2KQPS` slower compared to completion suggestions without `payload`.  

The same benchmark was run on pre-2.2 completion suggester. As `payloads` was an index-time setting the dataset produced a FST size of ~201.5mb without payload and ~352.5mb with payload. The alternate names of the cities were used for the `payload` values:
![benchmar_completion_2 2](https://cloud.githubusercontent.com/assets/753679/11201252/6dc6be9e-8caa-11e5-8297-2fb133b93b69.png)

The pre-2.2 completion suggester was at most `+19KQPS` faster and at least `+11KQPS` faster compared with its prefix query counterpart and completions with index-time payloads were at most `-3KQPS` slower and at least `-1KQPS` slower than completions with no payloads.

Note that the overall trend of the benchmarks are more interesting than the absolute KQPS values, as they seem to vary between runs unlike the overall trends seen in the graphs. 

Overall the query-time doc-values based payloads are slower than the previous index-time payloads that were stored as part of the FST, this is as expected and IMO justified by ensuring that the FST size is only bounded by its inputs instead of arbitrary index-time payload values (that could be anything from a small id to a full-blown json object) and the provided flexibility of specifying one or more query-time payloads. The current infrastructure also makes it possible to use FST index directly in the query DSL and for aggregations in the future. IMO, if needed, in a future optimization we can expose the index-time `payloads` option as a  more sane `_cache` option, where users can specify a single field to be stored in the FST for speeding up completions with payloads.
</comment><comment author="simplechris" created="2015-11-18T09:07:45Z" id="157650480">&gt; Generally speaking I tend to always follow the "golden rule" that says to "always put the user in control". So therefore allowing our users to choose which approach works best for them based upon a tradeoff of speed for increased memory, vs. large scale and off heap memory usage at the cost of some slight performance  - [djschny](https://github.com/elastic/elasticsearch/issues/10746#issuecomment-153724649)

:+1:  Yeah, I would like to have this option. I'm willing to use more memory to have my payloads stored within the FST.

&gt; ... where users can specify a single field to be stored in the FST for speeding up completions with payloads. - [areek](https://github.com/elastic/elasticsearch/issues/10746#issuecomment-157276138)

This sounds like a good compromise. Without it, honestly, I'd be very apprehensive about upgrading.

The rest of the ticket and progress sounds so exciting, I believe we should document this trade-off and allow users to make the informed decision of if/how to store their payloads. Let's keep the user in ultimate control, even if the default behaviour changes to best serve the 80% usecase. :)
</comment><comment author="djschny" created="2015-11-23T23:31:47Z" id="159103641">@areek Thanks for al the numbers, much appreciated. In your examples, do you have the average response time associated with performing a completion suggestion with the various prefix lengths? That is the more important measurement that I was expecting to see, as that results in the end user experience.
</comment><comment author="jimczi" created="2015-11-26T21:04:13Z" id="159993913">&gt; Overall the query-time doc-values based payloads are slower than the previous index-time payloads that were stored as part of the FST, this is as expected and IMO justified by ensuring that the FST size is only bounded by its inputs instead of arbitrary index-time payload values (that could be anything from a small id to a full-blown json object) and the provided flexibility of specifying one or more query-time payloads. - [areek](https://github.com/elastic/elasticsearch/issues/10746#issuecomment-157276138)

Have you considered replacing the BytesRef in the FST by a pointer to a file where the payloads would be written ? Something like Pair&lt;Long, Long&gt; where the first value would still be the weight but the second value would be a pointer to the start of the payload for this entry. This would ensure that the FST size is only bounded by its inputs and also speed up the performance compared to the docvalues case (though it would rely on mmap reads in both cases).
</comment><comment author="s1monw" created="2015-11-26T21:46:40Z" id="159997107">&gt; ave you considered replacing the BytesRef in the FST by a pointer to a file where the payloads would be written ? Something like Pair&lt;Long, Long&gt; where the first value would still be the weight but the second value would be a pointer to the start of the payload for this entry. This would ensure that the FST size is only bounded by its inputs and also speed up the performance compared to the docvalues case (though it would rely on mmap reads in both cases)

isn't this what we do now with putting the payloads in docvalues? I mean we use the doc ID to reference a payload on disk?
</comment><comment author="jimczi" created="2015-11-27T08:43:05Z" id="160071395">@s1monw right but my point is that we can make the FST size only bounded by its input without the docvalues. Having the docvalues in the process adds an indirection (docid =&gt; file_ptr = &gt; payload) that the direct pointer inside the FST could save. 
</comment><comment author="s1monw" created="2015-11-27T09:24:42Z" id="160088321">&gt; @s1monw right but my point is that we can make the FST size only bounded by its input without the docvalues. Having the docvalues in the process adds an indirection (docid =&gt; file_ptr = &gt; payload) that the direct pointer inside the FST could save.

this breaks all feature isolation with no benefit IMO. We are using a compressed integer to reference the payload (the docID) all optimization on top of this is premature and IMO of no benefit. We would need to invent and maintain yet another file format which is not sustainable in our environment. IMO the doc-values indirection is already as specialized as it gets here. And remember we only fetching this for the topN.
</comment><comment author="areek" created="2015-12-02T02:03:13Z" id="161155861">&gt; In your examples, do you have the average response time associated with performing a completion suggestion with the various prefix lengths? That is the more important measurement that I was expecting to see, as that results in the end user experience.

@djschny I think average response time for various prefix lengths is misleading when we are concerned with completion suggester's latency. If latency is a concern for completions, IMO the right question to ask is "What is the highest latency I can tolerate?" instead. From the result, we can see that latency decreases as the prefix length increases (as expected), hence the QPS for a prefix length of 1 is the most important measure in the benchmark, the rest just shows the relationship between the latency and increasing prefix lengths. So for this dataset, it happens to be in a range of `20` to `18` KQPS. In contrast, if we measure the average response time it would be somewhere in the low 30s in KQPS, which does not really help when you always see higher latency for smaller prefixes, which is the main use case for using completions to begin with.
</comment><comment author="djschny" created="2016-01-05T00:31:59Z" id="168853833">@areek That is exactly my point, some users may not be able to tolerate the highest latency. For example if folks see their average completion suggest response time go from 5 millis to 20 millis due to the hit on disk to fetch from _source, then they would ideally want the original behavior of storing a payload in the FST.

Most importantly, today you can make an FST and not actually have any relationship to source documents stored in ES. With the new change you cannot make a completion suggester with payloads but have your actual documents stored in a different system. Instead you would need to make fake/dummy documents that just contained the same fields that you would ultimately want in the FST. My apologies if I'm missing something there.
</comment><comment author="JeremyBYU" created="2016-05-27T14:54:54Z" id="222167944">I'm having a difficult time understanding how to use the new payloads option.  I see that we are supposed to "store" our payload on the actual document now and access these properties through the `payloads: [property'` syntax.
However my payload is a nested JSON document that has multiple fields, many of which are optional. So I map this JSON document to a property but I am forced to access the payload in this awkward way:

```
        "prefix": "coupon mi",
        "completion": {
            "payload": ["search.prop1.nested.prop2", "search.prop2.nestedprop3"], 
            "field": "suggest",
            "fuzzy":{},
            "size": 25
        }
```

Is there any way to just tell the suggestion engine to just return back a payload that is simply a nested JSON object? Or maybe more clearly is there any way to **not** index a JSON object on the property of a document? So that when the suggestion engine 'looks up' a property it just return the JSON.
</comment><comment author="areek" created="2016-05-30T15:37:01Z" id="222516551">Hey @JeremyBYU 

&gt; Is there any way to just tell the suggestion engine to just return back a payload that is simply a nested JSON object?

Currently no, you have to specify the payload fields as you are currently doing. But we plan to return the entire document a suggestion hit is associated with. When that is supported, you can use source filtering to exclude/include fields from the document. 

In general I would suggest using a dedicated index for suggestions, so the associated suggest documents contain fields only relevant for retrieving at suggest time. This might simplify the document structure and avoid the need to specify nested json fields as payloads altogether.
</comment><comment author="JeremyBYU" created="2016-06-01T16:16:13Z" id="223045070">Thanks for the explanation. I was indeed using a dedicated index.  The structure of the object is important, so if I flatten it into one document, I will have to regenerate it for the client. Thanks
</comment><comment author="Antonhansel" created="2016-11-04T21:56:05Z" id="258557551">Just dug up this issue, I'm trying to get duplicates documents.
I'm not sure about [this](https://github.com/elastic/elasticsearch/issues/10746#issuecomment-144050702) answer from @areek, does it mean it should return duplicate documents?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>core/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java</file><file>core/src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpFlag.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilders.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestContextParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestionSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionFieldStats.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestion.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/CategoryContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/CategoryQueryContext.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/ContextBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/ContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/ContextMappings.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/GeoContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/context/GeoQueryContext.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/context/CategoryContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/context/ContextBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestParser.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/completion/CompletionFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/CompletionFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextCompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CategoryContextMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/GeoContextMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java</file></files><comments><comment>Completion Suggester V2</comment></comments></commit></commits></item><item><title>Explicitly disallow multi fields from using object or nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10745</link><project id="" key="" /><description>Multi fields currently parse any field type passed in.  However, they
were only intended to support copying simple values from the outer
field. This change adds validation to ensure object and nested
fields are not used within multi fields.
</description><key id="70282214">10745</key><summary>Explicitly disallow multi fields from using object or nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T02:05:47Z</created><updated>2015-06-08T09:01:44Z</updated><resolved>2015-04-28T21:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T02:11:53Z" id="95392070">Note that this would have already failed with the current code if someone tried it, just with a nasty `ClassCastException`.
</comment><comment author="jpountz" created="2015-04-25T19:27:26Z" id="96267162">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file></files><comments><comment>Mappings: Explicitly disallow multi fields from using object or nested fields</comment></comments></commit></commits></item><item><title>Remove obsolete `expand_wildcards_open` and  `expand_wildcards_close` options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10744</link><project id="" key="" /><description>In #6097 we made snapshot/restore index option consistent with other API. Now we can remove old style options from master.

Closes #10743
</description><key id="70279197">10744</key><summary>Remove obsolete `expand_wildcards_open` and  `expand_wildcards_close` options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T01:43:46Z</created><updated>2015-08-13T17:47:59Z</updated><resolved>2015-04-23T19:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-23T06:27:16Z" id="95457658">LGTM can you add a small description of this to the migrate guide please?
</comment><comment author="imotov" created="2015-04-23T19:15:25Z" id="95689367">@javanna I added the note to the list of breaking changes and pushed it to master. Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/IndicesOptions.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java</file></files><comments><comment>Snapshot restore request should accept indices options</comment></comments></commit></commits></item><item><title>Snapshot/Restore: remove obsolete snapshot/restore expand_wildcards option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10743</link><project id="" key="" /><description>In #6097 we made snapshot/restore index option consistent with other API. Now we can remove old style options from master.
</description><key id="70277178">10743</key><summary>Snapshot/Restore: remove obsolete snapshot/restore expand_wildcards option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T01:34:06Z</created><updated>2015-04-23T19:13:25Z</updated><resolved>2015-04-23T19:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file></files><comments><comment>Snapshot/Restore: remove obsolete expand_wildcards_open and  expand_wildcards_close options</comment></comments></commit></commits></item><item><title>Score value is 0 in _explanation with random_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10742</link><project id="" key="" /><description>Sending random_score function query with explain=true to Elasticsearch 1.5.1, the value is always 0 in _explanation as below.

```
...
              {
                 "value": 0,
                 "description": "Math.min of",
                 "details": [
                    {
                       "value": 0,
                       "description": "product of:",
                       "details": [
                          {
                             "value": 0,
                             "description": "random score function (seed: 49)"
                          },
                          {
                             "value": 1,
                             "description": "weight"
                          }
                       ]
                    },
                    {
                       "value": 3.4028235e+38,
                       "description": "maxBoost"
                    }
                 ]
              },
...
```

I looked into RandomScoreFunction class. 
I think that exp.setValue(CombineFunction.toFloat(score(docId, subQueryScore.getValue()))) needs to be call in explainScore method.
</description><key id="70271801">10742</key><summary>Score value is 0 in _explanation with random_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marevol</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-23T00:46:58Z</created><updated>2015-04-26T11:22:54Z</updated><resolved>2015-04-23T02:58:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T02:38:31Z" id="95397760">I think this was actually fixed today as part of upgrading our lucene snapshot in master:
https://github.com/elastic/elasticsearch/pull/10727/files#diff-8

So this is already fixed in 2.0.  I will backport the fix 1.x.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pagination of percolate results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10741</link><project id="" key="" /><description>It is currently possible to set the "size" option to limit the number of percolate results, but it's not possible to set "from" (so we can paginate through the results).

If a percolate request matches thousands or millions of percolators, it'd be nice to be able to paginate through them rather than process them all in one go. Is this something that has been considered for an upcoming version?

Thanks!
</description><key id="70260884">10741</key><summary>Pagination of percolate results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertkern</reporter><labels><label>:Percolator</label><label>discuss</label></labels><created>2015-04-22T23:25:33Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-27T22:35:53Z" id="143599103">I'm not sure we want to support things like scroll for the percolator, but in general I think it'd be great if it worked in a more similar way to the search API. Maybe some refactoring happening on #13646 are going to help in that regard.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/index/memory/ExtendedMemoryIndex.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhase.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateException.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorModule.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhaseTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorQueryCacheTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryShardContextTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorServiceTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/TTLPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/percolator/PercolatorQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolStatsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>percolator: Replace percolate api with the new percolator query</comment></comments></commit><commit><files><file>core/src/main/java/org/apache/lucene/index/memory/ExtendedMemoryIndex.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhase.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateException.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorModule.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhaseTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorQueryCacheTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryShardContextTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorServiceTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/TTLPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/percolator/PercolatorQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolStatsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>percolator: Replace percolate api with the new percolator query</comment></comments></commit></commits></item><item><title>Add G1GC to JVMCheck and bail on G1GC usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10740</link><project id="" key="" /><description>We know based on tests that G1GC can cause corruptions on the index, though it seems like the JVM group is taking actions into fixing this issue, and some promising changes have been made to Java 8u40, its still an issue.

I suggest we bail ES from starting if G1GC is used, people can always override the JVM check flag if they are feeling adventurous. If we do this change, we need to make sure we disable this check when we test on G1GC in our testing infra, since we want to know when this is fixed eventually.

See https://issues.apache.org/jira/browse/LUCENE-5168 for example.
</description><key id="70260227">10740</key><summary>Add G1GC to JVMCheck and bail on G1GC usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Packaging</label><label>discuss</label><label>resiliency</label></labels><created>2015-04-22T23:22:08Z</created><updated>2016-02-28T20:30:44Z</updated><resolved>2016-02-23T00:53:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T09:44:11Z" id="95872797">Maybe another flag we should consider banning is `-XX:+AggressiveOpts`?
</comment><comment author="clintongormley" created="2015-04-26T16:37:06Z" id="96405843">@rmuir would you be keen to do this?
</comment><comment author="rmuir" created="2015-04-27T09:21:25Z" id="96580762">I think we need to do something, just not sure what the exact behavior should be. Yes we can look for the flag explicitly passed on the command line, but still its different than the current version-based checks because it will impact jvms to be released in the future...
</comment><comment author="kimchy" created="2015-04-27T09:24:48Z" id="96581747">yea, thats the tricky part, where if G1GC suddenly becomes good in 8u60, there is no way to undo this change. I don't have a good idea around how to solve this, we can decide to pin G1GC to last know Java versions, i.e. all the ones up to 8u40, but then the bad part is that if 8u60 fails and we don't fail, it might confuse users to think it fails.

Another option: fail if on 8u40 and before, and info log that check the docs for later versions? This does mean we need to keep on maintaining this check as new JVM versions gets released.
</comment><comment author="rmuir" created="2015-04-27T09:34:59Z" id="96584302">Even things as simple as range based version checks are hairy, if its based on anything other than java major version. Thats because, insanely its always 1.7.0 or 1.8.0 officially. Doing any kind of parsing here on these strings is a place i would rather not go. If some internal format changes we could easily land in a unhappy place...
</comment><comment author="kimchy" created="2015-04-27T10:36:48Z" id="96602621">@rmuir yea, I hear you, minor versions are a mess to parse. Best idea I can come up with is to list the known version in 7 and 8, and mark G1GC as problematic on them, and thats it? It won't help for future versions that might be a problem, but at least its progress in helping our users?

@jpountz yea, I think regardless of a version, certain JVM flags, like AggressiveOpts should probably cause us to bail as well
</comment><comment author="danielmitterdorfer" created="2015-05-02T11:31:37Z" id="98349640">Just as an FYI and in case you are not aware yet: G1 seems to become the default GC in JDK 9: http://openjdk.java.net/jeps/248
</comment><comment author="rtkmhart" created="2015-05-05T19:34:55Z" id="99198166">As a customer happily running G1GC (yes I'm aware of the risks) I'd be quite unhappy with this.

Reason for switching: we'd regularly see a JVM with CMS GC go from running happily at 75% heap with the normal sawtooth pattern for days, and then suddenly hit 100% and full GC within minutes for inexplicable reasons which we cannot replicate on demand, and the only recovery is to restart it. Switching to G1GC made this go away. We recently switched back after strong recommendations from support and immediately saw the same GC issue again. So we're going back to G1GC shortly. 

I understand the risks, and know what the implications are. I'd be happy with a strongly worded warning. And open to further discussion of course.
</comment><comment author="jprante" created="2015-05-18T15:38:57Z" id="103107064">Switching to G1GC and Java 8 made a lot of my ES JVM scaling issues caused by GC pausing go away. I do not have broken index data for years. I am aware there are bugs visible in ES testing, while it is hard for me to reproduce them in my environment.

So I would suggest a less rigorous and more risk mitigation focused strategy instead of simply bailing out.

For example
- discouraging Java 7
- enabling Java 9 support for ES
- embedding direct links in warning messages to failed ES tests where involved JVM versions and diagnostics can be revealed so risk mitigation is possible
- referencing OpenJDK issues at https://bugs.openjdk.java.net/browse/JDK/component/10304#selectedTab=com.atlassian.jira.plugin.system.project%3Acomponent-issues-panel so ES users can get advise from OpenJDK community how to fix / work around issues
</comment><comment author="rmuir" created="2015-05-19T03:03:34Z" id="103312980">&gt; Switching to G1GC and Java 8 made a lot of my ES JVM scaling issues caused by GC pausing go away. I do not have broken index data for years. I am aware there are bugs visible in ES testing, while it is hard for me to reproduce them in my environment.

It is dangerous to look at things this way, for example a lot of people could not reproduce the superword corruption bug (JDK-8024830), and that is because it required sandy bridge to easily reproduce.

&gt; discouraging Java 7

If I were to add any check here (as discussed above), I would only add it with a list of released hotspot versions &lt; java 8u40.

&gt; enabling Java 9 support for ES

I don't know what this means: tests in master pass with the latest EA snapshot of java 9, at least if you run them once. java 9 likely has some bugs, since its not released. There is nothing more we can do about this situation, its as good as it gets.

&gt; embedding direct links in warning messages to failed ES tests where involved JVM versions and diagnostics can be revealed so risk mitigation is possible
&gt; referencing OpenJDK issues at https://bugs.openjdk.java.net/browse/JDK/component/10304#selectedTab=com.atlassian.jira.plugin.system.project%3Acomponent-issues-panel so ES users can get advise from OpenJDK community how to fix / work around issues

This seems to imply we would waste time digging into/fixing bugs around G1GC: nobody has the time for that.
</comment><comment author="clintongormley" created="2016-01-17T17:48:31Z" id="172358146">@rmuir what's your take on this, 8 months later?  Should we do anything here?
</comment><comment author="jasontedor" created="2016-02-23T00:53:31Z" id="187457014">Closed by #16737
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Illegal instant due to time zone offset transition, Joda error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10739</link><project id="" key="" /><description>Hi there everyone,

I posted in another issue dealing with the "Illegal instant" error and it was suggested to create a new issue as my stack trace was somewhat different.  Here is the other issue thread: https://github.com/elastic/elasticsearch/issues/10025

My issue is regarding a JDBC SQL river using the elasticsearch-JDBC plugin
https://github.com/jprante/elasticsearch-jdbc

I am attempting to use field mapping to parse a timestamp in the following format 

```
2015-03-08 02:00:00 EST
```

This was needed because when timezone was not included, Kibana would display all data in the incorrect timezone.  

My field mappings look like this:

```
curl -XPUT 'localhost:9200/distribution_posting_status_sp' -d '
        { "mappings" : {
        "distribution_operation_posting_trend_historical": {
        "properties": {
          "day_name" : {
            "type" : "string"
          },
          "day_timeinterval" : {
            "type" : "date",
            "format": "yyyy-MM-dd HH:mm:ss z"
          },
          "posting_count" : {
            "type" : "long"
          },
          "posting_status" : {
            "type" : "string"
          }
        }
      },
      "distribution_operation_posting_trend_15min":{
        "properties": {
          "day_name" : {
            "type" : "string"
          },
          "day_timeinterval" : {
            "type" : "date",
            "format": "yyyy-MM-dd HH:mm:ss z"
          },
          "posting_count" : {
            "type" : "long"
          },
          "posting_status" : {
            "type" : "string"
          }
        }
      }
    }
}'
```

The Joda time format indicates that a "z" character should match an input of 
`Pacific Standard Time; PST`
So EST should not be an issue there.  

The stack trace that I got is as follows:

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [day_timeinterval]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:416)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:709)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:500)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:542)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:491)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:444)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:150)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [2015-03-08 02:15:00 EST], tried both date format [yyyy-MM-dd HH:mm:ss z], and timestamp number with locale []
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:621)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:549)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:235)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:406)
        ... 12 more
Caused by: org.elasticsearch.common.joda.time.IllegalInstantException: Cannot parse "2015-03-08 02:15:00 EST": Illegal instant due to time zone offset transition (America/New_York)
        at org.elasticsearch.common.joda.time.format.DateTimeParserBucket.computeMillis(DateTimeParserBucket.java:390)
        at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:749)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:615)
```

I suspect that this is caused by a Daylight Saving Time "gap", though I am unsure of what that implies. I do not know too much about the underworkings of Elasticsearch and am a rather new user.

Can anyone suggest a course of action?  I may modify the stored procedure to output the timestamp with a UTC offset intstead of the string representation of timezone, IE: -04:00 for DST EST and -05:00 for non-DST EST.

Thanks!
</description><key id="70238484">10739</key><summary>Illegal instant due to time zone offset transition, Joda error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">deusofnull</reporter><labels /><created>2015-04-22T21:22:25Z</created><updated>2015-07-23T15:44:54Z</updated><resolved>2015-04-23T18:07:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-04-23T09:46:22Z" id="95514620">It seems you are trying to index dates that are not valid in the specified time zone. The date "2015-03-08 02:15:00 EST" falls into the DST gap which last happened on Mar 8th for that time zone. This means that dates between "2015-03-08 02:00:00 EST" and "2015-03-08 02:59:59 EST" will be rejected as invalid because they don't exist for that time zone.
You should check your application logic how this invalid timestamp is produced and try to prevent this there. Either by using explicit timezone offsets (like -04:00 or -05:00) or by first converting to UTC.
</comment><comment author="cbuescher" created="2015-04-23T09:51:27Z" id="95516903">btw. Rivers will be deprecated: https://www.elastic.co/blog/deprecating_rivers
</comment><comment author="deusofnull" created="2015-04-23T13:57:53Z" id="95594750">Sadly, I am not the database guy at my org, and his teams are the ones writing these stored procedures.   I will have to go back to them with a request for any kind of SP change...

Honestly I might do what that elastic bog post suggests and build a custom import tool to do what this river is doing now.  I bet the Python-ES-dsl plugin would be great for that.

So the reason I do think this is more of an issue than just what you've accounted for is many other dates that have failed due to this error are outside of DST.

```
2015-01-23 10:00:00 EST

2015-01-23 11:15:00 EST

2015-01-23 19:45:00 EST

2015-01-24 00:00:00 EST

2015-01-30 01:15:00 EST

2015-02-27 12:00:00 EST

etc,etc,etc
```

Note: these are not consecutive as in the times between these failures did not necessarily fail.  There are thousands of dates that fail on my execution of the river.

These date times are on -05:00 UTC, not daylight savings rest hour (March 8th 2am).

I am going to build a work around using another system, but I'd still like this to work in the mean time.  

Any thoughts?
</comment><comment author="cbuescher" created="2015-04-23T14:20:16Z" id="95602605">Just out of curiosity, what seems to be the issue with the other dates you mentioned? I can see the error you mentioned when I use your mapping and index date with "2015-03-08 02:15:00 EST", but with the same mappings I can index the dates you mentioned above. Is this probably another issue?
</comment><comment author="deusofnull" created="2015-04-23T17:19:57Z" id="95660283">Hey!  When I set everything up again to cause the error to post here, everything ended up working... I can't explain it.  I replicated the bug... must have been 5 times yesterday. But today everything works.

_scratches head_

Sorry!
</comment><comment author="cbuescher" created="2015-04-23T18:07:54Z" id="95673683">No problem, I will close this then.
</comment><comment author="flyisland" created="2015-07-23T06:50:23Z" id="123998145">hi @cbuescher , I have no idea why 'Dates between "2015-03-08 02:00:00 EST" and "2015-03-08 02:59:59 EST"  don't exist for that time zone.'

I have just google it that "EST" is used in: North America, Caribbean, Central America (http://www.timeanddate.com/time/zones/est). I don't understand why these area doesn't have the "2015-03-08 02:15:00".

Would u please tell me where to find information about this question, thanks.
</comment><comment author="cbuescher" created="2015-07-23T07:44:00Z" id="124007719">Hi @flyisland, I think I was refering to daylight savings time. For 2015 this was March 8th, so at 02:00 clocks were forwarded one hour: http://www.timeanddate.com/time/change/usa/new-york?year=2015
</comment><comment author="flyisland" created="2015-07-23T15:44:54Z" id="124146816">thanks @cbuescher, I see now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restrict murmur3 field type to sane options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10738</link><project id="" key="" /><description>Disabling doc values or trying to index hash values are not
correct uses of this the murmur3 field type, and just cause
problems.  This disallows changing doc values or index options
for 2.0+.

closes #10465
</description><key id="70229521">10738</key><summary>Restrict murmur3 field type to sane options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T20:39:45Z</created><updated>2015-06-08T09:01:57Z</updated><resolved>2015-04-24T04:49:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-23T20:16:51Z" id="95707221">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SuggestCompletion , Failed to Execute Phase query : SearchPraseException issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10737</link><project id="" key="" /><description>Hi,

I am working on SuggestCompletion. After indexing all the records, when i execute a query i get below error message.

"Failed to execute phase [query], all shards failed; shardFailures {[TaTSmiM2TV27_ptdFdx5eQ][elasticsearchlocal][0]: SearchParseException[[elasticsearchlocal][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  \"suggest\": {\r\n    \"mysuggest\": {\r\n      \"text\": \"pota\",\r\n      \"completion\": {\r\n        \"field\": \"shortDescription\",\r\n        \"size\": 20\r\n      }\r\n    }\r\n  }\r\n}]]]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; }{[TaTSmiM2TV27_ptdFdx5eQ][elasticsearchlocal][1]: SearchParseException[[elasticsearchlocal][1]: from[-1],size[-1]: Parse Failure}"

This is how my code looks

Public Class AutoComplete
{
        public int ItemId { get; set; }

```
    public SuggestionField ShortDescription { get; set; }

    public string BrandName { get; set; }

    public string[] MenuNode { get; set; }

    public decimal Weight { get; set; 
```

}

public class SuggestionField
    {
        public string[] Input { get; set; }

```
    public string Output { get; set; }

}
```

Indexing Query  
        public static bool CreateIndex()
        {
            var result = ElasticClientSingleton.WriteInstance.CreateIndex(IndexName, index =&gt; index
                           .AddMapping&lt;AutoComplete&gt;(tmd =&gt; tmd
                               .Properties(props =&gt; props
                                   .Completion(s =&gt;
                                       s.Name(p =&gt; p.ShortDescription)
                                       .IndexAnalyzer("stop")
                                       .SearchAnalyzer("stop")
                                       .MaxInputLength(20)
                                       .Payloads(true)
                                       .PreservePositionIncrements()
                                       .PreserveSeparators()
                                       )
                                    .Completion(s =&gt;
                                       s.Name(p =&gt; p.BrandName)
                                       .IndexAnalyzer("stop")
                                       .SearchAnalyzer("stop")
                                       .MaxInputLength(20)
                                       .Payloads(true)
                                       .PreservePositionIncrements()
                                       .PreserveSeparators())
                                    .Completion(s =&gt;
                                       s.Name(p =&gt; p.MenuNode)
                                       .IndexAnalyzer("stop")
                                       .SearchAnalyzer("stop")
                                       .MaxInputLength(20)
                                       .Payloads(true)
                                       .PreservePositionIncrements()
                                       .PreserveSeparators())
                               )
                           )
                       );

```
        return result.IsValid;
    }
```

Query:
var query = ElasticClientSingleton.ReadInstance.Search&lt;SmartSiteAutoComplete&gt;(s =&gt; s
                .Index(IndexName)
                .SuggestCompletion("mysuggest", ts =&gt; ts
                    .Text(searchTerm)
                    .OnField(p =&gt; p.ShortDescription)
                    .Size(20)  
                    ));
</description><key id="70227914">10737</key><summary>SuggestCompletion , Failed to Execute Phase query : SearchPraseException issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iluvcode</reporter><labels /><created>2015-04-22T20:31:47Z</created><updated>2015-04-25T19:17:00Z</updated><resolved>2015-04-25T19:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T19:16:59Z" id="96264543">Hi @iluvcode 

Please ask questions like these on the mailing list, rather than here.  The issues list is for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate + Remove More Like This API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10736</link><project id="" key="" /><description>This API always caused more pain then use based on my experience. It tries to help out, and automatically fetch the fields from a doc and build a search request with MLT query in it, but almost always, the situation is more complicated than that (more complex search request, munging the source, no source / stored fields, ...), and the API ends up being misleading.

I think with the MLT query we have the right foundation for (certain type of) more like this functionality. And the user can "get" a document and more easily decide (with the use case in mind) how to build the best search request (which include the MLT query).

Also, I haven't seen in practice many users using the MLT API itself, and the ones I helped, we almost always ended up going for "get" + MLT Query for the flexibility it has.

So, I suggest we deprecate it in 1.6, and remove it in 2.0, thoughts?
</description><key id="70210512">10736</key><summary>Deprecate + Remove More Like This API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>deprecation</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T19:08:33Z</created><updated>2015-05-06T16:29:10Z</updated><resolved>2015-05-06T16:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-22T19:08:54Z" id="95306924">&gt; So, I suggest we deprecate it in 1.6, and remove it in 2.0, thoughts?

+1
</comment><comment author="clintongormley" created="2015-04-25T19:13:30Z" id="96264284">+1
</comment><comment author="alexksikes" created="2015-05-05T00:37:14Z" id="98892652">+1 I agree the MLT API is cumbersome and should be replaced with the query. However, the API is still more efficient because the document is fetched only once, and not on every shard. As soon as we have #10217 in place we can fully remove it. +1 to deprecate it in 1.6.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/package-info.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/mlt/RestMoreLikeThisAction.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/mlt/MoreLikeThisRequestTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/morelikethis/XMoreLikeThisTests.java</file><file>src/test/java/org/elasticsearch/search/morelikethis/ItemSerializationTests.java</file><file>src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisTests.java</file></files><comments><comment>More Like This: removal of the MLT API</comment></comments></commit></commits></item><item><title>Integration test failing due to assertion that does not allow calling blocking code from networking threads.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10735</link><project id="" key="" /><description>Sample project is available at: https://github.com/dr2014/ES_IntegrationTest_Issue

Elasticsearch version: 1.5.1

The assertion in Transports [https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/transport/Transports.java#L57] is failing integration test for HTTP requests.

What is the correct way to make a blocking call from RestHandler? 

Stacktrace:
Expected current thread [Thread[elasticsearch[node_s6][http_server_worker][T#1]{New I/O worker #75},5,SampleTest]] to not be a transport thread. Reason: 
java.lang.AssertionError: Expected current thread [Thread[elasticsearch[node_s6][http_server_worker][T#1]{New I/O worker #75},5,TGRP-SampleTest]] to not be a transport thread. Reason: 
    at org.elasticsearch.transport.Transports.assertNotTransportThread(Transports.java:57)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:117)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
    at plugin.TestRestHandler.handleRequest(TestRestHandler.java:37)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:225)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:170)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:329)
</description><key id="70201710">10735</key><summary>Integration test failing due to assertion that does not allow calling blocking code from networking threads.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dr2014</reporter><labels><label>non-issue</label></labels><created>2015-04-22T18:31:06Z</created><updated>2015-04-22T18:36:01Z</updated><resolved>2015-04-22T18:35:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-22T18:35:55Z" id="95294987">use the mainling list for questions like this... the answer to your question is in https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable node shutdown API for masters when gateway.recover_after_data_nodes is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10734</link><project id="" key="" /><description>After setting the gateway.recover_after_data_nodes any started masters will not respond to _shutdown.  It will result in:

```
{ 
   "error" : "ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/1/state not recovered /    initialized];]", 
   "status" : 503 
}
```

In this case the decision has been made to take down the cluster before any data nodes are started so is it necessary to continue to block operations in this case? 
</description><key id="70191691">10734</key><summary>Enable node shutdown API for masters when gateway.recover_after_data_nodes is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nemonster</reporter><labels /><created>2015-04-22T17:46:53Z</created><updated>2015-07-02T19:55:36Z</updated><resolved>2015-07-02T19:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T19:06:42Z" id="96264027">Interestingly, there is talk of removing the shutdown API altogether...  @s1monw ?
</comment><comment author="s1monw" created="2015-04-27T15:06:32Z" id="96694327">YEAH IMO we should remove the shutdown API completely without replacement. This is what OS scripts are for
</comment><comment author="s1monw" created="2015-04-27T15:25:48Z" id="96707259">see https://github.com/elastic/elasticsearch/pull/10831
</comment><comment author="pickypg" created="2015-07-02T19:55:34Z" id="118149041">This is closed by #10831.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>escaping appears to turn off fuzzy matching and wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10733</link><project id="" key="" /><description>I've got the folllowing query

'query': {
          'query_string': {
            'fields': ['*.searchable', 'name', 'account.name', 'subject'],
            'escape': true,
            'query': query
          }
        }

Using the (very poorly documented I might add) escape feature: https://github.com/elastic/elasticsearch/issues/41

This appears to turn of wild cards and fuzzy search, e.g.: searching for:
exp~+exp*

Does NOT return expansion.

Is this by design? If so, is there any way to escape characters, while still maintaining the Query DSL functionality?

This is desirable in case a customer wanted to search for say a string containing AND~
</description><key id="70179870">10733</key><summary>escaping appears to turn off fuzzy matching and wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apolishch</reporter><labels /><created>2015-04-22T16:56:47Z</created><updated>2015-04-25T19:04:26Z</updated><resolved>2015-04-25T19:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T19:04:25Z" id="96263937">@apolishch heh I didn't even know about the `escape` option, even though I apparently commented on that issue :)

Setting `escape` to true seems to defeat the point of using the `query_string` query.  Rather use the `match` query for these cases.

That said, you can escape individual special chars with `\`, eg:

```
PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "text": {
          "type": "string",
          "analyzer": "whitespace"
        }
      }
    }
  }
}

PUT test/test/1
{
  "text": "This AND~ that"
}

GET _search
{
  "query": {
    "query_string": {
      "default_field": "text",
      "query": "AND\\~"
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Actions for the two phases of synced flush </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10732</link><project id="" key="" /><description>- actions for gathering the commit ids after a flush for one shard and all of its copies
- write the sync id to all copies of a shard as TransportShardReplicationOperationAction

See https://github.com/elastic/elasticsearch/issues/10032#issuecomment-93706101 ("pre sync commit phase" and "sync commit phase")
</description><key id="70172504">10732</key><summary>Actions for the two phases of synced flush </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-04-22T16:21:23Z</created><updated>2015-04-30T08:56:54Z</updated><resolved>2015-04-30T08:56:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-23T15:04:13Z" id="95617613">looking good. left a bunch of comments..
</comment><comment author="brwe" created="2015-04-24T09:03:55Z" id="95860038">We decided to only implement the feedback on tests and push now anyway - @bleskes wants to see if this can be made even easier and implementing all the feedback would be a waste of time
</comment><comment author="brwe" created="2015-04-30T08:56:53Z" id="97711744">was already merged to feature/synced_flush
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: reduce dependencies on the Filter API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10731</link><project id="" key="" /><description>This commit changes aggregation and function_score so that they consume filters
using the Query API.
</description><key id="70171397">10731</key><summary>Search: reduce dependencies on the Filter API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-04-22T16:16:23Z</created><updated>2015-04-30T09:56:30Z</updated><resolved>2015-04-30T09:56:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-30T09:56:26Z" id="97725531">Superseded by https://github.com/elastic/elasticsearch/pull/10897
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor and cleanup transport request handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10730</link><project id="" key="" /><description>This refactoring and cleanup is that each request handler ends up
implementing too many methods that can be provided when the request handler itself
is registered, including a prototype like class that can be used to instantiate
new request instances for streaming.
</description><key id="70169287">10730</key><summary>Refactor and cleanup transport request handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T16:07:35Z</created><updated>2015-06-07T10:15:32Z</updated><resolved>2015-04-24T12:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-22T16:11:19Z" id="95251336">since this is a big change, I would recommend focusing on TransportService change, almost all the rest are simply the implications of this change.

The other interesting ones are 3 base transport classes where an inner non static class was used (we can't have it, yay reflection), which I moved to the request class itself and used the notion of package internal shard id, this helps remove another class which I think simplifies the code anyhow.
</comment><comment author="s1monw" created="2015-04-24T11:44:35Z" id="95907267">left a bunch of comments I love the cleanup and we should move quickly here
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/indexedscripts/delete/TransportDeleteIndexedScriptAction.java</file><file>src/main/java/org/elasticsearch/action/indexedscripts/get/TransportGetIndexedScriptAction.java</file><file>src/main/java/org/elasticsearch/action/indexedscripts/put/TransportPutIndexedScriptAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/action/support/HandledTransportAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeReadOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ShardReplicationOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/SingleCustomOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java</file><file>src/main/java/org/elasticsearch/river/cluster/PublishRiverClusterStateAction.java</file><file>src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/main/java/org/elasticsearch/transport/BaseTransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java</file><file>src/main/java/org/elasticsearch/transport/TransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/TransportService.java</file><file>src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/support/replication/ShardReplicationOperationTests.java</file><file>src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java</file><file>src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java</file><file>src/test/java/org/elasticsearch/test/transport/MockTransportService.java</file><file>src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java</file></files><comments><comment>refactor and cleanup transport request handling</comment></comments></commit></commits></item><item><title>Return term vectors as part of the search response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10729</link><project id="" key="" /><description>Adds a new parameter to the search API called `term_vectors` which takes as
input `true`, `false` or an `object` of parameters. The parameters are exactly
the same as the ones specified in the Term Vectors API, with the exception of
`_index`, `_type`, `_id`, `doc`, `_routing`, `_version` and `_version_type`.

Relates to #10823 
</description><key id="70159573">10729</key><summary>Return term vectors as part of the search response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>feature</label><label>stalled</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T15:27:01Z</created><updated>2015-07-22T16:10:58Z</updated><resolved>2015-07-01T13:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-04-22T16:57:56Z" id="95263636">Test suite does not seem to pass. Can you fix that before review?
</comment><comment author="jpountz" created="2015-04-28T16:33:08Z" id="97128183">The code looks good to me but I have concerns about the API: the ability to get term vectors for every search hit sounds a bit esoteric to me and we have been leaning towards removing esoteric features recently. I will let other chime in but at the minimum I think this should be marked experimental.
</comment><comment author="alexksikes" created="2015-04-28T16:57:08Z" id="97136560">Thanks for the review. I think this will be useful especially for NLP tasks. Note that term vectors are never returned by default.
</comment><comment author="clintongormley" created="2015-04-29T10:10:00Z" id="97377493">I agree with @jpountz here - I think that the existing term vectors APIs, plus this integration, plus vectorizers (https://github.com/elastic/elasticsearch/issues/10823) should be moved into a plugin to reduce the complexity in core.  See this issue about reducing complexity in core: https://github.com/elastic/elasticsearch/issues/10368
</comment><comment author="alexksikes" created="2015-05-04T15:38:49Z" id="98757098">I agree that the vectorize API should be moved to a plugin. However, I think that this would also be useful for MLT to find documents similar to a group of documents provided by a given query. So I'm not sure anymore whether this PR should be moved to the vectorize plugin. Any thoughts?
</comment><comment author="alexksikes" created="2015-06-29T16:01:46Z" id="116742431">What is the status quo concerning this PR. We can't remove the TVs API as it is being used by MLT. We agreed that the Vectorize API should be a plugin. However to its support this particular integration is useful:

1) for the Vectorize plugin
2) in MLT search by query scenario
3) in NLP tasks it could be useful to return the result of analysis for a set of documents.

Also this integration does not add much complexity and its usage is purely optional. On the other hand, it is not too difficult to leave this operation to the application client. It would just mean that the user would have to perform a lot of TVs requests for each document returned.

@brwe @jpountz @clintongormley WDYT?
</comment><comment author="jpountz" created="2015-07-01T09:25:15Z" id="117559344">&gt; Also this integration does not add much complexity and its usage is purely optional.

I think any new parameter adds complexity by increasing the surface area of an API.

&gt; it is not too difficult to leave this operation to the application client

+1 on this option

&gt;  It would just mean that the user would have to perform a lot of TVs requests for each document returned

You could still avoid the round trips with a multi-term-vectors request?
</comment><comment author="jpountz" created="2015-07-01T13:26:12Z" id="117674088">I just talked with @brwe and one option I had not considered was to add this option to the search API as a plugin. If I'm not mistaken, you can't plug custom phases in the search API today, so this is infrastructure we would need to add. But then I'm worried about exposing even more internals of elasticsearch than today, since it would make it harder to perform internal refactorings without impacting plugins and would potentially allow plugins to "poison" internal workings of elasticsearch.
</comment><comment author="clintongormley" created="2015-07-01T13:28:14Z" id="117675443">Another option we explored with @brwe was including a native script in a plugin, then the script can be called (with parameters) as a script_field in the search API.
</comment><comment author="brwe" created="2015-07-01T13:35:13Z" id="117678766">just for reference, here is my workaround script for the vectorizer right now, term_vectors would look similar: https://github.com/brwe/es-token-plugin/blob/master/src/main/java/org/elasticsearch/script/SparseVectorizerScript.java
just don't look too closely, just a hack...
</comment><comment author="alexksikes" created="2015-07-01T15:11:00Z" id="117711320">If understand correctly in order to have TVs returned as part of a scan and scroll request, I would have to:

1) either build the infrastructure to allow for fetch phase extensions
2) or perform a mTVs request on the client side
3) use a script_field

1) adds complexity for so far only one use case
2) is not doable on thousands of documents. This would be a huge request without the session mechanism of scan / scroll, and there may also be version conflicts.
3) seems to be more of hack

I'm sorry to insist but I think that support for this option is a natural one. We just ask for more information than just the source and it would make implementing certain features (vectorizer for example as a plugin) a lot easier.
</comment><comment author="brwe" created="2015-07-22T16:10:58Z" id="123774727">Since one of the arguments against pluggable sub-phases was that it might make it harder to maintain the code I took a closer look at what would need to be done. I made a pr with a prof of concept here to show what would happen: #12400 
I now actually think we would remove a little complexity by this. Let me know what you think!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: FielddataTermsFilter equality is based on hash codes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10728</link><project id="" key="" /><description>FielddataTermsFilter considers that two filters are equal if they apply to the same field name and the **hash code** of the wrapped terms are the same. It should compare terms directly.
</description><key id="70154779">10728</key><summary>Search: FielddataTermsFilter equality is based on hash codes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T15:11:55Z</created><updated>2015-04-26T11:27:59Z</updated><resolved>2015-04-22T16:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-22T15:58:55Z" id="95245663">Fixed through https://github.com/elastic/elasticsearch/pull/10727 and then backported.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to lucene-5.2-snapshot-1675363.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10727</link><project id="" key="" /><description>This snapshot contains in particular LUCENE-6446 (refactored explanation API)
and LUCENE-6448 (better equals/hashcode for filters).

Closes #10728
</description><key id="70149080">10727</key><summary>Upgrade to lucene-5.2-snapshot-1675363.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T14:48:28Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-22T15:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-22T15:17:16Z" id="95231642">Looks great. I think in general i prefer "min of" or "minimum of" where we currently say "Math.min of", because that only makes sense to java developers. Otherwise I am happy the equals/hashcode are all using superclass methods, and ComplexExplanation is out of here.
</comment><comment author="jpountz" created="2015-04-22T15:19:42Z" id="95232740">Thanks Robert. I kept the explanation descriptions as they were but I don't mind applying the fixes you suggest. We might find more bugs in equals/hashcode as we migrate to queries which recursively call QueryUtils.check on sub queries on the contrary to filters.
</comment><comment author="rmuir" created="2015-04-22T15:23:08Z" id="95234969">Indeed, they were in the CombineFunction before that way (I missed that). I thought they were newly introduced here.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java</file><file>src/main/java/org/elasticsearch/index/search/FieldDataTermsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/NumericRangeFieldDataFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NonNestedDocsFilter.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/explain/ExplainActionTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/TopChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScorePluginTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>Merge pull request #10727 from jpountz/upgrade/lucene-5.2-snapshot-1675363</comment></comments></commit></commits></item><item><title>Don't use nested classes for Rest tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10726</link><project id="" key="" /><description>Nested classes have the advantage of organizing the hack in a way
where its easy to see what is happening overall, but they have
the downside of class names with $ in them.

These names work just fine, but can require shell escaping
or other annoyances, which is the last thing you want if
you are trying to just reproduce.
</description><key id="70147737">10726</key><summary>Don't use nested classes for Rest tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T14:43:33Z</created><updated>2015-04-22T15:10:33Z</updated><resolved>2015-04-22T15:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-22T14:55:48Z" id="95221087">LGTM
</comment><comment author="jpountz" created="2015-04-22T14:55:54Z" id="95221109">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTestCase.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest0Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest1Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest2Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest3Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest4Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest5Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest6Tests.java</file><file>src/test/java/org/elasticsearch/test/rest/Rest7Tests.java</file></files><comments><comment>Merge pull request #10726 from rmuir/nuke_nested_classes</comment></comments></commit></commits></item><item><title>Add common SystemD file for RPM/DEB package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10725</link><project id="" key="" /><description>Since SystemD is well documented and supported, we don't need to have different files for DEB/RPM packages.

Tested on OpenSUSE13, Fedora 21, Debian 8 and Ubuntu 15.04 beta2.
</description><key id="70135974">10725</key><summary>Add common SystemD file for RPM/DEB package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T14:10:33Z</created><updated>2015-05-29T18:18:09Z</updated><resolved>2015-05-27T09:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-22T14:13:04Z" id="95195662">@electrical Can you have a look please? thanks
</comment><comment author="tlrx" created="2015-05-03T14:07:55Z" id="98484675">@spinscale Can you have a look please? thanks
</comment><comment author="spinscale" created="2015-05-04T06:58:29Z" id="98603437">when installing on jessie, all the output is occuring in `journalctl` after starting, guess this is not the desired behaviour

```
May 04 06:55:59 debian-800-jessie elasticsearch[664]: [2015-05-04 06:55:59,721][INFO ][node                     ] [Mar-Vell] version[2.0.0-SNAPSHOT], pid[664], build[cf7785d/2015-05-04T06:45:19Z]
May 04 06:55:59 debian-800-jessie elasticsearch[664]: [2015-05-04 06:55:59,723][INFO ][node                     ] [Mar-Vell] initializing ...
May 04 06:55:59 debian-800-jessie elasticsearch[664]: [2015-05-04 06:55:59,729][INFO ][plugins                  ] [Mar-Vell] loaded [], sites []
May 04 06:55:59 debian-800-jessie elasticsearch[664]: [2015-05-04 06:55:59,800][INFO ][env                      ] [Mar-Vell] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [7.1gb], net tota
May 04 06:56:03 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:03,336][INFO ][node                     ] [Mar-Vell] initialized
May 04 06:56:03 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:03,400][INFO ][node                     ] [Mar-Vell] starting ...
May 04 06:56:03 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:03,623][INFO ][transport                ] [Mar-Vell] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.0.2.15:9
May 04 06:56:03 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:03,641][INFO ][discovery                ] [Mar-Vell] elasticsearch/5OCAmEJIR6C2ALHieIfc7A
May 04 06:56:07 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:07,423][INFO ][cluster.service          ] [Mar-Vell] new_master [Mar-Vell][5OCAmEJIR6C2ALHieIfc7A][debian-800-jessie][inet[/10.0.2.1
May 04 06:56:07 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:07,450][INFO ][http                     ] [Mar-Vell] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.0.2.15:9
May 04 06:56:07 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:07,451][INFO ][node                     ] [Mar-Vell] started
May 04 06:56:07 debian-800-jessie elasticsearch[664]: [2015-05-04 06:56:07,476][INFO ][gateway                  ] [Mar-Vell] recovered [0] indices into cluster_state
May 04 06:57:30 debian-800-jessie elasticsearch[664]: [2015-05-04 06:57:30,514][INFO ][cluster.metadata         ] [Mar-Vell] [foo] creating index, cause [api], templates [], shards [5]/[1], mappings []
```

tested this against the pr rebased against master

**Update**: Same happens on opensuse
</comment><comment author="spinscale" created="2015-05-04T07:15:59Z" id="98605395">we also should update docs at http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html#_apt - as we always call `update-rc.d` in the apt parapgraph
</comment><comment author="tlrx" created="2015-05-07T08:55:36Z" id="99780363">@spinscale thanks for your review. I updated the code following your comments but I was forced to rebase because of #10986.

Having STDOUT in the journal did not choke me since I find this useful. But we already log everything in the log file so I agree with you, we can skip the journal log: I added option [StandardOutput](https://github.com/elastic/elasticsearch/pull/10725/files#diff-3a8b47152707cc3e2cf528afb30666b7R27) for that.

I also added option [StandardError](https://github.com/elastic/elasticsearch/pull/10725/files#diff-3a8b47152707cc3e2cf528afb30666b7R30) so that if startup failed it will be printed in journal.

I updated the documentation [here](https://github.com/elastic/elasticsearch/pull/10725/files#diff-bf8e9feb408d93bb0d85b3bba1f05ddfR54) and add a one line check in tests [here](https://github.com/elastic/elasticsearch/pull/10725/files#diff-36348e13fcc9bbcc47b1e9af995ad851R75) for #10986.

Tested on OpenSUSE13, Fedora 21, Debian 8 and Ubuntu 15.04 server.

Can you please have a look? I'd like to push this quickly and move forward on SysV init scripts. Thanks :) 
</comment><comment author="spinscale" created="2015-05-22T15:12:35Z" id="104685261">Tested on
- debian jessie: works
- ubuntu vivid/trusty: trusty has a perms startup issue, vivid looks fine with a minor thing, see below
- opensuse: works
- fedora 21: works

This is the `journalctl` output on vivid, fedora and opensuse, maybe we should change the name just to `Elasticsearch`?

```
May 22 14:46:33 vagrant-ubuntu-vivid-64 systemd[1]: Started Starts and stops a single elasticsearch instance on this system.
May 22 14:46:33 vagrant-ubuntu-vivid-64 systemd[1]: Starting Starts and stops a single elasticsearch instance on this system...
```

I get this on my ubuntu trusty system, did you change anything there? The process starts, but stops again with this message (added in the `-C` parameter to the `start-stop-daemon` in the init script revealed it)

```
ElasticsearchException[Failed to delete pid file /var/run/elasticsearch.pid]; nested: AccessDeniedException[/var/run/elasticsearch.pid];
    at org.elasticsearch.common.PidFile$1.run(PidFile.java:115)
Caused by: java.nio.file.AccessDeniedException: /var/run/elasticsearch.pid
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
    at java.nio.file.Files.deleteIfExists(Files.java:1165)
    at org.elasticsearch.common.PidFile$1.run(PidFile.java:113)
```
</comment><comment author="tlrx" created="2015-05-26T10:22:39Z" id="105480743">@spinscale thanks for your review!

&gt; This is the journalctl output on vivid, fedora and opensuse, maybe we should change the name just to Elasticsearch?

Done.

&gt; I get this on my ubuntu trusty system, did you change anything there?

No, but in the meanwhile the security manager has been enabled. I fixed few things in #10986 but I did not double check the init.d scripts... The Debian script was referring to `/var/run/elasticsearch.pid` whereas Redhat script was referring to `/var/run/elasticsearch/elasticsearch.pid`. I uniformized this in this pull request (see last commit).
</comment><comment author="spinscale" created="2015-05-26T12:53:09Z" id="105516039">another test

```
# start-stop-daemon --start -C -b --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.conf=/etc/elasticsearch

Error: You must build the project with Maven or download a pre-built package
before you can run Elasticsearch. See 'Building from Source' in README.textile
or visit http://www.elasticsearch.org/download to get a pre-built package.
```

reason in `/usr/share/elasticsearch/bin/elasticsearch`:

``` bash
IS_PACKAGED_VERSION='Elasticsearch core'
if [ "$IS_PACKAGED_VERSION" != "elasticsearch" ]; then
    cat &gt;&amp;2 &lt;&lt; EOF
Error: You must build the project with Maven or download a pre-built package
before you can run Elasticsearch. See 'Building from Source' in README.textile
or visit http://www.elasticsearch.org/download to get a pre-built package.
EOF
    exit 1
fi
```
</comment><comment author="tlrx" created="2015-05-26T12:59:05Z" id="105517852">@spinscale Yes, it has been introduced by commit 6051991 and should be fixed by #11348
</comment><comment author="spinscale" created="2015-05-26T13:57:05Z" id="105533906">rebased against master and tested against older and latest ubuntu, both now start ES.
init.d based startup correctly now has the pid-file in `/var/run/elasticsearch/elasticsearch.pid`

Also nice sideeffect with systemd, one catches stdout/stderr and thus finds out, in case of badly configured heap

```
journalctl -f
-- Logs begin at Tue 2015-05-26 13:48:53 UTC. --
May 26 13:52:41 vagrant-ubuntu-vivid-64 systemd[1]: Started Elasticsearch.
May 26 13:52:41 vagrant-ubuntu-vivid-64 systemd[1]: Starting Elasticsearch...
May 26 13:52:41 vagrant-ubuntu-vivid-64 elasticsearch[1492]: Invalid initial heap size: -Xms2gb
May 26 13:52:41 vagrant-ubuntu-vivid-64 elasticsearch[1492]: Error: Could not create the Java Virtual Machine.
May 26 13:52:41 vagrant-ubuntu-vivid-64 elasticsearch[1492]: Error: A fatal exception has occurred. Program will exit.
May 26 13:52:41 vagrant-ubuntu-vivid-64 systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE
May 26 13:52:41 vagrant-ubuntu-vivid-64 systemd[1]: Unit elasticsearch.service entered failed state.
May 26 13:52:41 vagrant-ubuntu-vivid-64 systemd[1]: elasticsearch.service failed.
```

LGTM, thanks so much for taking this up!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add more utilities for source/body handling in RestAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10724</link><project id="" key="" /><description /><key id="70134604">10724</key><summary>Add more utilities for source/body handling in RestAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T14:05:39Z</created><updated>2015-06-07T16:59:09Z</updated><resolved>2015-04-22T15:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-04-22T14:16:25Z" id="95198024">@s1monw Thanks for utility.
We should change RestSearchScrollAction.
</comment><comment author="s1monw" created="2015-04-22T14:19:30Z" id="95199909">&gt; We should change RestSearchScrollAction.

yeah will do
</comment><comment author="s1monw" created="2015-04-22T14:57:19Z" id="95221419">@johtani I pushed a new commit can you take another look?
</comment><comment author="johtani" created="2015-04-22T15:37:02Z" id="95240065">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvectors/RestTermVectorsAction.java</file></files><comments><comment>Merge pull request #10724 from s1monw/rest_actions_helper</comment></comments></commit></commits></item><item><title>Bulk freezes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10723</link><project id="" key="" /><description>If I send this json to elasticsearch, cluster was freezes.
https://dl.dropboxusercontent.com/u/1003531/bulk_freeze.txt
</description><key id="70122810">10723</key><summary>Bulk freezes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels><label>non-issue</label></labels><created>2015-04-22T13:15:28Z</created><updated>2015-04-24T13:47:25Z</updated><resolved>2015-04-24T13:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-04-22T19:06:51Z" id="95306545">@kiryam which version of elasticsearch is this? This bug looks very similar to #7266, which was fixed in 1.4.0 and 1.3.3.
</comment><comment author="kiryam" created="2015-04-22T19:49:04Z" id="95315260">@imotov 

```
{
"status": 200,
"name": "process",
"cluster_name": "elasticsearch",
"version": {
"number": "1.4.4",
"build_hash": "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
"build_timestamp": "2015-02-19T13:05:36Z",
"build_snapshot": false,
"lucene_version": "4.10.3"
},
"tagline": "You Know, for Search"
}
```
</comment><comment author="imotov" created="2015-04-22T20:17:05Z" id="95323690">@kiryam could you share the mapping for this index and type? I cannot reproduce the issue with v1.4.4 and default mapping?
</comment><comment author="kiryam" created="2015-04-23T04:49:48Z" id="95431381">@imotov https://gist.github.com/kiryam/93df99e99ffaf397072e
</comment><comment author="imotov" created="2015-04-23T14:06:26Z" id="95597219">@kiryam How are `ru` and `exact` analyzers defined? Could you post settings for this index? Are you using any analysis plugins? If you do which versions of the plugins are you using?
</comment><comment author="kiryam" created="2015-04-23T14:10:55Z" id="95599231">@imotov https://gist.github.com/kiryam/94a8c418e09025d7ef77

```
/usr/share/elasticsearch/plugins/analysis-morphology# ls
elasticsearch-analysis-morphology-1.2.0.jar  english-4.6.0.jar  morph-4.6.0.jar  russian-4.6.0.jar
```
</comment><comment author="imotov" created="2015-04-23T22:20:20Z" id="95734349">@kiryam the problem is in this regular expression:

```
exact_rstrip: 
        type: "pattern_replace"
        pattern: "[^#@a-zA-Zа-яА-Я0-9$%°~_]+($| )"
        replacement: "$1"
```

It's just takes too long to execute this regex on the string that consists of a million of "\" characters. I just did an [experiment with 50k string](https://gist.github.com/imotov/f56d090981e72dc87828) and it takes 17 seconds to process on my machine. 

What are you trying to achieve here?
</comment><comment author="kiryam" created="2015-04-24T07:44:55Z" id="95837061">@imotov I want to remove special characters exclude "[^#@a-zA-Zа-яА-Я0-9$%°~_] in end of word
</comment><comment author="clintongormley" created="2015-04-24T13:45:32Z" id="95937205">Could you replace all three character filters with the following?

```
PUT /test?pretty
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "exact": {
            "tokenizer": "whitespace",
            "char_filter": [
              "strip_chars"
            ]
          }
        },
        "char_filter": {
          "strip_chars": {
            "type": "pattern_replace",
            "pattern": "[^#@a-zA-Zа-яА-Я0-9$%°~_]+",
            "replacement": " "
          }
        }
      }
    }
  }
}
```

With @imotov's test string, this is pretty much instantaneous.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolation requests can be executed before all percolator queries are loaded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10722</link><project id="" key="" /><description>The percolator can sometimes fail to match queries right after shard recovery. Version observed in: 1.5.1. 

The percolator keeps all queries in an in-memory collection (shard by shard), which it reads from the index at startup. This is done by registering a listener to the IndicesLifecycle, with the listener loads the queries when afterIndexShardPostRecovery is called. 

This seems to not block the shard to be reported as initialised, so sometimes this is not completed before the cluster returns a yellow status. Thus, if a request comes in _before_ all queries have been loaded into the in-memory structure, the response will erroneously say that there were no matches.

I've been unable to create a predictively failing test for this. This test _sometimes_ exposes the error (by not passing). For me, it fails every 5-6th time:

```
package org.elasticsearch.test.integration;

import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
import org.elasticsearch.action.percolate.PercolateRequestBuilder;
import org.elasticsearch.action.percolate.PercolateResponse;
import org.elasticsearch.action.percolate.PercolateSourceBuilder;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.Requests;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.percolator.PercolatorService;
import org.testng.annotations.AfterClass;
import org.testng.annotations.Test;

import java.io.IOException;
import java.util.concurrent.ExecutionException;

import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
import static org.hamcrest.CoreMatchers.equalTo;
import static org.hamcrest.CoreMatchers.is;
import static org.hamcrest.MatcherAssert.assertThat;

/**
 *
 */
public class RecoveryTests extends AbstractNodesTests {
    @AfterClass
    public void closeNodes() {
        closeAllNodesAndClear();
    }

    @Test(enabled = true)
    public void testRestartNode() throws IOException, ExecutionException, InterruptedException {
        Settings extraSettings = ImmutableSettings.settingsBuilder()
                .put("index.gateway.type", "local").build();

        logger.info("--&gt; Starting one nodes");
        startNode("node1", extraSettings);
        Client client = client("node1");

        logger.info("--&gt; Add dummy doc");
        client.admin().indices().prepareDelete("_all").execute().actionGet();
        client.prepareIndex("test", "type", "1").setSource("field", "value").execute().actionGet();

        logger.info("--&gt; Register query");
        client.prepareIndex("test", PercolatorService.TYPE_NAME, "1")
                .setSource(jsonBuilder()
                                .startObject()
                                .field("query", matchQuery("field", "b"))
                                .field("id", 1)
                                .field("group", "g1")
                                .field("query_hash", "hash1")
                                .endObject()
                ).setRefresh(true).execute().actionGet();
        logger.info("--&gt; Restarting node");
        closeNode("node1");
        startNode("node1", extraSettings);
        client = client("node1");
        logger.info("Waiting for cluster health to be yellow");
        waitForYellowIndices(client);

        logger.info("--&gt; Percolate doc with field=b");
        PercolateResponse response = new PercolateRequestBuilder(client).setIndices("test").setDocumentType("type")
                .setSource(new PercolateSourceBuilder().setDoc(PercolateSourceBuilder.docBuilder().setDoc(jsonBuilder().startObject().field("_id", "1").field("field", "b").endObject())))
                .execute().actionGet();

        assertThat(response.getCount(), is(1l));

        logger.info("--&gt; Restarting node again (This will trigger another code-path since translog is flushed)");
        closeNode("node1");
        startNode("node1", extraSettings);
        client = client("node1");
        logger.info("Waiting for cluster health to be yellow");
        waitForYellowIndices(client);

        logger.info("--&gt; Percolate doc with field=b");
        response = new PercolateRequestBuilder(client).setIndices("test").setDocumentType("type")
                .setSource(new PercolateSourceBuilder().setDoc(PercolateSourceBuilder.docBuilder().setDoc(jsonBuilder().startObject().field("_id", "1").field("field", "b").endObject())))
                .execute().actionGet();

        assertThat(response.getCount(), is(1l));

    }

    private void waitForYellowIndices(Client client) {
        ClusterHealthResponse health = client.admin().cluster().health(Requests.clusterHealthRequest(new String[]{}).waitForYellowStatus().waitForActiveShards(5)).actionGet();
        assertThat(health.isTimedOut(), equalTo(false));
    }
}

```

There are tests similar to this one for the percolator, which are supposed to test the same thing. From what I understand though, those are very particular about the cluster setup.. might it be that they can't catch this issue?
</description><key id="70116835">10722</key><summary>Percolation requests can be executed before all percolator queries are loaded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">antonha</reporter><labels><label>:Percolator</label></labels><created>2015-04-22T12:54:15Z</created><updated>2015-06-23T18:12:48Z</updated><resolved>2015-06-23T18:12:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ekesken" created="2015-05-12T14:47:58Z" id="101306646">We live same problem during auto-scaling of our elasticsearch cluster in batch operations.

I described our problem and shared a script to reproduce problem in following stackoverflow post:

http://stackoverflow.com/questions/30194246/percolate-returns-empty-matches-under-heavy-load-during-elasticsearch-cluster-re

Is there any workaround that we can apply? It's critical for our scenario not to miss any content during percolation.
</comment><comment author="ekesken" created="2015-05-13T05:40:37Z" id="101519715">Is there a way to check every shard is really OK, before sending percolation requests? obviously checking green status does not work.
</comment><comment author="clintongormley" created="2015-05-27T09:23:38Z" id="105836748">@martijnvg can we somehow not mark a percolation shard as active until the percolation requests have been loaded?
</comment><comment author="clintongormley" created="2015-05-27T09:24:31Z" id="105836869">(Note: this is not new in 1.5.1 - it has worked this way since the beginning)
</comment><comment author="clintongormley" created="2015-06-23T18:12:47Z" id="114594633">Closed by #11799
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use of default CONF_DIR/CONF_FILE in plugin install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10721</link><project id="" key="" /><description>The bin/plugin script now uses the default CONF_DIR &amp; CONF_FILE environment vars. This allows to install a plugin even if Elasticsearch has been installed with a RPM or a DEB package. This commit also adds testing files for TAR archive and plugins installation. Related to ##7946.

Closes #10673
</description><key id="70114995">10721</key><summary>Use of default CONF_DIR/CONF_FILE in plugin install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>blocker</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T12:42:34Z</created><updated>2015-06-07T16:51:38Z</updated><resolved>2015-05-25T12:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-28T08:55:04Z" id="96981397">Removing review label for now, something breaks with the security manager.
</comment><comment author="tlrx" created="2015-04-29T15:28:18Z" id="97470794">@spinscale Could you please have a look at this one? It should be straightforward to test. Thanks
</comment><comment author="spinscale" created="2015-05-22T14:15:54Z" id="104671469">tested plugin installation (marvel and kopf) and removal with deb on debian jessie ubuntu trusty (initd), ubuntu vivid (systemd), with RPM on opensuse ... all worked flawless

Minor nitpick: Maybe put a "Packaging: " prefix in the commit message/PR title, so its easier for others what this is about

Last part: I'd love to get the BATS stuff into CI after this.

LGTM!
</comment><comment author="tlrx" created="2015-05-25T11:45:41Z" id="105216238">@spinscale thanks a lot for your review! I updated the commit/pr message.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify dynamic mappings updates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10720</link><project id="" key="" /><description>While dynamic mappings updates are using the same code path as updates from the
API when applied on a data node since #10593, they were still using a different
code path on the master node. This commit makes dynamic updates processed the
same way as updates from the API, which also seems to do a better way at
acknowledgements (I could not reproduce the ConcurrentDynamicTemplateTests
failure anymore). It also adds more checks, like for instance that indexing on
replicas should not trigger dynamic mapping updates since they should have been
handled on the primary before.
</description><key id="70110047">10720</key><summary>Simplify dynamic mappings updates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T12:19:00Z</created><updated>2015-06-07T11:43:26Z</updated><resolved>2015-04-23T09:53:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-22T20:54:15Z" id="95332389">left a couple of comments
</comment><comment author="s1monw" created="2015-04-23T08:57:24Z" id="95496438">LGTM I think we should add the UUID in a second step in a followup issue
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Merge pull request #10720 from jpountz/fix/simplify_mapperupdatedaction</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Merge pull request #10720 from jpountz/fix/simplify_mapperupdatedaction</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Mappings: simplify dynamic mappings updates.</comment></comments></commit></commits></item><item><title>Sampler Aggregator cannot be used in terms agg order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10719</link><project id="" key="" /><description>Sampler Aggregator is a single-bucket aggregator but if you try to use it as part of the order in a terms aggregation it fails. Below is a sense script to reproduce:

``` javascript
POST test/doc/1
{"color":"YELLOW","date":1500000009,"weight":105}

POST test/doc/2
{"color":"YELLOW","date":1500000008,"weight":104}

POST test/doc/3
{"color":"YELLOW","date":1500000007,"weight":103}

POST test/doc/11
{"color":"RED","date":1500000009,"weight":205}

GET test/doc/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "date": {
        "order": "desc"
      }
    }
  ],
  "aggregations": {
    "distinctColors": {
      "terms": {
        "field": "color",
        "size": 1,
        "order": {
          "sample&gt;max_weight.value": "asc"
        }
      },
      "aggregations": {
        "sample": {
          "sampler": {
            "shard_size": 1
          },
          "aggs": {
            "max_weight": {
              "max": {
                "field": "weight"
              }
            }
          }
        }
      }
    }
  }
}
```

The search request throws an ArrayStoreException:

```
org.elasticsearch.transport.RemoteTransportException: [Stallior][inet[/192.168.0.7:9300]][indices:data/read/search[phase/query]]
Caused by: java.lang.ArrayStoreException
    at java.lang.System.arraycopy(Native Method)
    at org.elasticsearch.search.aggregations.support.AggregationPath.subPath(AggregationPath.java:191)
    at org.elasticsearch.search.aggregations.support.AggregationPath.validate(AggregationPath.java:307)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder.validate(InternalOrder.java:145)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder.validate(InternalOrder.java:138)
    at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.&lt;init&gt;(TermsAggregator.java:141)
    at org.elasticsearch.search.aggregations.bucket.terms.AbstractStringTermsAggregator.&lt;init&gt;(AbstractStringTermsAggregator.java:39)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.&lt;init&gt;(GlobalOrdinalsStringTermsAggregator.java:75)
    at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory$ExecutionMode$2.create(TermsAggregatorFactory.java:70)
    at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:223)
    at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:57)
    at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:95)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:69)
    at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:77)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:96)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:296)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:307)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:422)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:1)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:340)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

```

but if you debug at `org.elasticsearch.search.aggregations.support.AggregationPath.subPath(AggregationPath.java:191)` you can see that the aggregator being tested is of type `AggregatorFactory$1` and wraps the `SamplerAggregator`. This is created by the `asMultiBucketAggregator(this, context, parent);` call in `SamplerAggregator$Factory.createInternal(...)`.

The reason SamplerAggregator has to be wrapped is that it's collectors do not take into account the `parentBucketOrdinal`.

We should update the SamplerAggregator (including the Diversity parts) to collect documents for each `parentBucketOrdinal` so that it doesn't need to be wrapped anymore and can be used in ordering like the other single-bucket aggregators
</description><key id="70090056">10719</key><summary>Sampler Aggregator cannot be used in terms agg order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T10:34:37Z</created><updated>2015-05-22T14:23:14Z</updated><resolved>2015-05-22T14:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-23T12:15:14Z" id="95564938">@jpountz , can you confirm my assumption: the parent bucket IDs aggs are asked to collect on are compact and ascending (0,1,2,3...) or do I have to allow for very sparse values (7,10342,...)?
This dictates if I use a map or an array in my sampler collection and also if I in turn should rebase IDs of the buckets that survive the "best docs" selection process.
</comment><comment author="jpountz" created="2015-04-23T12:33:56Z" id="95570842">@markharwood Indeed they are fine to use as array indices. However I'm confused why you are mentioning "surviving" bucket as the sampler aggregator should not filter buckets? My assumption was that it would just compute a different sample on each bucket?
</comment><comment author="markharwood" created="2015-04-23T12:45:38Z" id="95574249">&gt; My assumption was that it would just compute a different sample on each bucket?

My bad. You are correct.
On a separate point - when replaying the deferred collection(s) I need to replay collects in docId order along with the choice of bucket ID. There may be more than one bucket per doc id. A convenient way of doing this which avoids extra object allocations is to take the ScoreDocs produced from each of the samples and sneak the bucketID into the "shardIndex" int value they hold and then sort them for replay. A bit hacky (casting long bucket ids to ints) but should be OK?
</comment><comment author="jpountz" created="2015-04-23T12:51:45Z" id="95575518">This hack sounds ok to me, if you use more than Integer.MAX_VALUE buckets to collect such an aggregator, you will have other issues anyway.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedBytesHashSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedMapSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedNumericSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedOrdinalsSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java</file></files><comments><comment>Aggregation fix: Sampler agg could not be used with Terms agg’s order.</comment><comment>The Sampler agg was not capable of collecting samples for more than one parent bucket.</comment><comment>Added a Junit test case and changed BestDocsDeferringCollector to internally maintain collections per parent bucket.</comment></comments></commit></commits></item><item><title>Filter by empty value of some field doesn't work if index doesn't include document with not empty value for the field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10718</link><project id="" key="" /><description>Hey Guys,

I found strange behavior with my elastic.

So, to reproduce issue follow next steps:

1) Create mapping

```
curl -XPOST localhost:9200/_template/demo -d '{
  "template": "demo",
  "mappings" : {
    "_default_" : {
       "dynamic_templates" : [ {
         "string_fields" : {
           "match" : "*",
           "match_mapping_type" : "string",
           "mapping" : {
             "type" : "multi_field",
               "fields" : {
                 "{name}" : {"type": "string", "index" : "analyzed"},
                 "orig" : {"type": "string", "index" : "not_analyzed"}
               }
           }
         }
       } ]
    }
  }
}
'
```

2) Insert a document

```
curl -XPOST 'http://localhost:9200/demo/document/' -d '{
    "demoField": ""
}
'
```

3) Get terms for the field

```
curl -XPOST 'http://localhost:9200/demo/document/_search' -d '{
    "facets": {
        "terms": {
            "terms": {
                "field": "demoField.orig",
                "size": 10,
                "order": "count",
                "exclude": []
            }
        }
    },
    "size": 0
}
'
```

Result for step 3 includes following json:

```
{
    "took": 5,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0,
        "hits": []
    },
    "facets": {
        "terms": {
            "_type": "terms",
            "missing": 1,
            "total": 0,
            "other": 0,
            "terms": []
        }
    }
}
```

So, I observe that 1 document has been missed.. My expectation there should be found one term for "" (empty line)..

However, if you perform following steps, it begins work fine:

4) Add a new document with filled demoField property

```
curl -XPOST 'http://localhost:9200/demo/document/' -d '{
    "demoField": "testvalue"
}
'
```

5)  Repeat "terms" request

```
curl -XPOST 'http://localhost:9200/demo/document/_search' -d '{
    "facets": {
        "terms": {
            "terms": {
                "field": "demoField.orig",
                "size": 10,
                "order": "count",
                "exclude": []
            }
        }
    },
    "size": 0
}
'
```

Result:

```
{
    "took": 2,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 0,
        "hits": []
    },
    "facets": {
        "terms": {
            "_type": "terms",
            "missing": 1,
            "total": 1,
            "other": 0,
            "terms": [
                {
                    "term": "testvalue",
                    "count": 1
                }
            ]
        }
    }
}
```

6) Insert a document with empty demoField

```
curl -XPOST 'http://localhost:9200/demo/document/' -d '{
    "demoField": ""
}
'
```

7)  Repeat "terms" request

```
curl -XPOST 'http://localhost:9200/demo/document/_search' -d '{
    "facets": {
        "terms": {
            "terms": {
                "field": "demoField.orig",
                "size": 10,
                "order": "count",
                "exclude": []
            }
        }
    },
    "size": 0
}
'
```

Result:

```
{
    "took": 4,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 3,
        "max_score": 0,
        "hits": []
    },
    "facets": {
        "terms": {
            "_type": "terms",
            "missing": 1,
            "total": 2,
            "other": 0,
            "terms": [
                {
                    "term": "testvalue",
                    "count": 1
                },
                {
                    "term": "",
                    "count": 1
                }
            ]
        }
    }
}
```

So, just now we observe "" value in terms result.. btw, missing field continues to include 1 value, (guess it is first document)

Reproduced on ES 1.4.4 and 0.90.13
</description><key id="70054890">10718</key><summary>Filter by empty value of some field doesn't work if index doesn't include document with not empty value for the field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rusboy</reporter><labels /><created>2015-04-22T08:14:44Z</created><updated>2015-04-28T20:52:41Z</updated><resolved>2015-04-25T18:44:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T18:44:14Z" id="96258812">Hi @rusboy 

Thanks for the great bug report.  Just tried this out on 1.5.1 and it appears to have been fixed already. Btw, facets are deprecated and will be removed in 2.0.  Use aggregations instead.
</comment><comment author="rusboy" created="2015-04-28T20:52:41Z" id="97203855">thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable securitymanager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10717</link><project id="" key="" /><description>We have been using this in our tests for some time. Recently the permissions have gotten reasonable where we don't need read/write access everywhere.

I think we should start ES with securitymanager (just like tomcat and other things) to enhance security.

I ran the lucene benchmark suite comparing security manager off/on and there is no difference. I think a lot of concerns about perceived performance differences don't really make sense if you are managing files and sockets properly.

Here is a prototype, i tried to keep it simple (but damn if the SSD detection and other stuff we have going on fights me on that). There is nothing to configure, though security manager can be disabled in elasticsearch.yml or whatever with `security.enabled=false`. 

We use the same actual policy file for both tests and bin/elasticsearch. We just add additional permissions based on the environment (e.g. data paths and so on) so things will work. 

I didnt write any tests yet, but I can add some unit tests for the stuff here.
</description><key id="70044496">10717</key><summary>Enable securitymanager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T07:20:28Z</created><updated>2015-06-08T13:33:32Z</updated><resolved>2015-04-24T13:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-23T19:34:11Z" id="95694338">I pushed commits simplifying the SSD/filestore stuff after #10755 and adding the warning as @jpountz suggested.
</comment><comment author="s1monw" created="2015-04-23T20:28:37Z" id="95710708">this change looks good to me - i am curious if we see per drops with this enabled by default etc. but I even if we do we should push this (plus unittests please) and move the decision to default enable it to a different issue if we need to. I really like that we can offer this to the users and I am really leaning towards enable by default! 
</comment><comment author="mikemccand" created="2015-04-23T21:05:34Z" id="95718825">I tested indexing tiny (log line) docs, same test I run at http://benchmarks.elastic.co.

I ran each of master and this branch two times...

Master is 8999.7 and 9160.0 docs/sec overall (6.9 M docs), while this branch is 9610.9 and 9835.4 docs/sec.

Net/net I don't think security manager hurts indexing performance!
</comment><comment author="rmuir" created="2015-04-24T00:49:28Z" id="95764313">Unless someone can find real performance issues or problems, i plan to push the change in 72 hours. And you guys can bikeshed logging levels somewhere else, im not doing that.
</comment><comment author="kimchy" created="2015-04-24T01:07:38Z" id="95765964">Very promising on the perf test, its great. I will fix the logging post your push, I think that we should also work on a different issue on getting the test permissions out of this file, since it will be confusing for users who look at it, even though those env vars are not set.
</comment><comment author="rmuir" created="2015-04-24T01:08:36Z" id="95766042">i can remove the logging completely. i am happy with no logging and for someone else to deal with logging completely. its such a ridiculous thing to make bikesheds out of. I will not add any more logging statements to this codebase, trust me.
</comment><comment author="rmuir" created="2015-04-24T01:19:34Z" id="95766923">logging is removed.
</comment><comment author="kimchy" created="2015-04-24T01:24:30Z" id="95767429">LGTM, lets push it
</comment><comment author="rmuir" created="2015-04-24T02:05:34Z" id="95773973">I made the policy file a resource (not in conf/) and removed the docs of the option in elasticsearch.yml. Someone can turn it off with security.manager.enabled=false if they dont like it. And they can set JVM_OPTS with -D's to do whatever their heart desires rather than us supporting plumbing. I also added some really simple unit tests so we had something.
</comment><comment author="s1monw" created="2015-04-24T07:30:53Z" id="95834482">left one tiny nitpick - LGTM otherwise feel free to push after fixing the nitpick :)
</comment><comment author="kimchy" created="2015-04-24T10:12:50Z" id="95881802">LGTM, @rmuir if someone provides the `-Djava.security.policy` option, we will still run our code as far as I can tell, just double checking it won't be a problem?
</comment><comment author="rmuir" created="2015-04-24T12:43:49Z" id="95920743">Someone with a custom policy will have to also use the option (security.manager.enabled=false) to disable what we are doing.
</comment><comment author="kimchy" created="2015-04-24T12:54:52Z" id="95924004">@rmuir I see, will it be trappy? which one will be applied if just `-Djava.security.policy` is provided (not sure about the logic of which ones wins), maybe we should not do our thing if the system property exists as well?
</comment><comment author="rmuir" created="2015-04-24T13:01:06Z" id="95926146">I don't think we should have lots of logic to look if security manager is already set, to look if this or that system property is already set. There is no need or use-case for this IMO. It will just make our codebase even bigger than i already is.

Having a custom security policy is extremely expert. Those users can turn it off with the boolean i provided here. 

We only need one way to do this. This isn't perl.
</comment><comment author="kimchy" created="2015-04-24T13:05:14Z" id="95927214">@rmuir sounds good
</comment><comment author="tlrx" created="2015-04-27T11:37:59Z" id="96616869">@rmuir sorry to come a day after the fair but I think this change prevents writing in a local FsRepository no?
</comment><comment author="rmuir" created="2015-04-27T11:58:10Z" id="96621042">Probably. I have no idea about this one in particular, but i am sure this change breaks all kinds of things using rogue paths etc. We should likely review all places using paths.get and open bugs for anything except environment that uses it.
</comment><comment author="tlrx" created="2015-04-27T12:16:41Z" id="96627307">I agree, it will surely breaks things here and there. But in the case of snapshot repositories of type `fs`, users provide a custom path where the snapshots will be written/read (see the [doc example](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_repositories))... I suppose it won't work unless users provide a custom policy file (and restart the JVM) or disable the security manager.
</comment><comment author="rmuir" created="2015-04-27T12:25:50Z" id="96631045">Why doesnt that feature use a path configured in environment like all other paths. That is the way to fix it imo
</comment><comment author="s1monw" created="2015-04-27T12:26:26Z" id="96631294">&gt; Why doesnt that feature use a path configured in environment like all other paths. That is the way to fix it imo

I agree this should not be outside of the path or we need to have a dedicated path that is configured.
</comment><comment author="tlrx" created="2015-04-27T12:42:43Z" id="96635988">@rmuir @s1monw I agree and opened #10828 for that.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file></files><comments><comment>Merge pull request #10717 from rmuir/put_me_in_coach</comment></comments></commit></commits></item><item><title>ArgMax and ArgMin aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10716</link><project id="" key="" /><description>Have argmin and argmax aggregations [http://en.wikipedia.org/wiki/Arg_max]

As percentiles, that would be a multi-value metric aggregation, but size limit controls that.

Request

```
{
    "aggs" : {
        "product_with_max_price" : {
            "argmax" : { 
                "arg" : "price",
                "field" : "product",
                "size" : 5
              }
          }
    }
}
```

Response

```
{
    ...
    "aggregations": {
        "product_with_max_price": {
            "values": {
               "ferrari" : 500000,
               "lamborghini" : 500000
            }
        }
    }
}
```
</description><key id="70035357">10716</key><summary>ArgMax and ArgMin aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rcrezende</reporter><labels /><created>2015-04-22T06:23:47Z</created><updated>2015-04-25T18:49:25Z</updated><resolved>2015-04-25T18:49:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-22T09:32:23Z" id="95095524">@rcrezende you should be able to get this information with the existing aggregations by using the `terms` and `max` aggregations:

``` json
{
  "aggs": {
    "product_with_max_price": {
      "terms": {
        "field": "product",
        "size": 5,
        "order": {
          "max_price": "desc"
        }
      },
      "aggs": {
        "max_price": {
          "max": {
            "field": "price"
          }
        }
      }
    }
  }
}
```

This will get a list of the top 5 terms ordered by maximum price. Your client can then look for the top N which have the same value. With https://github.com/elastic/elasticsearch/issues/10000 coming in 2.0 you could use the `max_bucket` aggregation to do this last step for you and return only the terms which have the maximum value.

A similar approach could be used for the argmin.
</comment><comment author="clintongormley" created="2015-04-25T18:49:25Z" id="96259366">Agree with @colings86 - going to close this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>aggs (stats, value_counts) not working when other doc_types have similar mappings for 1.5.1, works for 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10715</link><project id="" key="" /><description>I saw an issue with aggs not working when upgrading from 1.4.4 to 1.5.1. I was able to reproduce with generic data in both a linux server (ubuntu on CircleCI) and OS X on my laptop locally. (nice to have CI) 

After some debugging, I was able to narrow it down to adding a second mapping where the second mapping's doc type has the same name as the first mapping's nested object. The following script below will return `0` or `null` aggs results for the `stats` aggregation in 1.5.1 when the second mapping is applied. Without the second mapping, this isn't an issue, and in 1.4.4 this isn't an issue. 

Cheers,
Aleck

``` bash
#!/bin/bash
# Aleck Landgraf - Elasticsearch 1.5.1 aggs tests with various mappings
# This test will return 0 or null aggregations for elasticsearch version 1.5.1
# and return valid aggs on 1.4.4. See output at the bottom.
#
# Comment out line 35 (mapping of building_snapshot) to work on 1.5.1.
#
# I've only tested for the value_counts and stats aggs, but more are probably
# affected.

echo ""
echo "get elasticsearch version for debug"
echo ""
curl -XGET localhost:9200
echo ""

echo ""
echo "drop index"
echo ""
curl -XDELETE localhost:9200/test
echo ""

echo ""
echo ""
echo "create index"
echo ""
curl -XPUT localhost:9200/test
echo ""

echo ""
echo "create mappings"
echo ""
curl -XPUT localhost:9200/test/_mapping/test -d '{"test": {"properties": {"building_snapshot": {"properties": {"lot_number": {"type": "string"}, "owner_address": {"type": "string"}, "owner_postal_code": {"type": "string"}, "block_number": {"type": "string"}, "project_buildings": {"type": "nested"}, "source_eui_weather_normalized": {"type": "float"}, "owner_email": {"type": "string"}, "year_ending": {"type": "date"}, "building_count": {"type": "float"}, "meters": {"index_name": "meter", "type": "string"}, "owner": {"type": "string"}, "site_eui": {"type": "float"}, "address_line_1": {"type": "string"}, "occupied_floor_area": {"type": "float"}, "source_eui": {"type": "float"}, "custom_id_1": {"index": "not_analyzed", "type": "string", "doc_values": true}, "city": {"type": "string"}, "property_notes": {"type": "string"}, "district": {"type": "string"}, "location": {"type": "geo_point"}, "latitude": {"type": "float"}, "generation_date": {"type": "date"}, "extra_data": {"type": "object", "properties": {"Wall Insulation Location": {"type": "string"}, "Audit Cost": {"ignore_malformed": true, "type": "float"}, "Design Food Sales - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Exterior Shading Type": {"type": "string"}, "Design Electricity Use - Grid Purchase (kBtu)": {"ignore_malformed": true, "type": "float"}, "Design Coal - Anthracite Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Static Pressure Reset Control": {"type": "string"}, "Annual Water Savings": {"ignore_malformed": true, "type": "float"}, "Library - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Evaporatively Cooled Condenser Minimum Temperature": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Site Natural Gas Use (therms)": {"ignore_malformed": true, "type": "float"}, "Mailing Center/Post Office - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Fuel Interruptibility": {"type": "string"}, "Building air leakage": {"ignore_malformed": true, "type": "float"}, "Indoor Water Cost (All Water Sources) ($)": {"ignore_malformed": true, "type": "float"}, "Footprint Shape": {"type": "string"}, "Guiding Principle 4.7 Indoor Environment - Tobacco Smoke Control": {"type": "string"}, "Hotel - Gym/fitness Center Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Refrigerant Return Line Diameter": {"ignore_malformed": true, "type": "float"}, "Transportation Terminal/Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Recreation - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Outside Air Reset Minimum Heating Supply Temperature": {"ignore_malformed": true, "type": "float"}, "Typical Exterior Shading Depth": {"ignore_malformed": true, "type": "float"}, "Annual Net Emissions": {"ignore_malformed": true, "type": "float"}, "Combustion Efficiency": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Design Other - Entertainment/Public Assembly - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Schedule End Month": {"ignore_malformed": true, "type": "float"}, "Police Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other - Mall - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Education - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Ignition Type": {"type": "string"}, "PV Module Rated Power": {"ignore_malformed": true, "type": "float"}, "Space Occupant Capacity": {"ignore_malformed": true, "type": "float"}, "Supply Duct Percent Conditioned Space": {"ignore_malformed": true, "type": "float"}, "Medical Office - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Water collected for Reuse": {"type": "string"}, "Convenience Store without Gas Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Minimum Fan Speed as a Fraction of Maximum - Heating": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Conveyance Standby Time": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Lighting Control Type Manual": {"type": "string"}, "Wood Cost ($)": {"ignore_malformed": true, "type": "float"}, "Hotel- Number of guest meals served per year": {"ignore_malformed": true, "type": "float"}, "Floors Above Grade": {"ignore_malformed": true, "type": "float"}, "Contact Name": {"type": "string"}, "Enclosed Mall - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Cooling Delivery Type": {"type": "string"}, "Dishwasher Year of Manufacture": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Percent of Total Electricity Generated from Onsite Renewable Systems": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Site EUI - Adjusted to Current Year (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Design Manufacturing/Industrial Plant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Commercial Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 1.4 Integrated -Occupant Feedback": {"type": "string"}, "Property Use Detail Alerts": {"type": "string"}, "Vertical Abutments": {"type": "string"}, "Premises Street Address 2": {"type": "string"}, "Evaporatively Cooled Condenser": {"type": "string"}, "Other - Stadium - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Premises Name": {"type": "string"}, "Foundation Wall Below Grade Depth": {"ignore_malformed": true, "type": "float"}, "Glass Type": {"type": "string"}, "Certification Version": {"type": "string"}, "Foundation Wall Insulation Continuity": {"type": "string"}, "Date Property Last Modified": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Bar/Nightclub - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Casino - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Other - Technology/Science - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Specular Reflectors": {"type": "string"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Stadium (Open) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Third Party Certification Date Achieved": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Design Other - Technology/Science - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Mailing Center/Post Office - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Casino - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Federal Agency/Department": {"type": "string"}, "Audit Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Aquarium - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Medical Office - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Owner City": {"type": "string"}, "Quantity of Processed Laundry": {"ignore_malformed": true, "type": "float"}, "NYC Borough, Block and Lot (BBL)": {"type": "string"}, "Premises Occupancy Classification": {"type": "string"}, "Other - Public Services - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.3 Energy - Onsite Renewable": {"type": "string"}, "Residence Hall/Dormitory - Room Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Interior Shading Type": {"type": "string"}, "Lifestyle Center - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Performing Arts - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "National Median Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Floor Framing Factor": {"ignore_malformed": true, "type": "float"}, "Cooling Efficiency Units": {"type": "string"}, "Fuel Oil #4 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Evaporatively Cooled Condenser Maximum Temperature": {"ignore_malformed": true, "type": "float"}, "Sequencing": {"type": "string"}, "Hotel- Hours per day guests on-site": {"ignore_malformed": true, "type": "float"}, "Other - Entertainment/Public Assembly - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Minimum Part Load Ratio": {"ignore_malformed": true, "type": "float"}, "Generation Capacity": {"ignore_malformed": true, "type": "float"}, "Water Fixture Fraction Hot Water": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Other - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Primary Service Hot Water Type": {"type": "string"}, "Source Energy Use Intensity": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Number of Bedrooms": {"ignore_malformed": true, "type": "float"}, "Shared Resource System": {"type": "string"}, "Worship Facility - Cooking Facilities": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 4.3 Indoor Environment - Automated Lighting Controls": {"type": "string"}, "K-12 School - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Design Medical Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Indoor Water Use (All Water Sources) (kgal)": {"ignore_malformed": true, "type": "float"}, "Movie Theater - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Regional Average EUI": {"type": "string"}, "Other - Education - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Dishwasher Quantity": {"ignore_malformed": true, "type": "float"}, "Design Electricity Use - Grid Purchase and Onsite Renewable Energy (kBtu)": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Pre-school/Daycare - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Motor Application": {"type": "string"}, "Premises County": {"type": "string"}, "Fire Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other - Lodging/Residential - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel- Number of Rooms": {"ignore_malformed": true, "type": "float"}, "Longitude": {"ignore_malformed": true, "type": "float"}, "Office - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Federal Property": {"type": "string"}, "Manufacturing/Industrial Plant - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Medical Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Outside Lighting Type": {"type": "string"}, "Pre-school/Daycare - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 1.3 Integrated - Plan": {"type": "string"}, "Medical Office - Surgery Center Size (ft2)": {"ignore_malformed": true, "type": "float"}, "Diesel Cost ($)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Living Unit Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Complete Resource": {"type": "string"}, "Contact Type": {"type": "string"}, "Fuel Oil #1 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Roof Insulation Material": {"type": "string"}, "Other - Technology/Science - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Hot Water Reset Control": {"type": "string"}, "College/University - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Collector Loop Type": {"type": "string"}, "Window Layout": {"type": "string"}, "Other - Specialty Hospital - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Most Recent Sale Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Social/Meeting Hall - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Other - Restaurant/Bar - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Residence Hall/Dormitory - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Daylighting Control Steps": {"type": "string"}, "Ice/Curling Rink - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Site Natural Gas Intensity (therms/ft2)": {"ignore_malformed": true, "type": "float"}, "Door Operation": {"type": "string"}, "Fitness Center/Health Club/Gym - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Adult Education - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Other Water Sources: Combined Indoor/Outdoor or Other Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Central Refrigeration System": {"type": "string"}, "Design Convenience Store with Gas Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Building Operator Type": {"type": "string"}, "Bank Branch - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Water Cost": {"ignore_malformed": true, "type": "float"}, "Pipe Location": {"type": "string"}, "Pump Control Type": {"type": "string"}, "Design Barracks - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Day Start Hour": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 5.5 Materials - Ozone Depleting Compounds": {"type": "string"}, "Skylight Layout": {"type": "string"}, "Design Fuel Oil #1 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Design Fitness Center/Health Club/Gym - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Cost Effectiveness Screening Method": {"type": "string"}, "Metered Areas (Energy)": {"type": "string"}, "Restaurant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Cooling Refrigerant": {"type": "string"}, "Certification Value": {"type": "string"}, "Financial Office - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Design Other - Stadium - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Pool Volume": {"ignore_malformed": true, "type": "float"}, "Floor Area Type": {"type": "string"}, "Hotel - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Roller Rink - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Static Pressure": {"ignore_malformed": true, "type": "float"}, "Other Water Sources - Outdoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Food Sales - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Part Load Ratio Below Which Hot Gas Bypass Operates": {"ignore_malformed": true, "type": "float"}, "Equipment Disposal and Salvage Costs": {"ignore_malformed": true, "type": "float"}, "Minimum Power Factor Without Penalty": {"ignore_malformed": true, "type": "float"}, "Wall Exterior Solar Absorptance": {"ignore_malformed": true, "type": "float"}, "HPWH Minimum Air Temperature": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other Water Sources - Indoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Percent Better than National Median Water/Wastewater Site EUI": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Number of Laundry Hookups in All Units": {"ignore_malformed": true, "type": "float"}, "Scenario Qualifier": {"type": "string"}, "Skylight Glazing": {"type": "string"}, "Audit Team Member Certification Type": {"type": "string"}, "Space Number of FTE Workers": {"ignore_malformed": true, "type": "float"}, "Convenience Store with Gas Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Courthouse - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Lamp Length": {"ignore_malformed": true, "type": "float"}, "Other - Entertainment/Public Assembly - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Fuel Oil (No. 5 and No. 6) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Azimuth": {"ignore_malformed": true, "type": "float"}, "Auditor Qualification": {"type": "string"}, "Swimming Pool - Approximate Pool Size": {"ignore_malformed": true, "type": "float"}, "Number of Buildings": {"ignore_malformed": true, "type": "float"}, "Wall Insulation Condition": {"type": "string"}, "Fan Type": {"type": "string"}, "Design Other - Specialty Hospital - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Number of Residential Electronic Lift Systems": {"ignore_malformed": true, "type": "float"}, "Design Target Energy Cost ($)": {"ignore_malformed": true, "type": "float"}, "SHW Efficiency": {"ignore_malformed": true, "type": "float"}, "Typical Wall R-Value": {"ignore_malformed": true, "type": "float"}, "Conveyance Peak Time": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Other - Stadium - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Design Senior Care Community - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Data Center - PDU Output Site Energy (kWh)": {"ignore_malformed": true, "type": "float"}, "Space Name": {"type": "string"}, "Premises Year Completed": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Weather Normalized Site Electricity Intensity (kWh/ft2)": {"ignore_malformed": true, "type": "float"}, "End Use": {"type": "string"}, "Compressor Staging": {"type": "string"}, "Senior Care Community - Commercial Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Recreation - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Motor Pole Count": {"ignore_malformed": true, "type": "float"}, "Number of Luminaires": {"ignore_malformed": true, "type": "float"}, "Schedule Day": {"ignore_malformed": true, "type": "float"}, "Room Density": {"ignore_malformed": true, "type": "float"}, "Adult Education - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Vocational School - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Reference For Rate Structure": {"type": "string"}, "Recirculation Flow Rate": {"ignore_malformed": true, "type": "float"}, "Indoor Water Intensity (All Water Sources) (gal/ft2)": {"ignore_malformed": true, "type": "float"}, "Floor Framing Material": {"type": "string"}, "Distribution Center - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Water Use Type": {"type": "string"}, "Space Floors Above Grade": {"ignore_malformed": true, "type": "float"}, "Natural Gas Cost ($)": {"ignore_malformed": true, "type": "float"}, "Design Convenience Store without Gas Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel - Type of Laundry Facility": {"type": "string"}, "Manufacturing/Industrial Plant - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Storage Volume": {"ignore_malformed": true, "type": "float"}, "Measure Life": {"ignore_malformed": true, "type": "float"}, "Parasitic Fuel Consumption Rate": {"ignore_malformed": true, "type": "float"}, "Propane Cost ($)": {"ignore_malformed": true, "type": "float"}, "Net Refrigeration Capacity": {"ignore_malformed": true, "type": "float"}, "Owner Name": {"type": "string"}, "Capacity Unit": {"type": "string"}, "Other - Public Services - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Percent Better than National Median Source EUI": {"ignore_malformed": true, "type": "float"}, "Humidity Control Minimum": {"ignore_malformed": true, "type": "float"}, "Maximum Outside Air Flow Rate": {"ignore_malformed": true, "type": "float"}, "Pump Power Demand": {"ignore_malformed": true, "type": "float"}, "Deck Type": {"type": "string"}, "Target Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Cooking Capacity Unit": {"type": "string"}, "Other - Public Services - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Primary Cooling Type": {"type": "string"}, "Financial Office - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Electronic Lift Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Site Energy Use Intensity": {"ignore_malformed": true, "type": "float"}, "Site EUI - Adjusted to Current Year (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "K-12 School - Months in Use": {"ignore_malformed": true, "type": "float"}, "Floor Insulation Condition": {"type": "string"}, "Electricity": {"type": "string"}, "Municipally Supplied Reclaimed Water - Outdoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Vocational School - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other Water Sources - Indoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Auditor Qualification State": {"type": "string"}, "Wholesale Club/Supercenter- Number of Open or Closed Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "PV System Racking System Tilt Angle Min": {"ignore_malformed": true, "type": "float"}, "Off-Cycle Heat Loss Coefficient": {"ignore_malformed": true, "type": "float"}, "Fast Food Restaurant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Data Center - PDU Input Site Energy (kWh)": {"ignore_malformed": true, "type": "float"}, "Dishwasher Hot Water Use": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Pump Efficiency": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Floors Below Grade": {"ignore_malformed": true, "type": "float"}, "Transportation Terminal/Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Fuel Oil #5 &amp; 6 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Other - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.1 Indoor Water - Option 1": {"type": "string"}, "Guiding Principle 3.1 Indoor Water - Option 2": {"type": "string"}, "Electricity Price Escalation Rate": {"type": "string"}, "Retail Store - Number of Cash Registers": {"ignore_malformed": true, "type": "float"}, "Laboratory - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Ice Events": {"ignore_malformed": true, "type": "float"}, "Dryer Secondary Energy Use Per Load": {"ignore_malformed": true, "type": "float"}, "Investment in Energy Projects, Cumulative ($)": {"ignore_malformed": true, "type": "float"}, "Conveyance Peak Power": {"ignore_malformed": true, "type": "float"}, "Design Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Suction Vapor Temperature": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Number of Residential Living Units": {"ignore_malformed": true, "type": "float"}, "Electrical Plug Load Intensity": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Hotel - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Alternative Water Generated On-Site - Outdoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Air Distribution Configuration": {"type": "string"}, "Electricity Use - Generated from Onsite Renewable Systems and Used Onsite (kWh)": {"ignore_malformed": true, "type": "float"}, "Skylight Window Treatments": {"type": "string"}, "Biomass GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Race Track - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Veterinary Office - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Clothes Washer Loader Type": {"type": "string"}, "Design Supermarket/Grocery Store - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design District Chilled Water Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Commercial Washing Machine Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Parking - Open Parking Lot Size (ft2)": {"ignore_malformed": true, "type": "float"}, "Direct GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Race Track - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Chiller Compressor Type": {"type": "string"}, "Fast Food Restaurant - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Veterinary Office - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Energy Cost Intensity ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Courthouse - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Cooling Equipment Maintenance Frequency": {"type": "string"}, "Medical Office - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Boiler Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Open or Closed Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Number of MRI Machines": {"ignore_malformed": true, "type": "float"}, "Property Data Administrator - Email": {"type": "string"}, "Coefficient of Performance": {"ignore_malformed": true, "type": "float"}, "Estimated Savings from Energy Projects, Cumulative ($)": {"ignore_malformed": true, "type": "float"}, "Other - Utility - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Social/Meeting Hall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "District Steam Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Design Source EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Design Source EUI": {"ignore_malformed": true, "type": "float"}, "Design Fuel Oil #2 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Measure End Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Other - Entertainment/Public Assembly - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Office - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Wholesale Club/Supercenter - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Resource Units": {"type": "string"}, "Typical Skylight Frame Type": {"type": "string"}, "Stadium (Open) - Ice Events": {"ignore_malformed": true, "type": "float"}, "Diesel #2 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Measure Description": {"type": "string"}, "Roof Insulation Continuity": {"type": "string"}, "Water Intensity": {"ignore_malformed": true, "type": "float"}, "Net Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Heat Pump Water Heater Refrigerant Designation": {"type": "string"}, "AC Adjusted": {"type": "string"}, "Medical Office - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Skylight Pitch": {"ignore_malformed": true, "type": "float"}, "Natural Gas": {"type": "string"}, "Supply Air Temperature Setpoint": {"ignore_malformed": true, "type": "float"}, "Publicly Subsidized": {"type": "string"}, "Kerosene Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Burner Turndown Ratio": {"ignore_malformed": true, "type": "float"}, "Skylights Visible Transmittance": {"ignore_malformed": true, "type": "float"}, "Ice/Curling Rink - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Design Target Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Exterior Roughness": {"type": "string"}, "Rate Structure Sector": {"type": "string"}, "Chilled Water Supply Temperature": {"ignore_malformed": true, "type": "float"}, "Exterior Wall Color": {"type": "string"}, "Emissions Factor": {"ignore_malformed": true, "type": "float"}, "Storage Tank Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Static Pressure - Installed": {"ignore_malformed": true, "type": "float"}, "Fixed Monthly Charge": {"ignore_malformed": true, "type": "float"}, "Lighting Efficacy": {"ignore_malformed": true, "type": "float"}, "Parking - Partially Enclosed Parking Garage Size (ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Average Number of Residents": {"ignore_malformed": true, "type": "float"}, "Exterior Wall Type": {"type": "string"}, "Guiding Principle 1.2 Integrated - Goals": {"type": "string"}, "Water/Wastewater Biomass GHG Emissions Intensity (kgCO2e/gpd)": {"ignore_malformed": true, "type": "float"}, "Premises Notes": {"type": "string"}, "Drinking Water Treatment &amp; Distribution - Average Flow (MGD)": {"ignore_malformed": true, "type": "float"}, "Prison/Incarceration - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Recreation - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "SHW Year installed": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Measure Notes": {"type": "string"}, "SHW is FEMP Designated Product": {"type": "string"}, "Data Center - UPS System Redundancy": {"type": "string"}, "Lamp Subtype": {"type": "string"}, "Owner State": {"type": "string"}, "Number of Lamps per Ballast": {"ignore_malformed": true, "type": "float"}, "Portfolio Manager Property ID": {"type": "string"}, "Makeup Air Source": {"type": "string"}, "PVc System Racking System Tilt Angle Max": {"ignore_malformed": true, "type": "float"}, "Zoo - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Services - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Vertical Fin Depth": {"ignore_malformed": true, "type": "float"}, "Transformer Nameplate Efficiency": {"ignore_malformed": true, "type": "float"}, "US Federal Real Property Unique Identifier": {"type": "string"}, "Guiding Principle 4.4 Daylighting and Occupant Controls - Option 2": {"type": "string"}, "Guiding Principle 4.4 Daylighting and Occupant Controls - Option 1": {"type": "string"}, "Scenario Name": {"type": "string"}, "Measure Start Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Indoor Arena - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Lighting Control Type Daylighting": {"type": "string"}, "Electricity Use - Grid Purchase (kBtu)": {"ignore_malformed": true, "type": "float"}, "Library - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "County": {"type": "string"}, "Process Load Heat Gain Fraction": {"ignore_malformed": true, "type": "float"}, "Design Electricity Use - Generated from Onsite Renewable Systems and Used Onsite (kBtu)": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Motor Efficiency": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Number of Laundry Hookups in Common Area(s)": {"ignore_malformed": true, "type": "float"}, "Hotel - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Residence Hall/Dormitory - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Site Energy Use": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Number of Sporting Events per Year": {"ignore_malformed": true, "type": "float"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "NYC Building Identification Number (BIN)": {"type": "string"}, "Installation Type": {"type": "string"}, "Courthouse - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Design Target % Better Than Median Source EUI": {"ignore_malformed": true, "type": "float"}, "Library - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Cell Count": {"ignore_malformed": true, "type": "float"}, "Aquarium - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Mall - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Heating Equipment Maintenance Frequency": {"type": "string"}, "Condenser Water Temperature": {"ignore_malformed": true, "type": "float"}, "Refrigerated Warehouse - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Refrigeration Unit is Third Party Certified": {"type": "string"}, "Heat Pump Backup system fuel": {"type": "string"}, "Motor Drive Efficiency": {"ignore_malformed": true, "type": "float"}, "Cooling Efficiency Value": {"ignore_malformed": true, "type": "float"}, "Electricity Use - Grid Purchase (kWh)": {"ignore_malformed": true, "type": "float"}, "Auditor Qualification Number": {"type": "string"}, "Coal (anthracite) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Roller Rink - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Recover Efficiency": {"ignore_malformed": true, "type": "float"}, "Outdoor Water Use (All Water Sources) (kgal)": {"ignore_malformed": true, "type": "float"}, "Terrace R-Value": {"ignore_malformed": true, "type": "float"}, "Typical Skylight Frames R-Value": {"ignore_malformed": true, "type": "float"}, "Off-Peak Occupancy Percentage": {"ignore_malformed": true, "type": "float"}, "Operable Windows": {"type": "string"}, "Site Energy Use - Adjusted to Current Year (kBtu)": {"ignore_malformed": true, "type": "float"}, "Motor Brake HP": {"ignore_malformed": true, "type": "float"}, "Motor Location Relative to Air Stream": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Staffed Bed Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Enclosed Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Software program version": {"type": "string"}, "Economizer Dry Bulb Control Point": {"ignore_malformed": true, "type": "float"}, "Design Non-Refrigerated Warehouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Parking - Supplemental Heating": {"type": "string"}, "Skylight Type": {"type": "string"}, "Rated Heat Pump Sensible Heat Ratio": {"ignore_malformed": true, "type": "float"}, "Daily Water Use": {"ignore_malformed": true, "type": "float"}, "Financial Office - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Premises State": {"type": "string"}, "Alternative Water Generated On-Site: Combined Indoor/Outdoor or Other Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Social/Meeting Hall - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Draft Type": {"type": "string"}, "Minimum Dimming Light Fraction": {"ignore_malformed": true, "type": "float"}, "Cooled Floor Area": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Number of Concert/Show Events per Year": {"ignore_malformed": true, "type": "float"}, "Ventilation Rate": {"ignore_malformed": true, "type": "float"}, "Refrigerated Case Doors": {"type": "string"}, "Guiding Principles - % Not Applicable": {"type": "string"}, "Name of Retro-commissioning Certification Holder": {"type": "string"}, "Metal Halide Start Type": {"type": "string"}, "Measure First Cost": {"ignore_malformed": true, "type": "float"}, "Library - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Perimeter": {"ignore_malformed": true, "type": "float"}, "Condenser Type": {"type": "string"}, "Adult Education - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Luminaire Height": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Cash Register Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design K-12 School - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Percent Better than National Median Site EUI": {"ignore_malformed": true, "type": "float"}, "Financial Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Zoo - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Direct GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Water Use": {"ignore_malformed": true, "type": "float"}, "Dehumidification Type": {"type": "string"}, "Prison/Incarceration - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.2 Energy - Efficient Products": {"type": "string"}, "Output Capacity": {"ignore_malformed": true, "type": "float"}, "Cooking Equipment is Third Party Certified": {"type": "string"}, "Guiding Principles - % Not Assessed": {"type": "string"}, "Urgent Care/Clinic/Other Outpatient - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fitness Center/Health Club/Gym - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.1 Energy - Energy Efficiency Any Option (Any Option)": {"type": "string"}, "Alternative Water Generated On-Site - Indoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Utility Account Number": {"type": "string"}, "Skylight to Roof Ratio": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Water/Wastewater Site Electricity Intensity (kWh/gpd)": {"ignore_malformed": true, "type": "float"}, "Weather Year": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Convention Center - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Winter Peak Electricity Reduction": {"ignore_malformed": true, "type": "float"}, "Target Source EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Number of Meals": {"ignore_malformed": true, "type": "float"}, "Indirect GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Design Ambulatory Surgical Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Number of Worker on Main Shift": {"ignore_malformed": true, "type": "float"}, "Humidification Type": {"type": "string"}, "Casino - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Site Electricity (kWh)": {"ignore_malformed": true, "type": "float"}, "Design Natural Gas Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Design Restaurant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Source EUI - Adjusted to Current Year (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 4.2 Indoor Environment - Moisture Control": {"type": "string"}, "Design Energy/Power Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Indirect GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Dishwasher Energy Factor": {"ignore_malformed": true, "type": "float"}, "Coal - Anthracite Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Other - Education - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Supply Fraction of Duct Leakage": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Weather Station Category": {"type": "string"}, "Foundation Wall Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Space Number Main Shift Workers": {"ignore_malformed": true, "type": "float"}, "Data Center - IT Equipment Input Site Energy (kWh)": {"ignore_malformed": true, "type": "float"}, "On-site Renewable Electricity": {"ignore_malformed": true, "type": "float"}, "Date of Last PM Modification": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Municipally Supplied Potable Water: Combined Indoor/Outdoor or Other Cost ($)": {"ignore_malformed": true, "type": "float"}, "Slab Insulation Condition": {"type": "string"}, "Police Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Number of Discrete Fan Speeds - Heating": {"ignore_malformed": true, "type": "float"}, "Number of Ballasts per Luminaire": {"ignore_malformed": true, "type": "float"}, "Outside Air Temperature Upper Limit Cooling Reset Control": {"ignore_malformed": true, "type": "float"}, "Installed Fan Flow Rate": {"ignore_malformed": true, "type": "float"}, "Fluorescent Start Type": {"type": "string"}, "Chilled Water Reset Control": {"type": "string"}, "Anti-Sweat Heater Power": {"ignore_malformed": true, "type": "float"}, "Federal Region/Sub-Department": {"type": "string"}, "Outpatient Rehabilitation/Physical Therapy - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other - Lodging/Residential - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design District Hot Water Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Boiler Type": {"type": "string"}, "Bar/Nightclub - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Point Of Use": {"type": "string"}, "Summer Peak Electricity Reduction": {"ignore_malformed": true, "type": "float"}, "Slab Perimeter": {"ignore_malformed": true, "type": "float"}, "Pre-school/Daycare - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Lighting Control Type Timer": {"type": "string"}, "Other HVAC Type": {"type": "string"}, "Guiding Principle 4.1 Indoor Environment - Ventilation and Thermal Comfort": {"type": "string"}, "Demand Reduction": {"ignore_malformed": true, "type": "float"}, "Green Power - Offsite (kWh)": {"ignore_malformed": true, "type": "float"}, "Other - Recreation - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Cooling Type": {"type": "string"}, "Food Service - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Mailing Center/Post Office - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Site EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Design Drinking Water Treatment &amp; Distribution - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Steam Boiler Maximum Operating Pressure": {"ignore_malformed": true, "type": "float"}, "Investment in Energy Projects, Cumulative ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Number of Special/Other Events per Year": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Potable Water - Indoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Occupant Activity Level": {"type": "string"}, "Worship Facility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Other - Education - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Reclaimed Water - Indoor Cost Intensity ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Manufacturing/Industrial Plant - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Target Total GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Setpoint temperature cooling": {"ignore_malformed": true, "type": "float"}, "Ice/Curling Rink - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fan Power Minimum Ratio": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Ice Events": {"ignore_malformed": true, "type": "float"}, "Weather Type": {"type": "string"}, "PM Profile Status": {"type": "string"}, "Other - Lodging/Residential - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Annual Combined Whole Building Annual Weather Normalized Site Resource Use": {"ignore_malformed": true, "type": "float"}, "Design Diesel #2 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Target Water/Wastewater Site EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Motor HP": {"ignore_malformed": true, "type": "float"}, "Convenience Store without Gas Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Police Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Chiller Compressor Driver": {"type": "string"}, "Multifamily Housing - Resident Population Type": {"type": "string"}, "Strip Mall - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Ventilation Type": {"type": "string"}, "Courthouse - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Refrigerated Warehouse - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Supply Air Control Strategy": {"type": "string"}, "Software program used": {"type": "string"}, "Duct Type": {"type": "string"}, "Cooling Supply Air Temperature": {"ignore_malformed": true, "type": "float"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Duct Leakage Test Method": {"type": "string"}, "Ventilation Zone Control": {"type": "string"}, "Funding from Tax Credits": {"type": "string"}, "Vocational School - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Absorption Heat Source": {"type": "string"}, "Transportation Terminal/Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Slab Insulation Orientation": {"ignore_malformed": true, "type": "float"}, "Alternative Water Generated On-Site - Outdoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Adult Education - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Target ENERGY STAR Score": {"ignore_malformed": true, "type": "float"}, "Design Vocational School - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Potable Water: Combined Indoor/Outdoor or Other Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Outside Air Reset Minimum Cooling Supply Temperature": {"ignore_malformed": true, "type": "float"}, "Normalization Years": {"type": "string"}, "Tank Heating Type": {"type": "string"}, "Electricity Use - Generated from Onsite Renewable Systems and Exported (kWh)": {"ignore_malformed": true, "type": "float"}, "Primary Contact": {"type": "string"}, "Average Daily Hours": {"ignore_malformed": true, "type": "float"}, "Courthouse - Number of Computers": {"ignore_malformed": true, "type": "float"}, "eGRID Output Emissions Rate (kgCO2e/MBtu)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Enclosed Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Number of Concert/Show Events per Year": {"ignore_malformed": true, "type": "float"}, "Design Financial Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Heat Pump Backup AFUE": {"ignore_malformed": true, "type": "float"}, "Target Finder EUI": {"ignore_malformed": true, "type": "float"}, "ENERGY STAR Certification - Year(s) Certified": {"type": "string"}, "Coke Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Adult Education - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "NAICS Code": {"type": "string"}, "Self-Storage Facility - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "ENERGY Star Score": {"ignore_malformed": true, "type": "float"}, "Process Load Duty Cycle": {"type": "string"}, "Alternative Water Generated On-Site: Combined Indoor/Outdoor or Other Cost ($)": {"ignore_malformed": true, "type": "float"}, "Other - Mall - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Number of Discrete Fan Speeds - Cooling": {"ignore_malformed": true, "type": "float"}, "Vocational School - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Recreation - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Museum - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Convenience Store without Gas Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fuel Oil #5 &amp; 6 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Receive Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Design Automobile Dealership - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Fenestration Area": {"ignore_malformed": true, "type": "float"}, "Aquarium - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "NPV of Tax Implications": {"ignore_malformed": true, "type": "float"}, "Casino - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Window to Wall Ratio": {"ignore_malformed": true, "type": "float"}, "Internal Rate of Return": {"ignore_malformed": true, "type": "float"}, "Residence Hall/ Dormitory - Computer Lab": {"type": "string"}, "Defrost Type": {"type": "string"}, "Supermarket/Grocery - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Design Greenhouse Gas Emissions": {"ignore_malformed": true, "type": "float"}, "Energy Cost Intensity ($)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.4 Energy - Measurement and Verification": {"type": "string"}, "Ambulatory Surgical Center - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Building Certification": {"type": "string"}, "Door Visible Transmittance": {"ignore_malformed": true, "type": "float"}, "Convenience Store without Gas Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Weather Data Station ID": {"type": "string"}, "Plug Load Peak Power": {"ignore_malformed": true, "type": "float"}, "Type of Measure": {"type": "string"}, "Active Dehumidification": {"type": "string"}, "Design ENERGY STAR Score": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Exterior Entrance to the Public": {"ignore_malformed": true, "type": "float"}, "College/University - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Kerosene Cost ($)": {"ignore_malformed": true, "type": "float"}, "Design Zoo - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Liquid Propane Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Window Orientation": {"ignore_malformed": true, "type": "float"}, "Fitness Center/Health Club/Gym - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "National Median Energy Cost ($)": {"ignore_malformed": true, "type": "float"}, "Dryer Primary Energy Use Per Load": {"ignore_malformed": true, "type": "float"}, "Library - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Alternative Water Generated On-Site - Indoor Intensity (gal/ft2)": {"ignore_malformed": true, "type": "float"}, "Fan Design Static Pressure": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Collector Type": {"type": "string"}, "Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Net Metering": {"type": "string"}, "Demand Window": {"type": "string"}, "Recirculation Loop Count": {"ignore_malformed": true, "type": "float"}, "Lot Size": {"ignore_malformed": true, "type": "float"}, "K-12 School - Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Compressor Unloader": {"type": "string"}, "Design Multifamily Housing - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Annual Fuel Use Native Units (Native Units)": {"type": "string"}, "Municipally Supplied Potable Water - Indoor Cost Intensity ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Convenience Store with Gas Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Clothes Washer Modified Energy Factor": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Number of Commercial Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Number of Refrigerant Return Lines": {"ignore_malformed": true, "type": "float"}, "National Median Source EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Property Data Administrator": {"type": "string"}, "Natural Ventilation Method": {"type": "string"}, "Outpatient Rehabilitation/Physical Therapy - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "K-12 School - Student Seating Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Performing Arts - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Percent of Roof Terraces": {"ignore_malformed": true, "type": "float"}, "Cooling Control Strategy": {"type": "string"}, "Rate Structure Effective Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Overhang Height above Window": {"ignore_malformed": true, "type": "float"}, "Typical Skylights SHGC": {"ignore_malformed": true, "type": "float"}, "MV Cost": {"ignore_malformed": true, "type": "float"}, "Transportation Terminal/Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Race Track - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Target % Better Than Median Source EUI": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.2 Water - Outdoor Water Any Option (Any Option)": {"type": "string"}, "Other - Utility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Alternative Water Generated On-Site - Indoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "PV Module Width": {"ignore_malformed": true, "type": "float"}, "District Hot Water Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "PM Sharing Account": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Owned By": {"type": "string"}, "Evaporative Cooling Entering Supply Air WB Temperature": {"ignore_malformed": true, "type": "float"}, "Roof Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "National Median Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Fitness Center/Health Club/Gym - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Percent Savings in Lighting Power Density": {"ignore_malformed": true, "type": "float"}, "Performing Arts - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Ambulatory Surgical Center - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Type of Rate Structure": {"type": "string"}, "Typical Window Frame R-Value": {"ignore_malformed": true, "type": "float"}, "Evaporative Cooling Operation": {"type": "string"}, "Liquid Propane Cost ($)": {"ignore_malformed": true, "type": "float"}, "Other - Specialty Hospital - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Self-Storage Facility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Outpatient Rehabilitation/Physical Therapy - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Number of Cash Registers": {"ignore_malformed": true, "type": "float"}, "Heating Type": {"type": "string"}, "Annual Heating Efficiency Value": {"ignore_malformed": true, "type": "float"}, "Percentage of Common Space": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Primary Hot Water Fuel Type for units (for units)": {"type": "string"}, "Duct Pressure Test Leakage Percentage (Percentage)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Financial Office - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Contact Address 1": {"type": "string"}, "Contact Address 2": {"type": "string"}, "Senior Care Community - Number of Commercial Refrigeration/ Freezer Units": {"ignore_malformed": true, "type": "float"}, "Design Swimming Pool - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Water Alerts": {"type": "string"}, "Annual Efficiency Unit": {"type": "string"}, "Barracks - Dining Hall": {"type": "string"}, "Zoo - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel - Full Service Spa Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Electric Distribution Utility": {"type": "string"}, "Design Total GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Prison/Incarceration - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Window Height": {"ignore_malformed": true, "type": "float"}, "Guiding Principles - Checklist Manager": {"type": "string"}, "Measure Capital Replacement Costs": {"ignore_malformed": true, "type": "float"}, "Rate Structure End Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Thermal Efficiency": {"ignore_malformed": true, "type": "float"}, "Replaced/modified/removed system identifier": {"type": "string"}, "Schedule Type": {"ignore_malformed": true, "type": "float"}, "District Chilled Water": {"type": "string"}, "Minimum Outside Air Percentage": {"ignore_malformed": true, "type": "float"}, "Lifestyle Center - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Typical Exterior Wall Type": {"type": "string"}, "Design Other - Lodging/Residential - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Laundry Type": {"type": "string"}, "Design Refrigerated Warehouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Compressor Unloader Stages": {"type": "string"}, "Design Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Wall Insulation Continuity": {"type": "string"}, "Financial Office - Number of Computers": {"ignore_malformed": true, "type": "float"}, "GHG Emissions": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Open or Closed Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Source EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Property Management Company": {"type": "string"}, "Municipally Supplied Reclaimed Water: Combined Indoor/Outdoor or Other Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Design Prison/Incarceration - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fan Flow Control Type": {"type": "string"}, "K-12 School - Gymnasium Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Heat Pump Backup Heating Switchover Temperature": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Year Of Latest Retrofit": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Fenestration R-value": {"ignore_malformed": true, "type": "float"}, "Return Duct Percent Conditioned Space": {"ignore_malformed": true, "type": "float"}, "Minimum Fan Speed as a Fraction of Maximum - Cooling": {"ignore_malformed": true, "type": "float"}, "Barracks - Computer Lab": {"type": "string"}, "Fuel Oil (No. 2) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Average Effluent Biological Oxygen Demand (BOD5) (mg/l)": {"ignore_malformed": true, "type": "float"}, "Ground Coupling": {"type": "string"}, "Diesel": {"type": "string"}, "Other - Education - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Other - Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Demand Ratchet Percentage": {"ignore_malformed": true, "type": "float"}, "Premises Gross Floor Area": {"ignore_malformed": true, "type": "float"}, "Scope": {"type": "string"}, "Design District Steam Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Education - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Water Current Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "normalized_address": {"index": "not_analyzed", "type": "string", "store": true}, "Design Temperature Difference": {"ignore_malformed": true, "type": "float"}, "Number of Months in Operation": {"ignore_malformed": true, "type": "float"}, "Barracks- Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Fast Food Restaurant - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Adult Education - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Weather Station ID": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Pump Application": {"type": "string"}, "Design Bank Branch - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Enclosed Mall - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel - Room Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "IT System Type": {"type": "string"}, "Desuperheat Valve": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 1.1 Integrated - Team": {"type": "string"}, "Wall Area": {"ignore_malformed": true, "type": "float"}, "Certification Year": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Roller Rink - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Third Party Certification Date Anticipated": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Target Finder Baseline": {"ignore_malformed": true, "type": "float"}, "Total Water Cost (All Water Sources) ($)": {"ignore_malformed": true, "type": "float"}, "Other - Entertainment/Public Assembly - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Water-Side Economizer Temperature Maximum": {"ignore_malformed": true, "type": "float"}, "Home Energy Score": {"ignore_malformed": true, "type": "float"}, "ENERGY STAR Application Status": {"type": "string"}, "Estimated Savings from Energy Projects, Cumulative ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Specialty Hospital - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Swimming Pool - Months in Use": {"ignore_malformed": true, "type": "float"}, "Skylight Solar tube": {"type": "string"}, "Avoided Emissions - Offsite Green Power (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Police Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Space Peak Number of Occupants": {"ignore_malformed": true, "type": "float"}, "Outpatient Rehabilitation/Physical Therapy - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Heating Setback Temperature": {"ignore_malformed": true, "type": "float"}, "Design Repair Services (Vehicle, Shoe, Locksmith, etc) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Courthouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Capacity": {"ignore_malformed": true, "type": "float"}, "Pump Maximum Flow Rate": {"ignore_malformed": true, "type": "float"}, "Design Museum - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Number of Sporting Events per Year": {"ignore_malformed": true, "type": "float"}, "Economizer Control": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Investment in Energy Projects, Cumulative ($/GPD)": {"ignore_malformed": true, "type": "float"}, "Design Other - Recreation - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Anti-Sweat Heaters": {"type": "string"}, "Coal (bituminous) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Window Frame Type": {"type": "string"}, "Other - Education - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Measure Implementation Status": {"type": "string"}, "Single Family Home - Bedroom Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Certification Expiration Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Design Mailing Center/Post Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Roof Color": {"type": "string"}, "Windows Gas Filled": {"type": "string"}, "Design Kerosene Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "SHW Control Type": {"type": "string"}, "Other Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Annual Savings Cost (Cost)": {"ignore_malformed": true, "type": "float"}, "PV System Location": {"type": "string"}, "Municipally Supplied Potable Water - Indoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Reclaimed Water - Indoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Latitude": {"type": "string"}, "Design College/University - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Vocational School - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Average Weekly Business Hours": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Lifestyle Center - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Reheat Control Strategy": {"type": "string"}, "Urgent Care/Clinic/Other Outpatient - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Schedule Begin Month": {"ignore_malformed": true, "type": "float"}, "Other - Services - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Country": {"type": "string"}, "Primary Service Hot Water Fuel": {"type": "string"}, "Plug Load Type": {"type": "string"}, "Number of Occupants": {"ignore_malformed": true, "type": "float"}, "Vocational School - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Data Center - UPS Output Site Energy (kWh)": {"ignore_malformed": true, "type": "float"}, "Green Power - Onsite and Offsite (kWh)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 5.3 Materials - Environmentally Preferred Products": {"type": "string"}, "Clothes Washer Capacity": {"ignore_malformed": true, "type": "float"}, "Motor Enclosure Type": {"type": "string"}, "PM Administrator": {"type": "string"}, "Data Center - Cooling Equipment Redundancy": {"type": "string"}, "Annual Combined Whole Builidng Annual Source Energy Use Intensity (EUI)": {"ignore_malformed": true, "type": "float"}, "Auditor Company": {"type": "string"}, "Restaurant - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Guiding Principles - Principles Date Anticipated": {"type": "string"}, "Urgent Care/Clinic/Other Outpatient - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Typical Window Frame": {"type": "string"}, "Roller Rink - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Percent Better than National Median Water/Wastewater Source EUI": {"ignore_malformed": true, "type": "float"}, "Fast Food Restaurant - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "ASHRAE Baseline Lighting Power Density": {"ignore_malformed": true, "type": "float"}, "First Cost": {"ignore_malformed": true, "type": "float"}, "Casino - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Laundry Equipment Usage": {"type": "string"}, "Retail Store - Cash Register Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.4 Water - Efficient Products": {"type": "string"}, "Source EUI - Adjusted to Current Year (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Barracks- Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Source Site Ratio": {"ignore_malformed": true, "type": "float"}, "ENERGY STAR Certification - Next Eligible Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Coal - Bituminous Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Water Cooled Condenser Flow Control": {"type": "string"}, "Design Pre-school/Daycare - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 4.4 Indoor Environment - Daylighting and Occupant Controls Any Option (Any Option)": {"type": "string"}, "Property Floor Area (Buildings and Parking) (ft2)": {"ignore_malformed": true, "type": "float"}, "Control Technology": {"type": "string"}, "Marginal Cost Rate": {"ignore_malformed": true, "type": "float"}, "Contact Postal Code": {"type": "string"}, "Fuel Oil #2 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Percent Better Than Baseline Design Site Energy Use Intensity": {"ignore_malformed": true, "type": "float"}, "Typical Floor R-Value": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Wall Framing Factor": {"ignore_malformed": true, "type": "float"}, "Outpatient Rehabilitation/Physical Therapy - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "District Chilled Water Cost ($)": {"ignore_malformed": true, "type": "float"}, "Veterinary Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Type": {"type": "string"}, "M&amp;V Option": {"type": "string"}, "Primary Heating Fuel Type": {"type": "string"}, "Refrigerant Subcooler": {"type": "string"}, "District Hot Water": {"type": "string"}, "Pre-school/Daycare - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Fire Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Wall Framing Material": {"type": "string"}, "Race Track - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Boiler Entering Water Temperature": {"ignore_malformed": true, "type": "float"}, "Refrigeration Unit is ENERGY STAR Rated": {"type": "string"}, "Pool Surface Area": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Utility Billpayer": {"type": "string"}, "Guiding Principles - % Complete (Yes or Not Applicable)": {"type": "string"}, "District Steam Cost ($)": {"ignore_malformed": true, "type": "float"}, "Percent of Electricity that is Green Power": {"ignore_malformed": true, "type": "float"}, "US Agency Designated Covered Facility ID": {"type": "string"}, "Library - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Fuel Generated": {"type": "string"}, "Supermarket/Grocery - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Hospital (General Medical &amp; Surgical) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Outside Air Reset Maximum Heating Supply Temperature": {"ignore_malformed": true, "type": "float"}, "Floor Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Avoided Emissions - Onsite and Offsite Green Power (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Federal Agency": {"type": "string"}, "Tank Height": {"ignore_malformed": true, "type": "float"}, "Heated Floor Area": {"ignore_malformed": true, "type": "float"}, "Floor Area Source": {"type": "string"}, "Stadium (Open) - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Social/Meeting Hall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Performing Arts - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Floor Construction Type": {"type": "string"}, "Energy Baseline Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Primary Property Type - EPA Calculated": {"type": "string"}, "Weather Normalized Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Self-Storage Facility - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Maximum Number of Floors": {"ignore_malformed": true, "type": "float"}, "Museum - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Size of Electronic Scoreboards (ft2)": {"ignore_malformed": true, "type": "float"}, "Target Site EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Annual Savings Source Energy (Source Energy)": {"ignore_malformed": true, "type": "float"}, "Design Target Total GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Recirculation Control Type": {"type": "string"}, "Design Other - Restaurant/Bar - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Quantity": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 4.6 Indoor Environment - Integrated Pest Management": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Laboratory": {"type": "string"}, "Resource Generated On Site": {"ignore_malformed": true, "type": "float"}, "Other - Utility - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Biomass GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Weather Station Name": {"type": "string"}, "Guiding Principles - % In Process": {"type": "string"}, "Retail Store - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Refrigerant Charge Factor": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Collector Area": {"ignore_malformed": true, "type": "float"}, "Aspect Ratio": {"ignore_malformed": true, "type": "float"}, "Other - Services - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Year of Last Energy Audit": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Indoor Arena - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Parking - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Percent of Window Area Shaded": {"ignore_malformed": true, "type": "float"}, "Daylight Sensors": {"type": "string"}, "Interior Visible Absorptance": {"ignore_malformed": true, "type": "float"}, "Crankcase Heater": {"type": "string"}, "Floor Covering": {"type": "string"}, "Simple Payback": {"ignore_malformed": true, "type": "float"}, "Metering Configuration": {"type": "string"}, "Refrigerated Warehouse - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Refrigeration Unit Size": {"ignore_malformed": true, "type": "float"}, "IT Energy Intensity": {"ignore_malformed": true, "type": "float"}, "Economizer": {"type": "string"}, "Wholesale Club/Supercenter- Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Auditor Team Member with Certification": {"type": "string"}, "K-12 School - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Site Address Line 2": {"type": "string"}, "Backup Generator": {"type": "string"}, "Roller Rink - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Site Address Line 1": {"type": "string"}, "Design Library - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "IT Standby Power": {"ignore_malformed": true, "type": "float"}, "Fire Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Premises Postal Code": {"type": "string"}, "Conveyance System Type": {"type": "string"}, "Race Track - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Site EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Roof Type": {"type": "string"}, "Convenience Store with Gas Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Target Total GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "K-12 School - Cooking Facilities": {"type": "string"}, "K-12 School - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Single Store": {"type": "string"}, "Design Site EUI": {"ignore_malformed": true, "type": "float"}, "IT Peak Power": {"ignore_malformed": true, "type": "float"}, "Energy Cost ($)": {"ignore_malformed": true, "type": "float"}, "Recirculation Energy Loss Rate": {"ignore_malformed": true, "type": "float"}, "Lighting is FEMP Designated Product": {"type": "string"}, "Design Fire Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Distribution Type": {"type": "string"}, "Design Roller Rink - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Heat Gain Coefficient SHGC (SHGC)": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Number of Residential Living Units": {"ignore_malformed": true, "type": "float"}, "Hotel - Amount of Laundry Processed On-site Annually (short tons/year)": {"ignore_malformed": true, "type": "float"}, "Doors Weather-Stripped": {"type": "string"}, "Restaurant - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Electricity Use - Generated from Onsite Renewable Systems and Used Onsite (kBtu)": {"ignore_malformed": true, "type": "float"}, "Electric Demand Rate": {"type": "string"}, "Food Service - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Specialty Hospital - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Heat Recovery Efficiency": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.1 Energy Efficiency - Option 3": {"type": "string"}, "Guiding Principle 2.1 Energy Efficiency - Option 2": {"type": "string"}, "Guiding Principle 2.1 Energy Efficiency - Option 1": {"type": "string"}, "Cooling Equipment Redundancy": {"type": "string"}, "Space Floors Below Grade": {"ignore_malformed": true, "type": "float"}, "Design Coal - Bituminous Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Split Condenser": {"type": "string"}, "Other Water Sources - Indoor Intensity (gal/ft2)": {"ignore_malformed": true, "type": "float"}, "Dishwasher Type": {"type": "string"}, "Movie Theater - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Light Shelf Exterior Protrusion": {"ignore_malformed": true, "type": "float"}, "Primary Air Distribution Type": {"type": "string"}, "Design Bowling Alley - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Day End Hour": {"ignore_malformed": true, "type": "float"}, "Boiler percent condensate return": {"ignore_malformed": true, "type": "float"}, "Weather Normalized Site EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Hot Water Boiler Maximum Flow Rate": {"ignore_malformed": true, "type": "float"}, "Mailing Center/Post Office - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Horizontal Abutments": {"type": "string"}, "Space Use Description": {"type": "string"}, "K-12 School - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Indoor Arena - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "On-Site Generation Type": {"type": "string"}, "Design Single Family Home - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Minimum Dimming Power Fraction": {"ignore_malformed": true, "type": "float"}, "Target Energy Cost ($)": {"ignore_malformed": true, "type": "float"}, "National Average EUI": {"ignore_malformed": true, "type": "float"}, "Owner Street Address": {"type": "string"}, "Other - Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Air Duct Configuration": {"type": "string"}, "Mailing Center/Post Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Indirect GHG Emissions Intensity (kgCO2e/gpd)": {"ignore_malformed": true, "type": "float"}, "PV System Inverter Efficiency": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical)- Full Time Equivalent (FTE) Workers": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Food Service - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Year of Last Major Remodel": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Water Fixture Volume per Cycle": {"ignore_malformed": true, "type": "float"}, "Water Fixture Cycles per day": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Social/Meeting Hall - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Office - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Maximum Fan Power": {"ignore_malformed": true, "type": "float"}, "Typical Skylight Area": {"ignore_malformed": true, "type": "float"}, "Visible Transmittance": {"ignore_malformed": true, "type": "float"}, "Number of Cooling Stages": {"ignore_malformed": true, "type": "float"}, "Veterinary Office - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Plant Design Flow Rate (MGD)": {"ignore_malformed": true, "type": "float"}, "Office - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Residual Value": {"type": "string"}, "Power Plant": {"type": "string"}, "Liquid Propane Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Window Glass Layers": {"ignore_malformed": true, "type": "float"}, "Design Other - Utility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Minimum Fan Flow Rate": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Number of Sporting Events per Year": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Size of Electronic Scoreboards (ft2)": {"ignore_malformed": true, "type": "float"}, "Fan Efficiency": {"ignore_malformed": true, "type": "float"}, "Design Other - Public Services - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Average Influent Biological Oxygen Demand (BOD5) (mg/l)": {"ignore_malformed": true, "type": "float"}, "Enclosed Mall - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "SHW Setpoint Temp": {"ignore_malformed": true, "type": "float"}, "Duty Cycle": {"type": "string"}, "College/University - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Floors Partially Below Grade": {"ignore_malformed": true, "type": "float"}, "Retail Store - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Natural Gas Use (therms)": {"ignore_malformed": true, "type": "float"}, "Energy Storage Type": {"type": "string"}, "Food Sales - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Collector Azimuth": {"ignore_malformed": true, "type": "float"}, "Ambulatory Surgical Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Manufacturing/Industrial Plant - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Occupancy Sensors": {"type": "string"}, "Case Return Line Diameter": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Design Aquarium - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Social/Meeting Hall - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Fixed Film Trickle Filtration Process": {"type": "string"}, "Design Courthouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Target Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Overhang Projection": {"type": "string"}, "National Median ENERGY STAR Score": {"ignore_malformed": true, "type": "float"}, "Strip Mall - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Barracks - Room Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Plug Load Equipment is ENERGY STAR Rated": {"type": "string"}, "Occupancy": {"type": "string"}, "Enclosed Mall - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fitness Center/Health Club/Gym - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Parent Property Name": {"type": "string"}, "Hotel- Number of Commercial Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Required Ventilation Rate": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 5.4 Materials - Waste and Materials Mgmt": {"type": "string"}, "Fast Food Restaurant - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.3 Water - Stormwater": {"type": "string"}, "Design Laboratory - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Premises Street Address 1": {"type": "string"}, "Other - Technology/Science - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Medical Office - Number of MRI Machines": {"ignore_malformed": true, "type": "float"}, "Heating Delivery Type": {"type": "string"}, "Other - Stadium - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Tertiary Care": {"type": "string"}, "Other - Public Services - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Wall R-Value": {"ignore_malformed": true, "type": "float"}, "Typical Window to Wall Ratio": {"ignore_malformed": true, "type": "float"}, "Refrigeration Energy": {"ignore_malformed": true, "type": "float"}, "Recommended Measure": {"type": "string"}, "K-12 School - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Wood Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Water Fixture Rated Flow Rate": {"ignore_malformed": true, "type": "float"}, "Number of Rooms": {"ignore_malformed": true, "type": "float"}, "Other Water Sources - Indoor Cost Intensity ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Door Glazed Area Fraction": {"ignore_malformed": true, "type": "float"}, "Recirculation": {"type": "string"}, "Potable Water Savings": {"ignore_malformed": true, "type": "float"}, "Design Casino - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Typical Window Sill Height": {"ignore_malformed": true, "type": "float"}, "Floor Type": {"type": "string"}, "Daylighting Illuminance Set Point": {"ignore_malformed": true, "type": "float"}, "Water-Side Economizer DB Temperature Maximum": {"ignore_malformed": true, "type": "float"}, "Strip Mall - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Fast Food Restaurant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Entertainment/Public Assembly - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Anti-Sweat Heater Controls Manufacturer": {"type": "string"}, "Design Wastewater Treatment Plant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Outpatient Rehabilitation/Physical Therapy - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Number of Staffed Beds": {"ignore_malformed": true, "type": "float"}, "Design Retail Store - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Worship Facility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Number of Commercial Washing Machines": {"ignore_malformed": true, "type": "float"}, "Burner Type": {"type": "string"}, "Other - Services - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Pump Minimum Flow Rate": {"ignore_malformed": true, "type": "float"}, "Movie Theater - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Convenience Store with Gas Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Maximum Number of Floors": {"ignore_malformed": true, "type": "float"}, "National Median Water/Wastewater Site EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Number of Open or Closed Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Guiding Principles - % Yes": {"type": "string"}, "Refrigeration Unit Type": {"type": "string"}, "Medical Office - MRI Machine Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Transformer Rated Power": {"ignore_malformed": true, "type": "float"}, "Rated Cooling Sensible Heat Ratio": {"ignore_malformed": true, "type": "float"}, "Domestic Hot Water Type": {"type": "string"}, "Typical Window Area": {"ignore_malformed": true, "type": "float"}, "Laboratory - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other Water Sources: Combined Indoor/Outdoor or Other Cost ($)": {"ignore_malformed": true, "type": "float"}, "Arch2030 Baseline": {"ignore_malformed": true, "type": "float"}, "Alternate Baseline Lighting Power Density": {"ignore_malformed": true, "type": "float"}, "Office - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Design Veterinary Office - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Serves Multiple Buildings": {"type": "string"}, "Performing Arts - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Humidity Control Maximum": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Onsite Laundry Facility": {"type": "string"}, "Pool Pump Duty Cycle": {"type": "string"}, "Process Load Installed Power": {"ignore_malformed": true, "type": "float"}, "Other - Public Services - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Ownership Status": {"type": "string"}, "Convenience Store with Gas Station - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Zoo - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Typical Window SHGC": {"ignore_malformed": true, "type": "float"}, "National Median Reference Property Type": {"type": "string"}, "Prison/Incarceration - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Roof Area": {"ignore_malformed": true, "type": "float"}, "Convention Center - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Fenestration Type": {"type": "string"}, "Economizer Low Temperature Lockout": {"type": "string"}, "ENERGY STAR Certification - Eligibility": {"type": "string"}, "Design Performing Arts - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Adult Education - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Open All Weekdays": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Number of Sporting Events per Year": {"ignore_malformed": true, "type": "float"}, "Average Weekly Hours": {"ignore_malformed": true, "type": "float"}, "Wood Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Performing Arts - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Generation Annual Operation Hours": {"ignore_malformed": true, "type": "float"}, "Total GHG Emissions Intensity (kgCO2e/ft2)": {"ignore_malformed": true, "type": "float"}, "Reactive Power Charge": {"ignore_malformed": true, "type": "float"}, "Technology Category": {"type": "string"}, "Roof Exterior Thermal Absorptance": {"ignore_malformed": true, "type": "float"}, "Other - Mall - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Interior Automated Shades": {"type": "string"}, "Design Ice/Curling Rink - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "College/University - Enrollment": {"ignore_malformed": true, "type": "float"}, "Roof Framing Depth": {"ignore_malformed": true, "type": "float"}, "Evaporative Cooling Entering Supply Air DB Temperature": {"ignore_malformed": true, "type": "float"}, "Annual Fuel Use Consistent Units (Consistent Units)": {"type": "string"}, "Design Target Site EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Pool Water Temperature": {"ignore_malformed": true, "type": "float"}, "Condensing Temperature": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Technology/Science - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Data Center - National Median PUE": {"ignore_malformed": true, "type": "float"}, "Cooling Stage Capacity": {"ignore_malformed": true, "type": "float"}, "District Chilled Water Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Outdoor Water Cost (All Water Sources) ($)": {"ignore_malformed": true, "type": "float"}, "Aquarium - Number of Computers": {"ignore_malformed": true, "type": "float"}, "K-12 School - High School": {"type": "string"}, "Other - Utility - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Food Service - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Laundry Unit Quantity": {"ignore_malformed": true, "type": "float"}, "Pool Type": {"type": "string"}, "Fuel Oil (No. 1) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Percent of Gross Floor Area That is Common Space Only": {"ignore_malformed": true, "type": "float"}, "Locations of exterior water intrusion damage": {"type": "string"}, "Police Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Electricity (Grid Purchase) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Light Shelf Distance from Top": {"ignore_malformed": true, "type": "float"}, "Laundry Unit Year of Manufacture": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Weather Data Source": {"type": "string"}, "Roof Exterior Solar Absorptance": {"ignore_malformed": true, "type": "float"}, "Boiler Insulation R Value": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Design PUE": {"ignore_malformed": true, "type": "float"}, "Food Service - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fitness Center/Health Club/Gym - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Residence Hall/ Dormitory - Dining Hall": {"type": "string"}, "Duct Pressure Test Leakage cfm (cfm)": {"ignore_malformed": true, "type": "float"}, "Data Center - IT Site Energy (kWh)": {"ignore_malformed": true, "type": "float"}, "Laboratory - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Setup temperature cooling": {"ignore_malformed": true, "type": "float"}, "Roof Framing Material": {"type": "string"}, "Lifestyle Center - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Roof Insulated Area": {"ignore_malformed": true, "type": "float"}, "CMU Fill": {"type": "string"}, "Evaporator Pressure Regulators": {"type": "string"}, "Cooling is FEMP Designated Product": {"type": "string"}, "Emissions Factor - Source": {"type": "string"}, "Other - Stadium - Size of Electronic Scoreboards (ft2)": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Number of Concert/Show Events per Year": {"ignore_malformed": true, "type": "float"}, "Courthouse - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Window Horizontal Spacing": {"ignore_malformed": true, "type": "float"}, "Winter Peak": {"ignore_malformed": true, "type": "float"}, "Roof R-Value": {"ignore_malformed": true, "type": "float"}, "Other - Lodging/Residential - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Source Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Residence Hall/Dormitory - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Restaurant/Bar - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Reclaimed Water - Indoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Economizer Enthalpy Control Point": {"ignore_malformed": true, "type": "float"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Data Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "College/University - Grant Dollars ($)": {"ignore_malformed": true, "type": "float"}, "Outpatient Rehabilitation/Physical Therapy - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Cash Register Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Self-Storage Facility - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Condensing Operation": {"type": "string"}, "Museum - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fan Placement": {"type": "string"}, "National Median Site EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Data Center - IT Energy Configuration": {"type": "string"}, "Medical Office - Surgical Operating Bed Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Ambulatory Surgical Center - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Potable": {"type": "string"}, "Quality Alert-Space Alert": {"type": "string"}, "Stadium (Closed) - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Percent of RECs Retained": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "National Median Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Lighting Type": {"type": "string"}, "REALPac Energy Benchmarking Program Building Name": {"type": "string"}, "Utility Meter Number": {"type": "string"}, "Stadium (Closed) - Number of Special/Other Events per Year": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Design Target Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Movie Theater - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Museum - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other - Technology/Science - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Other - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Other - Mall - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Restaurant/Bar - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Food Sales - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Installed Power": {"ignore_malformed": true, "type": "float"}, "Primary Service Hot Water Location": {"type": "string"}, "Rated Lamp Life": {"ignore_malformed": true, "type": "float"}, "Natural Gas Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Clothes Washer Water Factor": {"ignore_malformed": true, "type": "float"}, "Energy Cost": {"ignore_malformed": true, "type": "float"}, "Cooling Tower Control Type": {"type": "string"}, "Annual Water Cost Savings": {"ignore_malformed": true, "type": "float"}, "Ice/Curling Rink - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Water/Wastewater Site EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Movie Theater - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Plumbing Penetration Sealing": {"type": "string"}, "Personal Services (Health/Beauty, Dry Cleaning, etc.) - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Lodging/Residential - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Cooling Degree Days (CDD) (\\u00b0F)": {"ignore_malformed": true, "type": "float"}, "Enclosed Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Duct Surface Area": {"ignore_malformed": true, "type": "float"}, "Self-Storage Facility - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Owner Email Address": {"type": "string"}, "Pump Installed Flow Rate": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Licensed Bed Capacity Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Climate Zone": {"type": "string"}, "Police Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Net Present Value": {"ignore_malformed": true, "type": "float"}, "Other - Technology/Science - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Number of Computers": {"ignore_malformed": true, "type": "float"}, "ENERGY STAR Certification - Profile Published": {"type": "string"}, "Senior Care Community - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Input Voltage": {"ignore_malformed": true, "type": "float"}, "Electricity Sourced from Onsite Renewable Systems": {"ignore_malformed": true, "type": "float"}, "Motor RPM": {"ignore_malformed": true, "type": "float"}, "Ownership": {"type": "string"}, "Convenience Store without Gas Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Coke Cost ($)": {"ignore_malformed": true, "type": "float"}, "Fan Size": {"ignore_malformed": true, "type": "float"}, "Pool Hours Uncovered": {"ignore_malformed": true, "type": "float"}, "District Hot Water Cost ($)": {"ignore_malformed": true, "type": "float"}, "Veterinary Office - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Heating Degree Days (HDD) (\\u00b0F)": {"ignore_malformed": true, "type": "float"}, "Portfolio Manager Parent Property ID": {"type": "string"}, "Senior Care Community - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Type of resource meter": {"type": "string"}, "Museum - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Wall Exterior Thermal Absorptance": {"ignore_malformed": true, "type": "float"}, "Aquarium - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Percent Skylight Area": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other Financial Incentives": {"type": "string"}, "Target Site Energy Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "College/University - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.2 Outdoor Water - Option 3": {"type": "string"}, "Guiding Principle 3.2 Outdoor Water - Option 2": {"type": "string"}, "Guiding Principle 3.2 Outdoor Water - Option 1": {"type": "string"}, "Water-Side Economizer Temperature Setpoint": {"ignore_malformed": true, "type": "float"}, "Pump Operation": {"type": "string"}, "Other - Specialty Hospital - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Target Total GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Distribution Center - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Heat Recovery Type": {"type": "string"}, "Drinking Water Treatment &amp; Distribution - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Quality Alert-Energy Alerts": {"type": "string"}, "Urgent Care/Clinic/Other Outpatient - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Refrigeration Compressor Type": {"type": "string"}, "Floor Area Value": {"ignore_malformed": true, "type": "float"}, "Work Plane Height": {"ignore_malformed": true, "type": "float"}, "Worship Facility - Seating Capacity": {"ignore_malformed": true, "type": "float"}, "Avoided Emissions - Onsite Green Power (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Benchmark Type": {"type": "string"}, "Multifamily Housing - Number of Dishwasher Hookups": {"ignore_malformed": true, "type": "float"}, "Roof Slope": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Annual Combined Whole Building Annual Weather Normalized Source Resource Use": {"ignore_malformed": true, "type": "float"}, "Primary Heating Type": {"type": "string"}, "Data Center - PUE": {"ignore_malformed": true, "type": "float"}, "Ambulatory Surgical Center - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Roller Rink - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Race Track - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Primary Fan Configuration": {"type": "string"}, "ENERGY STAR Certification - Last Approval Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Avoided Emissions": {"ignore_malformed": true, "type": "float"}, "Generation Capacity Unit": {"type": "string"}, "Other - Restaurant/Bar - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Fast Food Restaurant - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Fuel Oil (No. 4) Cost ($)": {"ignore_malformed": true, "type": "float"}, "Marginal Sell Rate": {"ignore_malformed": true, "type": "float"}, "Pool Control Type": {"type": "string"}, "Name of Audit Certification Holder": {"type": "string"}, "Roof Framing Configuration": {"type": "string"}, "Design Race Track - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 4.5 Indoor Environment - Low-Emitting Materials": {"type": "string"}, "Service and Product Provider": {"type": "string"}, "Mailing Center/Post Office - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Laboratory - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Measure Name": {"type": "string"}, "Normalization Start Year": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Bar/Nightclub - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Outside Air Temperature Upper Limit Heating Reset Control": {"ignore_malformed": true, "type": "float"}, "Laboratory - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Direct GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Number of Heating Stages": {"ignore_malformed": true, "type": "float"}, "Barracks- Number of Rooms": {"ignore_malformed": true, "type": "float"}, "Pool is Heated": {"type": "string"}, "Vertical Edge Fin Only": {"type": "string"}, "Owner Postal Code": {"type": "string"}, "Stadium (Open) - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Potable Water - Outdoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Year PM Benchmarked": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Design Biomass GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Cooling is ENERGY STAR Rated": {"type": "string"}, "Wall Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Other - Specialty Hospital - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Design Convention Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Residence Hall/Dormitory - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Zoo - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Typical Skylight U-Value": {"ignore_malformed": true, "type": "float"}, "Other - Utility - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Prison/Incarceration - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Data Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 5.2 Materials - Biobased Content": {"type": "string"}, "Electricity Use - Grid Purchase and Generated from Onsite Renewable Systems (kBtu)": {"ignore_malformed": true, "type": "float"}, "Water Use (All Water Sources) (kgal)": {"ignore_malformed": true, "type": "float"}, "Other - Lodging/Residential - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Bowling Alley - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Indoor Water Cost Intensity (All Water Sources) ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Number of Cash Registers": {"ignore_malformed": true, "type": "float"}, "eGRID Region Code": {"type": "string"}, "Guiding Principle 5.1 Materials - Recycled Content": {"type": "string"}, "Year Occupied": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Building air leakage unit": {"type": "string"}, "Typical Window U-Value": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Heating Refrigerant Type": {"type": "string"}, "Senior Care Community - Residential Washing Machine Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Outside Air Temperature Lower Limit Heating Reset Control": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Funding from Rebates": {"type": "string"}, "Process Load Type": {"type": "string"}, "Design Propane Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Financial Office - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Average Cooling Operating Hours": {"ignore_malformed": true, "type": "float"}, "Building Operator Name": {"type": "string"}, "Supermarket/Grocery - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Social/Meeting Hall - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Premises Tax Map Number": {"type": "string"}, "Retro-commissioning Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Water/Wastewater Estimated Savings from Energy Projects, Cumulative ($/GPD)": {"ignore_malformed": true, "type": "float"}, "Case Door Orientation": {"type": "string"}, "Owner Telephone Number": {"type": "string"}, "Office - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Movie Theater - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Urgent Care/Clinic/Other Outpatient - Number of Computers": {"ignore_malformed": true, "type": "float"}, "U.S. Federal Campus": {"type": "string"}, "Tightness": {"type": "string"}, "K-12 School - Student Seating Capacity": {"ignore_malformed": true, "type": "float"}, "Zoo - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Multiple Building Heights": {"ignore_malformed": true, "type": "float"}, "Hotel - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Annual Combined Whole Builidng Annual Site Energy Use Intensity (EUI)": {"ignore_malformed": true, "type": "float"}, "Other - Services - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Prison/Incarceration - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Stadium - Number of Concert/Show Events per Year": {"ignore_malformed": true, "type": "float"}, "Number of Computers": {"ignore_malformed": true, "type": "float"}, "Scenario Type": {"type": "string"}, "Restaurant - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Reference Case": {"type": "string"}, "Wastewater Treatment Plant - Nutrient Removal": {"type": "string"}, "Municipally Supplied Potable Water - Indoor Intensity (gal/ft2)": {"ignore_malformed": true, "type": "float"}, "Condenser Fan Speed Operation": {"type": "string"}, "Weather Normalized Water/Wastewater Site Natural Gas Intensity (therms/gpd)": {"ignore_malformed": true, "type": "float"}, "Window Width": {"ignore_malformed": true, "type": "float"}, "Plug Load Standby Power": {"ignore_malformed": true, "type": "float"}, "Strip Mall - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Restaurant - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Government Subsidized Housing": {"type": "string"}, "Peak Occupancy Percentage": {"ignore_malformed": true, "type": "float"}, "Daily Hot Water Draw": {"ignore_malformed": true, "type": "float"}, "Analysis Period": {"type": "string"}, "Fan Application": {"type": "string"}, "Hotel - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Premises Conditioned Floor Area": {"ignore_malformed": true, "type": "float"}, "Design Other - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Onsite Renewable System Electricity Exported": {"ignore_malformed": true, "type": "float"}, "Pipe Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Site Use Description": {"type": "string"}, "PV System Maximum Power Output": {"ignore_malformed": true, "type": "float"}, "Measure Scale of Application": {"type": "string"}, "Transportation Terminal/Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Residence Hall/Dormitory - Number of Rooms": {"ignore_malformed": true, "type": "float"}, "Storage Tank Insulation R-Value": {"ignore_malformed": true, "type": "float"}, "Single Family Home - Number of Bedrooms": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Ventilation Control Method": {"type": "string"}, "Bank Branch - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Medical Office - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Cooking Facilities": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 3.1 Water - Indoor Water Any Option (Any Option)": {"type": "string"}, "Typical Ground Coupling Type": {"type": "string"}, "Contact City": {"type": "string"}, "Design Distribution Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Entertainment/Public Assembly - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Pre-school/Daycare - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Tightness/Fit Condition": {"type": "string"}, "Design Other - Services - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Utility - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "College/University - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Retail Store - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Wall Insulation Material": {"type": "string"}, "Gas Price Escalation Rate": {"ignore_malformed": true, "type": "float"}, "Absorption Stages": {"type": "string"}, "Floor R-Value": {"ignore_malformed": true, "type": "float"}, "Floor Area Custom Name": {"type": "string"}, "Water/Wastewater Direct GHG Emissions Intensity (kgCO2e/gpd)": {"ignore_malformed": true, "type": "float"}, "Multifamily Housing - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Retail Store - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Number of Open or Closed Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Supply Air Temp Reset Control": {"type": "string"}, "Stadium (Closed) - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Single Family Home - Density of People (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Hot Water Present": {"type": "string"}, "Wall Framing Configuration": {"type": "string"}, "National Median Total GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Hotel - Commercial Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "K-12 School - Weekend Operation": {"type": "string"}, "Fire Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "LEED Certification Audit Exemption": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Full Time Equivalent (FTE) Workers Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Premises Tax Book Number": {"type": "string"}, "Supermarket/Grocery - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Certification Program": {"type": "string"}, "Boiler Leaving Water Temperature": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Number of Computers": {"ignore_malformed": true, "type": "float"}, "Control Type": {"type": "string"}, "Premises Block Number": {"type": "string"}, "Repair Services (Vehicle, Shoe, Locksmith, etc.) - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Bar/Nightclub - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Summer Peak": {"ignore_malformed": true, "type": "float"}, "Convention Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Lamp Distribution Type": {"type": "string"}, "Water-Side Economizer Type": {"type": "string"}, "Manufacturing/Industrial Plant - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Transportation Terminal/Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "PV System Array Azimuth": {"ignore_malformed": true, "type": "float"}, "Roof Insulation Type": {"type": "string"}, "Premises Occupied Floor Area": {"ignore_malformed": true, "type": "float"}, "Skylight Operability": {"type": "string"}, "Financial Office - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "SHW is ENERGY STAR Rated": {"type": "string"}, "Stadium (Open) - Enclosed Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Indirect GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Guiding Principle 2.5 Energy - Benchmarking": {"type": "string"}, "Lighting Control Type Occupancy": {"type": "string"}, "Premises Custom ID": {"type": "string"}, "Hospital (General Medical &amp; Surgical) - Number of Workers on Main Shift Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Fuel Oil #4 Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "K-12 School - School District": {"type": "string"}, "Municipally Supplied Reclaimed Water: Combined Indoor/Outdoor or Other Cost ($)": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Retail Store - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "End Time Stamp": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Design Lifestyle Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Wholesale Club/Supercenter- Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Other Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Source Energy Use - Adjusted to Current Year (kBtu)": {"ignore_malformed": true, "type": "float"}, "Annual Savings Site Energy (Site Energy)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Economizer Type": {"type": "string"}, "Tank Volume": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Resident Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Number of Units": {"ignore_malformed": true, "type": "float"}, "Automobile Dealership - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Wastewater Treatment Plant - Average Influent Flow (MGD)": {"ignore_malformed": true, "type": "float"}, "K-12 School - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Swimming Pool - Location of Pool": {"type": "string"}, "Manufacturing/Industrial Plant - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "College/University - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Heating Staging": {"type": "string"}, "Distance Between Vertical Fins": {"ignore_malformed": true, "type": "float"}, "Total Heat Rejection": {"ignore_malformed": true, "type": "float"}, "Guiding Principles - % No": {"type": "string"}, "Floor Framing Configuration": {"type": "string"}, "Medical Office - Number of Surgical Operating Beds": {"ignore_malformed": true, "type": "float"}, "Restaurant - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Non-Refrigerated Warehouse - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Heating Setpoint setpoint": {"ignore_malformed": true, "type": "float"}, "Weather-Stripped": {"type": "string"}, "Fuel Use Intensity": {"ignore_malformed": true, "type": "float"}, "Contact State": {"type": "string"}, "Roof Framing Factor": {"ignore_malformed": true, "type": "float"}, "Bar/Nightclub - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Locations of interior water intrusion damage": {"type": "string"}, "Exterior Door Type": {"type": "string"}, "Enclosed Mall - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Slab Exposed Perimeter": {"ignore_malformed": true, "type": "float"}, "Type of Cooking Equipment": {"type": "string"}, "Target Water/Wastewater Source EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Enclosed Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Supermarket/Grocery - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Water/Wastewater Total GHG Emissions Intensity (kgCO2e/gpd)": {"ignore_malformed": true, "type": "float"}, "Discount Factor": {"ignore_malformed": true, "type": "float"}, "Retail Store - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Laboratory - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Premises Count": {"ignore_malformed": true, "type": "float"}, "Urgent Care/Clinic/Other Outpatient - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "OM Cost Annual Savings": {"ignore_malformed": true, "type": "float"}, "Casino - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Water Baseline Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Typical Window Type": {"type": "string"}, "Steam Boiler Minimum Operating Pressure": {"ignore_malformed": true, "type": "float"}, "Energy Recovery Efficiency": {"ignore_malformed": true, "type": "float"}, "Duct Insulation R-Value": {"ignore_malformed": true, "type": "float"}, "Premises City": {"type": "string"}, "Aquarium - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Strip Mall - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Vestibule": {"type": "string"}, "Municipally Supplied Reclaimed Water - Outdoor Cost ($)": {"ignore_malformed": true, "type": "float"}, "Indirect Tank Heating Source": {"type": "string"}, "Other - Public Services - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Federal Department": {"type": "string"}, "Other - Recreation - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Single Family Home - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Solar Thermal System Collector Tilt": {"ignore_malformed": true, "type": "float"}, "Fire Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Typical Exterior Shading Type": {"type": "string"}, "Anti-Sweat Heater Controls": {"type": "string"}, "Ice/Curling Rink - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other Peak Rate": {"type": "string"}, "Design Total GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - MRI Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "K-12 School - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Pre-school/Daycare - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Duct Insulation": {"type": "string"}, "Bar/Nightclub - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Property Floor Area (Parking) (ft2)": {"ignore_malformed": true, "type": "float"}, "Resource": {"type": "string"}, "PV Module Length": {"ignore_malformed": true, "type": "float"}, "Tank Perimeter": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Licensed Bed Capacity": {"ignore_malformed": true, "type": "float"}, "Food Sales - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Ballast Type": {"type": "string"}, "Indoor Arena - Number of Special/Other Events per Year": {"ignore_malformed": true, "type": "float"}, "Door Configuration": {"type": "string"}, "Guiding Principle 1.5 Integrated - Commissioning": {"type": "string"}, "Convention Center - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Convention Center - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Target Water/Wastewater Site EUI (kBtu/gpd)": {"ignore_malformed": true, "type": "float"}, "Calculation Method": {"type": "string"}, "Radiant Barrier": {"type": "string"}, "Bar/Nightclub - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Light Shelf Interior Protrusion": {"ignore_malformed": true, "type": "float"}, "Convenience Store with Gas Station - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "ENERGY STAR Certification - Application Status": {"type": "string"}, "Veterinary Office - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Conveyance Standby Power": {"ignore_malformed": true, "type": "float"}, "Outside Air Reset Maximum Cooling Supply Temperature": {"ignore_malformed": true, "type": "float"}, "Energy/Power Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Floor-to-Floor Height": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Ice Events": {"ignore_malformed": true, "type": "float"}, "Transportation Terminal/Station - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Primary Zonal Cooling Type": {"type": "string"}, "eGRID Subregion": {"type": "string"}, "Window Sill Height": {"ignore_malformed": true, "type": "float"}, "Food Sales - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Convention Center - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Foundation Wall Insulation Condition": {"type": "string"}, "Construction Status": {"type": "string"}, "Indoor Arena - Walk-in Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Roof Insulation Condition": {"type": "string"}, "Design Stadium (Closed) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Enclosed Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Electricity Use - Grid Purchase and Generated from Onsite Renewable Systems (kWh)": {"ignore_malformed": true, "type": "float"}, "Heating Setback Frequency": {"type": "string"}, "Barracks- Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Self-Storage Facility - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Ice/Curling Rink - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Anti-Sweat Heater Controls Model Number": {"type": "string"}, "Primary Zonal Cooling Fuel Type": {"type": "string"}, "Floor-to-Ceiling Height": {"ignore_malformed": true, "type": "float"}, "Total GHG Emissions (MtCO2e)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Reclaimed Water - Indoor Intensity (gal/ft2)": {"ignore_malformed": true, "type": "float"}, "Dishwasher Loads Per Week": {"ignore_malformed": true, "type": "float"}, "Other - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Food Sales - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Non-Potable Water used for Irrigation": {"type": "string"}, "Asset Score": {"ignore_malformed": true, "type": "float"}, "Collection Date": {"ignore_malformed": true, "type": "date", "format": "MM/dd/yyyy||yyyy-MM-dd||d-MMM-y||dd-MMM-y||dd-MMM-yy||date_optional_time"}, "Electricity Use - Generated from Onsite Renewable Systems (kWh)": {"ignore_malformed": true, "type": "float"}, "Other Water Sources - Outdoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Wall Framing Depth": {"ignore_malformed": true, "type": "float"}, "Natural Ventilation Rate": {"ignore_malformed": true, "type": "float"}, "Self-Storage Facility - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Target Source EUI (kBtu/ft2)": {"ignore_malformed": true, "type": "float"}, "Fire Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Refrigerated Warehouse - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Foundation Wall Type": {"type": "string"}, "Indoor Arena - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Input Capacity": {"ignore_malformed": true, "type": "float"}, "Natural Ventilation": {"type": "string"}, "Design Greenhouse Gas Emissions Intensity": {"ignore_malformed": true, "type": "float"}, "Ambulatory Surgical Center - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "Other - Restaurant/Bar - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Hot Water Boiler Minimum Flow Rate": {"ignore_malformed": true, "type": "float"}, "PV System Number of Modules per Array": {"ignore_malformed": true, "type": "float"}, "Propane Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Primary Zonal Heating Type": {"type": "string"}, "Lifestyle Center - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Senior Care Community - Number of Residential Washing Machines": {"ignore_malformed": true, "type": "float"}, "Retail Store - Open or Closed Refrigeration Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Municipally Supplied Potable Water - Outdoor Use (kgal)": {"ignore_malformed": true, "type": "float"}, "Floor Framing Depth": {"ignore_malformed": true, "type": "float"}, "Retail Store - Exterior Entrance to the Public": {"type": "string"}, "Lifestyle Center - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Food Service - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Alternative Water Generated On-Site - Indoor Cost Intensity ($/ft2)": {"ignore_malformed": true, "type": "float"}, "Bank Branch - Number of Computers": {"ignore_malformed": true, "type": "float"}, "Design Police Station - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "In Portfolio Manager": {"type": "string"}, "Utility Name": {"type": "string"}, "Design Ambient Temperature": {"ignore_malformed": true, "type": "float"}, "Primary Cooking Fuel": {"type": "string"}, "PV System Number of Arrays": {"ignore_malformed": true, "type": "float"}, "Courthouse - Percent That Can Be Cooled": {"ignore_malformed": true, "type": "float"}, "Lighting Power Density": {"ignore_malformed": true, "type": "float"}, "Stadium (Closed) - Size of Electronic Scoreboards (ft2)": {"ignore_malformed": true, "type": "float"}, "Parking - Completely Enclosed Parking Garage Size (ft2)": {"ignore_malformed": true, "type": "float"}, "Fixture Spacing": {"type": "string"}, "Heating Stage Capacity": {"ignore_malformed": true, "type": "float"}, "Number of Exterior Doors": {"ignore_malformed": true, "type": "float"}, "Museum - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Design Coke Use (kBtu)": {"ignore_malformed": true, "type": "float"}, "Design Personal Services (Health/Beauty, Dry Cleaning, etc) - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Lighting is ENERGY STAR Rated": {"type": "string"}, "Green Power - Onsite (kWh)": {"ignore_malformed": true, "type": "float"}, "Outside Air Temperature Lower Limit Cooling Reset Control": {"ignore_malformed": true, "type": "float"}, "Wall Insulation Type": {"type": "string"}, "Office - Worker Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Design Target ENERGY STAR Score": {"ignore_malformed": true, "type": "float"}, "Cooking Energy per Meal": {"ignore_malformed": true, "type": "float"}, "Ceiling Visible Absorptance": {"ignore_malformed": true, "type": "float"}, "Food Service - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Cooking Capacity": {"ignore_malformed": true, "type": "float"}, "Typical Window Visual Transmittance": {"ignore_malformed": true, "type": "float"}, "Stadium (Open) - Number of Special/Other Events per Year": {"ignore_malformed": true, "type": "float"}, "Motor Full Load Amps": {"ignore_malformed": true, "type": "float"}, "Number of Lamps per Luminaire": {"ignore_malformed": true, "type": "float"}, "Indoor Arena - Number of Walk-in Refrigeration/Freezer Units": {"ignore_malformed": true, "type": "float"}, "Duct Sealing": {"type": "string"}, "Design Parking - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Other - Restaurant/Bar - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Cooling Supply Air Temperature Control Type": {"type": "string"}, "Supermarket/Grocery - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Data Center - IT Source Energy (kBtu)": {"ignore_malformed": true, "type": "float"}, "Convenience Store without Gas Station - Computer Density (Number per 1,000 ft2)": {"ignore_malformed": true, "type": "float"}, "Slab Insulation Thickness": {"ignore_malformed": true, "type": "float"}, "Design Energy Cost ($)": {"ignore_malformed": true, "type": "float"}, "Pumping Configuration": {"type": "string"}, "Federal Sustainability Checklist Completion Percentage": {"ignore_malformed": true, "type": "float"}, "Hospital (General Medical &amp; Surgical) - Number of Workers on Main Shift": {"ignore_malformed": true, "type": "float"}, "Strip Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Metered Areas (Water)": {"type": "string"}, "Senior Care Community - Maximum Resident Capacity": {"ignore_malformed": true, "type": "float"}, "Other - Services - Weekly Operating Hours": {"ignore_malformed": true, "type": "float"}, "K-12 School - Percent That Can Be Heated": {"ignore_malformed": true, "type": "float"}, "Foundation Area": {"ignore_malformed": true, "type": "float"}, "Third party certification": {"type": "string"}, "Guiding Principles - Principles Date Achieved": {"type": "string"}, "Design Movie Theater - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Hotel - Cooking Facilities": {"type": "string"}, "Design Urgent Care/Clinic/Other Outpatient - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Single Family Home - Number of People": {"ignore_malformed": true, "type": "float"}, "Design Strip Mall - Gross Floor Area (ft2)": {"ignore_malformed": true, "type": "float"}, "Source Energy Use": {"ignore_malformed": true, "type": "float"}, "District Steam": {"type": "string"}, "Other Cost ($)": {"ignore_malformed": true, "type": "float"}}}, "space_alerts": {"type": "string"}, "pm_property_id": {"index": "not_analyzed", "type": "string", "doc_values": true}, "use_description": {"type": "string"}, "conditioned_floor_area": {"type": "float"}, "site_eui_weather_normalized": {"type": "float"}, "property_name": {"type": "string"}, "building_certification": {"type": "string"}, "state_province": {"type": "string"}, "energy_alerts": {"type": "string"}, "year_built": {"type": "float"}, "release_date": {"type": "date"}, "gross_floor_area": {"type": "float"}, "longitude": {"type": "float"}, "owner_city_state": {"type": "string"}, "owner_telephone": {"type": "string"}, "recent_sale_date": {"type": "date"}, "postal_code": {"type": "string"}, "tax_lot_id": {"index": "not_analyzed", "type": "string", "doc_values": true}, "address_line_2": {"type": "string"}, "energy_score": {"type": "float"}}}}, "dynamic_templates": [{"goals_as_floats": {"path_match": "building_snapshot.extra_data.*_goal", "mapping": {"ignore_malformed": true, "type": "float"}, "match_pattern": "regex", "match_mapping_type": "double|long|integer|float"}}, {"extra_data_as_string": {"path_match": "building_snapshot.extra_data.*", "mapping": {"type": "string"}, "match_pattern": "regex", "match_mapping_type": "date|boolean|double|long|integer|float"}}]}}'
# comment out the following line (mapping building_snapshot) to get aggs on 1.5.1
curl -XPUT localhost:9200/test/_mapping/building_snapshot -d '{"building_snapshot": {"properties": {"lot_number": {"type": "string"}, "owner_address": {"type": "string"}, "owner_postal_code": {"type": "string"}, "block_number": {"type": "string"}, "project_buildings": {"type": "nested"}, "source_eui_weather_normalized": {"type": "float"}, "owner_email": {"type": "string"}, "year_ending": {"type": "date"}, "building_count": {"type": "float"}, "postal_code": {"type": "string"}, "owner": {"type": "string"}, "site_eui": {"type": "float"}, "address_line_1": {"type": "string"}, "occupied_floor_area": {"type": "float"}, "source_eui": {"type": "float"}, "custom_id_1": {"index": "not_analyzed", "type": "string", "doc_values": true}, "city": {"type": "string"}, "property_notes": {"type": "string"}, "district": {"type": "string"}, "location": {"type": "geo_point"}, "latitude": {"type": "float"}, "generation_date": {"type": "date"}, "extra_data": {"dynamic": false, "type": "object", "properties": {"normalized_address": {"index": "not_analyzed", "type": "string", "store": true}}}, "space_alerts": {"type": "string"}, "pm_property_id": {"index": "not_analyzed", "type": "string", "doc_values": true}, "use_description": {"type": "string"}, "conditioned_floor_area": {"type": "float"}, "site_eui_weather_normalized": {"type": "float"}, "property_name": {"type": "string"}, "building_certification": {"type": "string"}, "state_province": {"type": "string"}, "energy_alerts": {"type": "string"}, "year_built": {"type": "float"}, "release_date": {"type": "date"}, "gross_floor_area": {"type": "float"}, "longitude": {"type": "float"}, "owner_city_state": {"type": "string"}, "owner_telephone": {"type": "string"}, "recent_sale_date": {"type": "date"}, "tax_lot_id": {"index": "not_analyzed", "type": "string", "doc_values": true}, "address_line_2": {"type": "string"}, "energy_score": {"type": "float"}}}}'
echo ""

echo ""
echo "check mappings"
echo ""
# comment in for mappings
# curl -XGET localhost:9200/test/_mapping
echo ""

echo ""
echo "create docs"
echo ""
curl -XPUT localhost:9200/test/test/1 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 10, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/2 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 11, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/3 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 12, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/4 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 13, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/5 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 14, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/6 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 8, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/7 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 4, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/8 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 1, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/9 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": 1995, "gross_floor_area": 39636.66217959946}}'
curl -XPUT localhost:9200/test/test/10 -d '{"building_snapshot": {"extra_data": {"test": 13}, "year_built": null, "gross_floor_area": 39636.66217959946}}'
echo ""

echo ""
echo "wait for index to refresh"
curl -XPOST localhost:9200/_refresh
echo ""

echo ""
echo "search with aggs"
echo ""
curl -XGET localhost:9200/test/test/_search?pretty -d '{"query": {"filtered": {"filter": {"range": {"building_snapshot.gross_floor_area": {"gte": 200}}}}}, "aggs": {"gfa": {"stats": {"field": "building_snapshot.year_built"}}}}'




# Output 1.5.1 building_snapshot mapping included
# "aggregations" : {
#   "gfa" : {
#     "count" : 0,
#     "min" : null,
#     "max" : null,
#     "avg" : null,
#     "sum" : null
#   }
# }
#
# Output 1.5.1 building_snapshot mapping excluded
# "aggregations" : {
#   "gfa" : {
#     "count" : 9,
#     "min" : 1.0,
#     "max" : 1995.0,
#     "avg" : 229.77777777777777,
#     "sum" : 2068.0,
#     "min_as_string" : "1.0",
#     "max_as_string" : "1995.0",
#     "avg_as_string" : "229.77777777777777",
#     "sum_as_string" : "2068.0"
#   }
# }
#
# Output 1.4.4 mapping included or excluded
# "aggregations" : {
#   "gfa" : {
#     "count" : 9,
#     "min" : 1.0,
#     "max" : 1995.0,
#     "avg" : 229.77777777777777,
#     "sum" : 2068.0
#   }
# }
```
</description><key id="69995039">10715</key><summary>aggs (stats, value_counts) not working when other doc_types have similar mappings for 1.5.1, works for 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alecklandgraf</reporter><labels /><created>2015-04-22T01:29:07Z</created><updated>2015-04-25T18:30:04Z</updated><resolved>2015-04-25T18:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T18:30:03Z" id="96257681">Hi @alecklandgraf 

Thanks for reporting. Actually, this problem is not new in 1.5, it is just a matter of luck that you didn't hit it before.  Field lookup in the 1.x series is ambiguous.  This issue is a duplicate of https://github.com/elastic/elasticsearch/issues/6108.  For related problems, see the issues linked to on this issue: https://github.com/elastic/elasticsearch/issues/4081

All of this is being fixed in 2.0 by #8870.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>aggs )not working  1.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10714</link><project id="" key="" /><description /><key id="69993557">10714</key><summary>aggs )not working  1.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alecklandgraf</reporter><labels /><created>2015-04-22T01:19:53Z</created><updated>2015-04-22T01:20:00Z</updated><resolved>2015-04-22T01:20:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Configuration API endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10713</link><project id="" key="" /><description>It'd be really useful if there was a single endpoint that could be called that gathers all the config from nodes in the cluster as well as any cluster level config, this includes the local elasticsearch.yml from all nodes.
</description><key id="69991166">10713</key><summary>Configuration API endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Settings</label><label>discuss</label><label>feature</label></labels><created>2015-04-22T01:06:47Z</created><updated>2016-01-13T21:36:44Z</updated><resolved>2016-01-13T21:36:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-07-28T23:57:12Z" id="125784913">This may be related to https://github.com/elastic/elasticsearch/issues/6732
</comment><comment author="eskibars" created="2016-01-13T21:36:43Z" id="171442236">Closing as a duplicate of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Extended mustach language to support direct array/collection access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10712</link><project id="" key="" /><description>Customzied our internal `MustacheFactory` to supports array/collection access syntax in the form of `array_or_collection.X` where `array_or_collection` refers to an array or a collection and `X` is the index into it. For example, referring to the "key" field in the 3rd item in the "array" array can be expressed as `array.3.key`.

Note, although not part of the official mustache specs, `mustache.js` support this and users may expect to have this support.
</description><key id="69990099">10712</key><summary>Extended mustach language to support direct array/collection access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2015-04-22T00:58:45Z</created><updated>2016-03-14T13:07:22Z</updated><resolved>2016-03-08T19:26:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-04-22T13:37:43Z" id="95179804">updated PR
- skipped the search template example as I don't find much value in providing it. It's less about search template support, more about adding the support in the language.
- limited this functionality to arrays, lists and sorted sets - collections where the order is deterministic.
- added javadocs
</comment><comment author="s1monw" created="2015-05-04T08:07:41Z" id="98621702">I think this looks good to me but I'd really like to get rid of the `CollectionMap` and maybe just transform the sorted collections into an array and then use the array one? 
</comment><comment author="s1monw" created="2015-05-29T09:00:29Z" id="106750441">@uboness can you revisit this we wanna get 1.6 out of the door soonish
</comment><comment author="markharwood" created="2015-08-14T10:21:51Z" id="131060159">Nice to have added array support but perhaps missing a feature?
The ["trailing comma" issue](http://stackoverflow.com/questions/6114435/in-mustache-templating-is-there-an-elegant-way-of-expressing-a-comma-seperated-l#comment48696688_6128029) looks to affect a lot of users and requires knowledge of what is the last element in an array. If we could have syntax for declaring the last element in an array users could include a `{{^last}}` type clause around the comma. 
</comment><comment author="clintongormley" created="2016-03-08T19:26:29Z" id="193931722">This PR is way out of date. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Execute tests with $JAVA_HOME.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10711</link><project id="" key="" /><description>Currently, crazy things happen in the build because of $PATH vs $JAVA_HOME.

Example, where some things are done with java 8, others with java 7:

```
rmuir@beast:~/workspace/elasticsearch$ export JAVA_HOME=$JAVA8_HOME
rmuir@beast:~/workspace/elasticsearch$ export PATH=$JAVA7_HOME/bin:$PATH
rmuir@beast:~/workspace/elasticsearch$ mvn clean test

main:
     [echo] Using Java(TM) SE Runtime Environment 1.8.0_45-b14 Oracle Corporation

  2&gt; NOTE: Linux 3.13.0-49-generic amd64/Oracle Corporation 1.7.0_55 (64-bit)/cpus=8,threads=1,free=423438432,total=515375104
```

This creates confusion because there is stuff in the build scripts e.g. configuring permgen based on the jvm running maven and it will do the wrong thing in this case. 

I think we should just always use JAVA_HOME for everything.
</description><key id="69986980">10711</key><summary>Execute tests with $JAVA_HOME.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-22T00:30:55Z</created><updated>2015-04-24T07:31:40Z</updated><resolved>2015-04-22T18:51:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-22T00:42:21Z" id="94984732">LGTM
</comment><comment author="tlrx" created="2015-04-22T07:26:24Z" id="95059497">LGTM
</comment><comment author="s1monw" created="2015-04-22T07:43:23Z" id="95063755">@mrsolo can you take a look at this please I think this affects our builds a lot
</comment><comment author="rmuir" created="2015-04-22T12:57:19Z" id="95164113">@s1monw I fixed the typo, thanks.
</comment><comment author="s1monw" created="2015-04-22T18:46:21Z" id="95298514">@rmuir LGTM I spoke to @mrsolo and we are good to go here lets try master first
</comment><comment author="mrsolo" created="2015-04-22T19:58:23Z" id="95317969">Change looks good https://build-us-00.elastic.co/job/es_core_master_debian/4945/
</comment><comment author="rmuir" created="2015-04-22T22:35:16Z" id="95357285">@s1monw @mrsolo Should I backport this change to any other branches?
</comment><comment author="mrsolo" created="2015-04-22T22:49:29Z" id="95359259">+1 for backport
</comment><comment author="rmuir" created="2015-04-24T01:30:18Z" id="95768036">@mrsolo i backported this to 1.x just now. let me know if we need other branches for jenkins.
</comment><comment author="mrsolo" created="2015-04-24T03:26:46Z" id="95788587">1.x should be sufficient for now, thanks
</comment><comment author="s1monw" created="2015-04-24T07:31:40Z" id="95834593">@rmuir if it cleanly applies to 1.5 I'd push it there too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10711 from rmuir/java_home_only</comment></comments></commit></commits></item><item><title>Phrase Suggester Collate Enhancements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10710</link><project id="" key="" /><description>Enhancements:
- Adds support for `routing` param in `collate` option to control which shards should execute `collate` requests.
- Ensures `collate` option works with concurrent client requests. (fixes #9377)
- doc clarification regarding use of `preference` or `routing` params.

closes #9377
</description><key id="69980924">10710</key><summary>Phrase Suggester Collate Enhancements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T23:50:43Z</created><updated>2015-05-15T18:27:50Z</updated><resolved>2015-05-14T04:51:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix JVM isolation in tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10709</link><project id="" key="" /><description>Currently security manager would allow for one JVM to muck
with the files (read, write, AND delete) of another JVM.

This is unnecessary.
</description><key id="69973261">10709</key><summary>Fix JVM isolation in tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T23:03:42Z</created><updated>2015-04-21T23:14:26Z</updated><resolved>2015-04-21T23:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-21T23:05:49Z" id="94967625">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10709 from rmuir/remove_this_insanity</comment></comments></commit></commits></item><item><title>Add Sequence Numbers to write operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10708</link><project id="" key="" /><description>## Introduction

An Elasticsearch shard can receive indexing, update, and delete commands. Those changes are applied first on the primary shard, maintaining per doc semantics and are then replicated to all the replicas. All these operations happen concurrently. While we maintain ordering on a per doc basis, using [versioning support](http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-versioning) there is no way to order them with respect to each other. Having such a per shard operation ordering will enable us to implement higher level features such as Changes API (follow changes to documents in a shard and index) and Reindexing API (take all data from a shard and reindex it into another, potentially mutating the data). Internally we could use this ordering to speed up shard recoveries, by identifying which specific operations need to be replayed to the recovering replica instead of falling back to a file based sync.

To get such ordering, each operation will be assigned a unique and ever increasing Sequence Number (in short, seq#). This sequence number will be assigned on the primary and replicated to all replicas. Seq# are to be indexed in Lucene to allow sorting, range filtering etc.
## Warning, research ahead

What follows in this ticket is the current thinking about how to best implement this feature. It may change in subtle or major ways as the work continues. Is is important to implement this infrastructure in a way that is correct, resilient to failures, and without slowing down indexing speed.  We feel confident with the approach described below, but we may have to backtrack or change the approach completely. 
## What is a Sequence #

Applying an operation order on a primary is a simple question of incrementing a local counter for every operation. However, this is not sufficient to guarantee global uniqueness and monotonicity under error conditions where the primary shard can be isolated by a network partition. For those, the identity of the current primary needs to be baked into each operation. For example, late to arrive operations from an old primary can be detected and rejected.

In short, each operation is assigned two numbers:
- a `term` - this number is incremented with every primary assignment and is determined by the cluster master. This is very similar to the notion of a `term` in Raft, a `view-number` in Viewstamped Replication or an `epoch` in Zab.
- a  `seq#` - this number is incremented by the primary with each operation it processes.

To achieve ordering, when comparing two operations , `o1` &amp; `o2`, we say that `o1` &lt; `o2` if and only if `s1.seq#` &lt; `s2.seq#` or (`s1.seq#` == `s2.seq#` and `s1.term` &lt; `s2.term`). Equality and greater than are defined in a similar fashion.

For reasons explained later on, we maintain for each shard copy two special seq#:
1. `local checkpoint#` - this is the highest seq# for which all lower seq# have been processed . Note that this is not the highest seq# the shard has processed due to the concurrent indexing, which means that some changes can be processed while previous more heavy ones can still be on going.
2. `global checkpoint#` (or just `checkpoint#`) - the highest seq# for which the local shard can guarantee that _all_ previous (included) seq# have been processed on all active shard copies (i.e., primary and replicas).

Those two numbers will be maintained in memory but also persisted in the metadata of every lucene commit. 
## Changes to indexing flow on primaries

Here is a sketch of the indexing code on primaries. Much of it is identical to the current logic. Changes or additions are marked in **bold** .
1. Validate write consistency based on routing tables.
2. Incoming indexing request is parsed first (rejected upon mapping/parsing failures)
3. Under uid lock:
   1. Versioning is resolved to a fixed version to be indexed.
   2. **Operation is assigned a seq#** and a term
   3. Doc is indexed into Lucene. 
   4. Doc is put into translog. 
4. Replication
   1. **Failures in step 3 above are also replicated (eg due to failure of lucene tokenization)**
   2. Send docs to all assigned replicas.
   3. **Replicas respond with their current `local checkpoint#`.**
   4. When all respond (or have failed), send answer to client.
5. **Checkpoint update**:
   1. Update the global `checkpoint# to the highest seq# for which all active replicas have processed  _all_ lower seq# (inclusive). This is based on information received in 4.3 . 
   2. If changed, send new global `checkpoint#` to replicas (can be folded into a heartbeat/next index req).
## Changes to indexing flow on replicas

As above, this is sketch of the indexing code on replicas. Changes with the current logic are marked as bold.
1. **Validate request**
   1. Seq#'s term is &gt;= locally known primary term.
2. Under uid lock:
   1. Index into Lucene **if seq# &gt; local copy and doesn't represent an error on primary**.
   2. Add to local translog.
3. Respond with the current `local checkpoint#`
## Global Checkpoint# increment on replicas

The primary advances its global `checkpoint#` based on its knowledge of its local and replica's `local checkpoint#`. Periodically it shares its knowledge with the replicas
1. Validate source:
   1. source's primary term is == locally known primary term.
2. Validate correctness:
   1. Check that  all sequence# below the new `global checkpoint#` were processed and local checkpoint# is of the same primary term. If not, fail shard.
3. Set the shard’s copy of `global checkpoint#`, if it's lower than the incoming global checkpoint.


Note that the global checkpoint is a local knowledge of that is update under the mandate of the primary. It may be that the primary information is lagging compared to a replica. This can happen when a replica is promoted to a primary (but still has stale info). 


## First use case - faster replica recovery

Have an ordering of operations allows us to speed up the recovery process of an existing replica and synchronization with the primary. At the moment, we do file based sync which typically results in over-copying of data. Having a clearly marked `checkpoint#` allows us to  limit the sync operation to just those documents that have changed subsequently. In many cases we expect to have no documents to sync at all. This improvement will be tracked in a separate issue. 
## Road map

### Basic infra
- [x] Introduce Primary Terms (#14062)
- [x] Introduce Seq# and index them (#14651)
- [x] Introduce local checkpoints (#15390)
- [x] Introduce global checkpoints (#15485)
- [x] Replicated failed operations for which a seq# was assigned (@Areek) #23314
- [x] Create testing infrastructure to allow testing replication as a unit test, but with real IndexShards (#18930)
- [x] Persist local and global checkpoints (#18949)
- [x] Update to Lucene 6.0  (#20793)
- [x] Persist global checkpoint in translog commits (@jasontedor) #21254
- [x] Reading global checkpoint from translog should be part of translog recovery (@jasontedor) #21934
- [x] transfer global checkpoint after peer recovery (@jasontedor) #22212
- [x] add translog no-op (@jasontedor) #22291
- [ ] Handle retry on primary exceptions in shard bulk action; we currently potentially reuse the answers of the previous primary. This is needed for versioned operations and mapping conflicts (we process ops, the third op needs a mapping change, we reach out to the master, the master already processed that change from another shard and responds with nothing to do, we reparse the op but the node doesn't have the mapping changes yet, we throw a retry and re-run the previous ops as well, potentiall running into conflicts) (@bleskes)
- [x] Don't fail shards as stale if they fail a global check point sync - this will happen all the time when a primary is started and wants to sync the global checkpoint. To achieve this we agreed to change default behavior of shard failures for replication operations (with the exception of write operations, see later). If an operation fails on a replica, the primary shouldn't fail the replica or mark it as stale. Instead it should report this to the user by failing the entire operation. Write Replication Operation (i.e., sub classes of `TransportWriteAction`) should keep the current behavior.  (@dakrone) #22874
- [x] Transfer sequence number "primary context" (i.e., in sync shards, tracked local checkpoints etc) upon primary relocation *AND* don't allow the old primary to operate as primary anymore (see `IndexShard#verifyPrimary`) (@jasontedor, #25122)

### Replica recovery (no rollback)

A best effort doc based replica recovery, based on local last commit. By best effort we refer to having no guarantees on the primary
translog state and the likelihood of doc based recovery to succeed and not requiring a file sync

- [x] Move local checkpoint to max seq# in commit when opening engine #22212
      We currently have no guarantee that *all* ops above the local checkpoint baked into the commit will be replayed. That means that delete operations with a seq# &gt; local checkpoint will not be replayed. To work around it (for now), we will move the local checkpoint artificially (at the *potential* expense of correctness) (@jasontedor)
- [ ] ~~Review correctness of POC and extract requirements for the primary side (@jasontedor)~~ replaced with TLA+ work
- [x] Use seq# checkpoints for replica recovery  (#22484 , @jasontedor)

### Translog seq# based API

Currently translog keeps all operations that are not persisted into the last lucene commit. This doesn't imply that it can serve all operations from a given seq# and up. We want to move seq# based recovery where a lucene commit indicates what seq# a fully baked into it and the translog recovers from there. 

- [x] Add min/max seq# to translog generations. (#22822, @jasontedor)
- [x] Add a smaller maximum generation size and automatically create new generations. Note that the total translog size can still grow to 512MB. However, dividing this in smaller pieces will allow a lucene commit will be able to trim the translog, even though we may need some operations from the non-current generation (see next bullet point). (#23606, @jasontedor)
- [x] Move translog trimming to use lastCommittedLocalCheckpoint and use that for recovering operations when opening a lucene index. BWC aspects (a replica on new node operating without seq#) TBD. (#24015, @jasontedor)
- [ ] Adapt snapshots to dedup sequence numbers by primary term (by reading in reverse)
- [ ] Allow to trim all ops above a certain seq# with a term lower than X

### Primary recovery (no rollback)

- [x] After recovering from the translog, the primary should close any gaps in its history by indexing no ops into the missing seq# (@s1monw, #24238)
- [x] After recovering, a primary should update its knowledge of its own local checkpoint (@ywelsch, #25468)

### Primary promotion

- Implement a clean transition between replica operations on the shard (with an older term) and operations with the new term.
   - [x] Replicas (triggered by indexing operations and live sync) (#24779 @jasontedor)
   - [x] Primary (via a cluster state update) (#24925 @jasontedor)
- [x] Should trigger a translog role over (so operations from the new primary will always have a higher translog generation than any operation they may conflict with) (#24825 @bleskes)
- [x] Should close gaps in history. Note that this doesn't imply this information will be transferred to the replicas. That is the job of the primary/replica sync. (#24945 @jasontedor)

### Live replica/primary sync (no rollback)

- [x] Add a task that streams all operations from the primary's global checkpoint to all shards. (@ywelsch #24858)
    - fails the shard on other failures (double think about shard closing)
    - triggers update of primary term on replica if needed
    - transfers shard's local checkpoint back to the new primary
    - progress reporting via the task manager infrastructure

- [x] When a replica shard increases its primary term under the mandate of a new primary, it should also update its global checkpoint; this gives us the guarantee that its global checkpoint is at least as high as the new primary and gives a starting point for the primary/replica resync (@ywelsch, #25422)
- [x] Replicas should throw their local checkpoint to the global checkpoint when detected a new primary. (@jasontedor, #25452)

### Primary recovery with rollback

Needed to deal with discrepancies between translog and commit point that can result of failure during primary/replica sync

- [ ] Make sure the translog keeps data based on the oldest commit 
- [ ] A custom deletion policy to keep old commits around
- [ ] Roll back before starting to recover from translog (lucene's deletion policy can clean commits before opening)
- [ ] Clean up commits once they are no longer needed. Lucene only calls the deletion policy upon commit. Since we keep older commit around based on the advancement of the global checkpoint, we can trim data earlier, i.e., once the global checkpoint has advanced enough.

### Replica recovery with rollback

Needed to throw away potential wrong doc versions that ended up in lucene. Those "wrong doc versions" may still be in the translog of the replica but since we ignore the translog on replica recovery they will be removed.

- [ ] Roll back before unsafe commits before opening engine (lucene's deletion policy can clean commits before opening)

### Live replica/primary sync with rollback

- [ ] Allow a shard to rollback to a seq# from before last known checkpoint# based on NRT readers
- [ ] Index all operations missing from the rollback point up to the global checkpoint from local translog

### Seq# as versioning

- [x] Change InternalEngine to to resolve collision based on seq# on replicas and recovery (@bleskes, #24060)
- [ ] Change write API to allow specifying desired current seq# for the operation to succeed
- [ ] Make doc level versioning an opt in feature (mostly for external versioning)

### Shrunk indices

Shrunk indices have mixed histories. 
- [x] The primary term of the new index should be higher than the source. (@jasontedor, #25307)
- [x] When recovering the primary shard we need to set it's maximum seqNo to the max of all the source shards. (@jasontedor, #25321)
- [x] (PS - while we're at it we can also transfer the max `max_unsafe_auto_id_timestamp` @jasontedor, #25356)


### Adopt Me
- [x] Properly store seq# in lucene: we expect to use the seq# for sorting, during collision checking and for doing range searches. The primary term will only be used during collision checking when the seq# of the two document copies is identical. Mapping this need to lucene means that the seq# it self should be stored both as a numeric doc value and as numerical indexed field (BKD). The primary term should be stored as a doc value field and doesn't need an indexed variant. We also considered the alternative of encoding both term and seq# into a single numeric/binary field as it may save on a the disk lookup implied by two separate fields. Since we expect the primary term to be rarely retrieved, we opted for the simplicity of the two doc value fields solution. We also expect it to mean better compression. (@dakrone) #21480
- [x] Add primary term to DocWriteResponse (@jasontedor) #24171
- [x] how do we deal with the BWC aspects in the case that - a primary is running on a new node will one replica is on an old node and one replica is on a new one. In that case the primary will maintain seq# and checkpoints for itself and the replica on the new node. However if the primary fails it may be the case that the old replica is elected as primary. That means that the other replica will suddenly stop receiving sequence numbers. It is not clear if this really a problem and if so what the best approach to solve it. (@dakrone, #25277)
- [ ] Introduce new shard states to indicated an ongoing primary sync on promotion. See https://github.com/elastic/elasticsearch/pull/24841#pullrequestreview-45689737
- [ ] Delay primary relocation if primary has not yet synced with replicas . See https://github.com/elastic/elasticsearch/pull/24841#pullrequestreview-45689737


### TBD
- [x] A primary that's allocated when in sync replicas aren't can advance the global checkpoint to a region that's unsafe - the primary doesn't about if it's local ops, which are above the global checkpoint actually exists on the other replicas (@ywelsch #25468).
- [ ] File based recovery (both local and remote) can recreate deleted docs. If a delete is out of order with a previous index operation, we may replay the index operation on recovery, but not the delete. This has to do with the fact that we trim the translog/capture a starting point at an arbitrary generation, and replay all of it. Current solution - change the indexing plan on non-primary origin to never index to lucene below the local checkpoint (5.x is addressed in #25592)
- [ ] How to deal with failures to sync the global check point? this is interesting as we will rely on the checkpoint to be eventually updated on all shard copies.
- [ ] Throwing back local checkpoint to global checkpoint may leave us in a situation we don't have all ops in the translog to do primary/replica sync. This is because we don't have (yet) any guarantee that the translog has all ops above the global checkpoint. That shard will be problematic when promoted to primary (it doesn't have the data to do a sync), causing replica's local checkpoints to get stuck. This will change when we have a custom deletion policy but we may want to double check that and fail the primary if it can't sync it's replica.
- [ ] When indexing stops, sync global checkpoint faster (now we wait on idle shard)
- [ ] Make local check point storage lazy intitialized to protect against memory usage during recovery (TBD)
- [ ] Snapshot and restore *may* create a primary that violates all our checkpointing logic. We should quantify the scenarios this can happen and discuss appropriate solutions. A similar issue occurs with force allocate empty &amp; stale primaries.
- [ ] When indexing on a replica InternalEngine loads the primary term to resolve a potential conflict between two indexing operations to the same doc with the same seq#. Once we have proper rollbacks that should never be needed. Instead we should assert that the term is identical (and potentially that the doc it self is identical). 

### Completed Miscellaneous
- [x] Review feasibility of old indices (done and implemented in #22185 ) (@bleskes)
- [x] Remove Shadow replicas (@dakrone #23906)
- [ ] ~~If minimum of all local checkpoints is less than global checkpoint on the primary, do we fail the shard?~~ No, this can happen when replicas pull back their local checkpoints to *their* version of the global checkpoint
- [ ] ~~Failed shards who's local checkpoint is lagging with more than 10000 (?) ops behind the primary . This is a temporary measure to allow merging into master without closing translog gaps during primary promotion on a live shard. Those will require the replicas to pick them up, which will take a replica/primary live sync~~
</description><key id="69958694">10708</key><summary>Add Sequence Numbers to write operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label><label>feature</label><label>Meta</label><label>resiliency</label></labels><created>2015-04-21T21:37:04Z</created><updated>2017-07-11T06:36:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shikhar" created="2015-05-05T17:53:50Z" id="99158111">&gt; First use case - faster replica recovery

I'd argue the first use case is making replication semantics more sound :)
</comment><comment author="rkonda" created="2016-03-08T06:53:13Z" id="193632541">It's not clear as to what would happen in the following split brain scenario (scenario-1):
1) split occurs, forming two networks
2) the network that didn't have a master, elects a master (call this network-2)
3) the master will elect a new primary (in network-2)
4) the primary in network-2 now has incremented `term` value (say 11). The primary in network-1 continues to have the same `term` value (10 in this example)
5) The connection between the networks is re-established.

In this case we need a strategy for reconciling the differences in the indexes, if there were change operations in both the networks. Does a strategy like that exist today? So far it seems like this situation is preventable by using min_master_nodes. However in case min_master_nodes is not set appropriately, some default strategy should come into effect I would think.

An example strategy could be:
1) Keep logs of write operations in both networks for a configurable amount of time. If the networks' connectivity is restored within this time period: (a) Drop all nodes in network-2 to read-only replica status (b) Attempt to reconcile the differences, and use network-1's state if the differences are not reconcilable. (c) Remove read-only status
If the connectivity isn't restored within that time, when connection is restored, all indices in network-2 that have competing primaries in network-1 will lose their shards, and replicas are created from network-1.

Another interesting situation (scenario-2) to consider:
1) Continuing with the scenario described above until (4) ...
2) network-1 has another split, creating network-1 and network-1a. Network-1a gives `term` value of 11 to the new primary in that network.
4) network-1 completely fails, and connectivity between network-1a and network-2 are restored. Now we may have a scenario where the subsequent change operations might not fail but still lead to different indexes in the replicas, with some operations failing some of the time, creating a messy situation.

This would happen if there is no reconciliation strategy in effect.

I do see that the sequence numbering method will keep shards that have connectivity to both the networks, in integral state, in the case of scenario-1. In the case of scenario-2, it is possible that the same shard gets operations with same `term` values from multiple primaries, and that again could create faulty index in that replica.

I am still trying to understand Elasticsearch's cluster behavior. It's possible that I might have made assumptions that aren't correct.
</comment><comment author="bleskes" created="2016-03-08T08:05:31Z" id="193654802">&gt; In this case we need a strategy for reconciling the differences in the indexes, if there were change operations in both the networks. Does a strategy like that exist today? 

The current strategy, which seq# will keep enforcing but in easier/faster way, is that all replicas are "reset" to be an exact copy of the primary currently chosen by the master. As you noted, this falls apart when there are two residing masters in the cluster. Indeed, the only way to prevent this is by setting minimum master nodes - which is the number one most important setting to set in ES (tell it what the expected cluster size is)

If min master nodes is not set and a split brain occurs, resolution will come when one of the masters steps down (either by manual intervention or by detecting the other one). In that case all replicas will "reset" to the primary designated by the left over master.

&gt;   Drop all nodes in network-2 to read-only replica status 

This is similar to what ES does - nodes with no master will only serve read requests and block writes (by default, it can be configured to block reads).

&gt;  it is possible that the same shard gets operations with same term values from multiple primaries, and that again could create faulty index in that replica.

If the term is the same from both primaries, the replica will accept them according to the current plan. The situation will be resolved when the network restores and the left over primary and replica sync but indeed there are potential troubles there. I have some ideas on how to fix this specific secondary failure (split brain is the true issue, after which all bets are off) but there are bigger fish to catch first :)
</comment><comment author="rkonda" created="2016-03-08T08:50:05Z" id="193667204">Thank you very much for your clarification. I rather enjoy all these discussions and your comments.

&gt; The current strategy, which seq# will keep enforcing but in easier/faster way, is that all replicas are "reset" to be an exact copy of the primary currently chosen by the master.

## 

&gt; The situation will be resolved when the network restores and the left over primary and replica sync

I would like to clearly understand the reset/sync scenarios. What triggers reset/sync?

I can think of a couple of "normal" operation scenarios
1) I would think that whenever a node joins a network, the master would initiate a sync/reset. 
2) If a replica fails for a request, I suppose the primary should keep attempting a sync/reset, otherwise the replica might keep diverging, and at some point the master has to decommission that replica, otherwise the reads would be inconsistent.

In the case of split brain, with multi-network replicas (assuming min master nodes is set), primary-1 has been assuming that this replica R (on this third node, say N-3) has been failing (because of its allegiance to primary-2 ) but still is in the network. Hence it would attempt sync/reset. How does this protocol work? Should master-1 attempt to decommission R at some point, going by assumption (2)?

This problem will occur in a loop if R is decommissioned but another replica is installed on N-3 in its place, by the same protocol. There will be contention on N-3 for "reset"-ing replica shards by both the masters.

I suppose one way to resolve this is by letting a node choose a master if there are multiple masters. If we did this, then whenever a node loses its master, it would choose the other master, and there will be a sync/reset and all is well.

However if the node chooses its master, the other master will lose quorum, and hence cease to exist, which is a good resolution for this issue in my opinion.
</comment><comment author="bleskes" created="2016-03-08T09:19:28Z" id="193683857">The two issues you mention indeed trigger a primary/replica sync. I'm not sure I follow the rest, I would like to ask you to continue the discussion on discuss.elastic.co . We try to keep github for issues and work items. Thx!
</comment><comment author="rkonda" created="2016-03-08T09:40:47Z" id="193689439">Sure. Posted it here: https://discuss.elastic.co/t/sequence-numbers-to-write-ops-split-brain-scenario/43748
</comment><comment author="makeyang" created="2016-04-05T08:19:52Z" id="205716717">any plan to release this?
it seems after this release, u guys will make ES a AP system? will u provide config paramters to allow users to control ES to be a AP or CP system eventually?
</comment><comment author="bleskes" created="2016-04-05T09:09:35Z" id="205721586">@makeyang  this will be released as soon as it is done. There's still a lot of work to do.

&gt; it seems after this release, u guys will make ES a AP system? will u provide config paramters to allow users to control ES to be a AP or CP system eventually?

ES is currently and will stay CP in the foreseeable future. If a node is partitioned away from the cluster it will serve read requests (configurable) but will block writes, in which case we drop availability. Of course in future there are many options but currently there are no concrete plans to make it any different.
</comment><comment author="milutz" created="2017-03-13T19:27:56Z" id="286216805">In the "Consistency and Replication in Elasticsearch" talk at Elastic{on} Mar 8, 2017 this issue was brought up as possibly providing groundwork for a change API.

@bleskes, @ywelsch, @jasontedor (pardon, I can't remember which of you brought this up): Was that in reference to issue #1242 or was that about a different form/meaning of change API ?

Many thanks for you talk!</comment><comment author="jasontedor" created="2017-03-13T19:32:27Z" id="286218014">@milutz Thank you for attending and your interest!

Yes, the open issue for the changes API is #1242. That issue gives a high-level overview of some possible goals for the changes API but the actual design is yet to be worked out. Sequence numbers will form a basis for what we will eventually build.</comment><comment author="milutz" created="2017-03-13T19:55:25Z" id="286224268">@jasontedor: Awesome!  I'm very interested in both of these efforts and wanted to confirm what I should subscribing-to-notifications for.  Many thanks, and again many thanks to all of you for efforts and your awesome talk - it filled in many questions I've had about the platform!</comment><comment author="andrewluetgers" created="2017-05-03T19:25:51Z" id="299010525">Very curious on this issue and #1242 are these high priority issues for elastic and is there any sense of a timeline?</comment><comment author="jasontedor" created="2017-05-03T22:20:56Z" id="299052084">@andrewluetgers We do not provide timelines. I can tell you this:
 - sequence IDs and some of the features they enable will ship in 6.0.0, it is one of our highest priorities for that release
 - the changes API is to be determined, it will definitely not ship with 6.0.0</comment><comment author="makeyang" created="2017-05-04T02:02:42Z" id="299081824">will Term be introduced for master election for ES ?</comment><comment author="jasontedor" created="2017-05-04T02:08:09Z" id="299082307">&gt; will Term be introduced for master election for ES ?

Yes.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/test/java/org/elasticsearch/index/IndexServiceTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalSettingsPlugin.java</file></files><comments><comment>Add a scheduled translog retention check (#25622)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file></files><comments><comment>Promote replica on the highest version node (#25277)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportWriteActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Update global checkpoint when increasing primary term on replica (#25422)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/PrimaryReplicaSyncer.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogDeletionPolicy.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/LocalCheckpointTrackerTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/PrimaryReplicaSyncerTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogDeletionPolicyTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Enable a long translog retention policy by default (#25294)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/translog/BaseTranslogReader.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogDeletionPolicy.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/test/java/org/elasticsearch/index/engine/CombinedDeletionPolicyTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogDeletionPolicyTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file></files><comments><comment>Introduce translog size and age based retention policies (#25147)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/LocalShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file></files><comments><comment>Move the IndexDeletionPolicy to be engine internal (#24930)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/TransportShardBulkActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java</file><file>test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java</file></files><comments><comment>Guarantee that translog generations are seqNo conflict free (#24825)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsRejectedExecutionException.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointService.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SeqNoStats.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SequenceNumbersService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/LocalCheckpointServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Introduce Local checkpoints</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file></files><comments><comment>Set an newly created IndexShard's ShardRouting before exposing it to operations</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/DocWriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SeqNoFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SequenceNumbersService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IllegalIndexShardStateException.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/transport/BaseTransportResponseHandler.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/document/ShardInfoIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/indexing/IndexingSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java</file><file>core/src/test/java/org/elasticsearch/routing/SimpleRoutingIT.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java</file></files><comments><comment>Add Sequence Numbers and enforce Primary Terms</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardStateIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PriorityComparatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushUnitTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file></files><comments><comment>Introduce Primary Terms</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SourceToParse.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java</file><file>test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java</file></files><comments><comment>Fill missing sequence IDs up to max sequence ID when recovering from store (#24238)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/uid/PerThreadIDVersionAndSeqNoLookup.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/uid/VersionsAndSeqNoResolver.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/uid/VersionsResolver.java</file><file>core/src/main/java/org/elasticsearch/index/engine/DeleteVersionValue.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/LiveVersionMap.java</file><file>core/src/main/java/org/elasticsearch/index/engine/VersionValue.java</file><file>core/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SeqNoFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/TermVectorsService.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/LiveVersionMapTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/VersionValueTests.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file></files><comments><comment>Use sequence numbers to identify out of order delivery in replicas &amp; recovery (#24060)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/RestRefreshAction.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ReplicationOperationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportWriteActionTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file></files><comments><comment>Change certain replica failures not to fail the replica shard</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointTracker.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SeqNoStats.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SequenceNumbersService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/PeerRecoverySourceService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/EvilPeerRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java</file><file>test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java</file></files><comments><comment>Seq Number based recovery should validate last lucene commit max seq# (#22851)</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SeqNoFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SeqNoFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/GeoFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldNamesFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorFieldMapperTests.java</file></files><comments><comment>Add internal _primary_term doc values field, fix _seq_no indexing</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionListenerResponseHandler.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexModule.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/GlobalCheckpointService.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/GlobalCheckpointSyncAction.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointService.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SeqNoStats.java</file><file>core/src/main/java/org/elasticsearch/index/seqno/SequenceNumbersService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetHandler.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTargetService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ReplicationOperationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexModuleTests.java</file><file>core/src/test/java/org/elasticsearch/index/IndexServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/GlobalCheckpointTests.java</file><file>core/src/test/java/org/elasticsearch/index/seqno/LocalCheckpointServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Introduced Global checkpoints for Sequence Numbers (#15485)</comment></comments></commit></commits></item><item><title>Config parsing problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10707</link><project id="" key="" /><description>A customer had a problem starting some nodes in their ES cluster.  The problem was traced to a problem in a puppet template used for creating the elasticsearch.yml file.  It looks like the cluster.routing.allocation.awareness.attributes has trailing whitespace after the 'zone' value that is tripping up the parser.  This was the only significant difference between the broken template and one that was working.

```
##################### Elasticsearch Configuration Example #####################

# This file contains an overview of various configuration settings,
# targeted at operations staff. Application developers should
# consult the guide at &lt;http://elasticsearch.org/guide&gt;.
#
# The installation procedure is covered at
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html&gt;.
#
# Elasticsearch comes with reasonable defaults for most settings,
# so you can try it out without bothering with configuration.
#
# Most of the time, these defaults are just fine for running a production
# cluster. If you're fine-tuning your cluster, or wondering about the
# effect of certain configuration option, please _do ask_ on the
# mailing list or IRC channel [http://elasticsearch.org/community].

# Any element in the configuration can be replaced with environment variables
# by placing them in ${...} notation. For example:
#
#node.rack: ${RACK_ENV_VAR}

# For information on supported formats and syntax for the config file, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html&gt;


################################### Cluster ###################################

# Cluster name identifies your cluster for auto-discovery. If you're running
# multiple clusters on the same network, make sure you're using unique names.
#
cluster.name: silver.elasticsearch


#################################### Node #####################################

# Node names are generated dynamically on startup, so you're relieved
# from configuring them manually. You can tie this node to a specific name:
#
#node.name: "Franz Kafka"
node.name: ES_Silver_${HOSTNAME}_N2

# Every node can be configured to allow or deny being eligible as the master,
# and to allow or deny to store the data.
#
# Allow this node to be eligible as a master node (enabled by default):
#
#node.master: true
#
# Allow this node to store data (enabled by default):
#
#node.data: true

# You can exploit these settings to design advanced cluster topologies.
#
#1. You want this node to never become a master node, only to hold data.
#    This will be the "workhorse" of your cluster.
#
node.master: false
node.data: true
#
#2. You want this node to only serve as a master: to not store any data and
#    to have free resources. This will be the "coordinator" of your cluster.
#
#node.master: true
#node.data: false
#
#3. You want this node to be neither master nor data node, but
#    to act as a "search load balancer" (fetching data from nodes,
#    aggregating results, etc.)
#
#node.master: false
#node.data: false

# Use the Cluster Health API [http://localhost:9200/_cluster/health], the
# Node Info API [http://localhost:9200/_nodes] or GUI tools
# such as &lt;http://www.elasticsearch.org/overview/marvel/&gt;,
# &lt;http://github.com/karmi/elasticsearch-paramedic&gt;,
# &lt;http://github.com/lukas-vlcek/bigdesk&gt; and
# &lt;http://mobz.github.com/elasticsearch-head&gt; to inspect the cluster state.

# A node can have generic attributes associated with it, which can later be used
# for customized shard allocation filtering, or allocation awareness. An attribute
# is a simple key value pair, similar to node.key: value, here is an example:
#
#node.rack: rack314

# By default, multiple nodes are allowed to start from the same installation location
# to disable it, set the following:
#node.max_local_storage_nodes: 1


########################### Forced Awarenes ###################################                                                                                                            
# Sometimes, we know in advance the number of values an awareness attribute                                                                                                                
# can have, and more over, we would like never to have more replicas then                                                                                                                  
# needed allocated on a specific group of nodes with the same awareness                                                                                                                    
# attribute value. For that, we can force awareness on specific attributes.                                                                                                                
# For example, lets say we have an awareness attribute called zone, and we know                                                                                                            
# we are going to have two zones, zone1 and zone2. Here is how we can force                                                                                                                
# awareness on a node                                                                                                                                                                      
cluster.routing.allocation.awareness.force.zone.values: lppbd3121.gso.domain.com,lppbd3122.gso.domain.com,lppbd30d1.gso.domain.com,lppbd30d2.gso.domain.com
cluster.routing.allocation.awareness.attributes: zone                                                                                                                                      
# Now, lets say we start 2 nodes with node.zone set to zone1 and create an                                                                                                                 
# index with 5 shards and 1 replica. The index will be created, but only 5                                                                                                                 
# shards will be allocated (with no replicas). Only when we start more shards                                                                                                              
# with node.zone set to zone2 will the replicas be allocated.                                                                                                                              
node.zone: ${HOSTNAME}

#################################### Index ####################################

# You can set a number of options (such as shard/replica options, mapping
# or analyzer definitions, translog settings, ...) for indices globally,
# in this file.
#
# Note, that it makes more sense to configure index settings specifically for
# a certain index, either when creating it or by using the index templates API.
#
# See &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html&gt; and
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html&gt;
# for more information.

# Set the number of shards (splits) of an index (5 by default):
#
#index.number_of_shards: 5

# Set the number of replicas (additional copies) of an index (1 by default):
#
#index.number_of_replicas: 1

# Note, that for development on a local machine, with small indices, it usually
# makes sense to "disable" the distributed features:
#
#index.number_of_shards: 1
#index.number_of_replicas: 0

# These settings directly affect the performance of index and search operations
# in your cluster. Assuming you have enough machines to hold shards and
# replicas, the rule of thumb is:
#
#1. Having more *shards* enhances the _indexing_ performance and allows to
#    _distribute_ a big index across machines.
#2. Having more *replicas* enhances the _search_ performance and improves the
#    cluster _availability_.
#
# The "number_of_shards" is a one-time setting for an index.
#
# The "number_of_replicas" can be increased or decreased anytime,
# by using the Index Update Settings API.
#
# Elasticsearch takes care about load balancing, relocating, gathering the
# results from nodes, etc. Experiment with different settings to fine-tune
# your setup.

# Use the Index Status API (&lt;http://localhost:9200/A/_status&gt;) to inspect
# the index status.

indices.fielddata.cache.size:  40%
indices.breaker.fielddata.limit: 50%
indices.breaker.request.limit: 35%
indices.breaker.total.limit: 60%

#################################### Paths ####################################

# Path to directory containing configuration (this file and logging.yml):
#
#path.conf: /path/to/conf
path.conf: /opt/elasticsearch/conf
# Path to directory where to store index data allocated for this node.
#
#path.data: /path/to/data
#
# Can optionally include more than one location, causing data to be striped across
# the locations (a la RAID 0) on a file level, favouring locations with most free
# space on creation. For example:
#
#path.data: /path/to/data1,/path/to/data2
path.data: /data/elasticsearch
# Path to temporary files:
#
#path.work: /path/to/work

# Path to log files:
#
#path.logs: /path/to/logs
path.logs: /data/elasticsearch/logs/N2/
# Path to where plugins are installed:
#
#path.plugins: /path/to/plugins
path.plugins: /opt/elasticsearch/plugins

#################################### Plugin ###################################

# If a plugin listed here is not installed for current node, the node will not start.
#
#plugin.mandatory: mapper-attachments,lang-groovy


################################### Memory ####################################

# Elasticsearch performs poorly when JVM starts swapping: you should ensure that
# it _never_ swaps.
#
# Set this property to true to lock the memory:
#
bootstrap.mlockall: true

# Make sure that the ES_MIN_MEM and ES_MAX_MEM environment variables are set
# to the same value, and that the machine has enough memory to allocate
# for Elasticsearch, leaving enough memory for the operating system itself.
#
# You should also make sure that the Elasticsearch process is allowed to lock
# the memory, eg. by using `ulimit -l unlimited`.


############################## Network And HTTP ###############################

# Elasticsearch, by default, binds itself to the 0.0.0.0 address, and listens
# on port [9200-9300] for HTTP traffic and on port [9300-9400] for node-to-node
# communication. (the range means that if the port is busy, it will automatically
# try the next port).

# Set the bind address specifically (IPv4 or IPv6):
#
#network.bind_host: 192.168.0.1

# Set the address other nodes will use to communicate with this node. If not
# set, it is automatically derived. It must point to an actual IP address.
#
#network.publish_host: 192.168.0.1

# Set both 'bind_host' and 'publish_host':
#
#network.host: 192.168.0.1

# Set a custom port for the node to node communication (9300 by default):
#
#transport.tcp.port: 9311

# Enable compression for all communication between nodes (disabled by default):
#
#transport.tcp.compress: true

# Set a custom port to listen for HTTP traffic:
#
#http.port: 9200

# Set a custom allowed content length:
#
#http.max_content_length: 100mb

# Disable HTTP completely:
#
#http.enabled: false

################################### Marvel  ###################################
# A list of hosts in hostname:port format to which statistics and events will
# be sent. Data will be sent to the first host, but will failover to the next
# host(s) if the first is not reachable. Defaults to ["localhost:9200"].

# [1.0.2] Added in 1.0.2. - HTTP Basic authentication credentials can be
# specified as part of the host name, i.e., ["user:pwd@host:9200"]
#marvel.agent.exporter.es.hosts: ["lppbd3121.gso.domain.com:9200","lppbd3122.gso.domain.com:9200","lppbd30d1.gso.domain.com:9200","lppbd30d2.gso.domain.com:9200"]
marvel.agent.enabled: false

################################### Gateway ###################################

# The gateway allows for persisting the cluster state between full cluster
# restarts. Every change to the state (such as adding an index) will be stored
# in the gateway, and when the cluster starts up for the first time,
# it will read its state from the gateway.

# There are several types of gateway implementations. For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html&gt;.

# The default gateway type is the "local" gateway (recommended):
#
#gateway.type: local

# Settings below control how and when to start the initial recovery process on
# a full cluster restart (to reuse as much local data as possible when using shared
# gateway).

# Allow recovery process after N nodes in a cluster are up:
#
gateway.recover_after_nodes: 8

# Set the timeout to initiate the recovery process, once the N nodes
# from previous setting are up (accepts time value):
#
gateway.recover_after_time: 5m

# Set how many nodes are expected in this cluster. Once these N nodes
# are up (and recover_after_nodes is met), begin recovery process immediately
# (without waiting for recover_after_time to expire):
#
gateway.expected_nodes: 11


############################# Recovery Throttling #############################

# These settings allow to control the process of shards allocation between
# nodes during initial recovery, replica allocation, rebalancing,
# or when adding and removing nodes.

# Set the number of concurrent recoveries happening on a node:
#
#1. During the initial recovery
#
#cluster.routing.allocation.node_initial_primaries_recoveries: 4
#
#2. During adding/removing nodes, rebalancing, etc
#
#cluster.routing.allocation.node_concurrent_recoveries: 2

# Set to throttle throughput when recovering (eg. 100mb, by default 20mb):
#
#indices.recovery.max_bytes_per_sec: 20mb

# Set to limit the number of open concurrent streams when
# recovering a shard from a peer:
#
#indices.recovery.concurrent_streams: 5


################################## Discovery ##################################

# Discovery infrastructure ensures nodes can be found within a cluster
# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered
# operational within the cluster. This should be set to a quorum/majority of
# the master-eligible nodes in the cluster.
#
discovery.zen.minimum_master_nodes: 2

# Set the time to wait for ping responses from other nodes when discovering.
# Set this option to a higher value on a slow or congested network
# to minimize discovery failures:
#
discovery.zen.ping.timeout: 30s

# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html&gt;

# Unicast discovery allows to explicitly control which nodes will be used
# to discover the cluster. It can be used when multicast is not present,
# or to restrict the cluster communication-wise.
#
#1. Disable multicast discovery (enabled by default):
#
discovery.zen.ping.multicast.enabled: false
#
#2. Configure an initial list of master nodes in the cluster
#    to perform discovery when new nodes (master or data) are started:
#
discovery.zen.ping.unicast.hosts: ["lppbd3121.gso.domain.com:9302","lppbd3122.gso.domain.com:9302","lppb30d1.gso.domain.com:9302"]

# EC2 discovery allows to use AWS EC2 API in order to perform discovery.
#
# You have to install the cloud-aws plugin for enabling the EC2 discovery.
#
# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-ec2.html&gt;
#
# See &lt;http://elasticsearch.org/tutorials/elasticsearch-on-ec2/&gt;
# for a step-by-step tutorial.

# GCE discovery allows to use Google Compute Engine API in order to perform discovery.
#
# You have to install the cloud-gce plugin for enabling the GCE discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-gce&gt;.

# Azure discovery allows to use Azure API in order to perform discovery.
#
# You have to install the cloud-azure plugin for enabling the Azure discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-azure&gt;.

# rejects read and write requests when node has lost it's master this is added to continue the
# behavior that was present in versions prior to 1.4.0. Default is write (potential partial/stale results)
discovery.zen.no_master_block: all

################################## Slow Log ##################################

# Shard level query and fetch threshold logging.

#index.search.slowlog.threshold.query.warn: 10s
#index.search.slowlog.threshold.query.info: 5s
#index.search.slowlog.threshold.query.debug: 2s
#index.search.slowlog.threshold.query.trace: 500ms

#index.search.slowlog.threshold.fetch.warn: 1s
#index.search.slowlog.threshold.fetch.info: 800ms
#index.search.slowlog.threshold.fetch.debug: 500ms
#index.search.slowlog.threshold.fetch.trace: 200ms

#index.indexing.slowlog.threshold.index.warn: 10s
#index.indexing.slowlog.threshold.index.info: 5s
#index.indexing.slowlog.threshold.index.debug: 2s
#index.indexing.slowlog.threshold.index.trace: 500ms

################################## GC Logging ################################

#monitor.jvm.gc.young.warn: 1000ms
#monitor.jvm.gc.young.info: 700ms
#monitor.jvm.gc.young.debug: 400ms

#monitor.jvm.gc.old.warn: 10s
#monitor.jvm.gc.old.info: 5s
#monitor.jvm.gc.old.debug: 2s

################################## Security ################################

# Uncomment if you want to enable JSONP as a valid return transport on the
# http server. With this enabled, it may pose a security risk, so disabling
# it unless you need it is recommended (it is disabled by default).
#
#http.jsonp.enable: true

################################# security fixes ###########################
# the following are required security fixes for 1.4.2
# for CVE-2015-1427 -- this is disabled by default in 1.4.3
script.groovy.sandbox.enabled: false


################################ multinode optimization ###############################
# processor optimization - when running multiple nodes on a physical host
# you must limit the number of processors that the node can take as they will take all
processors: 16
```
</description><key id="69958029">10707</key><summary>Config parsing problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2015-04-21T21:33:25Z</created><updated>2015-08-26T19:43:00Z</updated><resolved>2015-08-26T19:43:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T18:05:45Z" id="96248220">Hi @seang-es 

Could you provide some details about what kind of problems were seen? ie some way to replicate the issue?  I've tried adding `cluster.routing.allocation.awareness.attributes: zone` to the config file, but `GET _nodes` shows:

```
        "cluster": {
           "routing": {
              "allocation": {
                 "awareness": {
                    "attributes": "zone"
                 }
              }
           },
```

... which looks correct
</comment><comment author="clintongormley" created="2015-07-22T09:43:16Z" id="123644353">@seang-es any more info here, or can we close?
</comment><comment author="jpountz" created="2015-08-26T19:42:59Z" id="135149774">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Publish cluster nodes via mDNS to be able to build a unicast seed list for new host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10706</link><project id="" key="" /><description>It would be nice if mDNS could be used to discover an initial list of nodes for an ES cluster.  That list could then be used to set up a new node for unicast with a sensible initial list of nodes, without needing to have a static list known in advance.
</description><key id="69954958">10706</key><summary>Publish cluster nodes via mDNS to be able to build a unicast seed list for new host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bguezzie</reporter><labels><label>discuss</label></labels><created>2015-04-21T21:19:47Z</created><updated>2016-01-17T17:46:33Z</updated><resolved>2016-01-17T17:46:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:46:33Z" id="172357955">This ticket hasn't garnered any support in the last 9 months, and I think it is unlikely that we would add something like this to core.  This would probably be doable as a plugin if anybody has the TUITs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplified mapper lookups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10705</link><project id="" key="" /><description>We no longer support overriding field index names, but the lookup
data structures still optimize for this use case. This complicates
the work for #8871.  Instead, we can use a lookup structure
by making the legacy case slower.

This change simplifies the field mappers lookup to only
store a single map, keyed by the field's full name. It also
changes a lot of tests to decrease the uses of the older api
(looking up by index name where the index name is different
than the field name).
</description><key id="69938976">10705</key><summary>Simplified mapper lookups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T20:12:18Z</created><updated>2015-06-08T09:02:08Z</updated><resolved>2015-04-21T22:01:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-21T21:48:29Z" id="94953275">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/camelcase/CamelCaseFieldNameTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/completion/CompletionFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/path/PathMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java</file></files><comments><comment>Mappings: Simplified mapper lookups</comment></comments></commit></commits></item><item><title>Elasticsearch concurrent search requests block each other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10704</link><project id="" key="" /><description>I have a cluster of 5 nodes running ES 1.4.4 on Linux. My application sends search requests using the java transportClient. I've noticed a very unusual behavior:
If multiple requests (threads) come in, Elasticsearch won't return any of the thread's response until it's finished processing all of them and then sends all the responses at once.
I initially thought this might be because my transportClient was singleton, but the problem persisted when I made it create a new client per request. 
</description><key id="69935727">10704</key><summary>Elasticsearch concurrent search requests block each other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheGreatYamcha</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2015-04-21T19:55:30Z</created><updated>2015-06-04T12:24:48Z</updated><resolved>2015-06-04T12:24:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nilsga" created="2015-04-23T10:26:07Z" id="95526894">Do you have any thread dumps for when this happens? We seem to have some blocking issues with the TransportClient as well, and I'm curious if it could be the same thing we're experiencing. So comparing the thread dumps would be helpful.

Also, is this something you experienced after upgrading from some other version? If yes, which version were you running previosly?
</comment><comment author="clintongormley" created="2015-04-26T14:20:33Z" id="96390979">@TheGreatYamcha could you upload some code demonstrating the problem?
</comment><comment author="TheGreatYamcha" created="2015-04-27T15:25:12Z" id="96706827">I managed to fix the problem. This was the original code:

``` java
OrFilterBuilder fb = FilterBuilders.orFilter();

for (String key : indexValueMap.keySet()) {
fb.add(FilterBuilders.indicesFilter(
  FilterBuilders.termFilter("hashedValue", indexValueMap.get(key)), key).noMatchFilter("none"));
}
FilteredQueryBuilder qb = QueryBuilders.filteredQuery(null, fb);
SearchResponse sr = client.prepareSearch().setPostFilter(fb).execute().actionGet();
```

and then I simply changed the last line to:

``` java
SearchResponse sr = client.prepareSearch().setQuery(qb).execute().actionGet();
```

and everything worked i.e. I wasn't able to reproduce the problem.
Although the second code was significantly faster even on a single thread, so it might be that with this, there wasn't enough threads to replicate to problem.
</comment><comment author="clintongormley" created="2015-06-04T12:24:47Z" id="108871478">Closed by https://github.com/elastic/elasticsearch/pull/10940
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `fairness` option to KeyedLock.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10703</link><project id="" key="" /><description>This change adds the ability of a `KeyedLock` to be `fair` this means that threads will aquire the lock in the order they ask for it. 
Also add a test to ensure the fairness parameter is obeyed.
</description><key id="69912683">10703</key><summary>Add `fairness` option to KeyedLock.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T18:01:51Z</created><updated>2015-05-29T15:55:47Z</updated><resolved>2015-04-28T17:48:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-21T18:19:21Z" id="94894686">it looks ok to me but may I ask what triggered this change?
</comment><comment author="GaelTadh" created="2015-04-23T18:58:23Z" id="95685853">I have a use case for guaranteed order of execution.I'm seeing this test intermittently fail on my build so I'm investigating.
</comment><comment author="s1monw" created="2015-04-23T19:00:01Z" id="95686213">ok I see - please remove this test though we are not testing JDK functionality - the randomization is fine
</comment><comment author="GaelTadh" created="2015-04-23T19:39:17Z" id="95695410">Can I merge this to 1.x  and 1.5?
</comment><comment author="s1monw" created="2015-04-24T19:30:07Z" id="96042408">please squash and push to all relevant branches. 1.5 is bugfix only so please only to 1.x and master
</comment><comment author="s1monw" created="2015-04-28T09:25:25Z" id="96986926">@GaelTadh ping
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Add shortcut "all" to skip version ranges in rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10702</link><project id="" key="" /><description>This was suggested on #10656 as cleaner than " - " to indicate all
versions should be skipped.
</description><key id="69900266">10702</key><summary>Tests: Add shortcut "all" to skip version ranges in rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T17:05:36Z</created><updated>2015-04-22T18:41:17Z</updated><resolved>2015-04-22T18:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-21T17:06:09Z" id="94873417">@karmi I think this is what you wanted?
</comment><comment author="karmi" created="2015-04-21T18:02:19Z" id="94890580">@rjernst Yes, thanks, I think this better communicates the intent!
</comment><comment author="javanna" created="2015-04-22T07:09:52Z" id="95055097">LGTM thanks for doing this @rjernst 
</comment><comment author="s1monw" created="2015-04-22T18:37:16Z" id="95295518">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/section/SkipSection.java</file><file>src/test/java/org/elasticsearch/test/rest/test/SkipSectionParserTests.java</file></files><comments><comment>Tests: Add shortcut "all" to skip version ranges in rest tests</comment></comments></commit></commits></item><item><title>Validate number_of_shards/_replicas without index setting prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10701</link><project id="" key="" /><description>Settings: validate number_of_shards/number_of_replicas without index setting prefix

normalize settings before validating

Closes #10693

Now, this PR check CreateIndexRequest only. I think we should check other API related number_of_shards/number_of_replicas.
</description><key id="69870286">10701</key><summary>Validate number_of_shards/_replicas without index setting prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T15:00:44Z</created><updated>2015-06-08T00:30:28Z</updated><resolved>2015-04-27T06:41:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-21T20:31:38Z" id="94930984">@imotov can you review this please?
</comment><comment author="imotov" created="2015-04-21T23:02:12Z" id="94967104">The change itself LGTM. However, I tried changing the number of replicas to a negative number on restore and found very similar issue. So, I am wondering if it would make sense to move this validation logic into MetaDataCreateIndexService so it would be possible to reuse these checks during index restore as well. If this is out of scope for this PR, I can open a separate PR once this is merged in. What do you think?
</comment><comment author="johtani" created="2015-04-21T23:56:34Z" id="94977261">@imotov thx for reviewing. I change PR
</comment><comment author="johtani" created="2015-04-22T08:14:52Z" id="95069843">@imotov I move validation logic into MetaDataCreateIndexService and add test to SharedClusterSnapshotRestoreTests.changeSettingsOnRestoreTest().

One question about Validation.
In ActionRequest, we have the validation method and this method can handle multiple error messages.
If we support multiple error messages in Service class, it is helpful. What do you think?
</comment><comment author="johtani" created="2015-04-23T14:12:03Z" id="95599895">@imotov I change PR that validate both of settings and report multiple errors.
And fix your comments. Please review PR again.
</comment><comment author="johtani" created="2015-04-23T14:59:58Z" id="95615391">Push fix comment and change IndexCreationException instead of ElasticsearchIllegalArgumentException.
</comment><comment author="imotov" created="2015-04-23T15:20:08Z" id="95622607">There are a couple of unused imports. Otherwise LGTM.
</comment><comment author="johtani" created="2015-04-24T01:02:17Z" id="95765533">@imotov thx. I pushed removing some unused imports.
</comment><comment author="dakrone" created="2015-04-24T20:07:08Z" id="96048737">LGTM
</comment><comment author="johtani" created="2015-04-27T06:41:05Z" id="96520688">closed by 9745808c3f199218a42a7e37d5c3120ab5d700aa

closed by 77f350ab6f2544c28093f7da634149992ea6159e
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Share code for mock engines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10700</link><project id="" key="" /><description>Today we have duplicated logic in the MockInternal and MockShadowEngine
since they need to subclass the actual engine. This commit shares the most of
the code making it easier to add mock engines in the future.
</description><key id="69846077">10700</key><summary>[TEST] Share code for mock engines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T13:26:48Z</created><updated>2015-04-22T09:10:35Z</updated><resolved>2015-04-21T18:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-21T14:11:53Z" id="94809780">looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 5.2 r1675100</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10699</link><project id="" key="" /><description>This upgrade is for https://issues.apache.org/jira/browse/LUCENE-6442

It should improve test reproducibility, especially if you are on a mac
and want to reproduce a jenkins failure that happened on linux.
</description><key id="69841407">10699</key><summary>Upgrade to Lucene 5.2 r1675100</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T13:09:09Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-21T13:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-21T13:09:51Z" id="94787764">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #10699 from rmuir/lucene_1675100</comment></comments></commit></commits></item><item><title>parsing logs with same prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10698</link><project id="" key="" /><description>Hi,
I have been logstash to parse my logs for quite some time and its been cake walk for me. Now I had to add few more data point to some specific logs and its failing somewhere. Failing means logstash is not sending the data to ES properly.
For example:
previous logs:
1. A B C D
   new logs:
   1. A B C D
   2. A B C D E F  

I wrote corresponding rules for grok and placed rules for 2 and 1 in the same order in grok filter. Then logstash is not sending output to ES. If I put the 1st rule above the second rule, 'E' and 'F' 's data are missing in output. 

So please tell me, what is the mistake I am doing?
</description><key id="69823990">10698</key><summary>parsing logs with same prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rakesh91</reporter><labels /><created>2015-04-21T11:53:45Z</created><updated>2015-04-21T12:19:02Z</updated><resolved>2015-04-21T12:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-21T12:14:25Z" id="94770246">Not an Elasticsearch issue.

You should ask this on logstash mailing list and if it's an issue open it in logstash repository.

Closing.
</comment><comment author="rakesh91" created="2015-04-21T12:19:02Z" id="94773749">@dadoonet Thanks, Sorry for the mistake.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Kibana (or possibly ES) crashing every time Discover is clicked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10697</link><project id="" key="" /><description>Hi,

My rather a working setup has gone rouge all of a sudden and has started throwing below errors everytime I select my Kibana's Discover section. Even worse as my demostration date to management is approaching fast :-(

Any help will be highligh appreciated.

Error Throwing on Browser (Firefox):-
Oops!
Looks like something went wrong. Refreshing may do the trick.
Go Back or clear your session 

Discover: err.stack is undefined.
Notifier.prototype.error@http://localhost:5601/index.js?_b=5930:45607:7
bound@http://localhost:5601/index.js?_b=5930:32081:16
init&lt;/&lt;/&lt;@http://localhost:5601/index.js?_b=5930:118772:11
qFactory/defer/deferred.promise.then/wrappedCallback@http://localhost:5601/index.js?_b=5930:20873:31
qFactory/ref/&lt;.then/&lt;@http://localhost:5601/index.js?_b=5930:20959:26
$RootScopeProvider/this.$get&lt;/Scope.prototype.$eval@http://localhost:5601/index.js?_b=5930:22002:16
$RootScopeProvider/this.$get&lt;/Scope.prototype.$digest@http://localhost:5601/index.js?_b=5930:21814:15
$RootScopeProvider/this.$get&lt;/Scope.prototype.$apply@http://localhost:5601/index.js?_b=5930:22106:13
done@http://localhost:5601/index.js?_b=5930:17641:34
completeRequest@http://localhost:5601/index.js?_b=5930:17855:7
createHttpBackend/&lt;/xhr.onreadystatechange@http://localhost:5601/index.js?_b=5930:17794:1

Environmental Information:-

the version of JVM - "1.8.0_31" (for both Kibana and ES)
local configuration - All Default (for both Kibana and ES)
operating system - Linux 2.6.32-431.11.2.el6.x86_64 (for ES) and Win 7 (for Kibana)

Health Status of my ES Server:-
curl -XGET 'http://127.0.0.1:9200/_cluster/health?pretty'
{
  "cluster_name" : "fil_logs_cluster01",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 421,
  "active_shards" : 421,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 421
}

Cluster Stats of my ES Cluster:-
curl -XGET 'http://localhost:9200/_cluster/stats?human&amp;pretty'
{
  "timestamp" : 1429609506180,
  "cluster_name" : "fil_logs_cluster01",
  "status" : "yellow",
  "indices" : {
    "count" : 85,
    "shards" : {
      "total" : 421,
      "primaries" : 421,
      "replication" : 0.0,
      "index" : {
        "shards" : {
          "min" : 1,
          "max" : 5,
          "avg" : 4.952941176470588
        },
        "primaries" : {
          "min" : 1,
          "max" : 5,
          "avg" : 4.952941176470588
        },
        "replication" : {
          "min" : 0.0,
          "max" : 0.0,
          "avg" : 0.0
        }
      }
    },
    "docs" : {
      "count" : 11726,
      "deleted" : 4
    },
    "store" : {
      "size" : "14.1mb",
      "size_in_bytes" : 14789369,
      "throttle_time" : "0s",
      "throttle_time_in_millis" : 0
    },
    "fielddata" : {
      "memory_size" : "1.3mb",
      "memory_size_in_bytes" : 1411360,
      "evictions" : 0
    },
    "filter_cache" : {
      "memory_size" : "811.7kb",
      "memory_size_in_bytes" : 831188,
      "evictions" : 0
    },
    "id_cache" : {
      "memory_size" : "0b",
      "memory_size_in_bytes" : 0
    },
    "completion" : {
      "size" : "0b",
      "size_in_bytes" : 0
    },
    "segments" : {
      "count" : 513,
      "memory" : "20.9mb",
      "memory_in_bytes" : 21936938,
      "index_writer_memory" : "0b",
      "index_writer_memory_in_bytes" : 0,
      "index_writer_max_memory" : "205.5mb",
      "index_writer_max_memory_in_bytes" : 215552000,
      "version_map_memory" : "0b",
      "version_map_memory_in_bytes" : 0,
      "fixed_bit_set" : "0b",
      "fixed_bit_set_memory_in_bytes" : 0
    },
    "percolate" : {
      "total" : 0,
      "get_time" : "0s",
      "time_in_millis" : 0,
      "current" : 0,
      "memory_size_in_bytes" : -1,
      "memory_size" : "-1b",
      "queries" : 0
    }
  },
  "nodes" : {
    "count" : {
      "total" : 1,
      "master_only" : 0,
      "data_only" : 0,
      "master_data" : 1,
      "client" : 0
    },
    "versions" : [ "1.4.4" ],
    "os" : {
      "available_processors" : 2,
      "mem" : {
        "total" : "1.7gb",
        "total_in_bytes" : 1901912064
      },
      "cpu" : [ {
        "vendor" : "Intel",
        "model" : "Xeon",
        "mhz" : 2892,
        "total_cores" : 2,
        "total_sockets" : 1,
        "cores_per_socket" : 2,
        "cache_size" : "20kb",
        "cache_size_in_bytes" : 20480,
        "count" : 1
      } ]
    },
    "process" : {
      "cpu" : {
        "percent" : 0
      },
      "open_file_descriptors" : {
        "min" : 1477,
        "max" : 1477,
        "avg" : 1477
      }
    },
    "jvm" : {
      "max_uptime" : "1.1d",
      "max_uptime_in_millis" : 98348135,
      "versions" : [ {
        "version" : "1.8.0_31",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "25.31-b07",
        "vm_vendor" : "Oracle Corporation",
        "count" : 1
      } ],
      "mem" : {
        "heap_used" : "160.8mb",
        "heap_used_in_bytes" : 168635336,
        "heap_max" : "1007.3mb",
        "heap_max_in_bytes" : 1056309248
      },
      "threads" : 44
    },
    "fs" : {
      "total" : "1007.8mb",
      "total_in_bytes" : 1056858112,
      "free" : "526mb",
      "free_in_bytes" : 551628800,
      "available" : "474.8mb",
      "available_in_bytes" : 497942528,
      "disk_reads" : 410938,
      "disk_writes" : 525704,
      "disk_io_op" : 936642,
      "disk_read_size" : "1.8gb",
      "disk_read_size_in_bytes" : 2031592448,
      "disk_write_size" : "2gb",
      "disk_write_size_in_bytes" : 2153283584,
      "disk_io_size" : "3.8gb",
      "disk_io_size_in_bytes" : 4184876032,
      "disk_queue" : "0",
      "disk_service_time" : "1.7"
    },
    "plugins" : [ ]
  }
}
</description><key id="69800559">10697</key><summary>Kibana (or possibly ES) crashing every time Discover is clicked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashwgupt</reporter><labels /><created>2015-04-21T09:59:00Z</created><updated>2015-09-02T21:57:09Z</updated><resolved>2015-04-24T08:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-22T01:09:00Z" id="94989806">We reserve github for bugs and feature requests, while this appears to be an operational matter.

I'd suggest that you send something to the mailing list for assistance - https://groups.google.com/forum/#!forum/elasticsearch
</comment><comment author="ashwgupt" created="2015-04-24T08:16:36Z" id="95846555">Sure, thanks
</comment><comment author="programmer11" created="2015-09-02T21:54:00Z" id="137256129">I am having the same issue as above. Could you tell me the resolution to it?
</comment><comment author="markwalkom" created="2015-09-02T21:57:09Z" id="137257605">Please use the forums - https://discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make getFileStore a bit more defensive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10696</link><project id="" key="" /><description>This PR just makes the crazy code in NodeEnvironment that tries to locate the actual mount point (NodeEnvironment.getFileStore) for a given Path, a bit more defensive.

This just mirrors the fixes in Lucene from https://issues.apache.org/jira/browse/LUCENE-6428
</description><key id="69798967">10696</key><summary>Make getFileStore a bit more defensive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T09:51:31Z</created><updated>2015-04-22T10:22:43Z</updated><resolved>2015-04-22T10:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-21T15:17:24Z" id="94835916">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file></files><comments><comment>Make NodeEnvironment.getFileStore a bit more defensive</comment></comments></commit></commits></item><item><title>Chinese Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10695</link><project id="" key="" /><description>Dear team,

I have a document:
`
{"tags": [
"1",
"天猫",
"京东商城"
],
"name": "tao"}
`

When I use 
`{
    "query":{
        "bool" : {
            "must" : {
                "terms" : {
                    "tags" : ["1"],
                     minimum_should_match : 1
                 }
            }
         }
    }
}`
got right result.

But when I query with
`{
    "query":{
        "bool" : {
            "must" : {
                "terms" : {
                    "tags" : ["天猫"],
                     minimum_should_match : 1
                 }
            }
         }
    }
}`
got nothing.
</description><key id="69772684">10695</key><summary>Chinese Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cr2121</reporter><labels /><created>2015-04-21T07:52:21Z</created><updated>2015-04-21T08:15:34Z</updated><resolved>2015-04-21T08:15:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-04-21T08:15:33Z" id="94699125">Please ask questions like this on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch).

And your `tags` field analyzed by analyzer, however `terms` query does _not analyzed_.

See: http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Matched queries: Remove redundant and broken code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10694</link><project id="" key="" /><description>Because the fetch phase now has nested doc support, the logic that deals with detecting if a named nested query/filter matches with a hit can be removed.

PR for #10661
</description><key id="69759966">10694</key><summary>Matched queries: Remove redundant and broken code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-21T06:54:48Z</created><updated>2015-06-07T18:41:37Z</updated><resolved>2015-04-21T08:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-21T07:55:08Z" id="94683193">LGTM

Hurray for code removal. \o/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java</file><file>core/src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>inner_hits: Properly support named queries for both nested and parent child inner hits.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file></files><comments><comment>Merge pull request #10694 from martijnvg/matched_queries/cleanup</comment></comments></commit></commits></item><item><title>Can create index with negative shard count, deletion appears to fail (but actually succeeds)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10693</link><project id="" key="" /><description>```
shell&gt; curl -XPUT -d '{"number_of_shards":-100}' http://localhost:9200/asdf?pretty
{
  "acknowledged" : true
}
shell&gt; curl http://localhost:9200/_cat/indices?v
health status index  pri rep docs.count docs.deleted store.size pri.store.size
red    open   asdf  -100   1
shell&gt; time curl -XDELETE http://localhost:9200/asdf?pretty
{
  "acknowledged" : false
}

real    0m30.013s
user    0m0.002s
sys     0m0.007s
shell&gt; curl http://localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
shell&gt; curl http://localhost:9200/_cat/nodes?h=version
1.5.1
```
</description><key id="69721714">10693</key><summary>Can create index with negative shard count, deletion appears to fail (but actually succeeds)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">sorear</reporter><labels><label>:Settings</label><label>bug</label></labels><created>2015-04-21T02:03:24Z</created><updated>2015-04-27T04:56:20Z</updated><resolved>2015-04-27T04:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-04-21T10:55:21Z" id="94743825">@sorear Thanks reporting!
I reproduced it.
</comment><comment author="johtani" created="2015-04-21T10:56:36Z" id="94743986">related #7495
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateNumberOfReplicasTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Settings: validate number_of_shards/number_of_replicas without index setting prefix</comment></comments></commit></commits></item><item><title>Allow custom version_type comparison methods via plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10692</link><project id="" key="" /><description>It would be useful to be able to write custom document version comparison methods as plugins (or perhaps even scripts?) in order to support custom version semantics such as vector clocks.
</description><key id="69707882">10692</key><summary>Allow custom version_type comparison methods via plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schmichael</reporter><labels><label>feedback_needed</label></labels><created>2015-04-21T00:02:54Z</created><updated>2015-04-21T17:56:25Z</updated><resolved>2015-04-21T17:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-21T06:47:24Z" id="94655239">Hi Michael,

Can you elaborate on which API you would need and how you would implement the vector clocks? Remember that ES user a primary back up model where only accepts writes on the primary shard copy. If there is no primary available, writes will be rejected (with a default timeout of 1m). This means there is only 1 official version of each document.
</comment><comment author="schmichael" created="2015-04-21T17:56:25Z" id="94889519">@bleskes Sure, I'm still sketching out a new system, so this feature may not be necessary.

The system involves a pipeline in which mutations are merged into documents upstream from ES using vector clocks for partial ordering. By the time documents reach the end of the pipeline for indexing they may be indexed out of order such that a document with version `A:3,B:3` is indexed _after_ a document with version `A:9,B:3`. The `A:3,B:3` version of the document should be dropped as a later version is already indexed.

However, without a mechanism for resolving conflicts such as `A:4,B:2` vs `A:3,B:3`, a deterministic fallback algorithm would have to be used in its place (such as a simple lexicographical comparison) and undermine the consistency benefits of using vector clocks in the first place...

...so I think perhaps the better solution is for a system upstream in our pipeline to generate a monotonic version number for use by the indexer. Not sure it can be done without either a lot of coordination or stale (but always moving forward in time) documents in the index, but it's probably better than adding complexity to ES.

I'll go ahead and close this unless someone else can think of how vector clocks without a custom conflict resolution mechanism are safe/useful.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The name of scroll ID attribute in the response is "_scroll_id" </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10691</link><project id="" key="" /><description>rather than "scroll_id"
</description><key id="69706196">10691</key><summary>The name of scroll ID attribute in the response is "_scroll_id" </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mmollaverdi</reporter><labels /><created>2015-04-20T23:48:14Z</created><updated>2015-04-25T17:32:56Z</updated><resolved>2015-04-25T17:32:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T17:32:55Z" id="96241868">thanks @mmollaverdi - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: The name of scroll ID attribute in the response is "_scroll_id" rather than "scroll_id"</comment></comments></commit></commits></item><item><title>Pattern capture email example not complete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10690</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/blob/master/docs/reference/analysis/tokenfilters/pattern-capture-tokenfilter.asciidoc#pattern-capture-token-filter

The pattern capture example in the doc says it will produce the following tokens:

&gt; When the above analyzer is used on an email address like:
&gt; john-smith_123@foo-bar.com
&gt; it would produce the following tokens: [ john-smith_123, foo-bar.com, john, smith_123, smith, 123, foo, foo-bar.com, bar, com ]

The pattern in the doc example:

```
 "patterns" : [
                  "(\\w+)",
                  "(\\p{L}+)",
                  "(\\d+)",
                  "@(.+)"
               ]
```

The above does not actually produce the `john-smith_123` token.  Adding the last line in the patterns array will produce the additional `john-smith_123` token (though I am not sure if it is the best way since I'm not a regex guru):

```
               "patterns" : [
                  "(\\w+)",
                  "(\\p{L}+)",
                  "(\\d+)",
                  "@(.+)",
                  "([^@]+)"
               ]
```
</description><key id="69705036">10690</key><summary>Pattern capture email example not complete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-04-20T23:37:57Z</created><updated>2015-04-25T17:28:28Z</updated><resolved>2015-04-25T17:28:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T17:28:22Z" id="96241263">thanks @ppf2 - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fixed pattern-capture token filter example</comment></comments></commit></commits></item><item><title>Ensure that explanation descriptions are not null on serialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10689</link><project id="" key="" /><description>As requested on #10399
</description><key id="69692155">10689</key><summary>Ensure that explanation descriptions are not null on serialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T22:06:16Z</created><updated>2015-05-29T18:06:55Z</updated><resolved>2015-04-21T07:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-20T23:41:04Z" id="94593184">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file></files><comments><comment>Merge pull request #10689 from jpountz/enhancement/check_expl_desc</comment></comments></commit></commits></item><item><title>Implement retries for ShadowEngine creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10688</link><project id="" key="" /><description>When using a filesystem that may have lag between an index being created
on the primary and a on the replica, creation of the ShadowEngine can
fail because there are no segments in the directory.

In these situations, we retry during engine creation to wait until an
index is present in the directory. The number of retries and the delay
between retries are both configuration, defaulting to 5 retries and 1
second between each try.

Resolves #10637
</description><key id="69677648">10688</key><summary>Implement retries for ShadowEngine creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T20:52:01Z</created><updated>2015-05-29T16:06:36Z</updated><resolved>2015-04-22T20:34:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-22T17:46:39Z" id="95281027">@s1monw pushed a new commit to this refactoring the wait into `Lucene.waitForIndex` like we talked about.
</comment><comment author="s1monw" created="2015-04-22T18:26:04Z" id="95292068">left minor comments LGTM in general
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add CommitStats to supply information about the current commit point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10687</link><project id="" key="" /><description>Extends ShardStats with commit specific information. We currently expose commit id, generation and the user data map.

The information is also retrievable via the Rest API by using `GET _stats?level=shards`

Example output:

```
"commit": {
     "id": "UEMxOVFVbr/AGlOLg0xytQ==",
     "generation": 3,
     "user_data": {
        "translog_id": "4"
     }
}
```
</description><key id="69668967">10687</key><summary>Add CommitStats to supply information about the current commit point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T20:08:21Z</created><updated>2015-04-22T07:50:08Z</updated><resolved>2015-04-22T07:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-04-21T08:04:21Z" id="94691971">+1
</comment><comment author="s1monw" created="2015-04-21T10:23:25Z" id="94736257">left some minor comments! thanks boaz 
</comment><comment author="bleskes" created="2015-04-21T14:30:45Z" id="94814950">@s1monw thx. pushed another commit
</comment><comment author="s1monw" created="2015-04-22T07:28:04Z" id="95059781">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>src/main/java/org/elasticsearch/index/engine/CommitStats.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file></files><comments><comment>Stats: add CommitStats to supply information about the current commit point</comment></comments></commit></commits></item><item><title>Clarification/Examples in Disk-based Shard Allocation watermarks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10686</link><project id="" key="" /><description>http://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-allocation.html#disk

The docs make this setting pretty ambiguous - it's not really clear how to set a percentage value vs an absolute byte value. Even just showing example commands from the defaults and values already mentioned would be very helpful, such as `"cluster.routing.allocation.disk.watermark.low": "85%"` and `"cluster.routing.allocation.disk.watermark.low": "500mb"`

As-is, it's easy for a user to enter a value like "95" and think they are setting it to 95%, when in fact they are setting it to 95 _bytes_.
</description><key id="69658691">10686</key><summary>Clarification/Examples in Disk-based Shard Allocation watermarks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">w33ble</reporter><labels><label>docs</label></labels><created>2015-04-20T19:09:01Z</created><updated>2015-04-22T17:53:36Z</updated><resolved>2015-04-22T17:53:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Add example of setting disk threshold decider settings</comment></comments></commit></commits></item><item><title>[GEO] Optional exception for GeoPoints &gt; 2 dimensions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10685</link><project id="" key="" /><description>Related to #10510 and #10539 the discussion involves adding an optional mapping parameter to throw an exception if an input GeoJSON contains coordinates with &gt;2 dimensions. 

The loosely defined GeoJSON spec (at http://geojson.org/geojson-spec.html#positions) states: "There must be at least two elements, and may be more." With no limit on how many elements can be in a coordinate.  Since ES currently supports 2D only, it seems natural for default behavior to throw an exception if a coordinate is received with any more than 2 elements.  There are a few "user-friendly" questions surrounding this (which have cropped up more than once on the issue list)
1. Should another validation parameter be added to optionally fail on coordinates &gt; 2d?
2. Should it be part of the existing 'validate' parameter?
3. Should the default throw an exception for &gt;2d coordinates forcing users to set the parameter to handle pre-existing geojson data? 
</description><key id="69657734">10685</key><summary>[GEO] Optional exception for GeoPoints &gt; 2 dimensions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-04-20T19:03:45Z</created><updated>2015-04-21T12:17:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-20T23:38:38Z" id="94592907">@nknize You could change the behavior of validate, but only run the additional check if the index was created with new versions of ES (1.6+?).
</comment><comment author="nknize" created="2015-04-21T12:17:01Z" id="94772471">@rjernst I have outstanding pr #10248 that deprecates the "old" validate and normalize options. I can update the behavior there prior to merge /cc @colings86 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>audit third party deps and NOTICE.TXT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10684</link><project id="" key="" /><description>We should go through all the third party dependencies that we redistribute and make sure NOTICE.TXT has all the info needed to comply with the license terms of the dependencies.
</description><key id="69654515">10684</key><summary>audit third party deps and NOTICE.TXT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kevinkluge</reporter><labels /><created>2015-04-20T18:47:29Z</created><updated>2015-06-17T16:06:55Z</updated><resolved>2015-06-17T16:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-21T12:31:46Z" id="94776422">I looked into it a little yesterday: currently we use com.mycila license-maven-plugin to check source code license headers.
- codehaus license-maven-plugin (http://mojo.codehaus.org/license-maven-plugin/) seems more full-featured and can report licenses of third-party-dependencies. I was able to make this work and produce an XML file, but it just reports the licenses of every dep. This seems helpful though.
- apache whisker (http://creadur.apache.org/whisker/) has the goal of maintaining NOTICE/LICENSE type documentation in mind. However, I was unable to make it work at all. In its current state (0.1-snapshot, etc) I was unable to get past all the barriers, like configuring custom .m2/settings that allows its plugin prefix and making this work with the apache snapshot repository.
</comment><comment author="mrsolo" created="2015-04-22T20:13:58Z" id="95323122">We use https://github.com/elastic/elasticsearch/blob/master/pom.xml#L1753 license profile when doing license analysis which is essentially revert back to http://mojo.codehaus.org/license-maven-plugin/

'mvn site' will generate all kind of goodies, including third party dependencies
</comment><comment author="clintongormley" created="2015-04-29T17:23:03Z" id="97510950">I've been trying to automate this with http://mojo.codehaus.org/license-maven-plugin/ but I have a few questions:
- If three dependencies all use the BSD 3-clause license, do we need to distribute one copy of the BSD 3 -clause license, or 3 individual copies, each one with the copyright details filled in?
- We don't redistribute the source of the dependencies, so i assume we only need to distribute the licenses in the compiled version?
- We can generate a NOTICE file for our dependencies, but it looks like we have to include the NOTICE file from each dependency as well (see http://www.apache.org/licenses/LICENSE-2.0.html#redistribution). Am I reading this correctly?
</comment><comment author="kevinkluge" created="2015-04-29T22:42:54Z" id="97607832">@clintongormley 
1) we need to include a copy for every different copyright holder.  Just repeat the full BSD license, with only the copyright being different.
2) correct (compiled java counts as binary)
3) no.  see the definition of "derivative work".  We are shipping these jars unmodified so we are not distributing derivative works, and don't need to include the deps' NOTICE files as a result.    (If I'm wrong and we are modifying any deps from source, those resulting jars are derivative works, and the NOTICE inclusion clause does apply, for that jar.)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Packaging: Add LICENSE, NOTICE, and sha1 files and tests for all core dependencies</comment></comments></commit></commits></item><item><title>Update extendedstats-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10683</link><project id="" key="" /><description>code snippet should show ExtendedStats, not Stats
</description><key id="69642445">10683</key><summary>Update extendedstats-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amollenkopf</reporter><labels /><created>2015-04-20T17:45:31Z</created><updated>2015-04-25T17:07:46Z</updated><resolved>2015-04-25T17:07:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T17:07:38Z" id="96237390">thanks @amollenkopf - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update extendedstats-aggregation.asciidoc</comment></comments></commit></commits></item><item><title>Update bool-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10682</link><project id="" key="" /><description>from, to deprecated in favour of gt, lt
</description><key id="69628155">10682</key><summary>Update bool-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cy</reporter><labels /><created>2015-04-20T16:42:18Z</created><updated>2015-04-26T17:23:36Z</updated><resolved>2015-04-26T17:23:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T19:26:13Z" id="96041769">@clintongormley can you look
</comment><comment author="clintongormley" created="2015-04-26T17:23:33Z" id="96410545">Hi @cy 

thanks for the PR.  Actually, `from` translates to `gte` (while `to` is correct as `lt`).  I've fixed that and merged.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update bool-filter.asciidoc</comment></comments></commit></commits></item><item><title>[TEST] Add back old way for naming clusters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10681</link><project id="" key="" /><description>We messed this up in the mockFS branch and I am bringing back the pseudo isolation
</description><key id="69626616">10681</key><summary>[TEST] Add back old way for naming clusters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2015-04-20T16:33:09Z</created><updated>2015-04-20T16:39:49Z</updated><resolved>2015-04-20T16:37:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-20T16:34:31Z" id="94502350">+1. as a followup though, we need to fix the mvn configuration so that jvms are always isolated from each other completely (and enforce with security manager)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES do not wait for all shards being closed before shutdown/restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10680</link><project id="" key="" /><description>Hi!

I have a one node setup and spotted that if I create a lot of indices/shards and then immediatly will try to restart the server, it will come back online in the red state with a couple of unassigned shards and errors in log like this:

```
Caused by: java.io.EOFException: read past EOF:   
NIOFSIndexInput(path="/var/lib/elasticsearch/elasticsearch/nodes/0/indices/intelligence-reports-2014.05.01/3/index/segments_2”)
```

The problem is that ES starts to flush shards to the hard drive, but does not manage to do that in time, so some shards remain unflushed.

Init script that comes bundled with ES gives 20 seconds for it to finish all preparations:
start-stop-daemon --stop --pidfile "$PID_FILE" \ --user "$ES_USER" \ --retry=TERM/20/KILL/5 &gt; /dev/null

If you bump that period, ES have [hardcoded rule to do that in 30 secs](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/IndicesService.java#L153), otherwise:

```
[2015-04-21 15:04:03,558][INFO ][node ] [collaboration] stopping ...   
 [2015-04-21 15:04:33,581][WARN ][indices ] [collaboration] Not all shards are closed yet, waited 30sec - stopping service    
[2015-04-21 15:04:33,581][INFO ][node ] [collaboration] stopped    
[2015-04-21 15:04:33,581][INFO ][node ] [collaboration] closing ...    
[2015-04-21 15:04:33,587][INFO ][node ] [collaboration] closed
```

I know that I can "fix" this by send `flush` after massive write is finished, but why ES needs to kill himself after 30 secs and does not wait for shards to close?
</description><key id="69616913">10680</key><summary>ES do not wait for all shards being closed before shutdown/restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">somebody32</reporter><labels><label>:Packaging</label></labels><created>2015-04-20T15:45:14Z</created><updated>2015-05-20T07:54:58Z</updated><resolved>2015-05-04T14:57:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="saimonmoore" created="2015-04-23T09:50:43Z" id="95516562">yeah this happened to me too. Would like to know too.
</comment><comment author="ivanilves" created="2015-04-23T12:34:29Z" id="95570984">Confirm it happened to me too. Would like to have configurable **shard_close_timeout** or something...
</comment><comment author="clintongormley" created="2015-04-25T17:46:35Z" id="96245468">@tlrx do we force shutdown of ES nodes from the init scripts? I thought we'd removed that?
</comment><comment author="clintongormley" created="2015-04-25T17:48:06Z" id="96245630">Oh, it's not in the init script...

@s1monw what do you think about this?
</comment><comment author="tlrx" created="2015-04-25T18:08:50Z" id="96250446">&gt; @tlrx do we force shutdown of ES nodes from the init scripts? I thought we'd removed that?

We force shutdown by killing process after ~20 sec (see [here for RPM](https://github.com/elastic/elasticsearch/blob/master/src/packaging/rpm/init.d/elasticsearch#L113) and [here for DEB](https://github.com/elastic/elasticsearch/blob/master/src/packaging/deb/init.d/elasticsearch#L189)). I don't remind we decided to remove this for elasticsearch.

I'm not familiar with the IndicesService class but I find those 30 sec a bit strange.
</comment><comment author="clintongormley" created="2015-04-26T17:51:16Z" id="96413460">&gt; We force shutdown by killing process after ~20 sec (see here for RPM and here for DEB). I don't remind we decided to remove this for elasticsearch.

So it looks like `killproc` is sending a -9 after our specified 20 seconds (see https://github.com/elastic/elasticsearch/pull/3973#issuecomment-54807069) , but Elasticsearch is waiting for 30 seconds... Perhaps we should make the init script timeout longer.
</comment><comment author="clintongormley" created="2015-04-26T18:25:08Z" id="96418130">@somebody32 @saimonmoore @ivanilves what versions of Elasticsearch are you seeing this on?
</comment><comment author="somebody32" created="2015-04-26T18:35:44Z" id="96418686">@clintongormley 1.5.0 (and looks like no difference in 1.5.1 too)

And, as I said, the problem not only in init script. Bult-in 30 seconds grace period was not enough in our case to flush all shards
</comment><comment author="bleskes" created="2015-05-18T08:34:27Z" id="102975248">@tlrx  - @s1monw 's PR fix the internal wait in ES. I might be missing something here but I think the RPM/DEB still needs to be changed for it to be effective no? 
</comment><comment author="tlrx" created="2015-05-20T07:54:58Z" id="103800240">@bleskes right - I created #11248 to track this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file></files><comments><comment>[INDICES] Wait forever (or one day) for indices to close</comment></comments></commit></commits></item><item><title>[GEO] Update tree_level and precision parameter priorities</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10679</link><project id="" key="" /><description>If a user explicitly defined the tree_level or precision parameter in a geo_shape mapping their precision was always overridden by the distance_error_pct parameter (even though our docs say this parameter is a 'hint'). This lead to unexpected accuracy problems  (e.g., false positives) in the results of a geo_shape filter. (example provided in issue #9691)

This patch fixes this unexpected behavior by setting the distance_error_pct parameter to zero when the tree_level or precision parameters are provided by the user, but the distance_error_pct parameter is not. This enables a user to explicitly specify a precision and an error consciously knowing how the error factor will affect query results.

Under the covers the quadtree will now guarantee the precision defined by the user, eliminating this explanation that false positives are "like text based stemming". The docs will be updated to alert the user to exercise caution with these parameters.  Specifying a precision of "1m" for an index using large complex shapes can use a significant amount of memory.

closes #9691
</description><key id="69615369">10679</key><summary>[GEO] Update tree_level and precision parameter priorities</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.5.2</label></labels><created>2015-04-20T15:37:51Z</created><updated>2015-04-21T19:43:32Z</updated><resolved>2015-04-21T19:43:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-20T22:13:12Z" id="94581458">@nknize I left a couple comments.
</comment><comment author="rjernst" created="2015-04-21T15:26:52Z" id="94838638">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot indices with wildcard not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10678</link><project id="" key="" /><description>My indices naming convention is `[client_name]_movements_[year]` and I have set my backup repositories to backup data by client:

```
curl -XPUT 'http://localhost:9200/_snapshot/es_client_one' -d '{
    "type": "fs",
    "settings": {
        "indices": "client_one_*",
        "location": "/opt/backups/es_client_one",
        "compress": true,
        "verify" : "true"
    }
}'
```

when testing a first snapshot:

```
curl -XPUT "http://localhost:9200/_snapshot/es_client_one/snapshot_20150420_1437"
```

the result was that every index with no exceptions were backed up to "/opt/backups/es_client_one"

note: version 1.4.4
</description><key id="69607952">10678</key><summary>Snapshot indices with wildcard not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tostasqb</reporter><labels /><created>2015-04-20T15:05:51Z</created><updated>2015-04-20T15:37:53Z</updated><resolved>2015-04-20T15:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-04-20T15:17:15Z" id="94481267">@tostasqb please note that mailing list is much better place to ask questions like this. We are trying to keep github issue for tracking bugs and feature requests. The list of indices should go into the [snapshot command](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_snapshot) not into repository settings.  
</comment><comment author="tostasqb" created="2015-04-20T15:37:51Z" id="94486513">honest mistake.
Thank you sir
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure we can recover cleanly into a leftover shard directory from a different index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10677</link><project id="" key="" /><description>this is a followup issue for #10461 where we now throw an exception if we try to allocate a shard where there already exists such a shards with a non-matching index UUID. This is an edge case but we should be able to recover cleanly in such a case an remove that leftover shard directory.
</description><key id="69592125">10677</key><summary>Ensure we can recover cleanly into a leftover shard directory from a different index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T13:55:33Z</created><updated>2015-07-15T07:49:42Z</updated><resolved>2015-07-15T07:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Allow shards to be allocated if leftover shard from different index exists.</comment></comments></commit></commits></item><item><title>CPU is increasing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10676</link><project id="" key="" /><description>Hello,

We have 3 servers cluster. For around two weeks it runs OK, but then CPU on one of the servers starts to grow. When it becomes unacceptable, we restart that server and everything becomes normal again for two weeks. Below is hot threads report:

```
::: [es-02][L7i2hHdSSjSFW-CHEgS5ig][es-02][inet[/10.0.0.153:9300]]{master=true}

    0.7% (3.4ms out of 500ms) cpu usage by thread 'elasticsearch[es-02][refresh][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.2% (1.1ms out of 500ms) cpu usage by thread 'elasticsearch[es-02][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.0% (235.4micros out of 500ms) cpu usage by thread 'elasticsearch[es-02][flush][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

::: [es-03][wfw6V8x3Tsic1resYJ5Tcg][es-03][inet[/10.0.0.177:9300]]{master=true}

    0.2% (1.1ms out of 500ms) cpu usage by thread 'elasticsearch[es-03][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.0% (100.4micros out of 500ms) cpu usage by thread 'elasticsearch[es-03][transport_client_worker][T#2]{New I/O worker #2}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.0% (71.8micros out of 500ms) cpu usage by thread 'Abandoned connection cleanup thread'
     10/10 snapshots sharing following 3 elements
       java.lang.Object.wait(Native Method)
       java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
       com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43)

::: [es-01][LoW1ev29R5qzYjPl1PJu4A][es-01][inet[/10.0.0.108:9300]]

    0.3% (1.4ms out of 500ms) cpu usage by thread 'elasticsearch[es-01][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.1% (288.4micros out of 500ms) cpu usage by thread 'elasticsearch[es-01][flush][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)

    0.0% (156.6micros out of 500ms) cpu usage by thread 'elasticsearch[es-01][[transport_server_worker.default]][T#3]{New I/O worker #8}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
```

Below is CPU graph of servers performance:
![capture](https://cloud.githubusercontent.com/assets/1401980/7231395/bbaa96ee-e77d-11e4-96aa-994a6e8494e1.PNG)
As you see last CPU increase was between 04/01 and 04/02. Now latest CPU increase is on 04/20. Both times restart of es-01 node helped.

Any ideas?
Thank you in advance for your answers.

Gediminas
</description><key id="69592084">10676</key><summary>CPU is increasing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gediminasgu</reporter><labels /><created>2015-04-20T13:55:14Z</created><updated>2017-02-20T10:46:29Z</updated><resolved>2015-04-25T16:44:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T16:43:51Z" id="96234975">Hi @gediminasgu 

You should probably ask questions like these in the forum instead. I think you're probably experiencing memory pressure or having slow garbage collections.  You can see this in the logs.
</comment><comment author="huangyiminghappy" created="2017-02-20T10:46:28Z" id="281046459">all the threadpool's thread is waiting for notify by providers
,are there no comand to be offer to queue?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ipv6 support for unicast hosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10675</link><project id="" key="" /><description>Adding 

-Des.discovery.zen.ping.unicast.hosts="2a01:ddd:eeee:ffff:0:0:0:2" fails with address not reachable.

also
-Des.discovery.zen.ping.unicast.hosts="[2a01:ddd:eeee:ffff:0:0:0:2]" does not work.

Best regards
Dominic
</description><key id="69583876">10675</key><summary>ipv6 support for unicast hosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DominicBoettger</reporter><labels><label>:Network</label><label>discuss</label></labels><created>2015-04-20T13:13:18Z</created><updated>2015-08-21T06:40:53Z</updated><resolved>2015-08-21T06:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T06:40:53Z" id="133306089">Fixed by #12999
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over to IndexSearcher.count.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10674</link><project id="" key="" /><description>There is a new IndexSearcher.count method that makes it easier to count how
many documents match a particular query.
</description><key id="69580048">10674</key><summary>Cut over to IndexSearcher.count.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T12:50:53Z</created><updated>2015-06-07T11:43:45Z</updated><resolved>2015-04-20T12:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-20T12:54:07Z" id="94444568">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Merge pull request #10674 from jpountz/enhancement/indexsearcher_count</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Merge pull request #10674 from jpountz/enhancement/indexsearcher_count</comment></comments></commit></commits></item><item><title>Plugin installation script ignores path.plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10673</link><project id="" key="" /><description>Custom directory that is defined in the `elasticsearch.yml` under the `path.plugins` settings is being ignored by the `bin/plugin` script when installing new plugins.

It attempts to install the plugin to the default directory and there is no way of manually setting the plugin script to install a plugin to a different location.
</description><key id="69579042">10673</key><summary>Plugin installation script ignores path.plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">orweinberger</reporter><labels><label>:Packaging</label></labels><created>2015-04-20T12:45:28Z</created><updated>2016-01-14T23:18:05Z</updated><resolved>2015-05-25T12:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-20T12:47:20Z" id="94443282">@orweinberger thanks for reporting. Yes this is a known problem and I'm on it :)
</comment><comment author="ppf2" created="2015-04-21T23:21:05Z" id="94970999">+1
</comment><comment author="markwalkom" created="2015-07-26T22:13:56Z" id="125044355">Just wanted to check what version this fix was released in, I'm still seeing the same problem in 1.7.
</comment><comment author="tlrx" created="2015-07-27T07:56:51Z" id="125115114">@markwalkom I just tested with the cloud-azure plugin and ES 1.7, it seems OK. Can you please elaborate on your issue (paths, privileges, plugins, type of ES installation etc)? Thanks

My output:
 

```
cd elasticsearch-1.7.0  

cat config/elasticsearch.yml | grep path.plugins
  path.plugins: /tmp/my-plugins

bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.8.0
  -&gt; Installing elasticsearch/elasticsearch-cloud-azure/2.8.0...
  Trying http://download.elasticsearch.org/elasticsearch/elasticsearch-cloud-azure/elasticsearch-cloud-  azure-2.8.0.zip...
  Downloading...DONE
  Installed elasticsearch/elasticsearch-cloud-azure/2.8.0 into /tmp/my-plugins/cloud-azure
```
</comment><comment author="markwalkom" created="2015-07-27T21:17:31Z" id="125345670">This is ES 1.7.0 (tar.gz) on OS X.

I installed all of our commercial plugins;

```
$ grep plugins Confs/ES/elasticsearch.yml
path.plugins: /Users/markw/Workspace/elastic/Data/ES/plugins

$ elasticsearch-1.7.0/bin/plugin install elasticsearch/shield/latest
-&gt; Installing elasticsearch/shield/latest...
Trying http://download.elasticsearch.org/elasticsearch/shield/shield-latest.zip...
Downloading .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Installed elasticsearch/shield/latest into /Users/markw/Workspace/elastic/elasticsearch-1.7.0/plugins/shield
$ elasticsearch-1.7.0/bin/plugin install elasticsearch/license/latest
-&gt; Installing elasticsearch/license/latest...
Trying http://download.elasticsearch.org/elasticsearch/license/license-latest.zip...
Downloading ..........................................DONE
Installed elasticsearch/license/latest into /Users/markw/Workspace/elastic/elasticsearch-1.7.0/plugins/license
$ elasticsearch-1.7.0/bin/plugin install elasticsearch/watcher/latest
-&gt; Installing elasticsearch/watcher/latest...
Trying http://download.elasticsearch.org/elasticsearch/watcher/watcher-latest.zip...
Downloading .............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Installed elasticsearch/watcher/latest into /Users/markw/Workspace/elastic/elasticsearch-1.7.0/plugins/watcher
```

As you can see they just installed to $ES_HOME. Then when I checked what was installed;

```
$ elasticsearch-1.7.0/bin/plugin -l
Installed plugins:
    - No plugin detected in /Users/markw/Workspace/elastic/elasticsearch-1.7.0/plugins
$ ll /Users/markw/Workspace/elastic/Data/ES/plugins
total 0
drwxr-xr-x   3 markw  staff   102B  4 May 16:37 bigdesk
drwxr-xr-x  15 markw  staff   510B  4 May 16:37 inquisitor
drwxr-xr-x   3 markw  staff   102B  4 May 16:37 kibana3
drwxr-xr-x   3 markw  staff   102B  4 May 16:37 kopf
drwxr-xr-x   3 markw  staff   102B 27 Jul 08:05 license
drwxr-xr-x   5 markw  staff   170B  4 May 16:37 marvel
drwxr-xr-x   8 markw  staff   272B 27 Jul 08:05 shield
drwxr-xr-x   7 markw  staff   238B 27 Jul 08:05 watcher
```
</comment><comment author="tlrx" created="2015-07-28T07:24:43Z" id="125481519">~~@markwalkom thanks for the description. I think it doesn't work when executing from outside the elasticsearch directory (ie `elasticsearch-1.7.0/bin/plugin`) but `cd elasticsearch-1.7.0 &amp;&amp; bin/plugin` should work.~~

@markwalkom I could not reproduce it on Ubuntu or OSX. In your example, I think that the configuration file is not correctly resolved by the `bin/plugin` script and thus the custom `path.plugins` is not used.

Can you edit the `elasticsearch-1.7.0/config/elasticsearch.yml` file instead of `Confs/ES/elasticsearch.yml` ?

You can also pass the CONF_DIR and/or CONF_FILE as env vars like `CONF_DIR="Confs/ES/" elasticsearch-1.7.0/bin/plugin install elasticsearch/shield/latest`
</comment><comment author="trekr5" created="2016-01-14T23:08:37Z" id="171811118">Hi,

I get a CONF_FILE error when I now attempt to install any plugin in ES 2.1.1.

I'm upgrading from 1.7.3 (where all my plugins worked fine) to 2.1.1 where they now don't.

Can you help?

Thanks
</comment><comment author="markwalkom" created="2016-01-14T23:12:04Z" id="171812344">@trekr5 please ask on https://discuss.elastic.co.
</comment><comment author="trekr5" created="2016-01-14T23:18:05Z" id="171813807">Hi, 

I've asked the question here on discuss https://discuss.elastic.co/t/upgrading-elasticsearch-from-1-7-3-to-2-1-1/39281/2 with this same config issue that has stopped elasticsearch service from running properly on upgraded nodes and prevents correct install of plugins
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Packaging: Use of default CONF_DIR/CONF_FILE in plugin install</comment></comments></commit></commits></item><item><title>Remove working directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10672</link><project id="" key="" /><description>This commit removes the unused working directory and its associated environment variable "WORK_DIR"
</description><key id="69547094">10672</key><summary>Remove working directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-20T09:32:22Z</created><updated>2015-06-08T13:32:52Z</updated><resolved>2015-04-25T11:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-20T09:45:04Z" id="94409834">LGTM
</comment><comment author="tlrx" created="2015-04-24T12:59:07Z" id="95925458">I just rebased on master and add few changes due to recent pull request merges.
</comment><comment author="rmuir" created="2015-04-24T16:10:35Z" id="95981208">+1
</comment><comment author="s1monw" created="2015-04-24T18:29:43Z" id="96026207">@tlrx can we pull this in?
</comment><comment author="tlrx" created="2015-04-25T11:17:49Z" id="96176826">@s1monw Done
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Starts and stops a single elasticsearch instance on this system (code=exited, status=3)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10671</link><project id="" key="" /><description>$ sudo service elasticsearch start
$ sudo service elasticsearch status

```
 ● elasticsearch.service - Starts and stops a single elasticsearch instance on this system
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Пн. 2015-04-20 14:15:37 YEKT; 2s ago
     Docs: http://www.elasticsearch.org
  Process: 2478 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR (code=exited, status=3)
 Main PID: 2478 (code=exited, status=3)

апр. 20 14:15:37 MG392 elasticsearch[2478]: ... 4 more
апр. 20 14:15:37 MG392 elasticsearch[2478]: log4j:WARN No appenders could be found for logger (node).
апр. 20 14:15:37 MG392 elasticsearch[2478]: log4j:WARN Please initialize the log4j system properly.
апр. 20 14:15:37 MG392 elasticsearch[2478]: log4j:WARN See http://logging.apache.org/log4j/1.2/...fo.
апр. 20 14:15:37 MG392 elasticsearch[2478]: {1.5.1}: Initialization Failed ...
апр. 20 14:15:37 MG392 elasticsearch[2478]: - ElasticsearchIllegalStateException[Failed to crea...nt]
апр. 20 14:15:37 MG392 elasticsearch[2478]: AccessDeniedException[/usr/share/elasticsearch/data]
апр. 20 14:15:37 MG392 systemd[1]: elasticsearch.service: main process exited, code=exited, st...NTED
апр. 20 14:15:37 MG392 systemd[1]: Unit elasticsearch.service entered failed state.
апр. 20 14:15:37 MG392 systemd[1]: elasticsearch.service failed.
Hint: Some lines were ellipsized, use -l to show in full.
```

Linux MG392 3.19.0-14-generic # 14-Ubuntu SMP Mon Apr 13 22:18:24 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar 
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

/usr/share/elasticsearch/bin$ ./elasticsearch -v
Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar 
Version: 1.5.1, Build: 5e38401/2015-04-09T13:41:35Z, JVM: 1.8.0_45
</description><key id="69544829">10671</key><summary>Starts and stops a single elasticsearch instance on this system (code=exited, status=3)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">Fudoshiki</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-04-20T09:22:28Z</created><updated>2016-12-10T14:33:40Z</updated><resolved>2015-06-17T07:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-20T09:37:48Z" id="94408532">@Fudoshiki thanks for reporting.

Can you please provide:
- the complete error log (use `journalctl -u elasticsearch.service`)
- the paths declared in your `/etc/default/elasticsearch` file 
</comment><comment author="Fudoshiki" created="2015-04-20T09:38:57Z" id="94408712">```
апр. 20 14:22:00 MG392 systemd[1]: elasticsearch.service: main process exited, code=exited, status=3/NOTIMPLEMENTED
апр. 20 14:22:00 MG392 systemd[1]: Unit elasticsearch.service entered failed state.
апр. 20 14:22:00 MG392 systemd[1]: elasticsearch.service failed.
апр. 20 14:31:10 MG392 systemd[1]: Started Starts and stops a single elasticsearch instance on this system.
апр. 20 14:31:10 MG392 systemd[1]: Starting Starts and stops a single elasticsearch instance on this system...
апр. 20 14:31:10 MG392 elasticsearch[7352]: Failed to configure logging...
апр. 20 14:31:10 MG392 elasticsearch[7352]: org.elasticsearch.ElasticsearchException: Failed to load logging configuration
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:139)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:89)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:100)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:184)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
апр. 20 14:31:10 MG392 elasticsearch[7352]: Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.Files.readAttributes(Files.java:1686)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.Files.walkFileTree(Files.java:2602)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:123)
апр. 20 14:31:10 MG392 elasticsearch[7352]: ... 4 more
апр. 20 14:31:10 MG392 elasticsearch[7352]: log4j:WARN No appenders could be found for logger (node).
апр. 20 14:31:10 MG392 elasticsearch[7352]: log4j:WARN Please initialize the log4j system properly.
апр. 20 14:31:10 MG392 elasticsearch[7352]: log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
апр. 20 14:31:10 MG392 elasticsearch[7352]: {1.5.1}: Initialization Failed ...
апр. 20 14:31:10 MG392 elasticsearch[7352]: - ElasticsearchIllegalStateException[Failed to created node environment]
апр. 20 14:31:10 MG392 elasticsearch[7352]: AccessDeniedException[/usr/share/elasticsearch/data]
апр. 20 14:31:10 MG392 elasticsearch[7352]: org.elasticsearch.ElasticsearchIllegalStateException: Failed to created node environment
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:162)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
апр. 20 14:31:10 MG392 elasticsearch[7352]: Caused by: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/data
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:383)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.Files.createDirectory(Files.java:630)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.Files.createAndCheckIsDirectory(Files.java:734)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at java.nio.file.Files.createDirectories(Files.java:720)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:105)
апр. 20 14:31:10 MG392 elasticsearch[7352]: at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:160)
апр. 20 14:31:10 MG392 elasticsearch[7352]: ... 4 more
```
</comment><comment author="Fudoshiki" created="2015-04-20T09:40:15Z" id="94408891">```
# Run Elasticsearch as this user ID and group ID
#ES_USER=elasticsearch
#ES_GROUP=elasticsearch

# Heap Size (defaults to 256m min, 1g max)
#ES_HEAP_SIZE=2g

# Heap new generation
#ES_HEAP_NEWSIZE=

# max direct memory
#ES_DIRECT_SIZE=

# Maximum number of open files, defaults to 65535.
#MAX_OPEN_FILES=65535

# Maximum locked memory size. Set to "unlimited" if you use the
# bootstrap.mlockall option in elasticsearch.yml. You must also set
# ES_HEAP_SIZE.
#MAX_LOCKED_MEMORY=unlimited

# Maximum number of VMA (Virtual Memory Areas) a process can own
#MAX_MAP_COUNT=262144

# Elasticsearch log directory
#LOG_DIR=/var/log/elasticsearch

# Elasticsearch data directory
#DATA_DIR=/var/lib/elasticsearch

# Elasticsearch work directory
#WORK_DIR=/tmp/elasticsearch

# Elasticsearch configuration directory
#CONF_DIR=/etc/elasticsearch

# Elasticsearch configuration file (elasticsearch.yml)
#CONF_FILE=/etc/elasticsearch/elasticsearch.yml

# Additional Java OPTS
#ES_JAVA_OPTS=

# Configure restart on package upgrade (true, every other setting will lead to not restarting)
#RESTART_ON_UPGRADE=true
```
</comment><comment author="tlrx" created="2015-04-20T09:48:43Z" id="94410375">Which version of Ubuntu are you using?
</comment><comment author="Fudoshiki" created="2015-04-20T09:49:43Z" id="94410505">15.04
</comment><comment author="Fudoshiki" created="2015-04-20T09:54:30Z" id="94411158">/usr/share/elasticsearch/bin$ ./elasticsearch -v
Version: 1.4.4, Build: c88f77f/2015-02-19T13:05:36Z, JVM: 1.8.0_45

```
● elasticsearch.service - LSB: Starts elasticsearch
   Loaded: loaded (/etc/init.d/elasticsearch)
   Active: active (running) since Пн. 2015-04-20 14:52:37 YEKT; 1min 15s ago
     Docs: man:systemd-sysv-generator(8)
  Process: 11316 ExecStart=/etc/init.d/elasticsearch start (code=exited, status=0/SUCCESS)
 Main PID: 7352 (code=exited, status=3)
   CGroup: /system.slice/elasticsearch.service
           └─11335 /usr/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -...

апр. 20 14:52:37 MG392 systemd[1]: Starting LSB: Starts elasticsearch...
апр. 20 14:52:37 MG392 elasticsearch[11316]: * Starting Elasticsearch Server
апр. 20 14:52:37 MG392 elasticsearch[11316]: ...done.
апр. 20 14:52:37 MG392 systemd[1]: Started LSB: Starts elasticsearch.
```

working

1.5.0 too.
</comment><comment author="tlrx" created="2015-04-20T10:02:27Z" id="94413041">@Fudoshiki Elasticsearch version 1.4.4 does not install a SystemD file for Debian. In this case the SysV init.d script is used.
</comment><comment author="Fudoshiki" created="2015-04-20T10:11:25Z" id="94414316">On mint 17.1 elasticsearch 1.5.1 works too, I think this ubuntu bug
</comment><comment author="tlrx" created="2015-04-20T10:26:13Z" id="94416717">You are hitting the same bug as #10322. As a workaround you can uncomment the paths in the `/etc/default/elasticsearch` file and it should work.
</comment><comment author="Fudoshiki" created="2015-04-20T10:31:22Z" id="94417671">thank you, when will available new package in repo?
</comment><comment author="tlrx" created="2015-06-17T07:33:28Z" id="112693468">@Fudoshiki New packages (1.5.3, 1.6.0) have been released and integrate the fix.

This issue is resolved so I'm closing it, feel free to reopen if needed.
</comment><comment author="ayashjorden" created="2015-07-22T20:10:27Z" id="123848311">Hi, 
I think this ticket needs to be re-opened.

I've installed ES 1.7.0 using apt-get.
The system is Debian8 with openJDK 1.7.0.79.

After doing **everything** in [this](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/setup-service.html) page, mlock.all still didn't work, max_file_descriptors was still 65K (instead of 262144).

After digging in /etc/default/elasticsearch, I found in the comments a pointer to _/usr/lib/systemd/system/elasticsearch.service_.
After setting `LimitNOFILE=262144` and `LimitMEMLOCK=infinity`, restarting ES, mlock.all was finally true and max_file_descriptors was set to 262144.

I'll be happy to provide additional information as needed.

Thanks,
Yarden
</comment><comment author="tlrx" created="2015-07-22T20:22:28Z" id="123851248">@ayashjorden thanks for your feedback. You've done the right thing (uncommenting `LimitMEMLOCK`). I think we should make the documentation clearer about this, but I'm not sure why we should re open this issue. Can you elaborate please?
</comment><comment author="ayashjorden" created="2015-07-22T20:35:33Z" id="123854211">@tlrx, maybe I posted too fast.
Anyway, I've opened a new issue, so we can leave this one closed and if needed, I can remove my post from here.

Thanks,
Yarden
</comment><comment author="imsaar" created="2015-08-04T02:26:59Z" id="127457647">I encountered the same issue and finally figured it out that in the process of upgrading elasticsearch on centos (linux) my elasticsearch user and group was recreated and had a new user and group id. The logs and data directories and files within them were still owned by the old user and thus elasticsearch kept failing without giving a clue. Here is the command line that fixed it:

`sudo chown -R elasticsearch:elasticsearch /var/lib/elasticsearch/`
</comment><comment author="will-iam" created="2015-08-05T10:01:47Z" id="127943787">Thx imsaar ! It worked for me.
(full new installation with apt-get on debian 8, ES 1.6.2, openJDK 1.7.0.79)
</comment><comment author="clintongormley" created="2015-08-05T17:00:25Z" id="128070428">@imsaar was this upgrade via the rpm or deb packages, or did you do it manually with the zip or tar.gz packages?  If the former, please open a new issue about this.
</comment><comment author="will-iam" created="2015-08-06T07:40:11Z" id="128279089">I did it with the .deb package
</comment><comment author="clintongormley" created="2015-08-06T10:16:07Z" id="128316092">I opened a new issue here https://github.com/elastic/elasticsearch/issues/12688
</comment><comment author="gitaarik" created="2015-08-07T12:44:21Z" id="128691041">On Ubuntu I completely removed elasticsearch with these commands:

```
apt-get purge elasticsearch

rm /usr/lib/systemd/system/elasticsearch.service
rm /var/lib/dpkg/info/elasticsearch.list /var/lib/dpkg/info/elasticsearch.postrm
rm /etc/init.d/elasticsearch
rm /etc/default/elasticsearch
rm -rf /etc/elasticsearch

userdel elasticsearch
```

And then reinstalled and it worked
</comment><comment author="hrahal" created="2015-11-12T15:55:31Z" id="156146896">+1 @imsaar it worked on openSUSE 13.2 
</comment><comment author="KIVagant" created="2016-08-30T13:41:02Z" id="243441865">Thank you, @imsaar . I got same problem on Debian 8 with ES 1.6.2 started with openjdk "1.8.0_102"
</comment><comment author="ghoroubi" created="2016-12-10T14:33:40Z" id="266212848">-- Logs begin at Sat 2016-12-10 07:51:47 CST, end at Sat 2016-12-10 08:29:34 CST
Dec 10 07:52:06 Khalili-Mint systemd[1]: Starting LSB: Starts elasticsearch...
Dec 10 07:52:08 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 07:56:20 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 08:13:19 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 08:21:21 Khalili-Mint systemd[1]: Started Elasticsearch.
lines 1-6/6 (END)
___________________________________________________________________________
ervice elasticsearch status
● elasticsearch.service - Elasticsearch
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendo
   Active: active (exited) since Sat 2016-12-10 07:52:08 CST; 41min ago
     Docs: http://www.elastic.co

Dec 10 07:52:06 Khalili-Mint systemd[1]: Starting LSB: Starts elasticsearch...
Dec 10 07:52:08 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 07:56:20 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 08:13:19 Khalili-Mint systemd[1]: Started LSB: Starts elasticsearch.
Dec 10 08:21:21 Khalili-Mint systemd[1]: Started Elasticsearch.
l</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: IdsQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10670</link><project id="" key="" /><description>Split the parse(QueryParseContext ctx) method into a parsing and a query building part, adding Streamable for serialization and hashCode(), equals() for better testing.
Add basic unit test for Builder and Parser.

PR goes agains query-refacoring feature branch.
</description><key id="69535119">10670</key><summary>Query refactoring: IdsQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-20T08:25:51Z</created><updated>2015-05-08T13:24:10Z</updated><resolved>2015-05-08T13:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-05-05T13:58:27Z" id="99085330">I rebased this PR on current state of feature branch, added the changes to BaseQuery and the QueryBuilder interface that we already discussed in #10669.
</comment><comment author="javanna" created="2015-05-05T15:17:20Z" id="99112333">I quickly went through it and it LGTM besides the few minor comments I left. @dakrone can you have another look too please?
</comment><comment author="cbuescher" created="2015-05-06T13:13:58Z" id="99453401">Added changes adressing the comments and rebased on current feature branch. Will wait for another look from @dakrone.
</comment><comment author="javanna" created="2015-05-06T21:57:06Z" id="99620895">I just had another look, left a few comments
</comment><comment author="cbuescher" created="2015-05-07T15:02:49Z" id="99900327">Went over the comments and added serialization of string lists to the StreamInput/Output classes, also added test for checking exception on missing value field.
</comment><comment author="cbuescher" created="2015-05-08T13:08:20Z" id="100226518">Just rebased this on current state of feature branch which now includes the addition if streaming string lists &amp; adressed the last two comments.
</comment><comment author="javanna" created="2015-05-08T13:14:42Z" id="100228033">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file></files><comments><comment>Query refactoring: refactored IdsQueryBuilder and Parser and added test</comment></comments></commit></commits></item><item><title>Query refactoring: TermQueryBuilder refactoring and test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10669</link><project id="" key="" /><description>Split the parse(QueryParseContext ctx) method into a parsing and a query building part, adding Streamable for serialization and hashCode(), equals() for better testing.
Add basic unit test for Builder and Parser.

PR goes agains query-refacoring feature branch.
</description><key id="69533585">10669</key><summary>Query refactoring: TermQueryBuilder refactoring and test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-20T08:17:25Z</created><updated>2015-05-06T08:16:02Z</updated><resolved>2015-05-06T05:34:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-04-22T16:35:33Z" id="95257272">Rebased this PR on top of current tip of feature branch and changed test to make use of new BaseQueryTestCase.
</comment><comment author="javanna" created="2015-04-22T23:23:13Z" id="95366118">left a few comments
</comment><comment author="cbuescher" created="2015-04-23T13:19:16Z" id="95580454">I pulled out the validation of the two important fields into a method shared by builder and parser part, but not sure if this is the best way to go. Happy about comments there. Hope I adressed the rest of your comments, I would prefer tracking the use of QueryParseContext in toQuery() in a separate issue before making huge changes there. Same goes for generifying named queries (lookup but also maybe simplification)
</comment><comment author="cbuescher" created="2015-04-24T12:25:52Z" id="95917300">Changed variable name in constructor and added validate() to the QueryBuilder interface. As long as this is not implemented by all queries, added empty impl to BaseQueryBuilder. Also opened two separate issues to keep track of further ideas for validation (#10777) and for generifying named queries (#10776). 
I'm still not sure about how to improve the tests for `toQuery()` because the resulting lucene queries hide a lot of details, so assertions depend on explicit casts and knowing implementation details that might break often. I'm inclined to leave the `toQuery()`tests here very general and rely on other integration tests with real cluster setup / mappings etc. to check that queries work. Thoughts on this appreciated.
</comment><comment author="cbuescher" created="2015-04-28T14:17:37Z" id="97078147">@javanna went through the comments and current diff again, here are the things that are open from my point of view:
- using `context.setMapUnmappedFieldAsString(true)` in the BaseQueryTestCase
- using `BytesRefs.toBytesRef(value)` in `equals()` and `hashCode()` from my last update commit

Anything else open here that I'm missing? 
</comment><comment author="cbuescher" created="2015-04-29T11:10:11Z" id="97390246">I added the changes to the validate() signature as discussed. Also changed internal representation of the query value in case it is a String to BytesRef to be compatible with parser.objectBytes() on the parser side. If this looks okay to you, I can also merge the additions to StreamInput/Output to master in a separate PR already.
</comment><comment author="cbuescher" created="2015-04-29T21:22:00Z" id="97589489">Added one more change addressing the merging of two validation cases.
</comment><comment author="cbuescher" created="2015-05-05T09:22:14Z" id="99006589">Rebased this PR on current feature branch and changed validation exception class to own version similar to the previously suggested ActionRequestValidationException. Also opened separate issue #10974 for keeping track of adding tests for invalid json and set the TermQueryBuilder test to 20 repetitions.
</comment><comment author="javanna" created="2015-05-05T14:57:48Z" id="99104609">LGTM besides the minor comments left, if you can address them that would be great, this is good to merge then!
</comment><comment author="cbuescher" created="2015-05-05T21:22:19Z" id="99226924">Thanks for the quick response, pushed this to the feature branch.
</comment><comment author="cbuescher" created="2015-05-06T08:16:02Z" id="99372017">Thanks for the review and for closing the issue, somehow it got opened
again yesterday automatically.

On Wed, May 6, 2015 at 7:34 AM, Luca Cavanna notifications@github.com
wrote:

&gt; Closed #10669 https://github.com/elastic/elasticsearch/pull/10669.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/10669#event-297897046.

## 

Christoph Büscher
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: MatchAllQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10668</link><project id="" key="" /><description>Split the `parse(QueryParseContext ctx)` method into a parsing and a query building part, adding Streamable for serialization and hashCode(), equals() for better testing.
Add basic unit test for Builder and Parser.

PR goes agains query-refacoring feature branch.
</description><key id="69533016">10668</key><summary>Query refactoring: MatchAllQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-20T08:14:02Z</created><updated>2015-04-23T07:35:14Z</updated><resolved>2015-04-23T07:35:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-04-20T14:20:38Z" id="94465613">Added getter, pulled out test base class and removed usage of jdk 1.8 api.
</comment><comment author="javanna" created="2015-04-20T14:41:16Z" id="94470559">thanks @cbuescher left a few comments
</comment><comment author="cbuescher" created="2015-04-20T16:20:19Z" id="94498463">Pulled some more of the test setup in the BaseQueryTestCase, I was able to do a generic `testFromXContent` method but since not all queries will be Streamable from the start and to the readFrom/writeTo won't be in the interface until all classes implemen this I wasn't able to pull that up. Also, tests for `toQuery()` will probably do different assertions on the produced lucene queries, so I left it in the individual test.
</comment><comment author="cbuescher" created="2015-04-21T09:40:35Z" id="94722355">Small change to the equal() method to prevent possible npe. Other than that I went with reverting the base test so that injector is not static again because I'm not sure if that would work when multiple tests are running. Maybe we should revisit that question once we have more tests extending this base test. 
</comment><comment author="cbuescher" created="2015-04-21T14:20:07Z" id="94811534">Refactored the base test class even more, but I'm not sure if the test code is now spread too much between the base test and the abstract methods. The serialization test can be generalized even more once we put Streamable in the QueryBuilder interface. Not sure if the "testToQuery()" test is going to be very useful for more complex queries because it seems tricky to do check internals of the constructed lucene queries.
</comment><comment author="cbuescher" created="2015-04-21T16:44:16Z" id="94865895">@javanna Was able to follow your suggestions on pulling the serialization test up to base test class, this works great now, was also able to use this base class with the other queries I currently have open PRs for (IdsQuery, TermQuery). Let me know what you think.
</comment><comment author="javanna" created="2015-04-22T07:47:40Z" id="95064455">looks great, left a bunch of minor comments, @dakrone can you have a look too please so we can get it in and move on with other queries?
</comment><comment author="javanna" created="2015-04-22T13:00:57Z" id="95165835">LGTM
</comment><comment author="javanna" created="2015-04-22T22:49:56Z" id="95359322">this got merged right @cbuescher ? if so can you close it?
</comment><comment author="cbuescher" created="2015-04-23T07:35:13Z" id="95477461">Yes, merged on feature branch with eea7ee5d, sorry I didn't close yet. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field length norm calculation is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10667</link><project id="" key="" /><description>Hi,

the field-norm calculation is wrong.

```
# clear index
curl -XDELETE 'localhost:9200/test'

# add some data
curl 'http://localhost:9200/test/sample/five' -d '{ "name" : "one two three four five" }'
curl 'http://localhost:9200/test/sample/four' -d '{ "name" : "one two three four" }'
curl 'http://localhost:9200/test/sample/three' -d '{ "name" : "one two three" }'

# search for "one two three" and expect id:three at first result with best score
curl -s 'http://localhost:9200/test/sample/_search?q="one%20two%20three"&amp;pretty=true'

# explain the result, expected was 0.577, 0.5, 0.447
curl -s 'http://localhost:9200/test/sample/_search?q="one%20two%20three"&amp;pretty=true&amp;explain' | grep -B 1 'fieldNorm'
```

The Lucene results are in correct order with different scores.

Thanks in advanced,
Timo
</description><key id="69530926">10667</key><summary>Field length norm calculation is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">loevenwong</reporter><labels /><created>2015-04-20T07:59:12Z</created><updated>2015-05-11T07:52:04Z</updated><resolved>2015-04-23T06:53:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T06:53:57Z" id="95464808">@loevenwong Lucene encodes norms using 8 bits. This means precision can be lost when encoding. You can see it explained here:
https://lucene.apache.org/core/5_1_0/core/org/apache/lucene/search/similarities/DefaultSimilarity

The important bit to see is this:

&gt; The rationale supporting such lossy compression of norm values is that given the difficulty (and inaccuracy) of users to express their true information need by a query, only big differences matter. 
</comment><comment author="loevenwong" created="2015-05-11T07:52:04Z" id="100804186">@rjernst Thank you for your reply.
FYI: I've found a matching lucene issue: https://issues.apache.org/jira/browse/LUCENE-5005
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove dead code after previous refactorings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10666</link><project id="" key="" /><description>This is mostly removing code that handled deletion of types, which was
removed in #8877.
</description><key id="69472728">10666</key><summary>Remove dead code after previous refactorings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-19T22:48:35Z</created><updated>2015-06-08T09:02:22Z</updated><resolved>2015-04-20T17:08:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-20T01:53:41Z" id="94335155">I also cleaned up a some tests calling `smartName`, to reduce how many things rely on the Mappers (plural).
</comment><comment author="jpountz" created="2015-04-20T09:23:19Z" id="94404802">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file></files><comments><comment>Mappings: Remove dead code after previous refactorings</comment></comments></commit></commits></item><item><title>Scan and scroll query not working when combined with range query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10665</link><project id="" key="" /><description>Hi,

I am trying to use Java API for executing scan and scroll query while combining it with range query but I am not getting any hits. I am not sure if it's a problem with my query or my data but below is my code:

```
String from = "1429192800000";
String to = "1429193100000";

QueryBuilder qb = rangeQuery("myField")
             .gte(from)                          
                 .lte(to);

SearchRequestBuilder searchBuilder = client.prepareSearch(oldIndex)
            .setTypes("myType")
            .setSearchType(SearchType.SCAN)
            .setScroll(new TimeValue(Integer.parseInt(scrollOpenTime)*60000))
            .setQuery(qb)
            .setSize(Integer.parseInt(size));
```

And here's my output:

```
{
  "_scroll_id" : my64BitScrolliD,
  "took" : 576,
  "timed_out" : false,
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  }
}
```

I have tried querying without using java api and still it doesn't return any hits. Here's my query:

```
curl -XGET "http://localhost:9220/path/to/cluster/myIndex/myType/_search?search_type=scan&amp;scroll=1m -d '{
 "size": 100,
    "query": {
        "range": {
            "reqDT": {
                "from": "1429192800000",
                "to": "1429193100000"
            }
        }
  }
}'
```

And I get the same output as above. The field for which I have applied the range query holds an epoch timestamp and data exists between the specified range so there is no mistake with the query I suppose (I have tested it with just _search API and without using scan and scroll and it worked!). Could anyone point me in the right direction? Also please let me know if any more information is required.

Thank you!

EDIT 1: Corrected -XPOST to -XGET

EDIT 2: I would like to add that the scan and scroll query works fine if I don't combine range query with it! Since there is a really large amount of data, I want to get the results in batches by specifying the time range. 
</description><key id="69471515">10665</key><summary>Scan and scroll query not working when combined with range query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SrikanthKS</reporter><labels /><created>2015-04-19T22:30:03Z</created><updated>2016-12-14T06:18:16Z</updated><resolved>2015-04-25T15:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-20T02:48:16Z" id="94344004">I think it's not an issue but a usage question. Mailing list is a better place for that.

That said, could you provide a sample script which reproduces your problem?
Have a look at https://www.elastic.co/help.
</comment><comment author="clintongormley" created="2015-04-25T15:44:56Z" id="96230560">@SrikanthKS just to confirm: ranges work with scan/scroll as can be demonstrated here:

```
PUT my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "reqDt": {
          "type": "date"
        }
      }
    }
  }
}

PUT my_index/my_type/1
{
  "reqDt":  1429192800000
}

GET my_index/my_type/_search?search_type=scan&amp;scroll=1m
{
  "size": 100,
  "query": {
    "range": {
      "reqDt": {
        "from": 1429192800000,
        "to": 1429193100000
      }
    }
  }
}
```
</comment><comment author="henushang" created="2016-12-14T06:18:16Z" id="266952591">hi, I get the same question, and what's you solution?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Be explicit about scan doing no scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10664</link><project id="" key="" /><description>The scroll page doesn't explicitly mention that `search_type=scan` also means no scoring, not just no sorting and it can lead to confusion, for example https://github.com/elastic/elasticsearch-py/issues/220
</description><key id="69469871">10664</key><summary>[DOCS] Be explicit about scan doing no scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/HonzaKral/following{/other_user}', u'events_url': u'https://api.github.com/users/HonzaKral/events{/privacy}', u'organizations_url': u'https://api.github.com/users/HonzaKral/orgs', u'url': u'https://api.github.com/users/HonzaKral', u'gists_url': u'https://api.github.com/users/HonzaKral/gists{/gist_id}', u'html_url': u'https://github.com/HonzaKral', u'subscriptions_url': u'https://api.github.com/users/HonzaKral/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/32132?v=4', u'repos_url': u'https://api.github.com/users/HonzaKral/repos', u'received_events_url': u'https://api.github.com/users/HonzaKral/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/HonzaKral/starred{/owner}{/repo}', u'site_admin': False, u'login': u'HonzaKral', u'type': u'User', u'id': 32132, u'followers_url': u'https://api.github.com/users/HonzaKral/followers'}</assignee><reporter username="">HonzaKral</reporter><labels /><created>2015-04-19T22:17:58Z</created><updated>2015-04-20T16:07:48Z</updated><resolved>2015-04-20T16:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-19T23:20:48Z" id="94322485">LGTM
</comment><comment author="dakrone" created="2015-04-20T05:54:24Z" id="94364322">@HonzaKral can you add a mention that `track_scores` is supported for scan queries though?
</comment><comment author="HonzaKral" created="2015-04-20T11:46:40Z" id="94431151">updated to add track_scores param
</comment><comment author="dakrone" created="2015-04-20T14:56:49Z" id="94475174">LGTM also, thanks Honza
</comment><comment author="HonzaKral" created="2015-04-20T16:07:47Z" id="94494964">Merged as e929c1560d8ea85719480a2c9df3e11a7ab181e3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't use bitset cache for children filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10663</link><project id="" key="" /><description>Only parent filters should use bitset filter cache, to avoid memory being wasted.

Closes #10662 
Closes #10629
</description><key id="69468462">10663</key><summary>Don't use bitset cache for children filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-19T21:54:02Z</created><updated>2015-06-07T18:13:23Z</updated><resolved>2015-04-30T15:02:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T10:07:15Z" id="95880903">LGTM
</comment><comment author="jpountz" created="2015-04-24T10:07:30Z" id="95880939">I just left a comment about some missing null checks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>inner_hits: Ignore object fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10662</link><project id="" key="" /><description>Step over object fields when creating the nested identify and just use the object fieldname as a prefix for the nested field.

Closes #10629
</description><key id="69466713">10662</key><summary>inner_hits: Ignore object fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-04-19T21:27:20Z</created><updated>2015-05-29T16:27:04Z</updated><resolved>2015-04-30T15:02:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-04-30T15:02:49Z" id="97829448">fixed via #10663
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>inner_hits: Don't use bitset cache for children filters.</comment></comments></commit></commits></item><item><title>TopHits advance()'s backwards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10661</link><project id="" key="" /><description>I saw this by adding assertingcodec to the mix in our tests:

FAILURE 0.55s | TopHitsTests.testNestedFetchFeatures &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: Hit count is 1 but 2 was expected.  Total shards: 9 Successful shards: 8 &amp; 1 shard failures:
&gt;  shard [[qcEkX24CTsSgeHjwon4SYA][articles][5]], reason [ElasticsearchException[target must be &gt; docID(), got 1 &lt;= 3]; nested: AssertionError[target must be &gt; docID(), got 1 &lt;= 3]; ]
&gt;    at __randomizedtesting.SeedInfo.seed([EEBE1D571C1FD8E9:9F4B07A5A1F77FA4]:0)
&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:145)
&gt;    at org.elasticsearch.search.aggregations.bucket.TopHitsTests.testNestedFetchFeatures(TopHitsTests.java:804)
&gt;    at java.lang.Thread.run(Thread.java:745)
</description><key id="69443859">10661</key><summary>TopHits advance()'s backwards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label></labels><created>2015-04-19T17:05:17Z</created><updated>2015-06-25T18:59:32Z</updated><resolved>2015-04-21T08:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-19T17:44:38Z" id="94300013">I also hit this with SimpleNestedTests.simpleNestedMatchQueries() with AssertingCodec
</comment><comment author="martijnvg" created="2015-04-19T21:55:31Z" id="94318216">The failure in the TopHits is caused by the nested aggregator. The TopHitsAggregator doesn't invoke advance by itself. (the combination is being tested here)
</comment><comment author="s1monw" created="2015-04-20T09:49:10Z" id="94410434">@martijnvg  can you fix this?
</comment><comment author="martijnvg" created="2015-04-20T09:59:23Z" id="94412550">@s1monw sure, I'll fix this.
</comment><comment author="jpountz" created="2015-06-25T14:02:38Z" id="115268434">@martijnvg SimpleNestedTests still has an `AwaitsFix` with this bug url, should it be removed?
</comment><comment author="martijnvg" created="2015-06-25T18:59:32Z" id="115362532">@jpountz yes, it should! I guess I forgot that. the test needs to be changed a bit too... the matched queries should be asserted on the inner hits instead of the root hit.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file></files><comments><comment>matched queries: Remove redundant and broken code</comment></comments></commit></commits></item><item><title>ShardTermVectorsService calls docFreq() on unpositioned TermsEnum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10660</link><project id="" key="" /><description>when experimenting with lucene codec randomization, AssertingCodec hits this. I don't understand the code in question but it definitely ignores the result of seekExact()

ERROR   0.30s J0 | GetTermVectorsTests.testArtificialNonExistingField &lt;&lt;&lt;

&gt; Caused by: java.lang.AssertionError: docFreq() called on unpositioned TermsEnum
&gt;    at org.apache.lucene.index.AssertingLeafReader$AssertingTermsEnum.docFreq(AssertingLeafReader.java:188)
&gt;    at org.elasticsearch.action.termvectors.TermVectorsWriter.writeTermStatistics(TermVectorsWriter.java:230)
&gt;    at org.elasticsearch.action.termvectors.TermVectorsWriter.setFields(TermVectorsWriter.java:109)
&gt;    at org.elasticsearch.action.termvectors.TermVectorsResponse.setFields(TermVectorsResponse.java:365)
&gt;    at org.elasticsearch.index.termvectors.ShardTermVectorsService.getTermVectors(ShardTermVectorsService.java:152)
&gt;    ... 6 more
</description><key id="69443517">10660</key><summary>ShardTermVectorsService calls docFreq() on unpositioned TermsEnum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Term Vectors</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-19T16:59:48Z</created><updated>2015-04-26T11:25:36Z</updated><resolved>2015-04-20T12:14:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsWriter.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file></files><comments><comment>Use dummy TermStatistics when term is not found Closes #10660</comment><comment>Thanks for fixing this bug!</comment></comments></commit></commits></item><item><title>renames: ElasticSearchIntegrationTest/SingleNodeTest -&gt; TestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10659</link><project id="" key="" /><description>I wanted to do this in #10656, but lets do it as a followup, so we don't have to fight git's crappy merging on that issue.
</description><key id="69422819">10659</key><summary>renames: ElasticSearchIntegrationTest/SingleNodeTest -&gt; TestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-19T13:06:06Z</created><updated>2015-08-04T00:51:08Z</updated><resolved>2015-08-04T00:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilterTests.java</file><file>core/src/test/java/org/apache/lucene/analysis/miscellaneous/UniqueTokenFilterTests.java</file><file>core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTest.java</file><file>core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java</file><file>core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java</file><file>core/src/test/java/org/apache/lucene/util/SloppyMathTests.java</file><file>core/src/test/java/org/elasticsearch/ESExceptionTests.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/NamingConventionTests.java</file><file>core/src/test/java/org/elasticsearch/VersionTests.java</file><file>core/src/test/java/org/elasticsearch/action/IndicesRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/ListenerActionIT.java</file><file>core/src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file><file>core/src/test/java/org/elasticsearch/action/RejectionActionIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/HotThreadsIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/repositories/RepositoryBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/tasks/PendingTasksBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/flush/FlushBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/optimize/OptimizeBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/refresh/RefreshBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorClusterSettingsIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/count/CountResponseTests.java</file><file>core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTest.java</file><file>core/src/test/java/org/elasticsearch/action/get/MultiGetShardRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/indexedscripts/get/GetIndexedScriptRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/percolate/MultiPercolatorRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/IndicesOptionsTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/ListenableActionFutureTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file><file>core/src/test/java/org/elasticsearch/action/update/UpdateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java</file><file>core/src/test/java/org/elasticsearch/blocks/SimpleBlocksIT.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JNANativesTests.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicAnalysisBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/ClusterStateBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/StaticIndexBackwardCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/TransportClientBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/client/node/NodeClientIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SimpleDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/UpdateSettingsValidationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/FilteringAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/DateMathExpressionResolverTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/HumanReadableIndexSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/MappingMetaDataParserTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTest.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDeciderIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/SettingsFilteringIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/shards/ClusterSearchShardsIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/codecs/CodecTests.java</file><file>core/src/test/java/org/elasticsearch/common/Base64Test.java</file><file>core/src/test/java/org/elasticsearch/common/BooleansTests.java</file><file>core/src/test/java/org/elasticsearch/common/ChannelsTests.java</file><file>core/src/test/java/org/elasticsearch/common/ParseFieldTests.java</file><file>core/src/test/java/org/elasticsearch/common/PidFileTests.java</file><file>core/src/test/java/org/elasticsearch/common/StringsTests.java</file><file>core/src/test/java/org/elasticsearch/common/TableTests.java</file><file>core/src/test/java/org/elasticsearch/common/UUIDTests.java</file><file>core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/BytesReferenceTests.java</file><file>core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>core/src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java</file><file>core/src/test/java/org/elasticsearch/common/cli/CliToolTestCase.java</file><file>core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java</file><file>core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashSetTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTests.java</file><file>core/src/test/java/org/elasticsearch/common/compress/lzf/CorruptedCompressorTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/GeoHashTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>core/src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/common/hashing/MurmurHash3Tests.java</file><file>core/src/test/java/org/elasticsearch/common/hppc/HppcMapsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>core/src/test/java/org/elasticsearch/common/logging/jdk/JDKESLoggerTests.java</file><file>core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file><file>core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/IndexCacheableQueryTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/LuceneTest.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/ShardCoreKeyMapTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/all/SimpleAllTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/index/ESDirectoryReaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/XMoreLikeThisTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/store/ByteArrayIndexInputTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/store/InputStreamIndexInputTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java</file><file>core/src/test/java/org/elasticsearch/common/math/MathUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java</file><file>core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java</file><file>core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java</file><file>core/src/test/java/org/elasticsearch/common/regex/RegexTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsFilterTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/SettingsTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/JsonSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/YamlSettingsLoaderTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/ByteSizeUnitTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/ByteSizeValueTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/RatioValueTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/SizeValueTests.java</file><file>core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/ArrayUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/ByteUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/BytesRefHashTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTest.java</file><file>core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/LongHashTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/LongObjectHashMapTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/MultiDataPathUpgraderTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/SingleObjectCacheTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/URIPatternTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTest.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/EsExecutorsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTest.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/XContentFactoryTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentParserTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/cbor/JsonVsCborTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/smile/JsonVsSmileTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/XContentHelperTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTests.java</file><file>core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java</file><file>core/src/test/java/org/elasticsearch/count/simple/SimpleCountIT.java</file><file>core/src/test/java/org/elasticsearch/deps/jackson/JacksonLocationTests.java</file><file>core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java</file><file>core/src/test/java/org/elasticsearch/deps/lucene/SimpleLuceneTests.java</file><file>core/src/test/java/org/elasticsearch/deps/lucene/VectorHighlighterTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandlerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTest.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/document/BulkIT.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java</file><file>core/src/test/java/org/elasticsearch/document/ShardInfoIT.java</file><file>core/src/test/java/org/elasticsearch/env/EnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>core/src/test/java/org/elasticsearch/exists/SimpleExistsIT.java</file><file>core/src/test/java/org/elasticsearch/explain/ExplainActionIT.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/AsyncShardFetchTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayServiceTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaStateServiceTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PriorityComparatorTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/QuorumGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoverAfterNodesIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionIT.java</file><file>core/src/test/java/org/elasticsearch/http/netty/HttpPublishPortIT.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java</file></files><comments><comment>Tests: Rename base tests cases to use "TestCase" suffix</comment></comments></commit></commits></item><item><title>Shield - Circular Dependency - start failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10658</link><project id="" key="" /><description>Based on the latest version of shield and ES,
These other plugins are used :
   loaded [WebPlugin, license, shield, mapper-attachments, QuartzPlugin], sites [HQ]

But the start of ES fails:

[2015-04-17 16:47:10,470][INFO ][http                     ] [Node1] Using [org.elasticsearch.shield.transport.netty.ShieldNettyHttpServerTransport] as http transport, overridden by [shield]
{1.5.1}: Initialization Failed ...
1) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]2) Tried proxying org.elasticsearch.transport.TransportService to support a circular dependency, but it is not an interface.
</description><key id="69403554">10658</key><summary>Shield - Circular Dependency - start failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienlust</reporter><labels /><created>2015-04-19T09:15:40Z</created><updated>2015-04-25T16:26:48Z</updated><resolved>2015-04-25T15:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T15:15:41Z" id="96220731">Hi @damienlust 

I'm guessing the clash is with the WebPlugin?  Given that (a) we have no control over that plugin and (b) rivers are deprecated, I don't think there is much we can do to fix this.
</comment><comment author="damienlust" created="2015-04-25T15:54:21Z" id="96231753">Thanks for the feedback,
What's the alternative to crawl a website?
The river webplugin seems the most popular?
Thanks
</comment><comment author="clintongormley" created="2015-04-25T16:16:22Z" id="96233080">@damienlust I'm not sure. I had a look for a suitable logstash plugin, but can't find one. Really, you can use any web scraping framework, because it runs outside Elasticsearch.
</comment><comment author="dadoonet" created="2015-04-25T16:26:48Z" id="96233920">I heard about some efforts around nutch 2.
And some plugins like https://github.com/duffj/nutch-elasticsearch

Never tested it though.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item></channel></rss>